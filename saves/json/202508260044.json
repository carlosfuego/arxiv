[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.20776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v2",
                "updated": "2025-08-22T08:45:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    45,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16212v1",
                "updated": "2025-08-22T08:36:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    36,
                    58,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T08:36:58Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    36,
                    58,
                    4,
                    234,
                    0
                ],
                "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models"
                },
                "summary": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure.In addition, during cache reuse, we dynamically estimate the\ncorresponding noise and filter it out to reduce its impact on the sampling\ndirection.Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure.In addition, during cache reuse, we dynamically estimate the\ncorresponding noise and filter it out to reduce its impact on the sampling\ndirection.Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models."
                },
                "authors": [
                    {
                        "name": "Huanpeng Chu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Guanyu Fen"
                    },
                    {
                        "name": "Yutao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Zhang"
                },
                "author": "Yutao Zhang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16211v1",
                "updated": "2025-08-22T08:34:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    34,
                    3,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T08:34:03Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    34,
                    3,
                    4,
                    234,
                    0
                ],
                "title": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in\nhigh-fidelity image and video generation. To reduce their substantial\ncomputational costs, feature caching techniques have been proposed to\naccelerate inference by reusing hidden representations from previous timesteps.\nHowever, current methods often struggle to maintain generation quality at high\nacceleration ratios, where prediction errors increase sharply due to the\ninherent instability of long-step forecasting. In this work, we adopt an\nordinary differential equation (ODE) perspective on the hidden-feature\nsequence, modeling layer representations along the trajectory as a feature-ODE.\nWe attribute the degradation of existing caching strategies to their inability\nto robustly integrate historical features under large skipping intervals. To\naddress this, we propose FoCa (Forecast-then-Calibrate), which treats feature\ncaching as a feature-ODE solving problem. Extensive experiments on image\nsynthesis, video generation, and super-resolution tasks demonstrate the\neffectiveness of FoCa, especially under aggressive acceleration. Without\nadditional training, FoCa achieves near-lossless speedups of 5.50 times on\nFLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high\nquality with a 4.53 times speedup on DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in\nhigh-fidelity image and video generation. To reduce their substantial\ncomputational costs, feature caching techniques have been proposed to\naccelerate inference by reusing hidden representations from previous timesteps.\nHowever, current methods often struggle to maintain generation quality at high\nacceleration ratios, where prediction errors increase sharply due to the\ninherent instability of long-step forecasting. In this work, we adopt an\nordinary differential equation (ODE) perspective on the hidden-feature\nsequence, modeling layer representations along the trajectory as a feature-ODE.\nWe attribute the degradation of existing caching strategies to their inability\nto robustly integrate historical features under large skipping intervals. To\naddress this, we propose FoCa (Forecast-then-Calibrate), which treats feature\ncaching as a feature-ODE solving problem. Extensive experiments on image\nsynthesis, video generation, and super-resolution tasks demonstrate the\neffectiveness of FoCa, especially under aggressive acceleration. Without\nadditional training, FoCa achieves near-lossless speedups of 5.50 times on\nFLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high\nquality with a 4.53 times speedup on DiT."
                },
                "authors": [
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16184v1",
                "updated": "2025-08-22T07:57:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    57,
                    28,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T07:57:28Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    57,
                    28,
                    4,
                    234,
                    0
                ],
                "title": "Joint Cache Placement and Routing in Satellite-Terrestrial Edge\n  Computing Network: A GNN-Enabled DRL Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Cache Placement and Routing in Satellite-Terrestrial Edge\n  Computing Network: A GNN-Enabled DRL Approach"
                },
                "summary": "In this letter, we investigate the problem of joint content caching and\nrouting in satellite-terrestrial edge computing networks (STECNs) to improve\ncaching service for geographically distributed users. To handle the challenges\narising from dynamic low Earth orbit (LEO) satellite topologies and\nheterogeneous content demands, we propose a learning-based framework that\nintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).\nThe satellite network is represented as a dynamic graph, where GNNs are\nembedded within the DRL agent to capture spatial and topological dependencies\nand support routing-aware decision-making. The caching strategy is optimized by\nformulating the problem as a Markov decision process (MDP) and applying soft\nactor-critic (SAC) algorithm. Simulation results demonstrate that our approach\nsignificantly improves the delivery success rate and reduces communication\ntraffic cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter, we investigate the problem of joint content caching and\nrouting in satellite-terrestrial edge computing networks (STECNs) to improve\ncaching service for geographically distributed users. To handle the challenges\narising from dynamic low Earth orbit (LEO) satellite topologies and\nheterogeneous content demands, we propose a learning-based framework that\nintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).\nThe satellite network is represented as a dynamic graph, where GNNs are\nembedded within the DRL agent to capture spatial and topological dependencies\nand support routing-aware decision-making. The caching strategy is optimized by\nformulating the problem as a Markov decision process (MDP) and applying soft\nactor-critic (SAC) algorithm. Simulation results demonstrate that our approach\nsignificantly improves the delivery success rate and reduces communication\ntraffic cost."
                },
                "authors": [
                    {
                        "name": "Yuhao Zheng"
                    },
                    {
                        "name": "Ting You"
                    },
                    {
                        "name": "Kejia Peng"
                    },
                    {
                        "name": "Chang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Liu"
                },
                "author": "Chang Liu",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16134v1",
                "updated": "2025-08-22T06:55:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    55,
                    45,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T06:55:45Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    55,
                    45,
                    4,
                    234,
                    0
                ],
                "title": "CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing"
                },
                "summary": "Large Language Models (LLMs) confront significant memory challenges due to\nthe escalating KV cache with increasing sequence length. As a crucial\ntechnique, existing cross-layer KV cache sharing methods either necessitate\nmodified model architectures with subsequent pre-training or incur significant\nperformance degradation at high compression rates. To mitigate these\nchallenges, we propose CommonKV, a training-free method for cross-layer KV\ncache compression through adjacent parameters sharing. Inspired by the high\nsimilarity observed in cross-layer hidden states, we utilize Singular Value\nDecomposition (SVD) to achieve weight sharing across adjacent parameters,\nresulting in a more easily mergeable latent KV cache. Furthermore, we also\nintroduce an adaptive budget allocation strategy. It dynamically assigns\ncompression budgets based on cosine similarity, ensuring that dissimilar caches\nare not over-compressed. Experiments across multiple backbone models and\nbenchmarks including LongBench and Ruler demonstrate that the proposed method\nconsistently outperforms existing low-rank and cross-layer approaches at\nvarious compression ratios. Moreover, we find that the benefits of CommonKV are\northogonal to other quantization and eviction methods. By integrating these\napproaches, we can ultimately achieve a 98\\% compression ratio without\nsignificant performance loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) confront significant memory challenges due to\nthe escalating KV cache with increasing sequence length. As a crucial\ntechnique, existing cross-layer KV cache sharing methods either necessitate\nmodified model architectures with subsequent pre-training or incur significant\nperformance degradation at high compression rates. To mitigate these\nchallenges, we propose CommonKV, a training-free method for cross-layer KV\ncache compression through adjacent parameters sharing. Inspired by the high\nsimilarity observed in cross-layer hidden states, we utilize Singular Value\nDecomposition (SVD) to achieve weight sharing across adjacent parameters,\nresulting in a more easily mergeable latent KV cache. Furthermore, we also\nintroduce an adaptive budget allocation strategy. It dynamically assigns\ncompression budgets based on cosine similarity, ensuring that dissimilar caches\nare not over-compressed. Experiments across multiple backbone models and\nbenchmarks including LongBench and Ruler demonstrate that the proposed method\nconsistently outperforms existing low-rank and cross-layer approaches at\nvarious compression ratios. Moreover, we find that the benefits of CommonKV are\northogonal to other quantization and eviction methods. By integrating these\napproaches, we can ultimately achieve a 98\\% compression ratio without\nsignificant performance loss."
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoyu Qiao"
                    },
                    {
                        "name": "Lujun Li"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16121v1",
                "updated": "2025-08-22T06:28:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    28,
                    24,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T06:28:24Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    28,
                    24,
                    4,
                    234,
                    0
                ],
                "title": "Lightweight and Fast Real-time Image Enhancement via Decomposition of\n  the Spatial-aware Lookup Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight and Fast Real-time Image Enhancement via Decomposition of\n  the Spatial-aware Lookup Tables"
                },
                "summary": "The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently\nreduce both model size and runtime by interpolating pre-calculated values at\nthe vertices. However, the 3D LUT methods have a limitation due to their lack\nof spatial information, as they convert color values on a point-by-point basis.\nAlthough spatial-aware 3D LUT methods address this limitation, they introduce\nadditional modules that require a substantial number of parameters, leading to\nincreased runtime as image resolution increases. To address this issue, we\npropose a method for generating image-adaptive LUTs by focusing on the\nredundant parts of the tables. Our efficient framework decomposes a 3D LUT into\na linear sum of low-dimensional LUTs and employs singular value decomposition\n(SVD). Furthermore, we enhance the modules for spatial feature fusion to be\nmore cache-efficient. Extensive experimental results demonstrate that our model\neffectively decreases both the number of parameters and runtime while\nmaintaining spatial awareness and performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently\nreduce both model size and runtime by interpolating pre-calculated values at\nthe vertices. However, the 3D LUT methods have a limitation due to their lack\nof spatial information, as they convert color values on a point-by-point basis.\nAlthough spatial-aware 3D LUT methods address this limitation, they introduce\nadditional modules that require a substantial number of parameters, leading to\nincreased runtime as image resolution increases. To address this issue, we\npropose a method for generating image-adaptive LUTs by focusing on the\nredundant parts of the tables. Our efficient framework decomposes a 3D LUT into\na linear sum of low-dimensional LUTs and employs singular value decomposition\n(SVD). Furthermore, we enhance the modules for spatial feature fusion to be\nmore cache-efficient. Extensive experimental results demonstrate that our model\neffectively decreases both the number of parameters and runtime while\nmaintaining spatial awareness and performance."
                },
                "authors": [
                    {
                        "name": "Wontae Kim"
                    },
                    {
                        "name": "Keuntek Lee"
                    },
                    {
                        "name": "Nam Ik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Nam Ik Cho"
                },
                "author": "Nam Ik Cho",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v2",
                "updated": "2025-08-22T03:36:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    3,
                    36,
                    44,
                    4,
                    234,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1].This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1].This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhao"
                },
                "author": "Yang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v2",
                "updated": "2025-08-21T22:45:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    22,
                    45,
                    6,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "Added link to code repository. Fixed typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01225v2",
                "updated": "2025-08-21T20:13:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    20,
                    13,
                    40,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-02T06:43:43Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models"
                },
                "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance. Project Page available at:\nhttps://zhaihaotian.github.io/MCP-ICCV25/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance. Project Page available at:\nhttps://zhaihaotian.github.io/MCP-ICCV25/"
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Xiupeng Shi"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15919v1",
                "updated": "2025-08-21T18:40:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T18:40:20Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "title": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling"
                },
                "summary": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures.\n  We present \\textbf{HyperFlexis}, a unified LLM serving system that integrates\nalgorithmic and system-level innovations to jointly optimize scheduling and\nscaling under multiple SLOs. It features a multi-SLO-aware scheduler that\nleverages budget estimation and request prioritization to ensure proactive SLO\ncompliance for both new and ongoing requests. The system supports prefill- and\ndecode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV\ncache transfers. It also enables cost-effective scaling decisions,\nprefill-decode instance linking during scaling, and rapid P/D role transitions.\nTo accelerate scaling and reduce cold-start latency, a device-to-device (D2D)\nweight transfer mechanism is proposed that lowers weight loading overhead by up\nto \\textbf{19.39$\\times$}. These optimizations allow the system to achieve up\nto \\textbf{4.44$\\times$} higher SLO attainment, \\textbf{65.82\\%} lower request\nlatency, and cost parity with state-of-the-art baselines. The code will be\nreleased soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures.\n  We present \\textbf{HyperFlexis}, a unified LLM serving system that integrates\nalgorithmic and system-level innovations to jointly optimize scheduling and\nscaling under multiple SLOs. It features a multi-SLO-aware scheduler that\nleverages budget estimation and request prioritization to ensure proactive SLO\ncompliance for both new and ongoing requests. The system supports prefill- and\ndecode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV\ncache transfers. It also enables cost-effective scaling decisions,\nprefill-decode instance linking during scaling, and rapid P/D role transitions.\nTo accelerate scaling and reduce cold-start latency, a device-to-device (D2D)\nweight transfer mechanism is proposed that lowers weight loading overhead by up\nto \\textbf{19.39$\\times$}. These optimizations allow the system to achieve up\nto \\textbf{4.44$\\times$} higher SLO attainment, \\textbf{65.82\\%} lower request\nlatency, and cost parity with state-of-the-art baselines. The code will be\nreleased soon."
                },
                "authors": [
                    {
                        "name": "Zahra Yousefijamarani"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Taha Shabani"
                    },
                    {
                        "name": "Niloofar Gholipour"
                    },
                    {
                        "name": "Parham Yassini"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Qiantao Zhang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15717v1",
                "updated": "2025-08-21T16:56:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    56,
                    29,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T16:56:29Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    56,
                    29,
                    3,
                    233,
                    0
                ],
                "title": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video\n  Understanding"
                },
                "summary": "Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches."
                },
                "authors": [
                    {
                        "name": "Yanlai Yang"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Satya Narayan Shukla"
                    },
                    {
                        "name": "Aashu Singh"
                    },
                    {
                        "name": "Shlok Kumar Mishra"
                    },
                    {
                        "name": "Lizhu Zhang"
                    },
                    {
                        "name": "Mengye Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengye Ren"
                },
                "author": "Mengye Ren",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15694v1",
                "updated": "2025-08-21T16:21:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    21,
                    46,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T16:21:46Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    21,
                    46,
                    3,
                    233,
                    0
                ],
                "title": "GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector\n  Nearest Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector\n  Nearest Neighbor Search"
                },
                "summary": "Graph-based high-dimensional vector indices have become a mainstream solution\nfor large-scale approximate nearest neighbor search (ANNS). However, their\nsubstantial memory footprint often requires storage on secondary devices, where\nfrequent on-demand loading of graph and vector data leads to I/O becoming the\ndominant bottleneck, accounting for over 90\\% of query latency. Existing static\ncaching strategies mitigate this issue only in the initial navigation phase by\npreloading entry points and multi-hop neighbors, but they fail in the second\nphase where query-dependent nodes must be dynamically accessed to achieve high\nrecall. We propose GoVector, an I/O-efficient caching strategy tailored for\ndisk-based graph indices. GoVector combines (1) a static cache that stores\nentry points and frequently accessed neighbors, and (2) a dynamic cache that\nadaptively captures nodes with high spatial locality during the second search\nphase. To further align storage layout with similarity-driven search patterns,\nGoVector reorders nodes on disk so that similar vectors are colocated on the\nsame or adjacent pages, thereby improving locality and reducing I/O overhead.\nExtensive experiments on multiple public datasets show that GoVector achieves\nsubstantial performance improvements. At 90% recall, it reduces I/O operations\nby 46% on average, increases query throughput by 1.73x, and lowers query\nlatency by 42% compared to state-of-the-art disk-based graph indexing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based high-dimensional vector indices have become a mainstream solution\nfor large-scale approximate nearest neighbor search (ANNS). However, their\nsubstantial memory footprint often requires storage on secondary devices, where\nfrequent on-demand loading of graph and vector data leads to I/O becoming the\ndominant bottleneck, accounting for over 90\\% of query latency. Existing static\ncaching strategies mitigate this issue only in the initial navigation phase by\npreloading entry points and multi-hop neighbors, but they fail in the second\nphase where query-dependent nodes must be dynamically accessed to achieve high\nrecall. We propose GoVector, an I/O-efficient caching strategy tailored for\ndisk-based graph indices. GoVector combines (1) a static cache that stores\nentry points and frequently accessed neighbors, and (2) a dynamic cache that\nadaptively captures nodes with high spatial locality during the second search\nphase. To further align storage layout with similarity-driven search patterns,\nGoVector reorders nodes on disk so that similar vectors are colocated on the\nsame or adjacent pages, thereby improving locality and reducing I/O overhead.\nExtensive experiments on multiple public datasets show that GoVector achieves\nsubstantial performance improvements. At 90% recall, it reduces I/O operations\nby 46% on average, increases query throughput by 1.73x, and lowers query\nlatency by 42% compared to state-of-the-art disk-based graph indexing systems."
                },
                "authors": [
                    {
                        "name": "Yijie Zhou"
                    },
                    {
                        "name": "Shengyuan Lin"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shuhao Fan"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "arxiv_comment": "12 pages, 12 figures, this paper is the English version of our\n  Chinese paper accepted for publication in Journal of Software, Vol. 37, No.\n  3, 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15881v1",
                "updated": "2025-08-21T15:25:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    40,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T15:25:40Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    40,
                    3,
                    233,
                    0
                ],
                "title": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill \\& Decode Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill \\& Decode Inference"
                },
                "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration."
                },
                "authors": [
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15647v1",
                "updated": "2025-08-21T15:25:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    30,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T15:25:30Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    30,
                    3,
                    233,
                    0
                ],
                "title": "CausalMesh: A Formally Verified Causal Cache for Stateful Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalMesh: A Formally Verified Causal Cache for Stateful Serverless\n  Computing"
                },
                "summary": "Stateful serverless workflows consist of multiple serverless functions that\naccess state on a remote database. Developers sometimes add a cache layer\nbetween the serverless runtime and the database to improve I/O latency.\nHowever, in a serverless environment, functions in the same workflow may be\nscheduled to different nodes with different caches, which can cause\nnon-intuitive anomalies. This paper presents CausalMesh, a novel approach to\ncausally consistent caching in environments where a computation may migrate\nfrom one machine to another, such as in serverless computing. CausalMesh is the\nfirst cache system that supports coordination-free and abort-free read/write\noperations and read transactions when clients roam among multiple servers.\nCausalMesh also supports read-write transactional causal consistency in the\npresence of client roaming, but at the cost of abort-freedom.\n  We have formally verified CausalMesh's protocol in Dafny, and our\nexperimental evaluation shows that CausalMesh has lower latency and higher\nthroughput than existing proposals",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful serverless workflows consist of multiple serverless functions that\naccess state on a remote database. Developers sometimes add a cache layer\nbetween the serverless runtime and the database to improve I/O latency.\nHowever, in a serverless environment, functions in the same workflow may be\nscheduled to different nodes with different caches, which can cause\nnon-intuitive anomalies. This paper presents CausalMesh, a novel approach to\ncausally consistent caching in environments where a computation may migrate\nfrom one machine to another, such as in serverless computing. CausalMesh is the\nfirst cache system that supports coordination-free and abort-free read/write\noperations and read transactions when clients roam among multiple servers.\nCausalMesh also supports read-write transactional causal consistency in the\npresence of client roaming, but at the cost of abort-freedom.\n  We have formally verified CausalMesh's protocol in Dafny, and our\nexperimental evaluation shows that CausalMesh has lower latency and higher\nthroughput than existing proposals"
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Zihao Zhang"
                    },
                    {
                        "name": "Shuai Mu"
                    },
                    {
                        "name": "Sebastian Angel"
                    },
                    {
                        "name": "Vincent Liu"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Liu"
                },
                "author": "Vincent Liu",
                "arxiv_doi": "10.14778/3704965.3704969",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3704965.3704969",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.15647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Extended version from PVLDB Volume 17, Issue 13, 2024. This version\n  includes full proofs and formal verification in Dafny and fixes some small\n  bugs",
                "arxiv_journal_ref": "PVLDB Volume 17, Issue 13, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v2",
                "updated": "2025-08-21T14:58:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    58,
                    12,
                    3,
                    233,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15601v1",
                "updated": "2025-08-21T14:24:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    24,
                    52,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T14:24:52Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    24,
                    52,
                    3,
                    233,
                    0
                ],
                "title": "Efficient Mixed-Precision Large Language Model Inference with TurboMind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Mixed-Precision Large Language Model Inference with TurboMind"
                },
                "summary": "Mixed-precision inference techniques reduce the memory and computational\ndemands of Large Language Models (LLMs) by applying hybrid precision formats to\nmodel weights, activations, and KV caches. This work introduces mixed-precision\nLLM inference techniques that encompass (i) systematic memory and compute\noptimization across hierarchical storage and tensor core architectures, and\n(ii) comprehensive end-to-end mixed-precision optimization across diverse\nprecision formats and hardware configurations. Our approach features two novel\nmixed-precision pipelines designed for optimal hardware utilization: a General\nMatrix Multiply (GEMM) pipeline that optimizes matrix operations through\noffline weight packing and online acceleration, and an attention pipeline that\nenables efficient attention computation with arbitrary Query, Key, and Value\nprecision combinations. The key implementation of the pipelines includes (i)\nhardware-aware weight packing for automatic format optimization, (ii) adaptive\nhead alignment for efficient attention computation, (iii) instruction-level\nparallelism for memory hierarchy exploitation, and (iv) KV memory loading\npipeline for enhanced inference efficiency. We conduct comprehensive\nevaluations across 16 popular LLMs and 4 representative GPU architectures.\nResults demonstrate that our approach achieves up to 61% lower serving latency\n(30% on average) and up to 156% higher throughput (58% on average) in\nmixed-precision workloads compared to existing mixed-precision frameworks,\nestablishing consistent performance improvements across all tested\nconfigurations and hardware types. This work is integrated into TurboMind, a\nhigh-performance inference engine of the LMDeploy project, which is\nopen-sourced and publicly available at https://github.com/InternLM/lmdeploy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-precision inference techniques reduce the memory and computational\ndemands of Large Language Models (LLMs) by applying hybrid precision formats to\nmodel weights, activations, and KV caches. This work introduces mixed-precision\nLLM inference techniques that encompass (i) systematic memory and compute\noptimization across hierarchical storage and tensor core architectures, and\n(ii) comprehensive end-to-end mixed-precision optimization across diverse\nprecision formats and hardware configurations. Our approach features two novel\nmixed-precision pipelines designed for optimal hardware utilization: a General\nMatrix Multiply (GEMM) pipeline that optimizes matrix operations through\noffline weight packing and online acceleration, and an attention pipeline that\nenables efficient attention computation with arbitrary Query, Key, and Value\nprecision combinations. The key implementation of the pipelines includes (i)\nhardware-aware weight packing for automatic format optimization, (ii) adaptive\nhead alignment for efficient attention computation, (iii) instruction-level\nparallelism for memory hierarchy exploitation, and (iv) KV memory loading\npipeline for enhanced inference efficiency. We conduct comprehensive\nevaluations across 16 popular LLMs and 4 representative GPU architectures.\nResults demonstrate that our approach achieves up to 61% lower serving latency\n(30% on average) and up to 156% higher throughput (58% on average) in\nmixed-precision workloads compared to existing mixed-precision frameworks,\nestablishing consistent performance improvements across all tested\nconfigurations and hardware types. This work is integrated into TurboMind, a\nhigh-performance inference engine of the LMDeploy project, which is\nopen-sourced and publicly available at https://github.com/InternLM/lmdeploy."
                },
                "authors": [
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Guoliang He"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Han Lv"
                    },
                    {
                        "name": "Qian Yao"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15583v1",
                "updated": "2025-08-21T13:57:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    57,
                    9,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T13:57:09Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    57,
                    9,
                    3,
                    233,
                    0
                ],
                "title": "Time-Optimal Directed q-Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Optimal Directed q-Analysis"
                },
                "summary": "Directed q-analysis is a recent extension of q-analysis, an established\nmethod for extracting structure from networks, to directed graphs. Until\nrecently, a lack of efficient algorithms heavily restricted the application of\nthis technique: Previous approaches scale with the square of the input size,\nwhich is also the maximal size of the output, rendering such approaches\nworst-case optimal. In practice, output sizes of relevant networks are usually\nfar from the worst case, a fact that could be exploited by an (efficient)\noutput-sensitive algorithm. We develop such an algorithm and formally describe\nit in detail. The key insight, obtained by carefully studying various\napproaches to directed q-analysis and how they relate to each other, is that\ninverting the order of computation leads to significant complexity gains.\nTargeted precomputation and caching tactics further reduce the introduced\noverhead, enough to achieve (under mild assumptions) a time complexity that is\nlinear in output size. The resulting algorithm for performing directed\nq-analysis is shown to be time-optimal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Directed q-analysis is a recent extension of q-analysis, an established\nmethod for extracting structure from networks, to directed graphs. Until\nrecently, a lack of efficient algorithms heavily restricted the application of\nthis technique: Previous approaches scale with the square of the input size,\nwhich is also the maximal size of the output, rendering such approaches\nworst-case optimal. In practice, output sizes of relevant networks are usually\nfar from the worst case, a fact that could be exploited by an (efficient)\noutput-sensitive algorithm. We develop such an algorithm and formally describe\nit in detail. The key insight, obtained by carefully studying various\napproaches to directed q-analysis and how they relate to each other, is that\ninverting the order of computation leads to significant complexity gains.\nTargeted precomputation and caching tactics further reduce the introduced\noverhead, enough to achieve (under mild assumptions) a time complexity that is\nlinear in output size. The resulting algorithm for performing directed\nq-analysis is shown to be time-optimal."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Florian Unger"
                    }
                ],
                "author_detail": {
                    "name": "Florian Unger"
                },
                "author": "Florian Unger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15545v1",
                "updated": "2025-08-21T13:24:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    24,
                    13,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T13:24:13Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    24,
                    13,
                    3,
                    233,
                    0
                ],
                "title": "QVecOpt: An Efficient Storage and Computing Opti-mization Framework for\n  Large-scale Quantum State Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QVecOpt: An Efficient Storage and Computing Opti-mization Framework for\n  Large-scale Quantum State Simulation"
                },
                "summary": "In response to the challenges in large-scale quantum state simulation on\nclassical computing platforms, including memory limits, frequent disk I/O, and\nhigh computational complexity, this study builds upon a previously proposed\nhierarchical storage-based quantum simulation system and introduces an\noptimization framework, the Quantum Vector Optimization Framework (QVecOpt).\nQVecOpt integrates four strategies: amplitude pairing, cache optimization,\nblock storage optimization, and parallel optimization. These collectively\nenhance state vector storage and computational scheduling. The amplitude\npairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing\ntraversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache\noptimization pre-allocates buffers and loads only required data, cutting disk\nI/O. Block storage optimization partitions the state vector for on-demand\nloading and local updates, reducing redundant access. Parallel optimization\ndistributes the state vector across nodes for collaborative computation,\nachieving near-linear speedup. Complexity analysis shows that, compared with\nhierarchical storage simulation, the method reduces state vector traversals for\nsingle-qubit gates from $2^n$ to 1, removing the main bottleneck. It also\nlowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and\n$O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold,\nbreaking the memory bottleneck of existing tools and enabling high-bit quantum\ncircuit simulations beyond traditional methods. This work provides an\nefficient, scalable solution for classical simulation of large-scale quantum\ncomputation with significant academic and practical value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In response to the challenges in large-scale quantum state simulation on\nclassical computing platforms, including memory limits, frequent disk I/O, and\nhigh computational complexity, this study builds upon a previously proposed\nhierarchical storage-based quantum simulation system and introduces an\noptimization framework, the Quantum Vector Optimization Framework (QVecOpt).\nQVecOpt integrates four strategies: amplitude pairing, cache optimization,\nblock storage optimization, and parallel optimization. These collectively\nenhance state vector storage and computational scheduling. The amplitude\npairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing\ntraversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache\noptimization pre-allocates buffers and loads only required data, cutting disk\nI/O. Block storage optimization partitions the state vector for on-demand\nloading and local updates, reducing redundant access. Parallel optimization\ndistributes the state vector across nodes for collaborative computation,\nachieving near-linear speedup. Complexity analysis shows that, compared with\nhierarchical storage simulation, the method reduces state vector traversals for\nsingle-qubit gates from $2^n$ to 1, removing the main bottleneck. It also\nlowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and\n$O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold,\nbreaking the memory bottleneck of existing tools and enabling high-bit quantum\ncircuit simulations beyond traditional methods. This work provides an\nefficient, scalable solution for classical simulation of large-scale quantum\ncomputation with significant academic and practical value."
                },
                "authors": [
                    {
                        "name": "Mingyang Yu"
                    },
                    {
                        "name": "Haorui Yang"
                    },
                    {
                        "name": "Donglin Wang"
                    },
                    {
                        "name": "Desheng Kong"
                    },
                    {
                        "name": "Ji Du"
                    },
                    {
                        "name": "Yulong Fu"
                    },
                    {
                        "name": "Jing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Xu"
                },
                "author": "Jing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12892v2",
                "updated": "2025-08-21T12:52:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    12,
                    52,
                    11,
                    3,
                    233,
                    0
                ],
                "published": "2024-09-19T16:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt"
                },
                "summary": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 20% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 20% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS."
                },
                "authors": [
                    {
                        "name": "Lukas Hllein"
                    },
                    {
                        "name": "Alja Boi"
                    },
                    {
                        "name": "Michael Zollhfer"
                    },
                    {
                        "name": "Matthias Niener"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Niener"
                },
                "author": "Matthias Niener",
                "arxiv_comment": "Accepted to ICCV 2025. Project page:\n  https://lukashoel.github.io/3DGS-LM, Video:\n  https://www.youtube.com/watch?v=tDiGuGMssg8, Code:\n  https://github.com/lukasHoel/3DGS-LM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14204v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14204v3",
                "updated": "2025-08-21T11:43:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    11,
                    43,
                    48,
                    3,
                    233,
                    0
                ],
                "published": "2024-04-22T14:13:36Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    14,
                    13,
                    36,
                    0,
                    113,
                    0
                ],
                "title": "TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading"
                },
                "summary": "Next-generation mobile networks are expected to facilitate fast AI model\ndownloading to end users. By caching models on edge servers, mobile networks\ncan deliver models to end users with low latency, resulting in a paradigm of\nedge model caching. In this paper, we develop a novel model placement\nframework, called parameter-sharing model caching (TrimCaching). TrimCaching\nexploits the key observation that a wide range of AI models, such as\nconvolutional neural networks or large language models, can share a significant\nproportion of parameter blocks containing reusable knowledge, thereby improving\nstorage efficiency. To this end, we formulate a parameter-sharing model\nplacement problem to maximize the cache hit ratio in multi-edge wireless\nnetworks by balancing the fundamental tradeoff between storage efficiency and\nservice latency. We show that the formulated problem is a submodular\nmaximization problem with submodular constraints, for which no polynomial-time\napproximation algorithm exists. To tackle this challenge, we study an important\nspecial case, where a small fixed number of parameter blocks are shared across\nmodels, which often holds in practice. In such a case, a polynomial-time\nalgorithm with a $\\left(1-\\epsilon\\right)/2$-approximation guarantee is\ndeveloped. Subsequently, we address the original problem for the general case\nby developing a greedy algorithm. Simulation results demonstrate that the\nproposed TrimCaching framework significantly improves the cache hit ratio\ncompared with state-of-the-art content caching without exploiting shared\nparameters in AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation mobile networks are expected to facilitate fast AI model\ndownloading to end users. By caching models on edge servers, mobile networks\ncan deliver models to end users with low latency, resulting in a paradigm of\nedge model caching. In this paper, we develop a novel model placement\nframework, called parameter-sharing model caching (TrimCaching). TrimCaching\nexploits the key observation that a wide range of AI models, such as\nconvolutional neural networks or large language models, can share a significant\nproportion of parameter blocks containing reusable knowledge, thereby improving\nstorage efficiency. To this end, we formulate a parameter-sharing model\nplacement problem to maximize the cache hit ratio in multi-edge wireless\nnetworks by balancing the fundamental tradeoff between storage efficiency and\nservice latency. We show that the formulated problem is a submodular\nmaximization problem with submodular constraints, for which no polynomial-time\napproximation algorithm exists. To tackle this challenge, we study an important\nspecial case, where a small fixed number of parameter blocks are shared across\nmodels, which often holds in practice. In such a case, a polynomial-time\nalgorithm with a $\\left(1-\\epsilon\\right)/2$-approximation guarantee is\ndeveloped. Subsequently, we address the original problem for the general case\nby developing a greedy algorithm. Simulation results demonstrate that the\nproposed TrimCaching framework significantly improves the cache hit ratio\ncompared with state-of-the-art content caching without exploiting shared\nparameters in AI models."
                },
                "authors": [
                    {
                        "name": "Guanqiao Qu"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "18 pages, 13 figures. Part of this work has been accepted by ICDCS\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14204v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14204v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15290v1",
                "updated": "2025-08-21T06:26:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    6,
                    26,
                    18,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T06:26:18Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    6,
                    26,
                    18,
                    3,
                    233,
                    0
                ],
                "title": "Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional\n  Vector Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional\n  Vector Search"
                },
                "summary": "Similarity-based vector search underpins many important applications, but a\nkey challenge is processing massive vector datasets (e.g., in TBs). To reduce\ncosts, some systems utilize SSDs as the primary data storage. They employ a\nproximity graph, which connects similar vectors to form a graph and is the\nstate-of-the-art index for vector search. However, these systems are hindered\nby sub-optimal data layouts that fail to effectively utilize valuable memory\nspace to reduce disk access and suffer from poor locality for accessing\ndisk-resident data. Through extensive profiling and analysis, we found that the\nstructure of the proximity graph index is accessed more frequently than the\nvectors themselves, yet existing systems do not distinguish between the two. To\naddress this problem, we design the Gorgeous system with the principle of\nprioritizing graph structure over vectors. Specifically, Gorgeous features a\nmemory cache that keeps the adjacency lists of graph nodes to improve cache\nhits and a disk block format that explicitly stores neighbors' adjacency lists\nalong with a vector to enhance data locality. Experimental results show that\nGorgeous consistently outperforms two state-of-the-art disk-based systems for\nvector search, boosting average query throughput by over 60% and reducing query\nlatency by over 35%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity-based vector search underpins many important applications, but a\nkey challenge is processing massive vector datasets (e.g., in TBs). To reduce\ncosts, some systems utilize SSDs as the primary data storage. They employ a\nproximity graph, which connects similar vectors to form a graph and is the\nstate-of-the-art index for vector search. However, these systems are hindered\nby sub-optimal data layouts that fail to effectively utilize valuable memory\nspace to reduce disk access and suffer from poor locality for accessing\ndisk-resident data. Through extensive profiling and analysis, we found that the\nstructure of the proximity graph index is accessed more frequently than the\nvectors themselves, yet existing systems do not distinguish between the two. To\naddress this problem, we design the Gorgeous system with the principle of\nprioritizing graph structure over vectors. Specifically, Gorgeous features a\nmemory cache that keeps the adjacency lists of graph nodes to improve cache\nhits and a disk block format that explicitly stores neighbors' adjacency lists\nalong with a vector to enhance data locality. Experimental results show that\nGorgeous consistently outperforms two state-of-the-art disk-based systems for\nvector search, boosting average query throughput by over 60% and reducing query\nlatency by over 35%."
                },
                "authors": [
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Xiaolu Li"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Meiling Wang"
                    },
                    {
                        "name": "Xin Yao"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "12 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v1",
                "updated": "2025-08-21T03:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15036v1",
                "updated": "2025-08-20T20:02:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    20,
                    2,
                    35,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T20:02:35Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    20,
                    2,
                    35,
                    2,
                    232,
                    0
                ],
                "title": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in\n  Mixture-of-Experts LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in\n  Mixture-of-Experts LLMs"
                },
                "summary": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services."
                },
                "authors": [
                    {
                        "name": "Ruyi Ding"
                    },
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Xinyi Shen"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "This paper will appear in CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15033v1",
                "updated": "2025-08-20T19:54:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    19,
                    54,
                    41,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T19:54:41Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    19,
                    54,
                    41,
                    2,
                    232,
                    0
                ],
                "title": "Rethinking the Potential of Layer Freezing for Efficient DNN Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Potential of Layer Freezing for Efficient DNN Training"
                },
                "summary": "With the growing size of deep neural networks and datasets, the computational\ncosts of training have significantly increased. The layer-freezing technique\nhas recently attracted great attention as a promising method to effectively\nreduce the cost of network training. However, in traditional layer-freezing\nmethods, frozen layers are still required for forward propagation to generate\nfeature maps for unfrozen layers, limiting the reduction of computation costs.\nTo overcome this, prior works proposed a hypothetical solution, which caches\nfeature maps from frozen layers as a new dataset, allowing later layers to\ntrain directly on stored feature maps. While this approach appears to be\nstraightforward, it presents several major challenges that are severely\noverlooked by prior literature, such as how to effectively apply augmentations\nto feature maps and the substantial storage overhead introduced. If these\noverlooked challenges are not addressed, the performance of the caching method\nwill be severely impacted and even make it infeasible. This paper is the first\nto comprehensively explore these challenges and provides a systematic solution.\nTo improve training accuracy, we propose \\textit{similarity-aware channel\naugmentation}, which caches channels with high augmentation sensitivity with a\nminimum additional storage cost. To mitigate storage overhead, we incorporate\nlossy data compression into layer freezing and design a \\textit{progressive\ncompression} strategy, which increases compression rates as more layers are\nfrozen, effectively reducing storage costs. Finally, our solution achieves\nsignificant reductions in training cost while maintaining model accuracy, with\na minor time overhead. Additionally, we conduct a comprehensive evaluation of\nfreezing and compression strategies, providing insights into optimizing their\napplication for efficient DNN training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing size of deep neural networks and datasets, the computational\ncosts of training have significantly increased. The layer-freezing technique\nhas recently attracted great attention as a promising method to effectively\nreduce the cost of network training. However, in traditional layer-freezing\nmethods, frozen layers are still required for forward propagation to generate\nfeature maps for unfrozen layers, limiting the reduction of computation costs.\nTo overcome this, prior works proposed a hypothetical solution, which caches\nfeature maps from frozen layers as a new dataset, allowing later layers to\ntrain directly on stored feature maps. While this approach appears to be\nstraightforward, it presents several major challenges that are severely\noverlooked by prior literature, such as how to effectively apply augmentations\nto feature maps and the substantial storage overhead introduced. If these\noverlooked challenges are not addressed, the performance of the caching method\nwill be severely impacted and even make it infeasible. This paper is the first\nto comprehensively explore these challenges and provides a systematic solution.\nTo improve training accuracy, we propose \\textit{similarity-aware channel\naugmentation}, which caches channels with high augmentation sensitivity with a\nminimum additional storage cost. To mitigate storage overhead, we incorporate\nlossy data compression into layer freezing and design a \\textit{progressive\ncompression} strategy, which increases compression rates as more layers are\nfrozen, effectively reducing storage costs. Finally, our solution achieves\nsignificant reductions in training cost while maintaining model accuracy, with\na minor time overhead. Additionally, we conduct a comprehensive evaluation of\nfreezing and compression strategies, providing insights into optimizing their\napplication for efficient DNN training."
                },
                "authors": [
                    {
                        "name": "Chence Yang"
                    },
                    {
                        "name": "Ci Zhang"
                    },
                    {
                        "name": "Lei Lu"
                    },
                    {
                        "name": "Qitao Tan"
                    },
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Xulong Tang"
                    },
                    {
                        "name": "Shaoyi Huang"
                    },
                    {
                        "name": "Jinzhen Wang"
                    },
                    {
                        "name": "Guoming Li"
                    },
                    {
                        "name": "Jundong Li"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Geng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Geng Yuan"
                },
                "author": "Geng Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14468v1",
                "updated": "2025-08-20T06:48:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    48,
                    54,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T06:48:54Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    48,
                    54,
                    2,
                    232,
                    0
                ],
                "title": "Diverse Negative Sampling for Implicit Collaborative Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverse Negative Sampling for Implicit Collaborative Filtering"
                },
                "summary": "Implicit collaborative filtering recommenders are usually trained to learn\nuser positive preferences. Negative sampling, which selects informative\nnegative items to form negative training data, plays a crucial role in this\nprocess. Since items are often clustered in the latent space, existing negative\nsampling strategies normally oversample negative items from the dense regions.\nThis leads to homogeneous negative data and limited model expressiveness. In\nthis paper, we propose Diverse Negative Sampling (DivNS), a novel approach that\nexplicitly accounts for diversity in negative training data during the negative\nsampling process. DivNS first finds hard negative items with large preference\nscores and constructs user-specific caches that store unused but highly\ninformative negative samples. Then, its diversity-augmented sampler selects a\ndiverse subset of negative items from the cache while ensuring dissimilarity\nfrom the user's hard negatives. Finally, a synthetic negatives generator\ncombines the selected diverse negatives with hard negatives to form more\neffective training data. The resulting synthetic negatives are both informative\nand diverse, enabling recommenders to learn a broader item space and improve\ntheir generalisability. Extensive experiments on four public datasets\ndemonstrate the effectiveness of DivNS in improving recommendation quality\nwhile maintaining computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit collaborative filtering recommenders are usually trained to learn\nuser positive preferences. Negative sampling, which selects informative\nnegative items to form negative training data, plays a crucial role in this\nprocess. Since items are often clustered in the latent space, existing negative\nsampling strategies normally oversample negative items from the dense regions.\nThis leads to homogeneous negative data and limited model expressiveness. In\nthis paper, we propose Diverse Negative Sampling (DivNS), a novel approach that\nexplicitly accounts for diversity in negative training data during the negative\nsampling process. DivNS first finds hard negative items with large preference\nscores and constructs user-specific caches that store unused but highly\ninformative negative samples. Then, its diversity-augmented sampler selects a\ndiverse subset of negative items from the cache while ensuring dissimilarity\nfrom the user's hard negatives. Finally, a synthetic negatives generator\ncombines the selected diverse negatives with hard negatives to form more\neffective training data. The resulting synthetic negatives are both informative\nand diverse, enabling recommenders to learn a broader item space and improve\ntheir generalisability. Extensive experiments on four public datasets\ndemonstrate the effectiveness of DivNS in improving recommendation quality\nwhile maintaining computational efficiency."
                },
                "authors": [
                    {
                        "name": "Yueqing Xuan"
                    },
                    {
                        "name": "Kacper Sokol"
                    },
                    {
                        "name": "Mark Sanderson"
                    },
                    {
                        "name": "Jeffrey Chan"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Chan"
                },
                "author": "Jeffrey Chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14420v1",
                "updated": "2025-08-20T04:36:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    36,
                    25,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T04:36:25Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    36,
                    25,
                    2,
                    232,
                    0
                ],
                "title": "You Only Evaluate Once: A Tree-based Rerank Method at Meituan",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Only Evaluate Once: A Tree-based Rerank Method at Meituan"
                },
                "summary": "Reranking plays a crucial role in modern recommender systems by capturing the\nmutual influences within the list. Due to the inherent challenges of\ncombinatorial search spaces, most methods adopt a two-stage search paradigm: a\nsimple General Search Unit (GSU) efficiently reduces the candidate space, and\nan Exact Search Unit (ESU) effectively selects the optimal sequence. These\nmethods essentially involve making trade-offs between effectiveness and\nefficiency, while suffering from a severe \\textbf{inconsistency problem}, that\nis, the GSU often misses high-value lists from ESU. To address this problem, we\npropose YOLOR, a one-stage reranking method that removes the GSU while\nretaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context\nExtraction Module (TCEM) that hierarchically aggregates multi-scale contextual\nfeatures to achieve \"list-level effectiveness\", and (2) a Context Cache Module\n(CCM) that enables efficient feature reuse across candidate permutations to\nachieve \"permutation-level efficiency\". Extensive experiments across public and\nindustry datasets validate YOLOR's performance, and we have successfully\ndeployed YOLOR on the Meituan food delivery platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking plays a crucial role in modern recommender systems by capturing the\nmutual influences within the list. Due to the inherent challenges of\ncombinatorial search spaces, most methods adopt a two-stage search paradigm: a\nsimple General Search Unit (GSU) efficiently reduces the candidate space, and\nan Exact Search Unit (ESU) effectively selects the optimal sequence. These\nmethods essentially involve making trade-offs between effectiveness and\nefficiency, while suffering from a severe \\textbf{inconsistency problem}, that\nis, the GSU often misses high-value lists from ESU. To address this problem, we\npropose YOLOR, a one-stage reranking method that removes the GSU while\nretaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context\nExtraction Module (TCEM) that hierarchically aggregates multi-scale contextual\nfeatures to achieve \"list-level effectiveness\", and (2) a Context Cache Module\n(CCM) that enables efficient feature reuse across candidate permutations to\nachieve \"permutation-level efficiency\". Extensive experiments across public and\nindustry datasets validate YOLOR's performance, and we have successfully\ndeployed YOLOR on the Meituan food delivery platform."
                },
                "authors": [
                    {
                        "name": "Shuli Wang"
                    },
                    {
                        "name": "Yinqiu Huang"
                    },
                    {
                        "name": "Changhao Li"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Yonggang Liu"
                    },
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Yinhua Zhu"
                    },
                    {
                        "name": "Haitao Wang"
                    },
                    {
                        "name": "Xingxing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingxing Wang"
                },
                "author": "Xingxing Wang",
                "arxiv_doi": "10.1145/3746252.3761539",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761539",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.14420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by CIKM 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14148v2",
                "updated": "2025-08-23T20:28:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    45,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-19T16:56:51Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    56,
                    51,
                    1,
                    231,
                    0
                ],
                "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPad: Efficient Diffusion Language Models with Suffix Dropout"
                },
                "summary": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad."
                },
                "authors": [
                    {
                        "name": "Xinhua Chen"
                    },
                    {
                        "name": "Sitao Huang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Chiyue Wei"
                    },
                    {
                        "name": "Yintao He"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13935v1",
                "updated": "2025-08-19T15:26:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    26,
                    36,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:26:36Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    26,
                    36,
                    1,
                    231,
                    0
                ],
                "title": "Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated\n  LSM-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated\n  LSM-trees"
                },
                "summary": "Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are\nwidely used in storage systems but face significant challenges, such as high\nwrite amplification caused by compaction. KV-separated LSM-trees address write\namplification but introduce significant space amplification, a critical concern\nin cost-sensitive scenarios. Garbage collection (GC) can reduce space\namplification, but existing strategies are often inefficient and fail to\naccount for workload characteristics. Moreover, current key-value (KV)\nseparated LSM-trees overlook the space amplification caused by the index\nLSM-tree. In this paper, we systematically analyze the sources of space\namplification in KV-separated LSM-trees and propose Scavenger+, which achieves\na better performance-space trade-off. Scavenger+ introduces (1) an\nI/O-efficient garbage collection scheme to reduce I/O overhead, (2) a\nspace-aware compaction strategy based on compensated size to mitigate\nindex-induced space amplification, and (3) a dynamic GC scheduler that adapts\nto system load to make better use of CPU and storage resources. Extensive\nexperiments demonstrate that Scavenger+ significantly improves write\nperformance and reduces space amplification compared to state-of-the-art\nKV-separated LSM-trees, including BlobDB, Titan, and TerarkDB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are\nwidely used in storage systems but face significant challenges, such as high\nwrite amplification caused by compaction. KV-separated LSM-trees address write\namplification but introduce significant space amplification, a critical concern\nin cost-sensitive scenarios. Garbage collection (GC) can reduce space\namplification, but existing strategies are often inefficient and fail to\naccount for workload characteristics. Moreover, current key-value (KV)\nseparated LSM-trees overlook the space amplification caused by the index\nLSM-tree. In this paper, we systematically analyze the sources of space\namplification in KV-separated LSM-trees and propose Scavenger+, which achieves\na better performance-space trade-off. Scavenger+ introduces (1) an\nI/O-efficient garbage collection scheme to reduce I/O overhead, (2) a\nspace-aware compaction strategy based on compensated size to mitigate\nindex-induced space amplification, and (3) a dynamic GC scheduler that adapts\nto system load to make better use of CPU and storage resources. Extensive\nexperiments demonstrate that Scavenger+ significantly improves write\nperformance and reduces space amplification compared to state-of-the-art\nKV-separated LSM-trees, including BlobDB, Titan, and TerarkDB."
                },
                "authors": [
                    {
                        "name": "Jianshun Zhang"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Jiaxin Ou"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Sheng Qiu"
                    },
                    {
                        "name": "Junxun Huang"
                    },
                    {
                        "name": "Baoquan Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Dan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Feng"
                },
                "author": "Dan Feng",
                "arxiv_doi": "10.1109/TC.2025.3587513",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TC.2025.3587513",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Computers",
                "arxiv_journal_ref": "Year 2025, pp. 1-14,",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13909v1",
                "updated": "2025-08-19T15:08:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    8,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:08:39Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    8,
                    39,
                    1,
                    231,
                    0
                ],
                "title": "Scavenger: Better Space-Time Trade-Offs for Key-Value Separated\n  LSM-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scavenger: Better Space-Time Trade-Offs for Key-Value Separated\n  LSM-trees"
                },
                "summary": "Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)\nhave gained widespread acceptance in storage systems. Nonetheless, a\nsignificant challenge arises in the form of high write amplification due to the\ncompaction process. While KV-separated LSM-trees successfully tackle this\nissue, they also bring about substantial space amplification problems, a\nconcern that cannot be overlooked in cost-sensitive scenarios. Garbage\ncollection (GC) holds significant promise for space amplification reduction,\nyet existing GC strategies often fall short in optimization performance,\nlacking thorough consideration of workload characteristics. Additionally,\ncurrent KV-separated LSM-trees also ignore the adverse effect of the space\namplification in the index LSM-tree. In this paper, we systematically analyze\nthe sources of space amplification of KV-separated LSM-trees and introduce\nScavenger, which achieves a better trade-off between performance and space\namplification. Scavenger initially proposes an I/O-efficient garbage collection\nscheme to reduce I/O overhead and incorporates a space-aware compaction\nstrategy based on compensated size to minimize the space amplification of index\nLSM-trees. Extensive experiments show that Scavenger significantly improves\nwrite performance and achieves lower space amplification than other\nKV-separated LSM-trees (including BlobDB, Titan, and TerarkDB).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)\nhave gained widespread acceptance in storage systems. Nonetheless, a\nsignificant challenge arises in the form of high write amplification due to the\ncompaction process. While KV-separated LSM-trees successfully tackle this\nissue, they also bring about substantial space amplification problems, a\nconcern that cannot be overlooked in cost-sensitive scenarios. Garbage\ncollection (GC) holds significant promise for space amplification reduction,\nyet existing GC strategies often fall short in optimization performance,\nlacking thorough consideration of workload characteristics. Additionally,\ncurrent KV-separated LSM-trees also ignore the adverse effect of the space\namplification in the index LSM-tree. In this paper, we systematically analyze\nthe sources of space amplification of KV-separated LSM-trees and introduce\nScavenger, which achieves a better trade-off between performance and space\namplification. Scavenger initially proposes an I/O-efficient garbage collection\nscheme to reduce I/O overhead and incorporates a space-aware compaction\nstrategy based on compensated size to minimize the space amplification of index\nLSM-trees. Extensive experiments show that Scavenger significantly improves\nwrite performance and achieves lower space amplification than other\nKV-separated LSM-trees (including BlobDB, Titan, and TerarkDB)."
                },
                "authors": [
                    {
                        "name": "Jianshun Zhang"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Sheng Qiu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Jiaxin Ou"
                    },
                    {
                        "name": "Junxun Huang"
                    },
                    {
                        "name": "Baoquan Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Dan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Feng"
                },
                "author": "Dan Feng",
                "arxiv_doi": "10.1109/ICDE60146.2024.00312",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICDE60146.2024.00312",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, accepted by 2024 IEEE 40st International Conference on Data\n  Engineering (ICDE)",
                "arxiv_journal_ref": "Year: 2024, Pages: 4072-4085",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v1",
                "updated": "2025-08-19T14:30:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Inter-Core Cache Contention Analysis for WCET Estimation on\n  Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Inter-Core Cache Contention Analysis for WCET Estimation on\n  Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13859v1",
                "updated": "2025-08-19T14:18:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    18,
                    16,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:18:16Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    18,
                    16,
                    1,
                    231,
                    0
                ],
                "title": "Zobrist Hash-based Duplicate Detection in Symbolic Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zobrist Hash-based Duplicate Detection in Symbolic Regression"
                },
                "summary": "Symbolic regression encompasses a family of search algorithms that aim to\ndiscover the best fitting function for a set of data without requiring an a\npriori specification of the model structure. The most successful and commonly\nused technique for symbolic regression is Genetic Programming (GP), an\nevolutionary search method that evolves a population of mathematical\nexpressions through the mechanism of natural selection. In this work we analyze\nthe efficiency of the evolutionary search in GP and show that many points in\nthe search space are re-visited and re-evaluated multiple times by the\nalgorithm, leading to wasted computational effort. We address this issue by\nintroducing a caching mechanism based on the Zobrist hash, a type of hashing\nfrequently used in abstract board games for the efficient construction and\nsubsequent update of transposition tables. We implement our caching approach\nusing the open-source framework Operon and demonstrate its performance on a\nselection of real-world regression problems, where we observe up to 34\\%\nspeedups without any detrimental effects on search quality. The hashing\napproach represents a straightforward way to improve runtime performance while\nalso offering some interesting possibilities for adjusting search strategy\nbased on cached information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic regression encompasses a family of search algorithms that aim to\ndiscover the best fitting function for a set of data without requiring an a\npriori specification of the model structure. The most successful and commonly\nused technique for symbolic regression is Genetic Programming (GP), an\nevolutionary search method that evolves a population of mathematical\nexpressions through the mechanism of natural selection. In this work we analyze\nthe efficiency of the evolutionary search in GP and show that many points in\nthe search space are re-visited and re-evaluated multiple times by the\nalgorithm, leading to wasted computational effort. We address this issue by\nintroducing a caching mechanism based on the Zobrist hash, a type of hashing\nfrequently used in abstract board games for the efficient construction and\nsubsequent update of transposition tables. We implement our caching approach\nusing the open-source framework Operon and demonstrate its performance on a\nselection of real-world regression problems, where we observe up to 34\\%\nspeedups without any detrimental effects on search quality. The hashing\napproach represents a straightforward way to improve runtime performance while\nalso offering some interesting possibilities for adjusting search strategy\nbased on cached information."
                },
                "authors": [
                    {
                        "name": "Bogdan Burlacu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Burlacu"
                },
                "author": "Bogdan Burlacu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13756v1",
                "updated": "2025-08-19T11:54:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    54,
                    30,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:54:30Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    54,
                    30,
                    1,
                    231,
                    0
                ],
                "title": "INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video"
                },
                "summary": "Real-time streaming of point cloud video, characterized by massive data\nvolumes and high sensitivity to packet loss, remains a key challenge for\nimmersive applications under dynamic network conditions. While\nconnection-oriented protocols such as TCP and more modern alternatives like\nQUIC alleviate some transport-layer inefficiencies, including head-of-line\nblocking, they still retain a coarse-grained, segment-based delivery model and\na centralized control loop that limit fine-grained adaptation and effective\ncaching. We introduce INDS (Incremental Named Data Streaming), an adaptive\nstreaming framework based on Information-Centric Networking (ICN) that rethinks\ndelivery for hierarchical, layered media. INDS leverages the Octree structure\nof point cloud video and expressive content naming to support progressive,\npartial retrieval of enhancement layers based on consumer bandwidth and\ndecoding capability. By combining time-windows with Group-of-Frames (GoF),\nINDS's naming scheme supports fine-grained in-network caching and facilitates\nefficient multi-user data reuse. INDS can be deployed as an overlay, remaining\ncompatible with QUIC-based transport infrastructure as well as future\nMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IP\nnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%\nhigher throughput, and 20-30% increased cache hit rates compared to\nstate-of-the-art DASH-style systems. Together, these results establish INDS as\na scalable, cache-friendly solution for real-time point cloud streaming under\nvariable and lossy conditions, while its compatibility with MoQ overlays\nfurther positions it as a practical, forward-compatible architecture for\nemerging immersive media systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming of point cloud video, characterized by massive data\nvolumes and high sensitivity to packet loss, remains a key challenge for\nimmersive applications under dynamic network conditions. While\nconnection-oriented protocols such as TCP and more modern alternatives like\nQUIC alleviate some transport-layer inefficiencies, including head-of-line\nblocking, they still retain a coarse-grained, segment-based delivery model and\na centralized control loop that limit fine-grained adaptation and effective\ncaching. We introduce INDS (Incremental Named Data Streaming), an adaptive\nstreaming framework based on Information-Centric Networking (ICN) that rethinks\ndelivery for hierarchical, layered media. INDS leverages the Octree structure\nof point cloud video and expressive content naming to support progressive,\npartial retrieval of enhancement layers based on consumer bandwidth and\ndecoding capability. By combining time-windows with Group-of-Frames (GoF),\nINDS's naming scheme supports fine-grained in-network caching and facilitates\nefficient multi-user data reuse. INDS can be deployed as an overlay, remaining\ncompatible with QUIC-based transport infrastructure as well as future\nMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IP\nnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%\nhigher throughput, and 20-30% increased cache hit rates compared to\nstate-of-the-art DASH-style systems. Together, these results establish INDS as\na scalable, cache-friendly solution for real-time point cloud streaming under\nvariable and lossy conditions, while its compatibility with MoQ overlays\nfurther positions it as a practical, forward-compatible architecture for\nemerging immersive media systems."
                },
                "authors": [
                    {
                        "name": "Ruonan Chai"
                    },
                    {
                        "name": "Yixiang Zhu"
                    },
                    {
                        "name": "Xinjiao Li"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Zili Meng"
                    },
                    {
                        "name": "Dirk Kutscher"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Kutscher"
                },
                "author": "Dirk Kutscher",
                "arxiv_comment": "9 pages, 9 figures, 2 tables. To appear in Proc. of the 33rd ACM\n  International Conference on Multimedia (MM '25), October 27--31, 2025,\n  Dublin, Ireland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.1; C.2.4; H.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13716v1",
                "updated": "2025-08-19T10:21:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T10:21:33Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "title": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning"
                },
                "summary": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments."
                },
                "authors": [
                    {
                        "name": "Xianfeng Song"
                    },
                    {
                        "name": "Yi Zou"
                    },
                    {
                        "name": "Zheng Shi"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Shi"
                },
                "author": "Zheng Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v3",
                "updated": "2025-08-19T09:13:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    13,
                    13,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13523v1",
                "updated": "2025-08-19T05:27:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T05:27:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures"
                },
                "summary": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on all current US exascale\nmachines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the\nthree potentials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on all current US exascale\nmachines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the\nthree potentials."
                },
                "authors": [
                    {
                        "name": "Anders Johansson"
                    },
                    {
                        "name": "Evan Weinberg"
                    },
                    {
                        "name": "Christian R. Trott"
                    },
                    {
                        "name": "Megan J. McCarthy"
                    },
                    {
                        "name": "Stan G. Moore"
                    }
                ],
                "author_detail": {
                    "name": "Stan G. Moore"
                },
                "author": "Stan G. Moore",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v2",
                "updated": "2025-08-19T03:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    3,
                    13,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v2",
                "updated": "2025-08-19T01:38:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    1,
                    38,
                    23,
                    1,
                    231,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal Coulomb interactions. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal Coulomb interactions. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13382v1",
                "updated": "2025-08-18T21:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    21,
                    58,
                    18,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T21:58:18Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    21,
                    58,
                    18,
                    0,
                    230,
                    0
                ],
                "title": "Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data\n  Analysis"
                },
                "summary": "We present Datarus-R1-14B, a 14 B-parameter open-weights language model\nfine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and\ngraduate-level problem solver. Datarus is trained not on isolated\nquestion-answer pairs but on full analytical trajectories including reasoning\nsteps, code execution, error traces, self-corrections, and final conclusions,\nall captured in a ReAct-style notebook format spanning finance, medicine,\nnumerical analysis, and other quantitative domains. Our training pipeline\ncombines (i) a trajectory-centric synthetic data generator that yielded 144 000\ntagged notebook episodes, (ii) a dual-reward framework blending a lightweight\ntag-based structural signal with a Hierarchical Reward Model (HRM) that scores\nboth single-step soundness and end-to-end coherence, and (iii) a\nmemory-optimized implementation of Group Relative Policy Optimization (GRPO)\nfeaturing KV-cache reuse, sequential generation, and reference-model sharding.\nA cosine curriculum smoothly shifts emphasis from structural fidelity to\nsemantic depth, reducing the format collapse and verbosity that often plague\nRL-aligned LLMs. A central design choice in Datarus is it dual reasoning\ninterface. In agentic mode the model produces ReAct-tagged steps that invoke\nPython tools to execute real code; in reflection mode it outputs compact\nChain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On\ndemanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\"\npattern: it sketches hypotheses, revises them once or twice, and converges\navoiding the circular, token-inflating loops common to contemporary systems.\nAcross standard public benchmarks Datarus surpasses similar size models and\neven reaches the level of larger reasoning models such as QwQ-32B achieving up\nto 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting\n18-49% fewer tokens per solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Datarus-R1-14B, a 14 B-parameter open-weights language model\nfine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and\ngraduate-level problem solver. Datarus is trained not on isolated\nquestion-answer pairs but on full analytical trajectories including reasoning\nsteps, code execution, error traces, self-corrections, and final conclusions,\nall captured in a ReAct-style notebook format spanning finance, medicine,\nnumerical analysis, and other quantitative domains. Our training pipeline\ncombines (i) a trajectory-centric synthetic data generator that yielded 144 000\ntagged notebook episodes, (ii) a dual-reward framework blending a lightweight\ntag-based structural signal with a Hierarchical Reward Model (HRM) that scores\nboth single-step soundness and end-to-end coherence, and (iii) a\nmemory-optimized implementation of Group Relative Policy Optimization (GRPO)\nfeaturing KV-cache reuse, sequential generation, and reference-model sharding.\nA cosine curriculum smoothly shifts emphasis from structural fidelity to\nsemantic depth, reducing the format collapse and verbosity that often plague\nRL-aligned LLMs. A central design choice in Datarus is it dual reasoning\ninterface. In agentic mode the model produces ReAct-tagged steps that invoke\nPython tools to execute real code; in reflection mode it outputs compact\nChain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On\ndemanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\"\npattern: it sketches hypotheses, revises them once or twice, and converges\navoiding the circular, token-inflating loops common to contemporary systems.\nAcross standard public benchmarks Datarus surpasses similar size models and\neven reaches the level of larger reasoning models such as QwQ-32B achieving up\nto 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting\n18-49% fewer tokens per solution."
                },
                "authors": [
                    {
                        "name": "Ayoub Ben Chaliah"
                    },
                    {
                        "name": "Hela Dellagi"
                    }
                ],
                "author_detail": {
                    "name": "Hela Dellagi"
                },
                "author": "Hela Dellagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24584v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24584v3",
                "updated": "2025-08-18T16:52:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    16,
                    52,
                    22,
                    0,
                    230,
                    0
                ],
                "published": "2025-05-30T13:32:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical\n  Manufacturing Scale-Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical\n  Manufacturing Scale-Up"
                },
                "summary": "Recent advances in generative AI have accelerated the discovery of novel\nchemicals and materials. However, scaling these discoveries to industrial\nproduction remains a major bottleneck due to the synthesis gap -- the need to\ndevelop entirely new manufacturing processes. This challenge requires detailed\nengineering blueprints: PFDs for equipment layouts and material/energy flows,\nand PIDs for process plant operations. Current AI systems cannot yet reliably\ngenerate these critical engineering schematics, creating a fundamental obstacle\nto manufacturing scale-up of novel discoveries. We present a closed-loop,\nphysics-aware framework for automated generation of industrially viable PFDs\nand PIDs. The framework integrates three key components: (1) domain-specialized\nsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)\na hierarchical knowledge graph containing process flow and instrumentation\ndescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation\n(GRAG), and (3) an open-source chemical process simulator for modeling,\nsimulation, optimization, and analysis of novel chemical processes. The SLMs\nare trained through a multi-stage pipeline on synthetic datasets, with process\nsimulator-in-the-loop validation ensuring feasibility. To enhance computational\nefficiency, the framework implements structural pruning (width and depth)\nguided by importance heuristics to reduce language model size while preserving\naccuracy, followed by advanced inference optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test-Time Inference Scaling. Experimental results demonstrate that our\nframework generates simulator-validated process descriptions with high\nfidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative AI have accelerated the discovery of novel\nchemicals and materials. However, scaling these discoveries to industrial\nproduction remains a major bottleneck due to the synthesis gap -- the need to\ndevelop entirely new manufacturing processes. This challenge requires detailed\nengineering blueprints: PFDs for equipment layouts and material/energy flows,\nand PIDs for process plant operations. Current AI systems cannot yet reliably\ngenerate these critical engineering schematics, creating a fundamental obstacle\nto manufacturing scale-up of novel discoveries. We present a closed-loop,\nphysics-aware framework for automated generation of industrially viable PFDs\nand PIDs. The framework integrates three key components: (1) domain-specialized\nsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)\na hierarchical knowledge graph containing process flow and instrumentation\ndescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation\n(GRAG), and (3) an open-source chemical process simulator for modeling,\nsimulation, optimization, and analysis of novel chemical processes. The SLMs\nare trained through a multi-stage pipeline on synthetic datasets, with process\nsimulator-in-the-loop validation ensuring feasibility. To enhance computational\nefficiency, the framework implements structural pruning (width and depth)\nguided by importance heuristics to reduce language model size while preserving\naccuracy, followed by advanced inference optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test-Time Inference Scaling. Experimental results demonstrate that our\nframework generates simulator-validated process descriptions with high\nfidelity."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24584v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24584v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v2",
                "updated": "2025-08-18T16:06:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    16,
                    6,
                    9,
                    0,
                    230,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12767v1",
                "updated": "2025-08-18T09:41:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    41,
                    28,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T09:41:28Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    41,
                    28,
                    0,
                    230,
                    0
                ],
                "title": "Some optimization possibilities in data plane programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Some optimization possibilities in data plane programming"
                },
                "summary": "Software-defined networking (SDN) technology aims to create a highly flexible\nnetwork by decoupling control plane and the data plane and programming them\nindependently. There has been a lot of research on improving and optimizing the\ncontrol plane, and data plane programming is a relatively new concept, so study\non it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar,\nwell-known scientists on computer networking discussed challenges and problems\nin the field of data plane programming that need to be addressed over the next\n10 years. Based on this seminar issues and papers review, we suggested some\npossible solutions which are for optimizing data plane to improve packet\nprocessing performance and link utilization. The suggestions include (i)\nenriching data plane language with asynchronous external function, (ii)\ncompression based on payload size, (iii) in-network caching for fast packet\nprocessing, and (iv) offloading external functions to an additional thread,\nvirtual machine (VM) or server, etc. In addition, we implemented some of these\nin the P4 data plane language to illustrate the practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software-defined networking (SDN) technology aims to create a highly flexible\nnetwork by decoupling control plane and the data plane and programming them\nindependently. There has been a lot of research on improving and optimizing the\ncontrol plane, and data plane programming is a relatively new concept, so study\non it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar,\nwell-known scientists on computer networking discussed challenges and problems\nin the field of data plane programming that need to be addressed over the next\n10 years. Based on this seminar issues and papers review, we suggested some\npossible solutions which are for optimizing data plane to improve packet\nprocessing performance and link utilization. The suggestions include (i)\nenriching data plane language with asynchronous external function, (ii)\ncompression based on payload size, (iii) in-network caching for fast packet\nprocessing, and (iv) offloading external functions to an additional thread,\nvirtual machine (VM) or server, etc. In addition, we implemented some of these\nin the P4 data plane language to illustrate the practicality."
                },
                "authors": [
                    {
                        "name": "Altangerel Gereltsetseg"
                    },
                    {
                        "name": "Tejfel Mt"
                    }
                ],
                "author_detail": {
                    "name": "Tejfel Mt"
                },
                "author": "Tejfel Mt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12743v1",
                "updated": "2025-08-18T09:06:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    6,
                    49,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T09:06:49Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    6,
                    49,
                    0,
                    230,
                    0
                ],
                "title": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs"
                },
                "summary": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring\nmanagement of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)\nhas been proposed to ease the burden of memory management; however, at a high\ncost in performance. The recent introduction of AMD's MI300A Accelerated\nProcessing Units (APUs)--as deployed in the El Capitan supercomputer--enables\nHPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)\nfor the first time. This work presents the first comprehensive characterization\nof the UPM architecture on MI300A. We first analyze the UPM system properties,\nincluding memory latency, bandwidth, and coherence overhead. We then assess the\nefficiency of the system software in memory allocation, page fault handling,\nTLB management, and Infinity Cache utilization. We propose a set of porting\nstrategies for transforming applications for the UPM architecture and evaluate\nsix applications on the MI300A APU. Our results show that applications on UPM\nusing the unified memory model can match or outperform those in the explicitly\nmanaged model--while reducing memory costs by up to 44%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring\nmanagement of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)\nhas been proposed to ease the burden of memory management; however, at a high\ncost in performance. The recent introduction of AMD's MI300A Accelerated\nProcessing Units (APUs)--as deployed in the El Capitan supercomputer--enables\nHPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)\nfor the first time. This work presents the first comprehensive characterization\nof the UPM architecture on MI300A. We first analyze the UPM system properties,\nincluding memory latency, bandwidth, and coherence overhead. We then assess the\nefficiency of the system software in memory allocation, page fault handling,\nTLB management, and Infinity Cache utilization. We propose a set of porting\nstrategies for transforming applications for the UPM architecture and evaluate\nsix applications on the MI300A APU. Our results show that applications on UPM\nusing the unified memory model can match or outperform those in the explicitly\nmanaged model--while reducing memory costs by up to 44%."
                },
                "authors": [
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Ruimin Shi"
                    },
                    {
                        "name": "Edgar A. Len"
                    },
                    {
                        "name": "Roger Pearce"
                    },
                    {
                        "name": "Maya Gokhale"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "arxiv_comment": "To be published in IISWC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12691v1",
                "updated": "2025-08-18T07:49:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    7,
                    49,
                    33,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T07:49:33Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    7,
                    49,
                    33,
                    0,
                    230,
                    0
                ],
                "title": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration"
                },
                "summary": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Yuanxin Wei"
                    },
                    {
                        "name": "Lansong Diao"
                    },
                    {
                        "name": "Bujiao Chen"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Zhengping Qian"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Jiangsu Du"
                    }
                ],
                "author_detail": {
                    "name": "Jiangsu Du"
                },
                "author": "Jiangsu Du",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12485v1",
                "updated": "2025-08-17T20:01:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    20,
                    1,
                    12,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T20:01:12Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    20,
                    1,
                    12,
                    6,
                    229,
                    0
                ],
                "title": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for\n  NGINX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for\n  NGINX"
                },
                "summary": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs."
                },
                "authors": [
                    {
                        "name": "Aayush Gupta"
                    },
                    {
                        "name": "Arpit Bhayani"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Bhayani"
                },
                "author": "Arpit Bhayani",
                "arxiv_comment": "8 pages, 4 figures (system architecture, eviction path, training\n  pipeline, and DQN algorithm), 2 tables. Code available at\n  https://github.com/ayushgupta4897/DRL-Cache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; C.4; D.4.2; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v1",
                "updated": "2025-08-17T19:07:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12407v1",
                "updated": "2025-08-17T15:48:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    15,
                    48,
                    50,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T15:48:50Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    15,
                    48,
                    50,
                    6,
                    229,
                    0
                ],
                "title": "ZigzagAttention: Efficient Long-Context Inference with Exclusive\n  Retrieval and Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigzagAttention: Efficient Long-Context Inference with Exclusive\n  Retrieval and Streaming Heads"
                },
                "summary": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance."
                },
                "authors": [
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12357v1",
                "updated": "2025-08-17T13:05:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    13,
                    5,
                    52,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T13:05:52Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    13,
                    5,
                    52,
                    6,
                    229,
                    0
                ],
                "title": "Enhancement of the energy storage and electrocaloric effect performances\n  in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancement of the energy storage and electrocaloric effect performances\n  in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method"
                },
                "summary": "Based on the traditional polycrystalline ferroelectric\nBa0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6\nBa0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and\nelectrocaloric effect performances is designed and synthesized by the solgel\nmethod. The structural, dielectric, energy storage and electrocaloric effect\nproperties of the prepared sample were studied. The findings demonstrate that\nthe 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic\nsimultaneously has a significant recoverable energy storage density of 255.4\nmJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a\nhigh ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm.\nMoreover, excellent temperature stability of Wrec (less than 10%) was achieved\nin the investigated sample 0.4BCZT 0.6BSTSn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on the traditional polycrystalline ferroelectric\nBa0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6\nBa0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and\nelectrocaloric effect performances is designed and synthesized by the solgel\nmethod. The structural, dielectric, energy storage and electrocaloric effect\nproperties of the prepared sample were studied. The findings demonstrate that\nthe 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic\nsimultaneously has a significant recoverable energy storage density of 255.4\nmJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a\nhigh ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm.\nMoreover, excellent temperature stability of Wrec (less than 10%) was achieved\nin the investigated sample 0.4BCZT 0.6BSTSn."
                },
                "authors": [
                    {
                        "name": "S. Khardazi"
                    },
                    {
                        "name": "Z. Gargar"
                    },
                    {
                        "name": "A. Lyubchyk"
                    },
                    {
                        "name": "O. Zakir"
                    },
                    {
                        "name": "D. Mezzane"
                    },
                    {
                        "name": "M. Amjoud"
                    },
                    {
                        "name": "A. Alimoussa"
                    },
                    {
                        "name": "Z. Kutnjak"
                    }
                ],
                "author_detail": {
                    "name": "Z. Kutnjak"
                },
                "author": "Z. Kutnjak",
                "arxiv_doi": "10.1016/j.jssc.2025.125547",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jssc.2025.125547",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.12357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v5",
                "updated": "2025-08-16T23:41:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    23,
                    41,
                    48,
                    5,
                    228,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "15 pages, 3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v2",
                "updated": "2025-08-16T18:49:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    18,
                    49,
                    41,
                    5,
                    228,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation"
                },
                "summary": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "8 pages, 6 figures. Accepted to the 2025 IEEE-RAS 24th International\n  Conference on Humanoid Robots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10824v2",
                "updated": "2025-08-16T03:17:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    3,
                    17,
                    35,
                    5,
                    228,
                    0
                ],
                "published": "2025-08-14T16:48:38Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    48,
                    38,
                    3,
                    226,
                    0
                ],
                "title": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Enhanced Model Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Enhanced Model Architectures"
                },
                "summary": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures."
                },
                "authors": [
                    {
                        "name": "Parsa Omidi"
                    },
                    {
                        "name": "Xingshuai Huang"
                    },
                    {
                        "name": "Axel Laborieux"
                    },
                    {
                        "name": "Bahareh Nikpour"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Armaghan Eshaghi"
                    }
                ],
                "author_detail": {
                    "name": "Armaghan Eshaghi"
                },
                "author": "Armaghan Eshaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11495v1",
                "updated": "2025-08-15T14:17:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    17,
                    24,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T14:17:24Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    17,
                    24,
                    4,
                    227,
                    0
                ],
                "title": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value\n  Estimation"
                },
                "summary": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators."
                },
                "authors": [
                    {
                        "name": "Jingnan Xu"
                    },
                    {
                        "name": "Leixia Wang"
                    },
                    {
                        "name": "Xiaofeng Meng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Meng"
                },
                "author": "Xiaofeng Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11291v1",
                "updated": "2025-08-15T07:55:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:55:05Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "title": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks"
                },
                "summary": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks."
                },
                "authors": [
                    {
                        "name": "Rui Bao"
                    },
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Chen"
                },
                "author": "Zhiyong Chen",
                "arxiv_comment": "accepted by IEEE/CIC ICCC workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11260v1",
                "updated": "2025-08-15T06:53:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T06:53:28Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?"
                },
                "summary": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages."
                },
                "authors": [
                    {
                        "name": "Mukund Choudhary"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Gaurja Aeron"
                    },
                    {
                        "name": "Antara Raaghavi Bhattacharya"
                    },
                    {
                        "name": "Dang Khoa Dang Dinh"
                    },
                    {
                        "name": "Ikhlasul Akmal Hanif"
                    },
                    {
                        "name": "Daria Kotova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v2",
                "updated": "2025-08-15T04:27:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    27,
                    30,
                    4,
                    227,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we introduce Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we introduce Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10875v1",
                "updated": "2025-08-14T17:47:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:47:22Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "title": "A Survey on Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Diffusion Language Models"
                },
                "summary": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs."
                },
                "authors": [
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Mingda Chen"
                    },
                    {
                        "name": "Bowei Guo"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13109v2",
                "updated": "2025-08-14T16:12:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    12,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v2",
                "updated": "2025-08-14T15:37:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    37,
                    43,
                    3,
                    226,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache"
                },
                "summary": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10963v1",
                "updated": "2025-08-14T14:11:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    48,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:11:48Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    48,
                    3,
                    226,
                    0
                ],
                "title": "EVCtrl: Efficient Control Adapter for Visual Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVCtrl: Efficient Control Adapter for Visual Generation"
                },
                "summary": "Visual generation includes both image and video generation, training\nprobabilistic models to create coherent, diverse, and semantically faithful\ncontent from scratch. While early research focused on unconditional sampling,\npractitioners now demand controllable generation that allows precise\nspecification of layout, pose, motion, or style. While ControlNet grants\nprecise spatial-temporal control, its auxiliary branch markedly increases\nlatency and introduces redundant computation in both uncontrolled regions and\ndenoising steps, especially for video. To address this problem, we introduce\nEVCtrl, a lightweight, plug-and-play control adapter that slashes overhead\nwithout retraining the model. Specifically, we propose a spatio-temporal dual\ncaching strategy for sparse control information. For spatial redundancy, we\nfirst profile how each layer of DiT-ControlNet responds to fine-grained\ncontrol, then partition the network into global and local functional zones. A\nlocality-aware cache focuses computation on the local zones that truly need the\ncontrol signal, skipping the bulk of redundant computation in global regions.\nFor temporal redundancy, we selectively omit unnecessary denoising steps to\nimprove efficiency. Extensive experiments on CogVideo-Controlnet,\nWan2.1-Controlnet, and Flux demonstrate that our method is effective in image\nand video control generation without the need for training. For example, it\nachieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and\nWan2.1-Controlnet, respectively, with almost no degradation in generation\nquality.Codes are available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual generation includes both image and video generation, training\nprobabilistic models to create coherent, diverse, and semantically faithful\ncontent from scratch. While early research focused on unconditional sampling,\npractitioners now demand controllable generation that allows precise\nspecification of layout, pose, motion, or style. While ControlNet grants\nprecise spatial-temporal control, its auxiliary branch markedly increases\nlatency and introduces redundant computation in both uncontrolled regions and\ndenoising steps, especially for video. To address this problem, we introduce\nEVCtrl, a lightweight, plug-and-play control adapter that slashes overhead\nwithout retraining the model. Specifically, we propose a spatio-temporal dual\ncaching strategy for sparse control information. For spatial redundancy, we\nfirst profile how each layer of DiT-ControlNet responds to fine-grained\ncontrol, then partition the network into global and local functional zones. A\nlocality-aware cache focuses computation on the local zones that truly need the\ncontrol signal, skipping the bulk of redundant computation in global regions.\nFor temporal redundancy, we selectively omit unnecessary denoising steps to\nimprove efficiency. Extensive experiments on CogVideo-Controlnet,\nWan2.1-Controlnet, and Flux demonstrate that our method is effective in image\nand video control generation without the need for training. For example, it\nachieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and\nWan2.1-Controlnet, respectively, with almost no degradation in generation\nquality.Codes are available in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Zixiang Yang"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Yinhan Zhang"
                    },
                    {
                        "name": "Shanhui Mo"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15806v1",
                "updated": "2025-08-14T14:08:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    8,
                    58,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:08:58Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    8,
                    58,
                    3,
                    226,
                    0
                ],
                "title": "SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need\n  for Robust KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need\n  for Robust KV Cache Compression"
                },
                "summary": "The increasing input sequence length in Large Language Models (LLMs) puts\nsignificant pressure on key-value (KV) cache storage, making efficient\ninference challenging. Explicitly distinguishing attention behavior into our\nself-defined surface memorization and logic construction reveals essential\nroles in long-context reasoning. We observe that an individual attention head\ncan display various behaviors, with nearly 98.5% effectively ignoring\ncompletely irrelevant information. The remaining 1.5% behaves as logic\nconstruction, and 0.5% behaves as surface memorization. Based on layer- and\nhead-wise integration, we propose a novel two-stage SurfaceLogicKV method to\nutilize these attention behaviors for KV Cache compression. As a result, it\nachieves improved compressing robustness while maintaining competitive\nperformance across various tasks and long sequences compared to baselines or\neven FullKV in some specific situations",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing input sequence length in Large Language Models (LLMs) puts\nsignificant pressure on key-value (KV) cache storage, making efficient\ninference challenging. Explicitly distinguishing attention behavior into our\nself-defined surface memorization and logic construction reveals essential\nroles in long-context reasoning. We observe that an individual attention head\ncan display various behaviors, with nearly 98.5% effectively ignoring\ncompletely irrelevant information. The remaining 1.5% behaves as logic\nconstruction, and 0.5% behaves as surface memorization. Based on layer- and\nhead-wise integration, we propose a novel two-stage SurfaceLogicKV method to\nutilize these attention behaviors for KV Cache compression. As a result, it\nachieves improved compressing robustness while maintaining competitive\nperformance across various tasks and long sequences compared to baselines or\neven FullKV in some specific situations"
                },
                "authors": [
                    {
                        "name": "Mengjie Li"
                    },
                    {
                        "name": "William J. Song"
                    }
                ],
                "author_detail": {
                    "name": "William J. Song"
                },
                "author": "William J. Song",
                "arxiv_comment": "18 pages, 9 tables, 10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10613v1",
                "updated": "2025-08-14T13:10:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    10,
                    43,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T13:10:43Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    10,
                    43,
                    3,
                    226,
                    0
                ],
                "title": "Routing and Wavelength Assignment with Minimal Attack Radius for QKD\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Routing and Wavelength Assignment with Minimal Attack Radius for QKD\n  Networks"
                },
                "summary": "Quantum Key Distribution (QKD) can distribute keys with guaranteed security\nbut remains susceptible to key exchange interruption due to physical-layer\nthreats, such as high-power jamming attacks. To address this challenge, we\nfirst introduce a novel metric, namely Maximum Number of Affected Requests\n(maxNAR), to quantify the worst-case impact of a single physical-layer attack,\nand then we investigate a new problem of Routing and Wavelength Assignment with\nMinimal Attack Radius (RWA-MAR). We formulate the problem using an Integer\nLinear Programming (ILP) model and propose a scalable heuristic to efficiently\nminimize maxNAR. Our approach incorporates key caching through Quantum Key\nPools (QKPs) to enhance resilience and optimize resource utilization. Moreover,\nwe model the impact of different QKD network architectures, employing Optical\nBypass (OB) for optical switching of quantum channels and Trusted Relay (TR)\nfor secure key forwarding. Moreover, a tunable parameter is designed in the\nheuristic to guide the preference for OB or TR, offering enhanced adaptability\nand dynamic control in diverse network scenarios. Simulation results confirm\nthat our method significantly outperforms the baseline in terms of security and\nscalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Key Distribution (QKD) can distribute keys with guaranteed security\nbut remains susceptible to key exchange interruption due to physical-layer\nthreats, such as high-power jamming attacks. To address this challenge, we\nfirst introduce a novel metric, namely Maximum Number of Affected Requests\n(maxNAR), to quantify the worst-case impact of a single physical-layer attack,\nand then we investigate a new problem of Routing and Wavelength Assignment with\nMinimal Attack Radius (RWA-MAR). We formulate the problem using an Integer\nLinear Programming (ILP) model and propose a scalable heuristic to efficiently\nminimize maxNAR. Our approach incorporates key caching through Quantum Key\nPools (QKPs) to enhance resilience and optimize resource utilization. Moreover,\nwe model the impact of different QKD network architectures, employing Optical\nBypass (OB) for optical switching of quantum channels and Trusted Relay (TR)\nfor secure key forwarding. Moreover, a tunable parameter is designed in the\nheuristic to guide the preference for OB or TR, offering enhanced adaptability\nand dynamic control in diverse network scenarios. Simulation results confirm\nthat our method significantly outperforms the baseline in terms of security and\nscalability."
                },
                "authors": [
                    {
                        "name": "Mengyao Li"
                    },
                    {
                        "name": "Qiaolun Zhang"
                    },
                    {
                        "name": "Zongshuai Yang"
                    },
                    {
                        "name": "Stefano Bregni"
                    },
                    {
                        "name": "Alberto Gatto"
                    },
                    {
                        "name": "Raouf Boutaba"
                    },
                    {
                        "name": "Massimo Tornatore"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Tornatore"
                },
                "author": "Massimo Tornatore",
                "arxiv_comment": "6 pages, this paper has been successfully accepted by GLOBECOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08601v3",
                "updated": "2025-08-14T10:26:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    26,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T03:34:21Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    34,
                    21,
                    1,
                    224,
                    0
                ],
                "title": "Yan: Foundational Interactive Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yan: Foundational Interactive Video Generation"
                },
                "summary": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/."
                },
                "authors": [
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Fangyun Zhou"
                    },
                    {
                        "name": "Jiacheng Lv"
                    },
                    {
                        "name": "Jianqi Ma"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Junyan Lv"
                    },
                    {
                        "name": "Junyou Li"
                    },
                    {
                        "name": "Minwen Deng"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Qiang Fu"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Wenkai Lv"
                    },
                    {
                        "name": "Yangbin Yu"
                    },
                    {
                        "name": "Yewen Wang"
                    },
                    {
                        "name": "Yonghang Guan"
                    },
                    {
                        "name": "Zhihao Hu"
                    },
                    {
                        "name": "Zhongbin Fang"
                    },
                    {
                        "name": "Zhongqian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhongqian Sun"
                },
                "author": "Zhongqian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08895v2",
                "updated": "2025-08-14T09:04:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    4,
                    56,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T12:35:55Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    35,
                    55,
                    1,
                    224,
                    0
                ],
                "title": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs"
                },
                "summary": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines."
                },
                "authors": [
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Zhifeng Shen"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Haoqian Wu"
                    },
                    {
                        "name": "Wei Wen"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10424v1",
                "updated": "2025-08-14T07:54:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    54,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:54:44Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    54,
                    44,
                    3,
                    226,
                    0
                ],
                "title": "NanoControl: A Lightweight Framework for Precise and Efficient Control\n  in Diffusion Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoControl: A Lightweight Framework for Precise and Efficient Control\n  in Diffusion Transformer"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in\ntext-to-image synthesis. However, in the domain of controllable text-to-image\ngeneration using DiTs, most existing methods still rely on the ControlNet\nparadigm originally designed for UNet-based diffusion models. This paradigm\nintroduces significant parameter overhead and increased computational costs. To\naddress these challenges, we propose the Nano Control Diffusion Transformer\n(NanoControl), which employs Flux as the backbone network. Our model achieves\nstate-of-the-art controllable text-to-image generation performance while\nincurring only a 0.024\\% increase in parameter count and a 0.029\\% increase in\nGFLOPs, thus enabling highly efficient controllable generation. Specifically,\nrather than duplicating the DiT backbone for control, we design a LoRA-style\n(low-rank adaptation) control module that directly learns control signals from\nraw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation\nmechanism that integrates condition-specific key-value information into the\nbackbone in a simple yet highly effective manner, facilitating deep fusion of\nconditional features. Extensive benchmark experiments demonstrate that\nNanoControl significantly reduces computational overhead compared to\nconventional control approaches, while maintaining superior generation quality\nand achieving improved controllability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in\ntext-to-image synthesis. However, in the domain of controllable text-to-image\ngeneration using DiTs, most existing methods still rely on the ControlNet\nparadigm originally designed for UNet-based diffusion models. This paradigm\nintroduces significant parameter overhead and increased computational costs. To\naddress these challenges, we propose the Nano Control Diffusion Transformer\n(NanoControl), which employs Flux as the backbone network. Our model achieves\nstate-of-the-art controllable text-to-image generation performance while\nincurring only a 0.024\\% increase in parameter count and a 0.029\\% increase in\nGFLOPs, thus enabling highly efficient controllable generation. Specifically,\nrather than duplicating the DiT backbone for control, we design a LoRA-style\n(low-rank adaptation) control module that directly learns control signals from\nraw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation\nmechanism that integrates condition-specific key-value information into the\nbackbone in a simple yet highly effective manner, facilitating deep fusion of\nconditional features. Extensive benchmark experiments demonstrate that\nNanoControl significantly reduces computational overhead compared to\nconventional control approaches, while maintaining superior generation quality\nand achieving improved controllability."
                },
                "authors": [
                    {
                        "name": "Shanyuan Liu"
                    },
                    {
                        "name": "Jian Zhu"
                    },
                    {
                        "name": "Junda Lu"
                    },
                    {
                        "name": "Yue Gong"
                    },
                    {
                        "name": "Liuzhuozheng Li"
                    },
                    {
                        "name": "Bo Cheng"
                    },
                    {
                        "name": "Yuhang Ma"
                    },
                    {
                        "name": "Liebucha Wu"
                    },
                    {
                        "name": "Xiaoyu Wu"
                    },
                    {
                        "name": "Dawei Leng"
                    },
                    {
                        "name": "Yuhui Yin"
                    }
                ],
                "author_detail": {
                    "name": "Yuhui Yin"
                },
                "author": "Yuhui Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10395v1",
                "updated": "2025-08-14T06:52:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    52,
                    38,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T06:52:38Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    52,
                    38,
                    3,
                    226,
                    0
                ],
                "title": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization"
                },
                "summary": "Although LLM inference has emerged as a critical workload for many downstream\napplications, efficiently inferring LLMs is challenging due to the substantial\nmemory footprint and bandwidth requirements. In parallel, compute capabilities\nhave steadily outpaced both memory capacity and bandwidth over the last few\ndecades, a trend that remains evident in modern GPU hardware and exacerbates\nthe challenge of LLM inference. As such, new algorithms are emerging that trade\nincreased computation for reduced memory operations. To that end, we present\nXQuant, which takes advantage of this trend, enabling an order-of-magnitude\nreduction in memory consumption through low-bit quantization with substantial\naccuracy benefits relative to state-of-the-art KV cache quantization methods.\nWe accomplish this by quantizing and caching the layer input activations X,\ninstead of using standard KV caching, and then rematerializing the Keys and\nValues on-the-fly during inference. This results in an immediate 2$\\times$\nmemory savings compared to KV caching. By applying XQuant, we achieve up to\n$\\sim 7.7\\times$ memory savings with $<0.1$ perplexity degradation compared to\nthe FP16 baseline. Furthermore, our approach leverages the fact that X values\nare similar across layers. Building on this observation, we introduce\nXQuant-CL, which exploits the cross-layer similarity in the X embeddings for\nextreme compression. Across different models, XQuant-CL attains up to\n10$\\times$ memory savings relative to the FP16 baseline with only 0.01\nperplexity degradation, and 12.5$\\times$ memory savings with only $0.1$\nperplexity degradation. XQuant exploits the rapidly increasing compute\ncapabilities of hardware platforms to eliminate the memory bottleneck, while\nsurpassing state-of-the-art KV cache quantization methods and achieving\nnear-FP16 accuracy across a wide range of models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although LLM inference has emerged as a critical workload for many downstream\napplications, efficiently inferring LLMs is challenging due to the substantial\nmemory footprint and bandwidth requirements. In parallel, compute capabilities\nhave steadily outpaced both memory capacity and bandwidth over the last few\ndecades, a trend that remains evident in modern GPU hardware and exacerbates\nthe challenge of LLM inference. As such, new algorithms are emerging that trade\nincreased computation for reduced memory operations. To that end, we present\nXQuant, which takes advantage of this trend, enabling an order-of-magnitude\nreduction in memory consumption through low-bit quantization with substantial\naccuracy benefits relative to state-of-the-art KV cache quantization methods.\nWe accomplish this by quantizing and caching the layer input activations X,\ninstead of using standard KV caching, and then rematerializing the Keys and\nValues on-the-fly during inference. This results in an immediate 2$\\times$\nmemory savings compared to KV caching. By applying XQuant, we achieve up to\n$\\sim 7.7\\times$ memory savings with $<0.1$ perplexity degradation compared to\nthe FP16 baseline. Furthermore, our approach leverages the fact that X values\nare similar across layers. Building on this observation, we introduce\nXQuant-CL, which exploits the cross-layer similarity in the X embeddings for\nextreme compression. Across different models, XQuant-CL attains up to\n10$\\times$ memory savings relative to the FP16 baseline with only 0.01\nperplexity degradation, and 12.5$\\times$ memory savings with only $0.1$\nperplexity degradation. XQuant exploits the rapidly increasing compute\ncapabilities of hardware platforms to eliminate the memory bottleneck, while\nsurpassing state-of-the-art KV cache quantization methods and achieving\nnear-FP16 accuracy across a wide range of models."
                },
                "authors": [
                    {
                        "name": "Aditya Tomar"
                    },
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Rishabh Tiwari"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v3",
                "updated": "2025-08-13T17:55:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    55,
                    58,
                    2,
                    225,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v1",
                "updated": "2025-08-13T13:54:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining."
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v1",
                "updated": "2025-08-13T07:40:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v4",
                "updated": "2025-08-13T06:13:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    6,
                    13,
                    36,
                    2,
                    225,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v2",
                "updated": "2025-08-13T04:24:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    4,
                    24,
                    56,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under real-world distribution shifts. Code:\nhttps://github.com/Evelyn1ywliang/ReTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under real-world distribution shifts. Code:\nhttps://github.com/Evelyn1ywliang/ReTA."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v4",
                "updated": "2025-08-13T04:03:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    4,
                    3,
                    10,
                    2,
                    225,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09442v1",
                "updated": "2025-08-13T02:48:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T02:48:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference"
                },
                "summary": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment."
                },
                "authors": [
                    {
                        "name": "Zhifan Luo"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Su Zhang"
                    },
                    {
                        "name": "Lijing Zhou"
                    },
                    {
                        "name": "Yuke Hu"
                    },
                    {
                        "name": "Chenxu Zhao"
                    },
                    {
                        "name": "Zhihao Liu"
                    },
                    {
                        "name": "Zhan Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zhan Qin"
                },
                "author": "Zhan Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09419v1",
                "updated": "2025-08-13T01:39:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T01:39:09Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "title": "Design and Simulation of 6T SRAM Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Simulation of 6T SRAM Array"
                },
                "summary": "Conventional 6T SRAM is used in microprocessors in the cache memory design.\nThe basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit.\nThe design and analysis of key SRAM components, sense amplifiers, decoders,\nwrite drivers and precharge circuits are also provided. The pulse voltage\nwaveforms generated for read and write operations as well as Q and Qbar nodes\nare simulated in LTSpice. Parasitic capacitances are extracted and their impact\non the waveforms analyzed. Static noise margin, propagation delays, and power\ndissipation are calculated. Comparison of SRAM read and write operational\nperformance using CMOS transistors is made with edge-triggered D flip flops. If\ncertain size area and ratio constraints are satisfied, the 6T cell with CMOS\ntransistors will possess stability, speed, and power efficiency. Both\ntheoretical and simulated results are given.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional 6T SRAM is used in microprocessors in the cache memory design.\nThe basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit.\nThe design and analysis of key SRAM components, sense amplifiers, decoders,\nwrite drivers and precharge circuits are also provided. The pulse voltage\nwaveforms generated for read and write operations as well as Q and Qbar nodes\nare simulated in LTSpice. Parasitic capacitances are extracted and their impact\non the waveforms analyzed. Static noise margin, propagation delays, and power\ndissipation are calculated. Comparison of SRAM read and write operational\nperformance using CMOS transistors is made with edge-triggered D flip flops. If\ncertain size area and ratio constraints are satisfied, the 6T cell with CMOS\ntransistors will possess stability, speed, and power efficiency. Both\ntheoretical and simulated results are given."
                },
                "authors": [
                    {
                        "name": "Justin London"
                    }
                ],
                "author_detail": {
                    "name": "Justin London"
                },
                "author": "Justin London",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08744v2",
                "updated": "2025-08-13T01:39:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    3,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T08:39:32Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    39,
                    32,
                    1,
                    224,
                    0
                ],
                "title": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor\n  Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces\nhas a wide range of real-world applications. Numerous methods have been\nproposed to handle ANNS efficiently, while graph-based indexes have gained\nprominence due to their high accuracy and efficiency. However, the indexing\noverhead of graph-based indexes remains substantial. With exponential growth in\ndata volume and increasing demands for dynamic index adjustments, this overhead\ncontinues to escalate, posing a critical challenge. In this paper, we introduce\nTagore, a fast library accelerated by GPUs for graph indexing, which has\npowerful capabilities of constructing refinement-based graph indexes such as\nNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for\nefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up\nthe similarity comparison by a two-phase descent procedure and enables highly\nparallelized neighbor updates. Next, aiming to support various k-NN graph\npruning strategies, we formulate a universal computing procedure termed CFS and\ndevise two generalized GPU kernels for parallel processing complex dependencies\nin neighbor relationships. For large-scale datasets exceeding GPU memory\ncapacity, we propose an asynchronous GPU-CPU-disk indexing framework with a\ncluster-aware caching mechanism to minimize the I/O pressure on the disk.\nExtensive experiments on 7 real-world datasets exhibit that Tagore achieves\n1.32x-112.79x speedup while maintaining the index quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces\nhas a wide range of real-world applications. Numerous methods have been\nproposed to handle ANNS efficiently, while graph-based indexes have gained\nprominence due to their high accuracy and efficiency. However, the indexing\noverhead of graph-based indexes remains substantial. With exponential growth in\ndata volume and increasing demands for dynamic index adjustments, this overhead\ncontinues to escalate, posing a critical challenge. In this paper, we introduce\nTagore, a fast library accelerated by GPUs for graph indexing, which has\npowerful capabilities of constructing refinement-based graph indexes such as\nNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for\nefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up\nthe similarity comparison by a two-phase descent procedure and enables highly\nparallelized neighbor updates. Next, aiming to support various k-NN graph\npruning strategies, we formulate a universal computing procedure termed CFS and\ndevise two generalized GPU kernels for parallel processing complex dependencies\nin neighbor relationships. For large-scale datasets exceeding GPU memory\ncapacity, we propose an asynchronous GPU-CPU-disk indexing framework with a\ncluster-aware caching mechanism to minimize the I/O pressure on the disk.\nExtensive experiments on 7 real-world datasets exhibit that Tagore achieves\n1.32x-112.79x speedup while maintaining the index quality."
                },
                "authors": [
                    {
                        "name": "Zhonggen Li"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Bocheng Yu"
                    },
                    {
                        "name": "Baihua Zheng"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09262v1",
                "updated": "2025-08-12T18:05:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    18,
                    5,
                    33,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T18:05:33Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    18,
                    5,
                    33,
                    1,
                    224,
                    0
                ],
                "title": "Harnessing Input-Adaptive Inference for Efficient VLN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Input-Adaptive Inference for Efficient VLN"
                },
                "summary": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation."
                },
                "authors": [
                    {
                        "name": "Dongwoo Kang"
                    },
                    {
                        "name": "Akhil Perincherry"
                    },
                    {
                        "name": "Zachary Coalson"
                    },
                    {
                        "name": "Aiden Gabriel"
                    },
                    {
                        "name": "Stefan Lee"
                    },
                    {
                        "name": "Sanghyun Hong"
                    }
                ],
                "author_detail": {
                    "name": "Sanghyun Hong"
                },
                "author": "Sanghyun Hong",
                "arxiv_comment": "Accepted to ICCV 2025 [Poster]",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v1",
                "updated": "2025-08-12T16:47:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Yi Fei"
                    },
                    {
                        "name": "Zeng Weidi"
                    }
                ],
                "author_detail": {
                    "name": "Zeng Weidi"
                },
                "author": "Zeng Weidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09001v1",
                "updated": "2025-08-12T15:11:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:11:47Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "title": "Retrospective Sparse Attention for Efficient Long-Context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrospective Sparse Attention for Efficient Long-Context Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%."
                },
                "authors": [
                    {
                        "name": "Seonghwan Choi"
                    },
                    {
                        "name": "Beomseok Kang"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08978v1",
                "updated": "2025-08-12T14:40:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    40,
                    36,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:40:36Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    40,
                    36,
                    1,
                    224,
                    0
                ],
                "title": "TaoCache: Structure-Maintained Video Generation Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaoCache: Structure-Maintained Video Generation Acceleration"
                },
                "summary": "Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups."
                },
                "authors": [
                    {
                        "name": "Zhentao Fan"
                    },
                    {
                        "name": "Zongzuo Wang"
                    },
                    {
                        "name": "Weiwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weiwei Zhang"
                },
                "author": "Weiwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11488v2",
                "updated": "2025-08-12T10:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    43,
                    55,
                    1,
                    224,
                    0
                ],
                "published": "2023-11-30T16:02:04Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    16,
                    2,
                    4,
                    3,
                    334,
                    0
                ],
                "title": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows"
                },
                "summary": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes."
                },
                "authors": [
                    {
                        "name": "Thiago Garrett"
                    },
                    {
                        "name": "Weijia Song"
                    },
                    {
                        "name": "Roman Vitenberg"
                    },
                    {
                        "name": "Ken Birman"
                    }
                ],
                "author_detail": {
                    "name": "Ken Birman"
                },
                "author": "Ken Birman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v3",
                "updated": "2025-08-12T05:51:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    5,
                    51,
                    37,
                    1,
                    224,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08600v1",
                "updated": "2025-08-12T03:33:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    33,
                    15,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T03:33:15Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    33,
                    15,
                    1,
                    224,
                    0
                ],
                "title": "Rigorous quantum calculations for atom-molecule chemical reactions in\n  electric fields: from single to multiple partial wave regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous quantum calculations for atom-molecule chemical reactions in\n  electric fields: from single to multiple partial wave regimes"
                },
                "summary": "We present an efficient method for rigorous quantum calculations of cross\nsections for atom-molecule reactive scattering in the presence of a dc electric\nfield. The wavefunction of the reaction complex is expanded in an overcomplete\nset of arrangement-dependent Fock-Delves hyperspherical basis functions and the\ninteractions of the reactants and products with electric fields are accounted\nfor in the total angular momentum representation. A significant computational\nchallenge affecting our previously developed approach [Phys. Rev. Lett.\n$\\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame\ntransformation between the hyperspherical and Jacobi coordinates in the\npresence of an external field. Using accurate {\\it ab initio} potential energy\nsurfaces, we calculate total and state-resolved cross sections for the chemical\nreactions LiF$(v=1,j=0)$ + H $\\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$\n$\\to$ HF + D, DF + H as functions of collision energy and electric field\nstrength. The field dependence of the cross sections for the LiF + H chemical\nreaction exhibits resonance structure mediated by tunneling-driven interactions\nbetween reactants and products. No significant field effects are found for the\nF + HD $\\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for\nstate-resolved transitions and with field magnitudes reaching 200 kV/cm. Our\ncalculations illustrate the essential role of basis set convergence for the\nproper interpretation of external field effects on chemical reaction dynamics.\nWhile reduced-basis calculations for the F + HD reaction indicate significant\neffects of electric fields on product state distributions, these effects vanish\nwhen the number of total angular momentum basis states is increased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient method for rigorous quantum calculations of cross\nsections for atom-molecule reactive scattering in the presence of a dc electric\nfield. The wavefunction of the reaction complex is expanded in an overcomplete\nset of arrangement-dependent Fock-Delves hyperspherical basis functions and the\ninteractions of the reactants and products with electric fields are accounted\nfor in the total angular momentum representation. A significant computational\nchallenge affecting our previously developed approach [Phys. Rev. Lett.\n$\\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame\ntransformation between the hyperspherical and Jacobi coordinates in the\npresence of an external field. Using accurate {\\it ab initio} potential energy\nsurfaces, we calculate total and state-resolved cross sections for the chemical\nreactions LiF$(v=1,j=0)$ + H $\\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$\n$\\to$ HF + D, DF + H as functions of collision energy and electric field\nstrength. The field dependence of the cross sections for the LiF + H chemical\nreaction exhibits resonance structure mediated by tunneling-driven interactions\nbetween reactants and products. No significant field effects are found for the\nF + HD $\\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for\nstate-resolved transitions and with field magnitudes reaching 200 kV/cm. Our\ncalculations illustrate the essential role of basis set convergence for the\nproper interpretation of external field effects on chemical reaction dynamics.\nWhile reduced-basis calculations for the F + HD reaction indicate significant\neffects of electric fields on product state distributions, these effects vanish\nwhen the number of total angular momentum basis states is increased."
                },
                "authors": [
                    {
                        "name": "Timur V. Tscherbul"
                    },
                    {
                        "name": "Roman V. Krems"
                    }
                ],
                "author_detail": {
                    "name": "Roman V. Krems"
                },
                "author": "Roman V. Krems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07675v2",
                "updated": "2025-08-12T02:51:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    2,
                    51,
                    12,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T06:53:27Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    6,
                    53,
                    27,
                    0,
                    223,
                    0
                ],
                "title": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to\n  Online Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to\n  Online Adaptation"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Baran Atalar"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    }
                ],
                "author_detail": {
                    "name": "Carlee Joe-Wong"
                },
                "author": "Carlee Joe-Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08134v2",
                "updated": "2025-08-12T02:27:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    2,
                    27,
                    5,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T16:10:00Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    10,
                    0,
                    0,
                    223,
                    0
                ],
                "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control"
                },
                "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement."
                },
                "authors": [
                    {
                        "name": "Zeqian Long"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Kunyu Feng"
                    },
                    {
                        "name": "Xinhua Zhang"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Yue Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ma"
                },
                "author": "Yue Ma",
                "arxiv_comment": "Project webpage is available at https://follow-your-shape.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08531v1",
                "updated": "2025-08-12T00:06:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    0,
                    6,
                    34,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T00:06:34Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    0,
                    6,
                    34,
                    1,
                    224,
                    0
                ],
                "title": "Profiling Large Language Model Inference on Apple Silicon: A\n  Quantization Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Profiling Large Language Model Inference on Apple Silicon: A\n  Quantization Perspective"
                },
                "summary": "A systematic understanding of Apple Silicon is lacking in the current\nlandscape of hardware efficiency; research focus is largely centered on\naccelerating GPUs for large-scale training or inference on CUDA devices. This\npaper investigates Apple Silicon's unique memory architecture that offers a\nunified memory integrating CPU and GPU memory and its implications for\non-device LLM inference.\n  We decipher myths about whether Apple Silicon is efficient for on-device\ninference compared to competitors such as NVIDIA GPUs by directly conducting\nlatency and throughput comparison benchmarks. We explain the performance gap\nbetween them through profiling low level hardware metrics - ALU utilization,\nmemory bandwidth, buffer usage, cache residency etc. at runtime. We draw\nseveral insights regarding performance bottlenecks such as dequantization\noverhead, compute throughput and memory bandwidth. We debunk existing false\nclaims regarding large language model inference such as compressing models to\nlower bit precision is a defacto promise for faster inference across all\nhardware platforms. We find that the large unified memory enables Apple Silicon\nto be both cost effective and efficient against NVIDIA GPUs for ultra large\nlanguage models.\n  Our large scale evaluation on 5 hardware testbeds incorporating three Apple\nM-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX\nA6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from\n8B to 405B parameters and 14 quantization schemes gives an understanding of how\nApple Silicon fits within the paradigm of on-device LLM inference. Our analysis\nreveals multiple resource interdependencies and unexpected findings, while also\nquantifying established insights. To the best of our knowledge, this study\nmakes the first attempt to present a thorough characterization and analysis of\nApple Silicon for on-device inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A systematic understanding of Apple Silicon is lacking in the current\nlandscape of hardware efficiency; research focus is largely centered on\naccelerating GPUs for large-scale training or inference on CUDA devices. This\npaper investigates Apple Silicon's unique memory architecture that offers a\nunified memory integrating CPU and GPU memory and its implications for\non-device LLM inference.\n  We decipher myths about whether Apple Silicon is efficient for on-device\ninference compared to competitors such as NVIDIA GPUs by directly conducting\nlatency and throughput comparison benchmarks. We explain the performance gap\nbetween them through profiling low level hardware metrics - ALU utilization,\nmemory bandwidth, buffer usage, cache residency etc. at runtime. We draw\nseveral insights regarding performance bottlenecks such as dequantization\noverhead, compute throughput and memory bandwidth. We debunk existing false\nclaims regarding large language model inference such as compressing models to\nlower bit precision is a defacto promise for faster inference across all\nhardware platforms. We find that the large unified memory enables Apple Silicon\nto be both cost effective and efficient against NVIDIA GPUs for ultra large\nlanguage models.\n  Our large scale evaluation on 5 hardware testbeds incorporating three Apple\nM-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX\nA6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from\n8B to 405B parameters and 14 quantization schemes gives an understanding of how\nApple Silicon fits within the paradigm of on-device LLM inference. Our analysis\nreveals multiple resource interdependencies and unexpected findings, while also\nquantifying established insights. To the best of our knowledge, this study\nmakes the first attempt to present a thorough characterization and analysis of\nApple Silicon for on-device inference."
                },
                "authors": [
                    {
                        "name": "Afsara Benazir"
                    },
                    {
                        "name": "Felix Xiaozhu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Felix Xiaozhu Lin"
                },
                "author": "Felix Xiaozhu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08457v1",
                "updated": "2025-08-11T20:30:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    20,
                    30,
                    31,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T20:30:31Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    20,
                    30,
                    31,
                    0,
                    223,
                    0
                ],
                "title": "Architecting Long-Context LLM Acceleration with Packing-Prefetch\n  Scheduler and Ultra-Large Capacity On-Chip Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architecting Long-Context LLM Acceleration with Packing-Prefetch\n  Scheduler and Ultra-Large Capacity On-Chip Memories"
                },
                "summary": "Long-context Large Language Model (LLM) inference faces increasing compute\nbottlenecks as attention calculations scale with context length, primarily due\nto the growing KV-cache transfer overhead that saturates High Bandwidth Memory\n(HBM). While prefetching techniques mitigate cache misses by fetching KV data\nin advance, their spatial and temporal benefits present new opportunities to\nexploit. This work proposes a packing-prefetch scheduling architecture with\nmonolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with\nultra-large on-chip capacity to accelerate long-context LLM inference. Our\noptimizations demonstrate 8.06x decode speedup and 1.83x overall latency\nreduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL\nmemories over the serial execution. Evaluations of multi-request workloads on\nTPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM\nbandwidth reduction compared to packing-only methods on Llama3.1-8B and\nLlama3.1-70B models. With the co-design of packing, prefetching, and BEOL\nmemories, our approach alleviates HBM constraints and enables efficient\nlong-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Large Language Model (LLM) inference faces increasing compute\nbottlenecks as attention calculations scale with context length, primarily due\nto the growing KV-cache transfer overhead that saturates High Bandwidth Memory\n(HBM). While prefetching techniques mitigate cache misses by fetching KV data\nin advance, their spatial and temporal benefits present new opportunities to\nexploit. This work proposes a packing-prefetch scheduling architecture with\nmonolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with\nultra-large on-chip capacity to accelerate long-context LLM inference. Our\noptimizations demonstrate 8.06x decode speedup and 1.83x overall latency\nreduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL\nmemories over the serial execution. Evaluations of multi-request workloads on\nTPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM\nbandwidth reduction compared to packing-only methods on Llama3.1-8B and\nLlama3.1-70B models. With the co-design of packing, prefetching, and BEOL\nmemories, our approach alleviates HBM constraints and enables efficient\nlong-context LLM inference."
                },
                "authors": [
                    {
                        "name": "Ming-Yen Lee"
                    },
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Hanchen Yang"
                    },
                    {
                        "name": "Muhammed Ahosan Ul Karim"
                    },
                    {
                        "name": "Harsono Simka"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "7 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.3; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08438v1",
                "updated": "2025-08-11T19:55:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    19,
                    55,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T19:55:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    19,
                    55,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM\n  Inference"
                },
                "summary": "Global KV-cache sharing has emerged as a key optimization for accelerating\nlarge language model (LLM) inference. However, it exposes a new class of timing\nside-channel attacks, enabling adversaries to infer sensitive user inputs via\nshared cache entries. Existing defenses, such as per-user isolation, eliminate\nleakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),\nmaking them impractical for high-throughput deployment. To address this gap, we\nintroduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware\nKV-cache management framework that selectively shares non-sensitive entries\nwhile confining sensitive content to private caches. SafeKV comprises three\ncomponents: (i) a hybrid, multi-tier detection pipeline that integrates\nrule-based pattern matching, a general-purpose privacy detector, and\ncontext-aware validation; (ii) a unified radix-tree index that manages public\nand private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and\n(iii) entropy-based access monitoring to detect and mitigate residual\ninformation leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of\ntiming-based side-channel attacks. Compared to per-user isolation method,\nSafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across\ndiverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from\n50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with\nhigh cache reuse efficiency, SafeKV reclaims the performance advantages of\nglobal sharing while providing robust runtime privacy guarantees for LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global KV-cache sharing has emerged as a key optimization for accelerating\nlarge language model (LLM) inference. However, it exposes a new class of timing\nside-channel attacks, enabling adversaries to infer sensitive user inputs via\nshared cache entries. Existing defenses, such as per-user isolation, eliminate\nleakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),\nmaking them impractical for high-throughput deployment. To address this gap, we\nintroduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware\nKV-cache management framework that selectively shares non-sensitive entries\nwhile confining sensitive content to private caches. SafeKV comprises three\ncomponents: (i) a hybrid, multi-tier detection pipeline that integrates\nrule-based pattern matching, a general-purpose privacy detector, and\ncontext-aware validation; (ii) a unified radix-tree index that manages public\nand private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and\n(iii) entropy-based access monitoring to detect and mitigate residual\ninformation leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of\ntiming-based side-channel attacks. Compared to per-user isolation method,\nSafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across\ndiverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from\n50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with\nhigh cache reuse efficiency, SafeKV reclaims the performance advantages of\nglobal sharing while providing robust runtime privacy guarantees for LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Kexin Chu"
                    },
                    {
                        "name": "Zecheng Lin"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Zixu Shen"
                    },
                    {
                        "name": "Jianchang Su"
                    },
                    {
                        "name": "Cheng Chu"
                    },
                    {
                        "name": "Yiwei Yang"
                    },
                    {
                        "name": "Wenhui Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "17 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08081v1",
                "updated": "2025-08-11T15:28:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    28,
                    28,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:28:28Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    28,
                    28,
                    0,
                    223,
                    0
                ],
                "title": "Numerical computation of linearized KV and the Deligne-Drinfeld and\n  Broadhurst-Kreimer conjectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical computation of linearized KV and the Deligne-Drinfeld and\n  Broadhurst-Kreimer conjectures"
                },
                "summary": "We compute numerically the dimensions of the graded quotients of the\nlinearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a\nconjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in\na chain of inclusions of Lie algebras, including also the linearized double\nshuffle Lie algebra and the (depth associated graded of the)\nGrothendieck-Teichm\\\"uller Lie algebra. Hence our computations also allow us to\ncheck the validity of the Deligne-Drinfeld conjecture on the structure of the\nGrothendieck-Teichm\\\"uller group up to weight 29, and (a version of) the the\nBroadhurst-Kreimer conjecture on the number of multiple zeta values for a range\nof weight-depth pairs significantly exceeding the previous bounds. Our\ncomputations also verify a conjecture by Alekseev-Torossian on the\nKashiwara-Vergne Lie algebra up to weight 29.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compute numerically the dimensions of the graded quotients of the\nlinearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a\nconjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in\na chain of inclusions of Lie algebras, including also the linearized double\nshuffle Lie algebra and the (depth associated graded of the)\nGrothendieck-Teichm\\\"uller Lie algebra. Hence our computations also allow us to\ncheck the validity of the Deligne-Drinfeld conjecture on the structure of the\nGrothendieck-Teichm\\\"uller group up to weight 29, and (a version of) the the\nBroadhurst-Kreimer conjecture on the number of multiple zeta values for a range\nof weight-depth pairs significantly exceeding the previous bounds. Our\ncomputations also verify a conjecture by Alekseev-Torossian on the\nKashiwara-Vergne Lie algebra up to weight 29."
                },
                "authors": [
                    {
                        "name": "Florian Naef"
                    },
                    {
                        "name": "Thomas Willwacher"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Willwacher"
                },
                "author": "Thomas Willwacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.QA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v2",
                "updated": "2025-08-11T14:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    15,
                    27,
                    0,
                    223,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "15 pages, 14 figures; Accepted by ICCV2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08343v1",
                "updated": "2025-08-11T10:47:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T10:47:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "title": "Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical\n  Approach for Multi-Tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical\n  Approach for Multi-Tenant LLM Serving"
                },
                "summary": "Serving LLM adapters has gained significant attention as an effective\napproach to adapt general-purpose language models to diverse, task-specific use\ncases. However, serving a wide range of adapters introduces several and\nsubstantial overheads, leading to performance degradation and challenges in\noptimal placement. To address these challenges, we present an analytical,\nAI-driven pipeline that accurately determines the optimal allocation of\nadapters in single-node setups. This allocation maximizes performance,\neffectively using GPU resources, while preventing request starvation.\nCrucially, the proposed allocation is given based on current workload patterns.\nThese insights in single-node setups can be leveraged in multi-replica\ndeployments for overall placement, load balancing and server configuration,\nultimately enhancing overall performance and improving resource efficiency. Our\napproach builds on an in-depth analysis of LLM adapter serving, accounting for\noverheads and performance variability, and includes the development of the\nfirst Digital Twin capable of replicating online LLM-adapter serving systems\nwith matching key performance metrics. The experimental results demonstrate\nthat the Digital Twin achieves a SMAPE difference of no more than 5.5% in\nthroughput compared to real results, and the proposed pipeline accurately\npredicts the optimal placement with minimal latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving LLM adapters has gained significant attention as an effective\napproach to adapt general-purpose language models to diverse, task-specific use\ncases. However, serving a wide range of adapters introduces several and\nsubstantial overheads, leading to performance degradation and challenges in\noptimal placement. To address these challenges, we present an analytical,\nAI-driven pipeline that accurately determines the optimal allocation of\nadapters in single-node setups. This allocation maximizes performance,\neffectively using GPU resources, while preventing request starvation.\nCrucially, the proposed allocation is given based on current workload patterns.\nThese insights in single-node setups can be leveraged in multi-replica\ndeployments for overall placement, load balancing and server configuration,\nultimately enhancing overall performance and improving resource efficiency. Our\napproach builds on an in-depth analysis of LLM adapter serving, accounting for\noverheads and performance variability, and includes the development of the\nfirst Digital Twin capable of replicating online LLM-adapter serving systems\nwith matching key performance metrics. The experimental results demonstrate\nthat the Digital Twin achieves a SMAPE difference of no more than 5.5% in\nthroughput compared to real results, and the proposed pipeline accurately\npredicts the optimal placement with minimal latency."
                },
                "authors": [
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Joan Oliveras"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Under review for a computer science conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07811v1",
                "updated": "2025-08-11T09:54:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    54,
                    45,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T09:54:45Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    54,
                    45,
                    0,
                    223,
                    0
                ],
                "title": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration"
                },
                "summary": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions."
                },
                "authors": [
                    {
                        "name": "Sicheng Gao"
                    },
                    {
                        "name": "Nancy Mehta"
                    },
                    {
                        "name": "Zongwei Wu"
                    },
                    {
                        "name": "Radu Timofte"
                    }
                ],
                "author_detail": {
                    "name": "Radu Timofte"
                },
                "author": "Radu Timofte",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v2",
                "updated": "2025-08-11T08:10:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    8,
                    10,
                    21,
                    0,
                    223,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07570v1",
                "updated": "2025-08-11T03:03:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T03:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "title": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language\n  Models"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios."
                },
                "authors": [
                    {
                        "name": "Khanh-Binh Nguyen"
                    },
                    {
                        "name": "Phuoc-Nguyen Bui"
                    },
                    {
                        "name": "Hyunseung Choo"
                    },
                    {
                        "name": "Duc Thanh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Duc Thanh Nguyen"
                },
                "author": "Duc Thanh Nguyen",
                "arxiv_comment": "12 pages, Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09208v1",
                "updated": "2025-08-10T14:05:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    10,
                    14,
                    5,
                    36,
                    6,
                    222,
                    0
                ],
                "published": "2025-08-10T14:05:36Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    14,
                    5,
                    36,
                    6,
                    222,
                    0
                ],
                "title": "CoMoE: Collaborative Optimization of Expert Aggregation and Offloading\n  for MoE-based LLMs at Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMoE: Collaborative Optimization of Expert Aggregation and Offloading\n  for MoE-based LLMs at Edge"
                },
                "summary": "The proliferation of large language models (LLMs) has driven the adoption of\nMixture-of-Experts (MoE) architectures as a promising solution to scale model\ncapacity while controlling computational costs. However, deploying MoE models\nin resource-constrained mobile edge computing environments presents significant\nchallenges due to their large memory footprint and dynamic expert activation\npatterns. To address these challenges, we propose a novel dynamic\nresource-aware collaborative optimization framework that jointly optimizes\nexpert aggregation granularity and offloading strategies based on real-time\ndevice resource states, network conditions, and input characteristics in mobile\nedge environments, denoted as CoMoE. In CoMoE, we first systematically analyze\nexisting expert aggregation techniques, including expert parameter\nmerging,knowledge distillation,and parameter sharing decomposition, identifying\ntheir limitations in dynamic mobile environments.We then investigate expert\noffloading strategies encompassing expert prediction and prefetching, expert\ncaching and scheduling, and multi-tier storage architectures, revealing the\ninterdependencies between routing decisions and offloading performance.The\nCoMoE incorporates adaptive scheduling mechanisms that respond to user mobility\nand varying network conditions, enabling efficient MoE deployment across\nheterogeneous edge devices. Extensive experiments on real mobile edge testbeds\ndemonstrate that CoMoE achieves approximately 70% reduction in memory usage\ncompared to baseline methods, 10.5% lower inference latency than existing\nexpert offloading techniques, while maintaining model performance stability.\nFor large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE\nreduces memory requirements from 15.6GB to 4.7GB, enabling deployment on\nresource-constrained mobile edge devices that previously could only support\nmuch smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has driven the adoption of\nMixture-of-Experts (MoE) architectures as a promising solution to scale model\ncapacity while controlling computational costs. However, deploying MoE models\nin resource-constrained mobile edge computing environments presents significant\nchallenges due to their large memory footprint and dynamic expert activation\npatterns. To address these challenges, we propose a novel dynamic\nresource-aware collaborative optimization framework that jointly optimizes\nexpert aggregation granularity and offloading strategies based on real-time\ndevice resource states, network conditions, and input characteristics in mobile\nedge environments, denoted as CoMoE. In CoMoE, we first systematically analyze\nexisting expert aggregation techniques, including expert parameter\nmerging,knowledge distillation,and parameter sharing decomposition, identifying\ntheir limitations in dynamic mobile environments.We then investigate expert\noffloading strategies encompassing expert prediction and prefetching, expert\ncaching and scheduling, and multi-tier storage architectures, revealing the\ninterdependencies between routing decisions and offloading performance.The\nCoMoE incorporates adaptive scheduling mechanisms that respond to user mobility\nand varying network conditions, enabling efficient MoE deployment across\nheterogeneous edge devices. Extensive experiments on real mobile edge testbeds\ndemonstrate that CoMoE achieves approximately 70% reduction in memory usage\ncompared to baseline methods, 10.5% lower inference latency than existing\nexpert offloading techniques, while maintaining model performance stability.\nFor large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE\nreduces memory requirements from 15.6GB to 4.7GB, enabling deployment on\nresource-constrained mobile edge devices that previously could only support\nmuch smaller models."
                },
                "authors": [
                    {
                        "name": "Muqing Li"
                    },
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Wenchao Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Haijun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haijun Zhang"
                },
                "author": "Haijun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v2",
                "updated": "2025-08-09T11:31:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    31,
                    44,
                    5,
                    221,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06937v1",
                "updated": "2025-08-09T11:06:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    6,
                    58,
                    5,
                    221,
                    0
                ],
                "published": "2025-08-09T11:06:58Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    6,
                    58,
                    5,
                    221,
                    0
                ],
                "title": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing"
                },
                "summary": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods."
                },
                "authors": [
                    {
                        "name": "Weiyan Xie"
                    },
                    {
                        "name": "Han Gao"
                    },
                    {
                        "name": "Didan Deng"
                    },
                    {
                        "name": "Kaican Li"
                    },
                    {
                        "name": "April Hua Liu"
                    },
                    {
                        "name": "Yongxiang Huang"
                    },
                    {
                        "name": "Nevin L. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Nevin L. Zhang"
                },
                "author": "Nevin L. Zhang",
                "arxiv_comment": "Project Page: vaynexie.github.io/CannyEdit/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03974v2",
                "updated": "2025-08-09T02:33:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    2,
                    33,
                    21,
                    5,
                    221,
                    0
                ],
                "published": "2025-08-05T23:47:34Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    23,
                    47,
                    34,
                    1,
                    217,
                    0
                ],
                "title": "Managing Data for Scalable and Interactive Event Sequence Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing Data for Scalable and Interactive Event Sequence Visualization"
                },
                "summary": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization."
                },
                "authors": [
                    {
                        "name": "Sayef Azad Sakin"
                    },
                    {
                        "name": "Katherine E. Isaacs"
                    }
                ],
                "author_detail": {
                    "name": "Katherine E. Isaacs"
                },
                "author": "Katherine E. Isaacs",
                "arxiv_comment": "The 15th IEEE Workshop on Large Data Analysis and Visualization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v2",
                "updated": "2025-08-09T00:12:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    0,
                    12,
                    1,
                    5,
                    221,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data and labels to the server. To address those\nissues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method\nwhich can be deployed on the mobile device via server-assisted additive\nside-tuning. To further accelerate FT convergence and improve computing\nefficiency, PAE MobiLLM integrates activation caching on the server side, which\nallows the server to reuse historical activations and saves the mobile device\nfrom repeatedly computing forward passes for the recurring data samples.\nBesides, to reduce communication cost, PAE MobiLLM develops an activation\nshortcut that transmits only the token involved in the loss calculation instead\nof full activation matrices to guide the side network tuning. Last but not\nleast, PAE MobiLLM introduces the additive adapter side-network design which\nmakes the server train the adapter modules based on device-defined prediction\ndifferences rather than raw ground-truth labels. In this way, the server can\nonly assist device-defined side-network computing, and learn nothing about data\nand labels. Extensive experimental results demonstrate PAE MobiLLM's\nsuperiority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data and labels to the server. To address those\nissues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method\nwhich can be deployed on the mobile device via server-assisted additive\nside-tuning. To further accelerate FT convergence and improve computing\nefficiency, PAE MobiLLM integrates activation caching on the server side, which\nallows the server to reuse historical activations and saves the mobile device\nfrom repeatedly computing forward passes for the recurring data samples.\nBesides, to reduce communication cost, PAE MobiLLM develops an activation\nshortcut that transmits only the token involved in the loss calculation instead\nof full activation matrices to guide the side network tuning. Last but not\nleast, PAE MobiLLM introduces the additive adapter side-network design which\nmakes the server train the adapter modules based on device-defined prediction\ndifferences rather than raw ground-truth labels. In this way, the server can\nonly assist device-defined side-network computing, and learn nothing about data\nand labels. Extensive experimental results demonstrate PAE MobiLLM's\nsuperiority."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v3",
                "updated": "2025-08-08T18:16:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    18,
                    16,
                    33,
                    4,
                    220,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06447v1",
                "updated": "2025-08-08T16:42:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:42:38Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning"
                },
                "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Lingkun Long"
                    },
                    {
                        "name": "Rubing Yang"
                    },
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Desheng Hui"
                    },
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Jianlei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianlei Yang"
                },
                "author": "Jianlei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v3",
                "updated": "2025-08-08T14:25:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    25,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06297v1",
                "updated": "2025-08-08T13:19:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:19:30Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "title": "KV Cache Compression for Inference Efficiency in LLMs: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression for Inference Efficiency in LLMs: A Review"
                },
                "summary": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanyu Liu"
                    },
                    {
                        "name": "Jingying Fu"
                    },
                    {
                        "name": "Sixiang Liu"
                    },
                    {
                        "name": "Yitian Zou"
                    },
                    {
                        "name": "You Fu"
                    },
                    {
                        "name": "Jiehan Zhou"
                    },
                    {
                        "name": "Shouhua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shouhua Zhang"
                },
                "arxiv_affiliation": "University of Oulu",
                "author": "Shouhua Zhang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.16577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16577v1",
                "updated": "2025-08-22T17:59:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    59,
                    40,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T17:59:40Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    59,
                    40,
                    4,
                    234,
                    0
                ],
                "title": "MV-RAG: Retrieval Augmented Multiview Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MV-RAG: Retrieval Augmented Multiview Diffusion"
                },
                "summary": "Text-to-3D generation approaches have advanced significantly by leveraging\npretrained 2D diffusion priors, producing high-quality and 3D-consistent\noutputs. However, they often fail to produce out-of-domain (OOD) or rare\nconcepts, yielding inconsistent or inaccurate results. To this end, we propose\nMV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images\nfrom a large in-the-wild 2D database and then conditions a multiview diffusion\nmodel on these images to synthesize consistent and accurate multiview outputs.\nTraining such a retrieval-conditioned model is achieved via a novel hybrid\nstrategy bridging structured multiview data and diverse 2D image collections.\nThis involves training on multiview data using augmented conditioning views\nthat simulate retrieval variance for view-specific reconstruction, alongside\ntraining on sets of retrieved real-world 2D images using a distinctive held-out\nview prediction objective: the model predicts the held-out view from the other\nviews to infer 3D consistency from 2D data. To facilitate a rigorous OOD\nevaluation, we introduce a new collection of challenging OOD prompts.\nExperiments against state-of-the-art text-to-3D, image-to-3D, and\npersonalization baselines show that our approach significantly improves 3D\nconsistency, photorealism, and text adherence for OOD/rare concepts, while\nmaintaining competitive performance on standard benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-3D generation approaches have advanced significantly by leveraging\npretrained 2D diffusion priors, producing high-quality and 3D-consistent\noutputs. However, they often fail to produce out-of-domain (OOD) or rare\nconcepts, yielding inconsistent or inaccurate results. To this end, we propose\nMV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images\nfrom a large in-the-wild 2D database and then conditions a multiview diffusion\nmodel on these images to synthesize consistent and accurate multiview outputs.\nTraining such a retrieval-conditioned model is achieved via a novel hybrid\nstrategy bridging structured multiview data and diverse 2D image collections.\nThis involves training on multiview data using augmented conditioning views\nthat simulate retrieval variance for view-specific reconstruction, alongside\ntraining on sets of retrieved real-world 2D images using a distinctive held-out\nview prediction objective: the model predicts the held-out view from the other\nviews to infer 3D consistency from 2D data. To facilitate a rigorous OOD\nevaluation, we introduce a new collection of challenging OOD prompts.\nExperiments against state-of-the-art text-to-3D, image-to-3D, and\npersonalization baselines show that our approach significantly improves 3D\nconsistency, photorealism, and text adherence for OOD/rare concepts, while\nmaintaining competitive performance on standard benchmarks."
                },
                "authors": [
                    {
                        "name": "Yosef Dayani"
                    },
                    {
                        "name": "Omer Benishu"
                    },
                    {
                        "name": "Sagie Benaim"
                    }
                ],
                "author_detail": {
                    "name": "Sagie Benaim"
                },
                "author": "Sagie Benaim",
                "arxiv_comment": "Project page: https://yosefdayani.github.io/MV-RAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00998v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00998v2",
                "updated": "2025-08-22T17:56:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    56,
                    26,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-01T18:06:13Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    18,
                    6,
                    13,
                    4,
                    213,
                    0
                ],
                "title": "Are LLM-Powered Social Media Bots Realistic?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLM-Powered Social Media Bots Realistic?"
                },
                "summary": "As Large Language Models (LLMs) become more sophisticated, there is a\npossibility to harness LLMs to power social media bots. This work investigates\nthe realism of generating LLM-Powered social media bot networks. Through a\ncombination of manual effort, network science and LLMs, we create synthetic bot\nagent personas, their tweets and their interactions, thereby simulating social\nmedia networks. We compare the generated networks against empirical bot/human\ndata, observing that both network and linguistic properties of LLM-Powered Bots\ndiffer from Wild Bots/Humans. This has implications towards the detection and\neffectiveness of LLM-Powered Bots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become more sophisticated, there is a\npossibility to harness LLMs to power social media bots. This work investigates\nthe realism of generating LLM-Powered social media bot networks. Through a\ncombination of manual effort, network science and LLMs, we create synthetic bot\nagent personas, their tweets and their interactions, thereby simulating social\nmedia networks. We compare the generated networks against empirical bot/human\ndata, observing that both network and linguistic properties of LLM-Powered Bots\ndiffer from Wild Bots/Humans. This has implications towards the detection and\neffectiveness of LLM-Powered Bots."
                },
                "authors": [
                    {
                        "name": "Lynnette Hui Xian Ng"
                    },
                    {
                        "name": "Kathleen M. Carley"
                    }
                ],
                "author_detail": {
                    "name": "Kathleen M. Carley"
                },
                "author": "Kathleen M. Carley",
                "arxiv_comment": "Accepted into SBP-BRiMS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00998v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00998v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16571v1",
                "updated": "2025-08-22T17:50:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    50,
                    0,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T17:50:00Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    50,
                    0,
                    4,
                    234,
                    0
                ],
                "title": "LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due\n  Diligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due\n  Diligence"
                },
                "summary": "In this paper, we describe and benchmark a competitor-discovery component\nused within an agentic AI system for fast drug asset due diligence. A\ncompetitor-discovery AI agent, given an indication, retrieves all drugs\ncomprising the competitive landscape of that indication and extracts canonical\nattributes for these drugs. The competitor definition is investor-specific, and\ndata is paywalled/licensed, fragmented across registries, ontology-mismatched\nby indication, alias-heavy for drug names, multimodal, and rapidly changing.\nAlthough considered the best tool for this problem, the current LLM-based AI\nsystems aren't capable of reliably retrieving all competing drug names, and\nthere is no accepted public benchmark for this task. To address the lack of\nevaluation, we use LLM-based agents to transform five years of multi-modal,\nunstructured diligence memos from a private biotech VC fund into a structured\nevaluation corpus mapping indications to competitor drugs with normalized\nattributes. We also introduce a competitor validating LLM-as-a-judge agent that\nfilters out false positives from the list of predicted competitors to maximize\nprecision and suppress hallucinations. On this benchmark, our\ncompetitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research\n(65%) and Perplexity Labs (60%). The system is deployed in production with\nenterprise users; in a case study with a biotech VC investment fund, analyst\nturnaround time dropped from 2.5 days to $\\sim$3 hours ($\\sim$20x) for the\ncompetitive analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we describe and benchmark a competitor-discovery component\nused within an agentic AI system for fast drug asset due diligence. A\ncompetitor-discovery AI agent, given an indication, retrieves all drugs\ncomprising the competitive landscape of that indication and extracts canonical\nattributes for these drugs. The competitor definition is investor-specific, and\ndata is paywalled/licensed, fragmented across registries, ontology-mismatched\nby indication, alias-heavy for drug names, multimodal, and rapidly changing.\nAlthough considered the best tool for this problem, the current LLM-based AI\nsystems aren't capable of reliably retrieving all competing drug names, and\nthere is no accepted public benchmark for this task. To address the lack of\nevaluation, we use LLM-based agents to transform five years of multi-modal,\nunstructured diligence memos from a private biotech VC fund into a structured\nevaluation corpus mapping indications to competitor drugs with normalized\nattributes. We also introduce a competitor validating LLM-as-a-judge agent that\nfilters out false positives from the list of predicted competitors to maximize\nprecision and suppress hallucinations. On this benchmark, our\ncompetitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research\n(65%) and Perplexity Labs (60%). The system is deployed in production with\nenterprise users; in a case study with a biotech VC investment fund, analyst\nturnaround time dropped from 2.5 days to $\\sim$3 hours ($\\sim$20x) for the\ncompetitive analysis."
                },
                "authors": [
                    {
                        "name": "Alisa Vinogradova"
                    },
                    {
                        "name": "Vlad Vinogradov"
                    },
                    {
                        "name": "Dmitrii Radkevich"
                    },
                    {
                        "name": "Ilya Yasny"
                    },
                    {
                        "name": "Dmitry Kobyzev"
                    },
                    {
                        "name": "Ivan Izmailov"
                    },
                    {
                        "name": "Katsiaryna Yanchanka"
                    },
                    {
                        "name": "Andrey Doronichev"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Doronichev"
                },
                "arxiv_affiliation": "Optic Inc",
                "author": "Andrey Doronichev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16560v1",
                "updated": "2025-08-22T17:26:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    26,
                    33,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T17:26:33Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    26,
                    33,
                    4,
                    234,
                    0
                ],
                "title": "Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse\n  Autoencoders"
                },
                "summary": "Sparse Autoencoders (SAEs) extract features from LLM internal activations,\nmeant to correspond to single concepts. A core SAE training hyperparameter is\nL0: how many features should fire per token on average. Existing work compares\nSAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a\nfree parameter with no single correct value. In this work we study the effect\nof L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE\nfails to learn the underlying features of the LLM. If L0 is too low, the SAE\nwill mix correlated features to improve reconstruction. If L0 is too high, the\nSAE finds degenerate solutions that also mix features. Further, we demonstrate\na method to determine the correct L0 value for an SAE on a given training\ndistribution, which finds the true L0 in toy models and coincides with peak\nsparse probing performance in LLMs. We find that most commonly used SAEs have\nan L0 that is too low. Our work shows that, to train SAEs with correct\nfeatures, practitioners must set L0 correctly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) extract features from LLM internal activations,\nmeant to correspond to single concepts. A core SAE training hyperparameter is\nL0: how many features should fire per token on average. Existing work compares\nSAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a\nfree parameter with no single correct value. In this work we study the effect\nof L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE\nfails to learn the underlying features of the LLM. If L0 is too low, the SAE\nwill mix correlated features to improve reconstruction. If L0 is too high, the\nSAE finds degenerate solutions that also mix features. Further, we demonstrate\na method to determine the correct L0 value for an SAE on a given training\ndistribution, which finds the true L0 in toy models and coincides with peak\nsparse probing performance in LLMs. We find that most commonly used SAEs have\nan L0 that is too low. Our work shows that, to train SAEs with correct\nfeatures, practitioners must set L0 correctly."
                },
                "authors": [
                    {
                        "name": "David Chanin"
                    },
                    {
                        "name": "Adri Garriga-Alonso"
                    }
                ],
                "author_detail": {
                    "name": "Adri Garriga-Alonso"
                },
                "author": "Adri Garriga-Alonso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16556v1",
                "updated": "2025-08-22T17:23:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    23,
                    25,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T17:23:25Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    23,
                    25,
                    4,
                    234,
                    0
                ],
                "title": "Spherical latent space models for social network analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spherical latent space models for social network analysis"
                },
                "summary": "This article introduces a spherical latent space model for social network\nanalysis, embedding actors on a hypersphere rather than in Euclidean space as\nin standard latent space models. The spherical geometry facilitates the\nrepresentation of transitive relationships and community structure, naturally\ncaptures cyclical patterns, and ensures bounded distances, thereby mitigating\ndegeneracy issues common in traditional approaches. Bayesian inference is\nperformed via Markov chain Monte Carlo methods to estimate both latent\npositions and other model parameters. The approach is demonstrated using two\nbenchmark social network datasets, yielding improved model fit and\ninterpretability relative to conventional latent space models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article introduces a spherical latent space model for social network\nanalysis, embedding actors on a hypersphere rather than in Euclidean space as\nin standard latent space models. The spherical geometry facilitates the\nrepresentation of transitive relationships and community structure, naturally\ncaptures cyclical patterns, and ensures bounded distances, thereby mitigating\ndegeneracy issues common in traditional approaches. Bayesian inference is\nperformed via Markov chain Monte Carlo methods to estimate both latent\npositions and other model parameters. The approach is demonstrated using two\nbenchmark social network datasets, yielding improved model fit and\ninterpretability relative to conventional latent space models."
                },
                "authors": [
                    {
                        "name": "Juan Sosa"
                    },
                    {
                        "name": "Carlos Nosa"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Nosa"
                },
                "author": "Carlos Nosa",
                "arxiv_comment": "37 pages, 8 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16553v1",
                "updated": "2025-08-22T17:21:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    21,
                    56,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T17:21:56Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    21,
                    56,
                    4,
                    234,
                    0
                ],
                "title": "TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a\n  Milling Machine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a\n  Milling Machine"
                },
                "summary": "In the context of industry 4.0, long-serving industrial machines can be\nretrofitted with process monitoring capabilities for future use in a smart\nfactory. One possible approach is the deployment of wireless monitoring\nsystems, which can benefit substantially from the TinyML paradigm. This work\npresents a complete TinyML flow from dataset generation, to machine learning\nmodel development, up to implementation and evaluation of a full preprocessing\nand classification pipeline on a microcontroller. After a short review on\nTinyML in industrial process monitoring, the creation of the novel MillingVibes\ndataset is described. The feasibility of a TinyML system for\nstructure-integrated process quality monitoring could be shown by the\ndevelopment of an 8-bit-quantized convolutional neural network (CNN) model with\n12.59kiB parameter storage. A test accuracy of 100.0% could be reached at\n15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex\nM4F microcontroller, serving as a reference for future TinyML process\nmonitoring solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context of industry 4.0, long-serving industrial machines can be\nretrofitted with process monitoring capabilities for future use in a smart\nfactory. One possible approach is the deployment of wireless monitoring\nsystems, which can benefit substantially from the TinyML paradigm. This work\npresents a complete TinyML flow from dataset generation, to machine learning\nmodel development, up to implementation and evaluation of a full preprocessing\nand classification pipeline on a microcontroller. After a short review on\nTinyML in industrial process monitoring, the creation of the novel MillingVibes\ndataset is described. The feasibility of a TinyML system for\nstructure-integrated process quality monitoring could be shown by the\ndevelopment of an 8-bit-quantized convolutional neural network (CNN) model with\n12.59kiB parameter storage. A test accuracy of 100.0% could be reached at\n15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex\nM4F microcontroller, serving as a reference for future TinyML process\nmonitoring solutions."
                },
                "authors": [
                    {
                        "name": "Tim Langer"
                    },
                    {
                        "name": "Matthias Widra"
                    },
                    {
                        "name": "Volkhard Beyer"
                    }
                ],
                "author_detail": {
                    "name": "Volkhard Beyer"
                },
                "author": "Volkhard Beyer",
                "arxiv_comment": "10 pages, 5 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.5.4; C.5.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16552v1",
                "updated": "2025-08-22T17:21:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    21,
                    21,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T17:21:21Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    21,
                    21,
                    4,
                    234,
                    0
                ],
                "title": "Data Gluttony: Epistemic Risks, Dependent Testing and Data Reuse in\n  Large Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Gluttony: Epistemic Risks, Dependent Testing and Data Reuse in\n  Large Datasets"
                },
                "summary": "Large-scale registries have collected vast amounts of data which has enabled\ninvestigators to efficiently conduct studies of observational data. Common\npractice is for investigators to use all data meeting the inclusion criteria of\ntheir study to perform their analysis. We term this common practice data\ngluttony. It has apparent formal justification insofar as this approach\nmaximizes per-study power. But this comes at a cost: data reuse affects the\nshape of the tail distribution of inferential errors. Using the theory of risk\norderings we demonstrate how positively dependent testing procedures result in\nstrictly riskier distributions of inferential error.\n  We identify two remedies to this state of affairs: research portfolio\noptimization and what we term data temperance. Research portfolio optimization\nrequires that we formulate the enterprise of inference in a utility theoretic\nframework: associated to each hypothesis to be evaluated is some utility\ndependent on its truth as well as the impact of the statistical decision\nrendered on the basis of the data. Under certain models of data governance,\nthis approach can be used to optimally allocate data usage across multiple\ninferential tasks. On the other hand, data temperance is a more flexible\nstrategy for managing the distribution of inferential errors. Data temperance\nis the principle that an investigator use only as much data as is necessary to\nperform the task at hand. This is possible due to the diminishing marginal\nreturns in power and precision in sample size. We analyze the effectiveness of\ndata temperance at reducing the dependence across testing and develop a theory\nof the capacity of a static database to sustain large numbers of inferential\ntasks with low probability of inducing pairwise dependent testing procedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale registries have collected vast amounts of data which has enabled\ninvestigators to efficiently conduct studies of observational data. Common\npractice is for investigators to use all data meeting the inclusion criteria of\ntheir study to perform their analysis. We term this common practice data\ngluttony. It has apparent formal justification insofar as this approach\nmaximizes per-study power. But this comes at a cost: data reuse affects the\nshape of the tail distribution of inferential errors. Using the theory of risk\norderings we demonstrate how positively dependent testing procedures result in\nstrictly riskier distributions of inferential error.\n  We identify two remedies to this state of affairs: research portfolio\noptimization and what we term data temperance. Research portfolio optimization\nrequires that we formulate the enterprise of inference in a utility theoretic\nframework: associated to each hypothesis to be evaluated is some utility\ndependent on its truth as well as the impact of the statistical decision\nrendered on the basis of the data. Under certain models of data governance,\nthis approach can be used to optimally allocate data usage across multiple\ninferential tasks. On the other hand, data temperance is a more flexible\nstrategy for managing the distribution of inferential errors. Data temperance\nis the principle that an investigator use only as much data as is necessary to\nperform the task at hand. This is possible due to the diminishing marginal\nreturns in power and precision in sample size. We analyze the effectiveness of\ndata temperance at reducing the dependence across testing and develop a theory\nof the capacity of a static database to sustain large numbers of inferential\ntasks with low probability of inducing pairwise dependent testing procedures."
                },
                "authors": [
                    {
                        "name": "Reid Dale"
                    },
                    {
                        "name": "Jordan Rodu"
                    },
                    {
                        "name": "Maria E. Currie"
                    },
                    {
                        "name": "Mike Baiocchi"
                    }
                ],
                "author_detail": {
                    "name": "Mike Baiocchi"
                },
                "author": "Mike Baiocchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16546v1",
                "updated": "2025-08-22T17:10:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    10,
                    37,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T17:10:37Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    10,
                    37,
                    4,
                    234,
                    0
                ],
                "title": "RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs.\n  Reinforcement Learning Fine-Tuning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs.\n  Reinforcement Learning Fine-Tuning for LLMs"
                },
                "summary": "Training large language models (LLMs) from scratch is increasingly\nimpractical, making post-training methods such as supervised fine-tuning (SFT)\nand reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern\npractice. Using an out-of-distribution (OOD) variant of the 24-point card game\nand new spectrum-based diagnostics, we revisit how these two stages reshape\nmodel representation and OOD performance. Our key findings are- (1) RL-FT can\nrestore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to\n15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and\na clear distribution shift, RL-FT cannot fully recover OOD performance. (2)\nDirection shifts of singular vectors matter more than singular value\nmagnitudes. These shifts concentrate on directions linked to the largest and\nsmallest singular values, leaving the bulk spectrum intact. (3) Low-rank and\nshallow recovery is effective: restoring singular vector directions for the top\n20% of values or first 25% of layers recovers 70-80% of OOD performance. (4)\nStronger SFT checkpoints enable better recovery by RL, while overfitted ones\nresist restoration. These results reconcile prior reports of RL superior OOD\nperformance: RL primarily counteracts SFT-induced directional drift rather than\nfinding new solutions. Our spectrum-aware analysis highlights inexpensive\nrecovery knobs low-rank UV merging and shallow-layer resets that practitioners\ncan use before costly RL fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) from scratch is increasingly\nimpractical, making post-training methods such as supervised fine-tuning (SFT)\nand reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern\npractice. Using an out-of-distribution (OOD) variant of the 24-point card game\nand new spectrum-based diagnostics, we revisit how these two stages reshape\nmodel representation and OOD performance. Our key findings are- (1) RL-FT can\nrestore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to\n15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and\na clear distribution shift, RL-FT cannot fully recover OOD performance. (2)\nDirection shifts of singular vectors matter more than singular value\nmagnitudes. These shifts concentrate on directions linked to the largest and\nsmallest singular values, leaving the bulk spectrum intact. (3) Low-rank and\nshallow recovery is effective: restoring singular vector directions for the top\n20% of values or first 25% of layers recovers 70-80% of OOD performance. (4)\nStronger SFT checkpoints enable better recovery by RL, while overfitted ones\nresist restoration. These results reconcile prior reports of RL superior OOD\nperformance: RL primarily counteracts SFT-induced directional drift rather than\nfinding new solutions. Our spectrum-aware analysis highlights inexpensive\nrecovery knobs low-rank UV merging and shallow-layer resets that practitioners\ncan use before costly RL fine-tuning."
                },
                "authors": [
                    {
                        "name": "Hangzhan Jin"
                    },
                    {
                        "name": "Sicheng Lv"
                    },
                    {
                        "name": "Sifan Wu"
                    },
                    {
                        "name": "Mohammad Hamdaqa"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Hamdaqa"
                },
                "author": "Mohammad Hamdaqa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11401v2",
                "updated": "2025-08-22T17:10:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    10,
                    33,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-15T11:10:40Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    10,
                    40,
                    4,
                    227,
                    0
                ],
                "title": "FACET: Teacher-Centred LLM-Based Multi-Agent Systems-Towards\n  Personalized Educational Worksheets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FACET: Teacher-Centred LLM-Based Multi-Agent Systems-Towards\n  Personalized Educational Worksheets"
                },
                "summary": "The increasing heterogeneity of student populations poses significant\nchallenges for teachers, particularly in mathematics education, where\ncognitive, motivational, and emotional differences strongly influence learning\noutcomes. While AI-driven personalization tools have emerged, most remain\nperformance-focused, offering limited support for teachers and neglecting\nbroader pedagogical needs. This paper presents the FACET framework, a\nteacher-facing, large language model (LLM)-based multi-agent system designed to\ngenerate individualized classroom materials that integrate both cognitive and\nmotivational dimensions of learner profiles. The framework comprises three\nspecialized agents: (1) learner agents that simulate diverse profiles\nincorporating topic proficiency and intrinsic motivation, (2) a teacher agent\nthat adapts instructional content according to didactical principles, and (3)\nan evaluator agent that provides automated quality assurance. We tested the\nsystem using authentic grade 8 mathematics curriculum content and evaluated its\nfeasibility through a) automated agent-based assessment of output quality and\nb) exploratory feedback from K-12 in-service teachers. Results from ten\ninternal evaluations highlighted high stability and alignment between generated\nmaterials and learner profiles, and teacher feedback particularly highlighted\nstructure and suitability of tasks. The findings demonstrate the potential of\nmulti-agent LLM architectures to provide scalable, context-aware\npersonalization in heterogeneous classroom settings, and outline directions for\nextending the framework to richer learner profiles and real-world classroom\ntrials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing heterogeneity of student populations poses significant\nchallenges for teachers, particularly in mathematics education, where\ncognitive, motivational, and emotional differences strongly influence learning\noutcomes. While AI-driven personalization tools have emerged, most remain\nperformance-focused, offering limited support for teachers and neglecting\nbroader pedagogical needs. This paper presents the FACET framework, a\nteacher-facing, large language model (LLM)-based multi-agent system designed to\ngenerate individualized classroom materials that integrate both cognitive and\nmotivational dimensions of learner profiles. The framework comprises three\nspecialized agents: (1) learner agents that simulate diverse profiles\nincorporating topic proficiency and intrinsic motivation, (2) a teacher agent\nthat adapts instructional content according to didactical principles, and (3)\nan evaluator agent that provides automated quality assurance. We tested the\nsystem using authentic grade 8 mathematics curriculum content and evaluated its\nfeasibility through a) automated agent-based assessment of output quality and\nb) exploratory feedback from K-12 in-service teachers. Results from ten\ninternal evaluations highlighted high stability and alignment between generated\nmaterials and learner profiles, and teacher feedback particularly highlighted\nstructure and suitability of tasks. The findings demonstrate the potential of\nmulti-agent LLM architectures to provide scalable, context-aware\npersonalization in heterogeneous classroom settings, and outline directions for\nextending the framework to richer learner profiles and real-world classroom\ntrials."
                },
                "authors": [
                    {
                        "name": "Jana Gonnermann-Mller"
                    },
                    {
                        "name": "Jennifer Haase"
                    },
                    {
                        "name": "Konstantin Fackeldey"
                    },
                    {
                        "name": "Sebastian Pokutta"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Pokutta"
                },
                "author": "Sebastian Pokutta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10652v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10652v3",
                "updated": "2025-08-22T17:01:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    1,
                    56,
                    4,
                    234,
                    0
                ],
                "published": "2025-03-07T10:37:31Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    10,
                    37,
                    31,
                    4,
                    66,
                    0
                ],
                "title": "Can Large Language Models Simulate Human Responses? A Case Study of\n  Stated Preference Experiments in the Context of Heating-related Choices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Simulate Human Responses? A Case Study of\n  Stated Preference Experiments in the Context of Heating-related Choices"
                },
                "summary": "Stated preference (SP) surveys are a key method to research how individuals\nmake trade-offs in hypothetical, also futuristic, scenarios. In energy context\nthis includes key decarbonisation enablement contexts, such as low-carbon\ntechnologies, distributed renewable energy generation, and demand-side response\n[1,2]. However, they tend to be costly, time-consuming, and can be affected by\nrespondent fatigue and ethical constraints. Large language models (LLMs) have\ndemonstrated remarkable capabilities in generating human-like textual\nresponses, prompting growing interest in their application to survey research.\nThis study investigates the use of LLMs to simulate consumer choices in\nenergy-related SP surveys and explores their integration into data analysis\nworkflows. A series of test scenarios were designed to systematically assess\nthe simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5 and\nDeepSeek-R1) at both individual and aggregated levels, considering contexts\nfactors such as prompt design, in-context learning (ICL), chain-of-thought\n(CoT) reasoning, LLM types, integration with traditional choice models, and\npotential biases. Cloud-based LLMs do not consistently outperform smaller local\nmodels. In this study, the reasoning model DeepSeek-R1 achieves the highest\naverage accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factor\nidentification, and choice distribution alignment. Across models, systematic\nbiases are observed against the gas boiler and no-retrofit options, with a\npreference for more energy-efficient alternatives. The findings suggest that\nprevious SP choices are the most effective input factor, while longer prompts\nwith additional factors and varied formats can cause LLMs to lose focus,\nreducing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stated preference (SP) surveys are a key method to research how individuals\nmake trade-offs in hypothetical, also futuristic, scenarios. In energy context\nthis includes key decarbonisation enablement contexts, such as low-carbon\ntechnologies, distributed renewable energy generation, and demand-side response\n[1,2]. However, they tend to be costly, time-consuming, and can be affected by\nrespondent fatigue and ethical constraints. Large language models (LLMs) have\ndemonstrated remarkable capabilities in generating human-like textual\nresponses, prompting growing interest in their application to survey research.\nThis study investigates the use of LLMs to simulate consumer choices in\nenergy-related SP surveys and explores their integration into data analysis\nworkflows. A series of test scenarios were designed to systematically assess\nthe simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5 and\nDeepSeek-R1) at both individual and aggregated levels, considering contexts\nfactors such as prompt design, in-context learning (ICL), chain-of-thought\n(CoT) reasoning, LLM types, integration with traditional choice models, and\npotential biases. Cloud-based LLMs do not consistently outperform smaller local\nmodels. In this study, the reasoning model DeepSeek-R1 achieves the highest\naverage accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factor\nidentification, and choice distribution alignment. Across models, systematic\nbiases are observed against the gas boiler and no-retrofit options, with a\npreference for more energy-efficient alternatives. The findings suggest that\nprevious SP choices are the most effective input factor, while longer prompts\nwith additional factors and varied formats can cause LLMs to lose focus,\nreducing accuracy."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Jacek Pawlak"
                    },
                    {
                        "name": "Aruna Sivakumar"
                    }
                ],
                "author_detail": {
                    "name": "Aruna Sivakumar"
                },
                "author": "Aruna Sivakumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10652v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10652v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14295v2",
                "updated": "2025-08-22T16:49:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    49,
                    10,
                    4,
                    234,
                    0
                ],
                "published": "2025-07-18T18:07:38Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    18,
                    7,
                    38,
                    4,
                    199,
                    0
                ],
                "title": "A Simple \"Try Again\" Can Elicit Multi-Turn LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple \"Try Again\" Can Elicit Multi-Turn LLM Reasoning"
                },
                "summary": "Multi-turn problem solving is critical yet challenging for Large Reasoning\nModels (LRMs) to reflect on their reasoning and revise from feedback. Existing\nReinforcement Learning (RL) methods train large reasoning models on a\nsingle-turn paradigm with verifiable rewards. However, we observe that models\ntrained with existing RL paradigms often lose their ability to solve problems\nacross multiple turns and struggle to revise answers based on contextual\nfeedback, leading to repetitive responses. We ask: can LRMs learn to reflect\ntheir answers in a multi-turn context? In this work, we find that training\nmodels with multi-turn RL using only unary feedback (e.g., \"Let's try again\")\nafter wrong answers can improve both single-turn performance and multi-turn\nreasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement\nlearning, which uses minimal yet common unary user feedback during iterative\nproblem solving. It can be easily applied to existing single-turn RL training\nsetups. Experimental results show that RL training with UFO keeps single-turn\nperformance and improves multi-turn reasoning accuracy by up to 14%, enabling\nlanguage models to better react to feedback in multi-turn problem solving. To\nfurther minimize the number of turns needed for a correct answer while\nencouraging diverse reasoning when mistakes occur, we design reward structures\nthat guide models to produce careful and deliberate answers in each turn. Code:\nhttps://github.com/lichengliu03/unary-feedback",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn problem solving is critical yet challenging for Large Reasoning\nModels (LRMs) to reflect on their reasoning and revise from feedback. Existing\nReinforcement Learning (RL) methods train large reasoning models on a\nsingle-turn paradigm with verifiable rewards. However, we observe that models\ntrained with existing RL paradigms often lose their ability to solve problems\nacross multiple turns and struggle to revise answers based on contextual\nfeedback, leading to repetitive responses. We ask: can LRMs learn to reflect\ntheir answers in a multi-turn context? In this work, we find that training\nmodels with multi-turn RL using only unary feedback (e.g., \"Let's try again\")\nafter wrong answers can improve both single-turn performance and multi-turn\nreasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement\nlearning, which uses minimal yet common unary user feedback during iterative\nproblem solving. It can be easily applied to existing single-turn RL training\nsetups. Experimental results show that RL training with UFO keeps single-turn\nperformance and improves multi-turn reasoning accuracy by up to 14%, enabling\nlanguage models to better react to feedback in multi-turn problem solving. To\nfurther minimize the number of turns needed for a correct answer while\nencouraging diverse reasoning when mistakes occur, we design reward structures\nthat guide models to produce careful and deliberate answers in each turn. Code:\nhttps://github.com/lichengliu03/unary-feedback"
                },
                "authors": [
                    {
                        "name": "Licheng Liu"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Linjie Li"
                    },
                    {
                        "name": "Chenwei Xu"
                    },
                    {
                        "name": "Yiping Lu"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Avirup Sil"
                    },
                    {
                        "name": "Manling Li"
                    }
                ],
                "author_detail": {
                    "name": "Manling Li"
                },
                "author": "Manling Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16517v1",
                "updated": "2025-08-22T16:40:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    40,
                    17,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T16:40:17Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    40,
                    17,
                    4,
                    234,
                    0
                ],
                "title": "ARSP: Automated Repair of Verilog Designs via Semantic Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARSP: Automated Repair of Verilog Designs via Semantic Partitioning"
                },
                "summary": "Debugging functional Verilog bugs consumes a significant portion of front-end\ndesign time. While Large Language Models (LLMs) have demonstrated great\npotential in mitigating this effort, existing LLM-based automated debugging\nmethods underperform on industrial-scale modules. A major reason for this is\nbug signal dilution in long contexts, where a few bug-relevant tokens are\noverwhelmed by hundreds of unrelated lines, diffusing the model's attention. To\naddress this issue, we introduce ARSP, a two-stage system that mitigates\ndilution via semantics-guided fragmentation. A Partition LLM splits a module\ninto semantically tight fragments; a Repair LLM patches each fragment; edits\nare merged without altering unrelated logic. A synthetic data framework\ngenerates fragment-level training pairs spanning bug types, design styles, and\nscales to supervise both models. Experiments show that ARSP achieves 77.92%\npass@1 and 83.88% pass@5, outperforming mainstream commercial LLMs including\nClaude-3.7 and SOTA automated Verilog debugging tools Strider and MEIC. Also,\nsemantic partitioning improves pass@1 by 11.6% and pass@5 by 10.2% over\nwhole-module debugging, validating the effectiveness of fragment-level scope\nreduction in LLM-based Verilog debugging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging functional Verilog bugs consumes a significant portion of front-end\ndesign time. While Large Language Models (LLMs) have demonstrated great\npotential in mitigating this effort, existing LLM-based automated debugging\nmethods underperform on industrial-scale modules. A major reason for this is\nbug signal dilution in long contexts, where a few bug-relevant tokens are\noverwhelmed by hundreds of unrelated lines, diffusing the model's attention. To\naddress this issue, we introduce ARSP, a two-stage system that mitigates\ndilution via semantics-guided fragmentation. A Partition LLM splits a module\ninto semantically tight fragments; a Repair LLM patches each fragment; edits\nare merged without altering unrelated logic. A synthetic data framework\ngenerates fragment-level training pairs spanning bug types, design styles, and\nscales to supervise both models. Experiments show that ARSP achieves 77.92%\npass@1 and 83.88% pass@5, outperforming mainstream commercial LLMs including\nClaude-3.7 and SOTA automated Verilog debugging tools Strider and MEIC. Also,\nsemantic partitioning improves pass@1 by 11.6% and pass@5 by 10.2% over\nwhole-module debugging, validating the effectiveness of fragment-level scope\nreduction in LLM-based Verilog debugging."
                },
                "authors": [
                    {
                        "name": "Bingkun Yao"
                    },
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Yuxin Du"
                    },
                    {
                        "name": "Yuchen Hu"
                    },
                    {
                        "name": "Hong Gao"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16514v1",
                "updated": "2025-08-22T16:37:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    37,
                    40,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T16:37:40Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    37,
                    40,
                    4,
                    234,
                    0
                ],
                "title": "FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the\n  Data Synthesis Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the\n  Data Synthesis Pipeline"
                },
                "summary": "Recent works improving LLM math reasoning with synthetic data have used\nunique setups, making comparison of data synthesis strategies impractical. This\nleaves many unanswered questions about the roles of different factors in the\nsynthetic data pipeline, such as the impact of filtering low-quality problems.\nTo address this gap, we introduce FLAMES, a Framework for LLM Assessment of\nMath rEasoning Data Synthesis, and perform a systematic study of 10 existing\ndata synthesis strategies and multiple other factors impacting the performance\nof synthetic math reasoning data. Our FLAMES experiments provide several\nvaluable insights about the optimal balance of difficulty and diversity of\nsynthetic data. First, data agents designed to increase problem complexity lead\nto best improvements on most math metrics. Second, with a fixed data generation\nbudget, keeping higher problem coverage is more important than keeping only\nproblems with reliable solutions. Third, GSM8K- and MATH-based synthetic data\ncan lead to improvements on competition-level benchmarks, showcasing\neasy-to-hard generalization. Leveraging insights from our FLAMES experiments,\nwe design two novel data synthesis strategies for improving out-of-domain\ngeneralization and robustness. Further, we develop the FLAMES dataset, an\neffective blend of our novel and existing data synthesis strategies,\noutperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5),\nGSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES\ndataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and\nClaude 3.5 Sonnet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works improving LLM math reasoning with synthetic data have used\nunique setups, making comparison of data synthesis strategies impractical. This\nleaves many unanswered questions about the roles of different factors in the\nsynthetic data pipeline, such as the impact of filtering low-quality problems.\nTo address this gap, we introduce FLAMES, a Framework for LLM Assessment of\nMath rEasoning Data Synthesis, and perform a systematic study of 10 existing\ndata synthesis strategies and multiple other factors impacting the performance\nof synthetic math reasoning data. Our FLAMES experiments provide several\nvaluable insights about the optimal balance of difficulty and diversity of\nsynthetic data. First, data agents designed to increase problem complexity lead\nto best improvements on most math metrics. Second, with a fixed data generation\nbudget, keeping higher problem coverage is more important than keeping only\nproblems with reliable solutions. Third, GSM8K- and MATH-based synthetic data\ncan lead to improvements on competition-level benchmarks, showcasing\neasy-to-hard generalization. Leveraging insights from our FLAMES experiments,\nwe design two novel data synthesis strategies for improving out-of-domain\ngeneralization and robustness. Further, we develop the FLAMES dataset, an\neffective blend of our novel and existing data synthesis strategies,\noutperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5),\nGSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES\ndataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and\nClaude 3.5 Sonnet."
                },
                "authors": [
                    {
                        "name": "Parker Seegmiller"
                    },
                    {
                        "name": "Kartik Mehta"
                    },
                    {
                        "name": "Soumya Saha"
                    },
                    {
                        "name": "Chenyang Tao"
                    },
                    {
                        "name": "Shereen Oraby"
                    },
                    {
                        "name": "Arpit Gupta"
                    },
                    {
                        "name": "Tagyoung Chung"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "arxiv_comment": "To appear at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16513v1",
                "updated": "2025-08-22T16:36:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    36,
                    52,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T16:36:52Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    36,
                    52,
                    4,
                    234,
                    0
                ],
                "title": "Extreme Lithium Depletion in Solar Twins: Challenging Non-Standard\n  Mixing Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme Lithium Depletion in Solar Twins: Challenging Non-Standard\n  Mixing Models"
                },
                "summary": "Lithium (Li) is a powerful tracer of stellar mixing, gradually depleted in\nsolar twins by non-standard transport below the convective zone. Here, we\nidentify six new solar twins with exceptionally low Li levels that are not\nexplained by current non-standard mixing models and, together with our\npreviously reported anomalous solar twin HIP 8522, suggest a distinct\npopulation marked by a violent evolutionary past. Employing high-resolution\nspectra ($R=60,000 - 165,000$), we infer precise stellar parameters and\nchemical compositions, including Li abundances. We consider possible scenarios\ngenerating enhanced mixing, including planetary engulfment, blue straggler\nstars (BSSs), and early episodic accretion. Our planet engulfment simulations\nindicate that only one star may have engulfed an exoplanet, rapidly depleting\nLi via thermohaline convection. In the BSS scenario, radial velocity data rule\nout binary mass transfer, revealing no stellar companions but instead two new\nexoplanets. If these stars are field BSSs, a binary merger is likely though\nuncertain given that current BSS models focus mostly on stars in open clusters.\nUsing pre-main-sequence episodic accretion models, we find that solar-mass\nstars can experience enhanced Li depletion without significant beryllium (Be)\ndepletion. This is consistent with the Be abundances measured in two of our\nstars and represents the most plausible scenario, pending Be measurements for\nthe remaining stars. These unique stars, together with HIP 8522, represent\nexceptional cases for testing stellar evolution models and probing internal\nmixing processes in Sun-like stars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lithium (Li) is a powerful tracer of stellar mixing, gradually depleted in\nsolar twins by non-standard transport below the convective zone. Here, we\nidentify six new solar twins with exceptionally low Li levels that are not\nexplained by current non-standard mixing models and, together with our\npreviously reported anomalous solar twin HIP 8522, suggest a distinct\npopulation marked by a violent evolutionary past. Employing high-resolution\nspectra ($R=60,000 - 165,000$), we infer precise stellar parameters and\nchemical compositions, including Li abundances. We consider possible scenarios\ngenerating enhanced mixing, including planetary engulfment, blue straggler\nstars (BSSs), and early episodic accretion. Our planet engulfment simulations\nindicate that only one star may have engulfed an exoplanet, rapidly depleting\nLi via thermohaline convection. In the BSS scenario, radial velocity data rule\nout binary mass transfer, revealing no stellar companions but instead two new\nexoplanets. If these stars are field BSSs, a binary merger is likely though\nuncertain given that current BSS models focus mostly on stars in open clusters.\nUsing pre-main-sequence episodic accretion models, we find that solar-mass\nstars can experience enhanced Li depletion without significant beryllium (Be)\ndepletion. This is consistent with the Be abundances measured in two of our\nstars and represents the most plausible scenario, pending Be measurements for\nthe remaining stars. These unique stars, together with HIP 8522, represent\nexceptional cases for testing stellar evolution models and probing internal\nmixing processes in Sun-like stars."
                },
                "authors": [
                    {
                        "name": "Isabelle Winnick"
                    },
                    {
                        "name": "Jhon Yana Galarza"
                    },
                    {
                        "name": "Henrique Reggiani"
                    },
                    {
                        "name": "Thiago Ferreira"
                    },
                    {
                        "name": "Isabelle Baraffe"
                    },
                    {
                        "name": "Diego Lorenzo-Oliveira"
                    },
                    {
                        "name": "Micaela Oyague"
                    },
                    {
                        "name": "Rita Valle"
                    },
                    {
                        "name": "Renzo Trujillo Diaz"
                    },
                    {
                        "name": "Nathan Leigh"
                    },
                    {
                        "name": "Matias Flores Trivigno"
                    },
                    {
                        "name": "Ricardo Lopez-Valdivia"
                    },
                    {
                        "name": "Gabriela Carvalho Silva"
                    },
                    {
                        "name": "Eder Martioli"
                    },
                    {
                        "name": "Helio Perottoni"
                    }
                ],
                "author_detail": {
                    "name": "Helio Perottoni"
                },
                "author": "Helio Perottoni",
                "arxiv_comment": "32 pages, 12 figures, 9 tables, submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16499v1",
                "updated": "2025-08-22T16:25:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    25,
                    8,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T16:25:08Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    25,
                    8,
                    4,
                    234,
                    0
                ],
                "title": "How Small is Enough? Empirical Evidence of Quantized Small Language\n  Models for Automated Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Small is Enough? Empirical Evidence of Quantized Small Language\n  Models for Automated Program Repair"
                },
                "summary": "Background: Large language models (LLMs) have greatly improved the accuracy\nof automated program repair (APR) methods. However, LLMs are constrained by\nhigh computational resource requirements. Aims: We focus on small language\nmodels (SLMs), which perform well even with limited computational resources\ncompared to LLMs. We aim to evaluate whether SLMs can achieve competitive\nperformance in APR tasks. Method: We conducted experiments on the QuixBugs\nbenchmark to compare the bug-fixing accuracy of SLMs and LLMs. We also analyzed\nthe impact of int8 quantization on APR performance. Results: The latest SLMs\ncan fix bugs as accurately as--or even more accurately than--LLMs. Also, int8\nquantization had minimal effect on APR accuracy while significantly reducing\nmemory requirements. Conclusions: SLMs present a viable alternative to LLMs for\nAPR, offering competitive accuracy with lower computational costs, and\nquantization can further enhance their efficiency without compromising\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Large language models (LLMs) have greatly improved the accuracy\nof automated program repair (APR) methods. However, LLMs are constrained by\nhigh computational resource requirements. Aims: We focus on small language\nmodels (SLMs), which perform well even with limited computational resources\ncompared to LLMs. We aim to evaluate whether SLMs can achieve competitive\nperformance in APR tasks. Method: We conducted experiments on the QuixBugs\nbenchmark to compare the bug-fixing accuracy of SLMs and LLMs. We also analyzed\nthe impact of int8 quantization on APR performance. Results: The latest SLMs\ncan fix bugs as accurately as--or even more accurately than--LLMs. Also, int8\nquantization had minimal effect on APR accuracy while significantly reducing\nmemory requirements. Conclusions: SLMs present a viable alternative to LLMs for\nAPR, offering competitive accuracy with lower computational costs, and\nquantization can further enhance their efficiency without compromising\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Kazuki Kusama"
                    },
                    {
                        "name": "Honglin Shu"
                    },
                    {
                        "name": "Masanari Kondo"
                    },
                    {
                        "name": "Yasutaka Kamei"
                    }
                ],
                "author_detail": {
                    "name": "Yasutaka Kamei"
                },
                "author": "Yasutaka Kamei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07908v2",
                "updated": "2025-08-22T16:20:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    20,
                    32,
                    4,
                    234,
                    0
                ],
                "published": "2025-07-10T16:39:49Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    16,
                    39,
                    49,
                    3,
                    191,
                    0
                ],
                "title": "Not Only Consistency: Enhance Test-Time Adaptation with Spatio-temporal\n  Inconsistency for Remote Physiological Measurement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not Only Consistency: Enhance Test-Time Adaptation with Spatio-temporal\n  Inconsistency for Remote Physiological Measurement"
                },
                "summary": "Remote physiological measurement (RPM) has emerged as a promising\nnon-invasive method for monitoring physiological signals using the non-contact\ndevice. Although various domain adaptation and generalization methods were\nproposed to promote the adaptability of deep-based RPM models in unseen\ndeployment environments, considerations in aspects such as privacy concerns and\nreal-time adaptation restrict their application in real-world deployment. Thus,\nwe aim to propose a novel fully Test-Time Adaptation (TTA) strategy tailored\nfor RPM tasks in this work. Specifically, based on prior knowledge in\nphysiology and our observations, we noticed not only there is spatio-temporal\nconsistency in the frequency domain of BVP signals, but also that inconsistency\nin the time domain was significant. Given this, by leveraging both consistency\nand inconsistency priors, we introduce an innovative expert knowledge-based\nself-supervised\n\\textbf{C}onsistency-\\textbf{i}n\\textbf{C}onsistency-\\textbf{i}ntegration\n(\\textbf{CiCi}) framework to enhances model adaptation during inference.\nBesides, our approach further incorporates a gradient dynamic control mechanism\nto mitigate potential conflicts between priors, ensuring stable adaptation\nacross instances. Through extensive experiments on five diverse datasets under\nthe TTA protocol, our method consistently outperforms existing techniques,\npresenting state-of-the-art performance in real-time self-supervised adaptation\nwithout accessing source data. The code will be released later.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote physiological measurement (RPM) has emerged as a promising\nnon-invasive method for monitoring physiological signals using the non-contact\ndevice. Although various domain adaptation and generalization methods were\nproposed to promote the adaptability of deep-based RPM models in unseen\ndeployment environments, considerations in aspects such as privacy concerns and\nreal-time adaptation restrict their application in real-world deployment. Thus,\nwe aim to propose a novel fully Test-Time Adaptation (TTA) strategy tailored\nfor RPM tasks in this work. Specifically, based on prior knowledge in\nphysiology and our observations, we noticed not only there is spatio-temporal\nconsistency in the frequency domain of BVP signals, but also that inconsistency\nin the time domain was significant. Given this, by leveraging both consistency\nand inconsistency priors, we introduce an innovative expert knowledge-based\nself-supervised\n\\textbf{C}onsistency-\\textbf{i}n\\textbf{C}onsistency-\\textbf{i}ntegration\n(\\textbf{CiCi}) framework to enhances model adaptation during inference.\nBesides, our approach further incorporates a gradient dynamic control mechanism\nto mitigate potential conflicts between priors, ensuring stable adaptation\nacross instances. Through extensive experiments on five diverse datasets under\nthe TTA protocol, our method consistently outperforms existing techniques,\npresenting state-of-the-art performance in real-time self-supervised adaptation\nwithout accessing source data. The code will be released later."
                },
                "authors": [
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Jiyao Wang"
                    },
                    {
                        "name": "Yuxuan Fan"
                    },
                    {
                        "name": "Can Liu"
                    },
                    {
                        "name": "Houcheng Su"
                    },
                    {
                        "name": "Weichen Guo"
                    },
                    {
                        "name": "Zitong Yu"
                    },
                    {
                        "name": "Dengbo He"
                    },
                    {
                        "name": "Kaishun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kaishun Wu"
                },
                "author": "Kaishun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16495v1",
                "updated": "2025-08-22T16:17:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    17,
                    31,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T16:17:31Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    17,
                    31,
                    4,
                    234,
                    0
                ],
                "title": "Post Hoc Regression Refinement via Pairwise Rankings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post Hoc Regression Refinement via Pairwise Rankings"
                },
                "summary": "Accurate prediction of continuous properties is essential to many scientific\nand engineering tasks. Although deep-learning regressors excel with abundant\nlabels, their accuracy deteriorates in data-scarce regimes. We introduce\nRankRefine, a model-agnostic, plug-and-play post hoc method that refines\nregression with expert knowledge coming from pairwise rankings. Given a query\nitem and a small reference set with known properties, RankRefine combines the\nbase regressor's output with a rank-based estimate via inverse variance\nweighting, requiring no retraining. In molecular property prediction task,\nRankRefine achieves up to 10% relative reduction in mean absolute error using\nonly 20 pairwise comparisons obtained through a general-purpose large language\nmodel (LLM) with no finetuning. As rankings provided by human experts or\ngeneral-purpose LLMs are sufficient for improving regression across diverse\ndomains, RankRefine offers practicality and broad applicability, especially in\nlow-data settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate prediction of continuous properties is essential to many scientific\nand engineering tasks. Although deep-learning regressors excel with abundant\nlabels, their accuracy deteriorates in data-scarce regimes. We introduce\nRankRefine, a model-agnostic, plug-and-play post hoc method that refines\nregression with expert knowledge coming from pairwise rankings. Given a query\nitem and a small reference set with known properties, RankRefine combines the\nbase regressor's output with a rank-based estimate via inverse variance\nweighting, requiring no retraining. In molecular property prediction task,\nRankRefine achieves up to 10% relative reduction in mean absolute error using\nonly 20 pairwise comparisons obtained through a general-purpose large language\nmodel (LLM) with no finetuning. As rankings provided by human experts or\ngeneral-purpose LLMs are sufficient for improving regression across diverse\ndomains, RankRefine offers practicality and broad applicability, especially in\nlow-data settings."
                },
                "authors": [
                    {
                        "name": "Kevin Tirta Wijaya"
                    },
                    {
                        "name": "Michael Sun"
                    },
                    {
                        "name": "Minghao Guo"
                    },
                    {
                        "name": "Hans-Peter Seidel"
                    },
                    {
                        "name": "Wojciech Matusik"
                    },
                    {
                        "name": "Vahid Babaei"
                    }
                ],
                "author_detail": {
                    "name": "Vahid Babaei"
                },
                "author": "Vahid Babaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16491v1",
                "updated": "2025-08-22T16:15:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    15,
                    53,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T16:15:53Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    15,
                    53,
                    4,
                    234,
                    0
                ],
                "title": "Tracking flat bands via phonon-mediated interband scattering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracking flat bands via phonon-mediated interband scattering"
                },
                "summary": "Flat-band (FB) materials have emerged as promising platforms for exploring\nexotic quantum phases. While numerous candidates have recently been identified\nthrough spectroscopic techniques such as angle-resolved photoemission\nspectroscopy, central challenges remain on how to tune FBs towards the Fermi\nlevel $E_F$ and to understand their impact on low-energy excitations probed in\nelectronic transport experiments. Here, we show that, by attributing the\ntemperature dependence of the electrical resistivity at elevated temperatures\nto electron-phonon interband scattering, one can infer the position of FBs near\n$E_F$ across diverse material classes. As charge carriers scatter off phonons,\ninterband transitions into FB states lead to distinctive sub- or superlinear\nresistivity at elevated temperatures, governed by the proximity of the FB to\n$E_F$. Our phenomenological model captures these universal transport behaviors\nobserved across several recently studied FB compounds and offers a simple,\nbroadly applicable method for detecting flat bands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flat-band (FB) materials have emerged as promising platforms for exploring\nexotic quantum phases. While numerous candidates have recently been identified\nthrough spectroscopic techniques such as angle-resolved photoemission\nspectroscopy, central challenges remain on how to tune FBs towards the Fermi\nlevel $E_F$ and to understand their impact on low-energy excitations probed in\nelectronic transport experiments. Here, we show that, by attributing the\ntemperature dependence of the electrical resistivity at elevated temperatures\nto electron-phonon interband scattering, one can infer the position of FBs near\n$E_F$ across diverse material classes. As charge carriers scatter off phonons,\ninterband transitions into FB states lead to distinctive sub- or superlinear\nresistivity at elevated temperatures, governed by the proximity of the FB to\n$E_F$. Our phenomenological model captures these universal transport behaviors\nobserved across several recently studied FB compounds and offers a simple,\nbroadly applicable method for detecting flat bands."
                },
                "authors": [
                    {
                        "name": "Fabian Garmroudi"
                    },
                    {
                        "name": "Xinlin Yan"
                    },
                    {
                        "name": "Silke Paschen"
                    },
                    {
                        "name": "Sean M. Thomas"
                    },
                    {
                        "name": "Eric D. Bauer"
                    },
                    {
                        "name": "Andrej Pustogow"
                    },
                    {
                        "name": "Priscila F. S. Rosa"
                    }
                ],
                "author_detail": {
                    "name": "Priscila F. S. Rosa"
                },
                "author": "Priscila F. S. Rosa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13082v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13082v3",
                "updated": "2025-08-22T16:12:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    12,
                    45,
                    4,
                    234,
                    0
                ],
                "published": "2024-09-19T20:40:52Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    20,
                    40,
                    52,
                    3,
                    263,
                    0
                ],
                "title": "AutoVerus: Automated Proof Generation for Rust Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoVerus: Automated Proof Generation for Rust Code"
                },
                "summary": "Generative AI has shown its values for many software engineering tasks. Still\nin its infancy, large language model (LLM)-based proof generation lags behind\nLLM-based code generation. In this paper, we present AutoVerus. AutoVerus uses\nLLMs to automatically generate correctness proof for Rust code. AutoVerus is\ndesigned to match the unique features of Verus, a verification tool that can\nprove the correctness of Rust code using proofs and specifications also written\nin Rust. AutoVerus consists of a network of LLM agents that are crafted and\norchestrated to mimic human experts' three phases of proof construction:\npreliminary proof generation, proof refinement guided by generic tips, and\nproof debugging guided by verification errors. To thoroughly evaluate AutoVerus\nand help foster future research in this direction, we have built a benchmark\nsuite of 150 non-trivial proof tasks, based on existing code-generation\nbenchmarks and verification benchmarks. Our evaluation shows that AutoVerus can\nautomatically generate correct proof for more than 90% of them, with more than\nhalf of them tackled in less than 30 seconds or 3 LLM calls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI has shown its values for many software engineering tasks. Still\nin its infancy, large language model (LLM)-based proof generation lags behind\nLLM-based code generation. In this paper, we present AutoVerus. AutoVerus uses\nLLMs to automatically generate correctness proof for Rust code. AutoVerus is\ndesigned to match the unique features of Verus, a verification tool that can\nprove the correctness of Rust code using proofs and specifications also written\nin Rust. AutoVerus consists of a network of LLM agents that are crafted and\norchestrated to mimic human experts' three phases of proof construction:\npreliminary proof generation, proof refinement guided by generic tips, and\nproof debugging guided by verification errors. To thoroughly evaluate AutoVerus\nand help foster future research in this direction, we have built a benchmark\nsuite of 150 non-trivial proof tasks, based on existing code-generation\nbenchmarks and verification benchmarks. Our evaluation shows that AutoVerus can\nautomatically generate correct proof for more than 90% of them, with more than\nhalf of them tackled in less than 30 seconds or 3 LLM calls."
                },
                "authors": [
                    {
                        "name": "Chenyuan Yang"
                    },
                    {
                        "name": "Xuheng Li"
                    },
                    {
                        "name": "Md Rakib Hossain Misu"
                    },
                    {
                        "name": "Jianan Yao"
                    },
                    {
                        "name": "Weidong Cui"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Chris Hawblitzel"
                    },
                    {
                        "name": "Shuvendu Lahiri"
                    },
                    {
                        "name": "Jacob R. Lorch"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Ziqiao Zhou"
                    },
                    {
                        "name": "Shan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shan Lu"
                },
                "author": "Shan Lu",
                "arxiv_doi": "10.1145/3763174",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3763174",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.13082v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13082v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "OOPSLA 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14879v2",
                "updated": "2025-08-22T16:12:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    12,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-20T17:50:15Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    50,
                    15,
                    2,
                    232,
                    0
                ],
                "title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds"
                },
                "summary": "Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding. The project homepage is\navailable at \\href{https://daibingquan.github.io/MeshCoder}{this link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding. The project homepage is\navailable at \\href{https://daibingquan.github.io/MeshCoder}{this link}."
                },
                "authors": [
                    {
                        "name": "Bingquan Dai"
                    },
                    {
                        "name": "Li Ray Luo"
                    },
                    {
                        "name": "Qihong Tang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xinyu Lian"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Minghan Qin"
                    },
                    {
                        "name": "Xudong Xu"
                    },
                    {
                        "name": "Bo Dai"
                    },
                    {
                        "name": "Haoqian Wang"
                    },
                    {
                        "name": "Zhaoyang Lyu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10969v2",
                "updated": "2025-08-22T16:11:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    11,
                    58,
                    4,
                    234,
                    0
                ],
                "published": "2024-09-17T08:11:07Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    11,
                    7,
                    1,
                    261,
                    0
                ],
                "title": "Enhancing Code-switched Text-to-Speech Synthesis Capability in Large\n  Language Models with only Monolingual Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Code-switched Text-to-Speech Synthesis Capability in Large\n  Language Models with only Monolingual Corpora"
                },
                "summary": "While Large Language Models (LLMs) have shown potential in speech generation\nand recognition, their applications are mainly confined to monolingual\nscenarios, with limited explorations in code-switched (CS) contexts. In this\npaper, we propose a Code-Switched Large Language Model (CS-LLM) to enhance the\ncode-switched text-to-speech synthesis (CS TTS) capability in LLMs with only\nmonolingual corpora. Specifically, we begin by enhancing the multilingual\nspeech processing ability of LLMs through multilingual speech recognition and\nsynthesis tasks. Then, we develop an effective code-switched (CS) data\nconstruction strategy that splits and concatenates words from different\nmonolingual speech corpora to equip LLMs with improved CS TTS ability.\nExperiments show that our approach outperforms baselines in CS TTS in terms of\nnaturalness, speaker consistency and similarity even with limited data.\nAdditionally, the constructed CS data further improves multilingual speech\nsynthesis and recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have shown potential in speech generation\nand recognition, their applications are mainly confined to monolingual\nscenarios, with limited explorations in code-switched (CS) contexts. In this\npaper, we propose a Code-Switched Large Language Model (CS-LLM) to enhance the\ncode-switched text-to-speech synthesis (CS TTS) capability in LLMs with only\nmonolingual corpora. Specifically, we begin by enhancing the multilingual\nspeech processing ability of LLMs through multilingual speech recognition and\nsynthesis tasks. Then, we develop an effective code-switched (CS) data\nconstruction strategy that splits and concatenates words from different\nmonolingual speech corpora to equip LLMs with improved CS TTS ability.\nExperiments show that our approach outperforms baselines in CS TTS in terms of\nnaturalness, speaker consistency and similarity even with limited data.\nAdditionally, the constructed CS data further improves multilingual speech\nsynthesis and recognition."
                },
                "authors": [
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Daxin Tan"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Xiao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Chen"
                },
                "author": "Xiao Chen",
                "arxiv_comment": "Accepted to ASRU2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10848v2",
                "updated": "2025-08-22T16:11:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    11,
                    37,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-14T17:18:35Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    18,
                    35,
                    3,
                    226,
                    0
                ],
                "title": "Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy,\n  Expertise, and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy,\n  Expertise, and Reasoning"
                },
                "summary": "Amidst a shortage of qualified mental health professionals, the integration\nof large language models (LLMs) into psychological applications offers a\npromising way to alleviate the growing burden of mental health disorders.\nRecent reasoning-augmented LLMs have achieved remarkable performance in\nmathematics and programming, while research in the psychological domain has\npredominantly emphasized emotional support and empathetic dialogue, with\nlimited attention to reasoning mechanisms that are beneficial to generating\nreliable responses. Therefore, in this paper, we propose Psyche-R1, the first\nChinese psychological LLM that jointly integrates empathy, psychological\nexpertise, and reasoning, built upon a novel data curation pipeline.\nSpecifically, we design a comprehensive data synthesis pipeline that produces\nover 75k high-quality psychological questions paired with detailed rationales,\ngenerated through chain-of-thought (CoT) reasoning and iterative\nprompt-rationale optimization, along with 73k empathetic dialogues.\nSubsequently, we employ a hybrid training strategy wherein challenging samples\nare identified through a multi-LLM cross-selection strategy for group relative\npolicy optimization (GRPO) to improve reasoning ability, while the remaining\ndata is used for supervised fine-tuning (SFT) to enhance empathetic response\ngeneration and psychological domain knowledge. Extensive experiment results\ndemonstrate the effectiveness of the Psyche-R1 across several psychological\nbenchmarks, where our 7B Psyche-R1 achieves comparable results to 671B\nDeepSeek-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amidst a shortage of qualified mental health professionals, the integration\nof large language models (LLMs) into psychological applications offers a\npromising way to alleviate the growing burden of mental health disorders.\nRecent reasoning-augmented LLMs have achieved remarkable performance in\nmathematics and programming, while research in the psychological domain has\npredominantly emphasized emotional support and empathetic dialogue, with\nlimited attention to reasoning mechanisms that are beneficial to generating\nreliable responses. Therefore, in this paper, we propose Psyche-R1, the first\nChinese psychological LLM that jointly integrates empathy, psychological\nexpertise, and reasoning, built upon a novel data curation pipeline.\nSpecifically, we design a comprehensive data synthesis pipeline that produces\nover 75k high-quality psychological questions paired with detailed rationales,\ngenerated through chain-of-thought (CoT) reasoning and iterative\nprompt-rationale optimization, along with 73k empathetic dialogues.\nSubsequently, we employ a hybrid training strategy wherein challenging samples\nare identified through a multi-LLM cross-selection strategy for group relative\npolicy optimization (GRPO) to improve reasoning ability, while the remaining\ndata is used for supervised fine-tuning (SFT) to enhance empathetic response\ngeneration and psychological domain knowledge. Extensive experiment results\ndemonstrate the effectiveness of the Psyche-R1 across several psychological\nbenchmarks, where our 7B Psyche-R1 achieves comparable results to 671B\nDeepSeek-R1."
                },
                "authors": [
                    {
                        "name": "Chongyuan Dai"
                    },
                    {
                        "name": "Jinpeng Hu"
                    },
                    {
                        "name": "Hongchang Shi"
                    },
                    {
                        "name": "Zhuo Li"
                    },
                    {
                        "name": "Xun Yang"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24102v3",
                "updated": "2025-08-22T15:58:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    58,
                    31,
                    4,
                    234,
                    0
                ],
                "published": "2025-03-31T13:56:03Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    56,
                    3,
                    0,
                    90,
                    0
                ],
                "title": "Is Small Language Model the Silver Bullet to Low-Resource Languages\n  Machine Translation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Small Language Model the Silver Bullet to Low-Resource Languages\n  Machine Translation?"
                },
                "summary": "Low-resource languages (LRLs) lack sufficient linguistic resources and are\nunderrepresented in benchmark datasets, resulting in persistently lower\ntranslation quality than high-resource languages, especially in\nprivacy-sensitive and resource-limited contexts. Firstly, this study\nsystematically evaluates state-of-the-art smaller Large Language Models in 200\nlanguages using the FLORES-200 benchmark, highlighting persistent deficiencies\nand disparities in the translation of LRLs. To mitigate these limitations, we\ninvestigate knowledge distillation from large pre-trained teacher models to\nSmall Language Models (SLMs) through supervised fine-tuning. The results show\nsubstantial improvements; for example, the translation performance of English\nto Luxembourgish (EN to LB), measured by the LLM-as-a-Judge score, increases\nfrom 0.36 to 0.89 in the validation set for Llama-3.2-3B. We further\ninvestigate various fine-tuning configurations and tasks to clarify the\ntrade-offs between data scale and training efficiency, verify that the model\nretains its general capabilities without significant catastrophic forgetting\nafter training, and explore the distillation benefits to other LRLs on SLMs\n(Khasi, Assamese, and Ukrainian). In general, this work exposes the limitations\nand fairness issues of current SLMs in LRL translation and systematically\nexplores the potential of using the distillation of knowledge from large to\nsmall models, offering practical, empirically grounded recommendations to\nimprove LRL translation systems",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-resource languages (LRLs) lack sufficient linguistic resources and are\nunderrepresented in benchmark datasets, resulting in persistently lower\ntranslation quality than high-resource languages, especially in\nprivacy-sensitive and resource-limited contexts. Firstly, this study\nsystematically evaluates state-of-the-art smaller Large Language Models in 200\nlanguages using the FLORES-200 benchmark, highlighting persistent deficiencies\nand disparities in the translation of LRLs. To mitigate these limitations, we\ninvestigate knowledge distillation from large pre-trained teacher models to\nSmall Language Models (SLMs) through supervised fine-tuning. The results show\nsubstantial improvements; for example, the translation performance of English\nto Luxembourgish (EN to LB), measured by the LLM-as-a-Judge score, increases\nfrom 0.36 to 0.89 in the validation set for Llama-3.2-3B. We further\ninvestigate various fine-tuning configurations and tasks to clarify the\ntrade-offs between data scale and training efficiency, verify that the model\nretains its general capabilities without significant catastrophic forgetting\nafter training, and explore the distillation benefits to other LRLs on SLMs\n(Khasi, Assamese, and Ukrainian). In general, this work exposes the limitations\nand fairness issues of current SLMs in LRL translation and systematically\nexplores the potential of using the distillation of knowledge from large to\nsmall models, offering practical, empirically grounded recommendations to\nimprove LRL translation systems"
                },
                "authors": [
                    {
                        "name": "Yewei Song"
                    },
                    {
                        "name": "Lujun Li"
                    },
                    {
                        "name": "Cedric Lothritz"
                    },
                    {
                        "name": "Saad Ezzini"
                    },
                    {
                        "name": "Lama Sleem"
                    },
                    {
                        "name": "Niccolo Gentile"
                    },
                    {
                        "name": "Radu State"
                    },
                    {
                        "name": "Tegawend F. Bissyand"
                    },
                    {
                        "name": "Jacques Klein"
                    }
                ],
                "author_detail": {
                    "name": "Jacques Klein"
                },
                "author": "Jacques Klein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16484v1",
                "updated": "2025-08-22T15:57:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    57,
                    57,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:57:57Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    57,
                    57,
                    4,
                    234,
                    0
                ],
                "title": "HAMSA: Hijacking Aligned Compact Models via Stealthy Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAMSA: Hijacking Aligned Compact Models via Stealthy Automation"
                },
                "summary": "Large Language Models (LLMs), especially their compact efficiency-oriented\nvariants, remain susceptible to jailbreak attacks that can elicit harmful\noutputs despite extensive alignment efforts. Existing adversarial prompt\ngeneration techniques often rely on manual engineering or rudimentary\nobfuscation, producing low-quality or incoherent text that is easily flagged by\nperplexity-based filters. We present an automated red-teaming framework that\nevolves semantically meaningful and stealthy jailbreak prompts for aligned\ncompact LLMs. The approach employs a multi-stage evolutionary search, where\ncandidate prompts are iteratively refined using a population-based strategy\naugmented with temperature-controlled variability to balance exploration and\ncoherence preservation. This enables the systematic discovery of prompts\ncapable of bypassing alignment safeguards while maintaining natural language\nfluency. We evaluate our method on benchmarks in English (In-The-Wild Jailbreak\nPrompts on LLMs), and a newly curated Arabic one derived from In-The-Wild\nJailbreak Prompts on LLMs and annotated by native Arabic linguists, enabling\nmultilingual assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), especially their compact efficiency-oriented\nvariants, remain susceptible to jailbreak attacks that can elicit harmful\noutputs despite extensive alignment efforts. Existing adversarial prompt\ngeneration techniques often rely on manual engineering or rudimentary\nobfuscation, producing low-quality or incoherent text that is easily flagged by\nperplexity-based filters. We present an automated red-teaming framework that\nevolves semantically meaningful and stealthy jailbreak prompts for aligned\ncompact LLMs. The approach employs a multi-stage evolutionary search, where\ncandidate prompts are iteratively refined using a population-based strategy\naugmented with temperature-controlled variability to balance exploration and\ncoherence preservation. This enables the systematic discovery of prompts\ncapable of bypassing alignment safeguards while maintaining natural language\nfluency. We evaluate our method on benchmarks in English (In-The-Wild Jailbreak\nPrompts on LLMs), and a newly curated Arabic one derived from In-The-Wild\nJailbreak Prompts on LLMs and annotated by native Arabic linguists, enabling\nmultilingual assessment."
                },
                "authors": [
                    {
                        "name": "Alexey Krylov"
                    },
                    {
                        "name": "Iskander Vagizov"
                    },
                    {
                        "name": "Dmitrii Korzh"
                    },
                    {
                        "name": "Maryam Douiba"
                    },
                    {
                        "name": "Azidine Guezzaz"
                    },
                    {
                        "name": "Vladimir Kokh"
                    },
                    {
                        "name": "Sergey D. Erokhin"
                    },
                    {
                        "name": "Elena V. Tutubalina"
                    },
                    {
                        "name": "Oleg Y. Rogov"
                    }
                ],
                "author_detail": {
                    "name": "Oleg Y. Rogov"
                },
                "author": "Oleg Y. Rogov",
                "arxiv_comment": "9 pages, 1 figure; article under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14034v2",
                "updated": "2025-08-22T15:53:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    53,
                    30,
                    4,
                    234,
                    0
                ],
                "published": "2023-09-25T11:01:20Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    11,
                    1,
                    20,
                    0,
                    268,
                    0
                ],
                "title": "A unified worst case for classical simplex and policy iteration pivot\n  rules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A unified worst case for classical simplex and policy iteration pivot\n  rules"
                },
                "summary": "We construct a family of Markov decision processes for which the policy\niteration algorithm needs an exponential number of improving switches with\nDantzig's rule, with Bland's rule, and with the Largest Increase pivot rule.\nThis immediately translates to a family of linear programs for which the\nsimplex algorithm needs an exponential number of pivot steps with the same\nthree pivot rules. Our results yield a unified construction that simultaneously\nreproduces well-known lower bounds for these classical pivot rules, and we are\nable to infer that any (deterministic or randomized) combination of them cannot\navoid an exponential worst-case behavior. Regarding the policy iteration\nalgorithm, pivot rules typically switch multiple edges simultaneously and our\nlower bound for Dantzig's rule and the Largest Increase rule, which perform\nonly single switches, seem novel. Regarding the simplex algorithm, the\nindividual lower bounds were previously obtained separately via deformed\nhypercube constructions. In contrast to previous bounds for the simplex\nalgorithm via Markov decision processes, our rigorous analysis is reasonably\nconcise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We construct a family of Markov decision processes for which the policy\niteration algorithm needs an exponential number of improving switches with\nDantzig's rule, with Bland's rule, and with the Largest Increase pivot rule.\nThis immediately translates to a family of linear programs for which the\nsimplex algorithm needs an exponential number of pivot steps with the same\nthree pivot rules. Our results yield a unified construction that simultaneously\nreproduces well-known lower bounds for these classical pivot rules, and we are\nable to infer that any (deterministic or randomized) combination of them cannot\navoid an exponential worst-case behavior. Regarding the policy iteration\nalgorithm, pivot rules typically switch multiple edges simultaneously and our\nlower bound for Dantzig's rule and the Largest Increase rule, which perform\nonly single switches, seem novel. Regarding the simplex algorithm, the\nindividual lower bounds were previously obtained separately via deformed\nhypercube constructions. In contrast to previous bounds for the simplex\nalgorithm via Markov decision processes, our rigorous analysis is reasonably\nconcise."
                },
                "authors": [
                    {
                        "name": "Yann Disser"
                    },
                    {
                        "name": "Nils Mosis"
                    }
                ],
                "author_detail": {
                    "name": "Nils Mosis"
                },
                "author": "Nils Mosis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90C05 (Primary) 90C40 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; G.1.6; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16481v1",
                "updated": "2025-08-22T15:53:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    53,
                    22,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:53:22Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    53,
                    22,
                    4,
                    234,
                    0
                ],
                "title": "Benchmarking the Robustness of Agentic Systems to Adversarially-Induced\n  Harms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking the Robustness of Agentic Systems to Adversarially-Induced\n  Harms"
                },
                "summary": "Ensuring the safe use of agentic systems requires a thorough understanding of\nthe range of malicious behaviors these systems may exhibit when under attack.\nIn this paper, we evaluate the robustness of LLM-based agentic systems against\nattacks that aim to elicit harmful actions from agents. To this end, we propose\na novel taxonomy of harms for agentic systems and a novel benchmark, BAD-ACTS,\nfor studying the security of agentic systems with respect to a wide range of\nharmful actions. BAD-ACTS consists of 4 implementations of agentic systems in\ndistinct application environments, as well as a dataset of 188 high-quality\nexamples of harmful actions. This enables a comprehensive study of the\nrobustness of agentic systems across a wide range of categories of harmful\nbehaviors, available tools, and inter-agent communication structures. Using\nthis benchmark, we analyze the robustness of agentic systems against an\nattacker that controls one of the agents in the system and aims to manipulate\nother agents to execute a harmful target action. Our results show that the\nattack has a high success rate, demonstrating that even a single adversarial\nagent within the system can have a significant impact on the security. This\nattack remains effective even when agents use a simple prompting-based defense\nstrategy. However, we additionally propose a more effective defense based on\nmessage monitoring. We believe that this benchmark provides a diverse testbed\nfor the security research of agentic systems. The benchmark can be found at\ngithub.com/JNoether/BAD-ACTS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safe use of agentic systems requires a thorough understanding of\nthe range of malicious behaviors these systems may exhibit when under attack.\nIn this paper, we evaluate the robustness of LLM-based agentic systems against\nattacks that aim to elicit harmful actions from agents. To this end, we propose\na novel taxonomy of harms for agentic systems and a novel benchmark, BAD-ACTS,\nfor studying the security of agentic systems with respect to a wide range of\nharmful actions. BAD-ACTS consists of 4 implementations of agentic systems in\ndistinct application environments, as well as a dataset of 188 high-quality\nexamples of harmful actions. This enables a comprehensive study of the\nrobustness of agentic systems across a wide range of categories of harmful\nbehaviors, available tools, and inter-agent communication structures. Using\nthis benchmark, we analyze the robustness of agentic systems against an\nattacker that controls one of the agents in the system and aims to manipulate\nother agents to execute a harmful target action. Our results show that the\nattack has a high success rate, demonstrating that even a single adversarial\nagent within the system can have a significant impact on the security. This\nattack remains effective even when agents use a simple prompting-based defense\nstrategy. However, we additionally propose a more effective defense based on\nmessage monitoring. We believe that this benchmark provides a diverse testbed\nfor the security research of agentic systems. The benchmark can be found at\ngithub.com/JNoether/BAD-ACTS"
                },
                "authors": [
                    {
                        "name": "Jonathan Nther"
                    },
                    {
                        "name": "Adish Singla"
                    },
                    {
                        "name": "Goran Radanovic"
                    }
                ],
                "author_detail": {
                    "name": "Goran Radanovic"
                },
                "author": "Goran Radanovic",
                "arxiv_comment": "52 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16479v1",
                "updated": "2025-08-22T15:51:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    51,
                    33,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:51:33Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    51,
                    33,
                    4,
                    234,
                    0
                ],
                "title": "Disentangled Multi-modal Learning of Histology and Transcriptomics for\n  Cancer Characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangled Multi-modal Learning of Histology and Transcriptomics for\n  Cancer Characterization"
                },
                "summary": "Histopathology remains the gold standard for cancer diagnosis and prognosis.\nWith the advent of transcriptome profiling, multi-modal learning combining\ntranscriptomics with histology offers more comprehensive information. However,\nexisting multi-modal approaches are challenged by intrinsic multi-modal\nheterogeneity, insufficient multi-scale integration, and reliance on paired\ndata, restricting clinical applicability. To address these challenges, we\npropose a disentangled multi-modal framework with four contributions: 1) To\nmitigate multi-modal heterogeneity, we decompose WSIs and transcriptomes into\ntumor and microenvironment subspaces using a disentangled multi-modal fusion\nmodule, and introduce a confidence-guided gradient coordination strategy to\nbalance subspace optimization. 2) To enhance multi-scale integration, we\npropose an inter-magnification gene-expression consistency strategy that aligns\ntranscriptomic signals across WSI magnifications. 3) To reduce dependency on\npaired data, we propose a subspace knowledge distillation strategy enabling\ntranscriptome-agnostic inference through a WSI-only student model. 4) To\nimprove inference efficiency, we propose an informative token aggregation\nmodule that suppresses WSI redundancy while preserving subspace semantics.\nExtensive experiments on cancer diagnosis, prognosis, and survival prediction\ndemonstrate our superiority over state-of-the-art methods across multiple\nsettings. Code is available at\nhttps://github.com/helenypzhang/Disentangled-Multimodal-Learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Histopathology remains the gold standard for cancer diagnosis and prognosis.\nWith the advent of transcriptome profiling, multi-modal learning combining\ntranscriptomics with histology offers more comprehensive information. However,\nexisting multi-modal approaches are challenged by intrinsic multi-modal\nheterogeneity, insufficient multi-scale integration, and reliance on paired\ndata, restricting clinical applicability. To address these challenges, we\npropose a disentangled multi-modal framework with four contributions: 1) To\nmitigate multi-modal heterogeneity, we decompose WSIs and transcriptomes into\ntumor and microenvironment subspaces using a disentangled multi-modal fusion\nmodule, and introduce a confidence-guided gradient coordination strategy to\nbalance subspace optimization. 2) To enhance multi-scale integration, we\npropose an inter-magnification gene-expression consistency strategy that aligns\ntranscriptomic signals across WSI magnifications. 3) To reduce dependency on\npaired data, we propose a subspace knowledge distillation strategy enabling\ntranscriptome-agnostic inference through a WSI-only student model. 4) To\nimprove inference efficiency, we propose an informative token aggregation\nmodule that suppresses WSI redundancy while preserving subspace semantics.\nExtensive experiments on cancer diagnosis, prognosis, and survival prediction\ndemonstrate our superiority over state-of-the-art methods across multiple\nsettings. Code is available at\nhttps://github.com/helenypzhang/Disentangled-Multimodal-Learning."
                },
                "authors": [
                    {
                        "name": "Yupei Zhang"
                    },
                    {
                        "name": "Xiaofei Wang"
                    },
                    {
                        "name": "Anran Liu"
                    },
                    {
                        "name": "Lequan Yu"
                    },
                    {
                        "name": "Chao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chao Li"
                },
                "author": "Chao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16478v1",
                "updated": "2025-08-22T15:47:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    47,
                    17,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:47:17Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    47,
                    17,
                    4,
                    234,
                    0
                ],
                "title": "LLM-as-classifier: Semi-Supervised, Iterative Framework for Hierarchical\n  Text Classification using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-classifier: Semi-Supervised, Iterative Framework for Hierarchical\n  Text Classification using Large Language Models"
                },
                "summary": "The advent of Large Language Models (LLMs) has provided unprecedented\ncapabilities for analyzing unstructured text data. However, deploying these\nmodels as reliable, robust, and scalable classifiers in production environments\npresents significant methodological challenges. Standard fine-tuning approaches\ncan be resource-intensive and often struggle with the dynamic nature of\nreal-world data distributions, which is common in the industry. In this paper,\nwe propose a comprehensive, semi-supervised framework that leverages the zero-\nand few-shot capabilities of LLMs for building hierarchical text classifiers as\na framework for a solution to these industry-wide challenges. Our methodology\nemphasizes an iterative, human-in-the-loop process that begins with domain\nknowledge elicitation and progresses through prompt refinement, hierarchical\nexpansion, and multi-faceted validation. We introduce techniques for assessing\nand mitigating sequence-based biases and outline a protocol for continuous\nmonitoring and adaptation. This framework is designed to bridge the gap between\nthe raw power of LLMs and the practical need for accurate, interpretable, and\nmaintainable classification systems in industry applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has provided unprecedented\ncapabilities for analyzing unstructured text data. However, deploying these\nmodels as reliable, robust, and scalable classifiers in production environments\npresents significant methodological challenges. Standard fine-tuning approaches\ncan be resource-intensive and often struggle with the dynamic nature of\nreal-world data distributions, which is common in the industry. In this paper,\nwe propose a comprehensive, semi-supervised framework that leverages the zero-\nand few-shot capabilities of LLMs for building hierarchical text classifiers as\na framework for a solution to these industry-wide challenges. Our methodology\nemphasizes an iterative, human-in-the-loop process that begins with domain\nknowledge elicitation and progresses through prompt refinement, hierarchical\nexpansion, and multi-faceted validation. We introduce techniques for assessing\nand mitigating sequence-based biases and outline a protocol for continuous\nmonitoring and adaptation. This framework is designed to bridge the gap between\nthe raw power of LLMs and the practical need for accurate, interpretable, and\nmaintainable classification systems in industry applications."
                },
                "authors": [
                    {
                        "name": "Doohee You"
                    },
                    {
                        "name": "Andy Parisi"
                    },
                    {
                        "name": "Zach Vander Velden"
                    },
                    {
                        "name": "Lara Dantas Inojosa"
                    }
                ],
                "author_detail": {
                    "name": "Lara Dantas Inojosa"
                },
                "author": "Lara Dantas Inojosa",
                "arxiv_comment": "20 pages excluding reference list, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12983v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12983v2",
                "updated": "2025-08-22T15:39:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    39,
                    27,
                    4,
                    234,
                    0
                ],
                "published": "2024-12-17T15:02:33Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    2,
                    33,
                    1,
                    352,
                    0
                ],
                "title": "Exploring natural variation in tendon constitutive parameters via\n  Bayesian data selection and mixed effects models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring natural variation in tendon constitutive parameters via\n  Bayesian data selection and mixed effects models"
                },
                "summary": "Combining microstructural mechanical models with experimental data enhances\nour understanding of the mechanics of soft tissue, such as tendons. In previous\nwork, a Bayesian framework was used to infer constitutive parameters from\nuniaxial stress-strain experiments on horse tendons, specifically the\nsuperficial digital flexor tendon (SDFT) and common digital extensor tendon\n(CDET), on a per-experiment basis. Here, we extend this analysis to investigate\nthe natural variation of these parameters across a population of horses. Using\na Bayesian mixed effects model, we infer population distributions of these\nparameters. Given that the chosen hyperelastic model does not account for\ntendon damage, careful data selection is necessary. Avoiding ad hoc methods, we\nintroduce a hierarchical Bayesian data selection method. This two-stage\napproach selects data per experiment, and integrates data weightings into the\nBayesian mixed effects model. Our results indicate that the CDET is stiffer\nthan the SDFT, likely due to a higher collagen volume fraction. The modes of\nthe parameter distributions yield estimates of the product of the collagen\nvolume fraction and Young's modulus as 811.5 MPa for the SDFT and 1430.2 MPa\nfor the CDET. This suggests that positional tendons have stiffer collagen\nfibrils and/or higher collagen volume density than energy-storing tendons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining microstructural mechanical models with experimental data enhances\nour understanding of the mechanics of soft tissue, such as tendons. In previous\nwork, a Bayesian framework was used to infer constitutive parameters from\nuniaxial stress-strain experiments on horse tendons, specifically the\nsuperficial digital flexor tendon (SDFT) and common digital extensor tendon\n(CDET), on a per-experiment basis. Here, we extend this analysis to investigate\nthe natural variation of these parameters across a population of horses. Using\na Bayesian mixed effects model, we infer population distributions of these\nparameters. Given that the chosen hyperelastic model does not account for\ntendon damage, careful data selection is necessary. Avoiding ad hoc methods, we\nintroduce a hierarchical Bayesian data selection method. This two-stage\napproach selects data per experiment, and integrates data weightings into the\nBayesian mixed effects model. Our results indicate that the CDET is stiffer\nthan the SDFT, likely due to a higher collagen volume fraction. The modes of\nthe parameter distributions yield estimates of the product of the collagen\nvolume fraction and Young's modulus as 811.5 MPa for the SDFT and 1430.2 MPa\nfor the CDET. This suggests that positional tendons have stiffer collagen\nfibrils and/or higher collagen volume density than energy-storing tendons."
                },
                "authors": [
                    {
                        "name": "James Casey"
                    },
                    {
                        "name": "Jessica Forsyth"
                    },
                    {
                        "name": "Timothy Waite"
                    },
                    {
                        "name": "Simon Cotter"
                    },
                    {
                        "name": "Tom Shearer"
                    }
                ],
                "author_detail": {
                    "name": "Tom Shearer"
                },
                "author": "Tom Shearer",
                "arxiv_comment": "24 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12983v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14313v2",
                "updated": "2025-08-22T15:37:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    37,
                    12,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-19T23:41:15Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    23,
                    41,
                    15,
                    1,
                    231,
                    0
                ],
                "title": "Your Reward Function for RL is Your Best PRM for Search: Unifying RL and\n  Search-Based TTS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Reward Function for RL is Your Best PRM for Search: Unifying RL and\n  Search-Based TTS"
                },
                "summary": "Test-time scaling (TTS) for large language models (LLMs) has thus far fallen\ninto two largely separate paradigms: (1) reinforcement learning (RL) methods\nthat optimize sparse outcome-based rewards, yet suffer from instability and low\nsample efficiency; and (2) search-based techniques guided by independently\ntrained, static process reward models (PRMs), which require expensive human- or\nLLM-generated labels and often degrade under distribution shifts. In this\npaper, we introduce AIRL-S, the first natural unification of RL-based and\nsearch-based TTS. Central to AIRL-S is the insight that the reward function\nlearned during RL training inherently represents the ideal PRM for guiding\ndownstream search. Specifically, we leverage adversarial inverse reinforcement\nlearning (AIRL) combined with group relative policy optimization (GRPO) to\nlearn a dense, dynamic PRM directly from correct reasoning traces, entirely\neliminating the need for labeled intermediate process data. At inference, the\nresulting PRM simultaneously serves as the critic for RL rollouts and as a\nheuristic to effectively guide search procedures, facilitating robust reasoning\nchain extension, mitigating reward hacking, and enhancing cross-task\ngeneralization. Experimental results across eight benchmarks, including\nmathematics, scientific reasoning, and code generation, demonstrate that our\nunified approach improves performance by 9 % on average over the base model,\nmatching GPT-4o. Furthermore, when integrated into multiple search algorithms,\nour PRM consistently outperforms all baseline PRMs trained with labeled data.\nThese results underscore that, indeed, your reward function for RL is your best\nPRM for search, providing a robust and cost-effective solution to complex\nreasoning tasks in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) for large language models (LLMs) has thus far fallen\ninto two largely separate paradigms: (1) reinforcement learning (RL) methods\nthat optimize sparse outcome-based rewards, yet suffer from instability and low\nsample efficiency; and (2) search-based techniques guided by independently\ntrained, static process reward models (PRMs), which require expensive human- or\nLLM-generated labels and often degrade under distribution shifts. In this\npaper, we introduce AIRL-S, the first natural unification of RL-based and\nsearch-based TTS. Central to AIRL-S is the insight that the reward function\nlearned during RL training inherently represents the ideal PRM for guiding\ndownstream search. Specifically, we leverage adversarial inverse reinforcement\nlearning (AIRL) combined with group relative policy optimization (GRPO) to\nlearn a dense, dynamic PRM directly from correct reasoning traces, entirely\neliminating the need for labeled intermediate process data. At inference, the\nresulting PRM simultaneously serves as the critic for RL rollouts and as a\nheuristic to effectively guide search procedures, facilitating robust reasoning\nchain extension, mitigating reward hacking, and enhancing cross-task\ngeneralization. Experimental results across eight benchmarks, including\nmathematics, scientific reasoning, and code generation, demonstrate that our\nunified approach improves performance by 9 % on average over the base model,\nmatching GPT-4o. Furthermore, when integrated into multiple search algorithms,\nour PRM consistently outperforms all baseline PRMs trained with labeled data.\nThese results underscore that, indeed, your reward function for RL is your best\nPRM for search, providing a robust and cost-effective solution to complex\nreasoning tasks in LLMs."
                },
                "authors": [
                    {
                        "name": "Can Jin"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Qixin Zhang"
                    },
                    {
                        "name": "Hongwu Peng"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Marco Pavone"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Zhang-Wei Hong"
                    },
                    {
                        "name": "Tong Che"
                    },
                    {
                        "name": "Dimitris N. Metaxas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris N. Metaxas"
                },
                "author": "Dimitris N. Metaxas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10991v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10991v2",
                "updated": "2025-08-22T15:35:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    35,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-14T18:00:25Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    18,
                    0,
                    25,
                    3,
                    226,
                    0
                ],
                "title": "MCP-Guard: A Defense Framework for Model Context Protocol Integrity in\n  Large Language Model Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCP-Guard: A Defense Framework for Model Context Protocol Integrity in\n  Large Language Model Applications"
                },
                "summary": "The integration of Large Language Models (LLMs) with external tools via\nprotocols such as the Model Context Protocol (MCP) introduces critical security\nvulnerabilities, including prompt injection, data exfiltration, and other\nthreats. To counter these challenges, we propose MCP-Guard, a robust, layered\ndefense architecture designed for LLM--tool interactions. MCP-Guard employs a\nthree-stage detection pipeline that balances efficiency with accuracy: it\nprogresses from lightweight static scanning for overt threats and a deep neural\ndetector for semantic attacks, to our fine-tuned E5-based model achieves\n(96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM\narbitrator synthesizes these signals to deliver the final decision while\nminimizing false positives. To facilitate rigorous training and evaluation, we\nalso introduce MCP-AttackBench, a comprehensive benchmark of over 70,000\nsamples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench\nsimulates diverse, real-world attack vectors in the MCP format, providing a\nfoundation for future research into securing LLM-tool ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) with external tools via\nprotocols such as the Model Context Protocol (MCP) introduces critical security\nvulnerabilities, including prompt injection, data exfiltration, and other\nthreats. To counter these challenges, we propose MCP-Guard, a robust, layered\ndefense architecture designed for LLM--tool interactions. MCP-Guard employs a\nthree-stage detection pipeline that balances efficiency with accuracy: it\nprogresses from lightweight static scanning for overt threats and a deep neural\ndetector for semantic attacks, to our fine-tuned E5-based model achieves\n(96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM\narbitrator synthesizes these signals to deliver the final decision while\nminimizing false positives. To facilitate rigorous training and evaluation, we\nalso introduce MCP-AttackBench, a comprehensive benchmark of over 70,000\nsamples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench\nsimulates diverse, real-world attack vectors in the MCP format, providing a\nfoundation for future research into securing LLM-tool ecosystems."
                },
                "authors": [
                    {
                        "name": "Wenpeng Xing"
                    },
                    {
                        "name": "Zhonghao Qi"
                    },
                    {
                        "name": "Yupeng Qin"
                    },
                    {
                        "name": "Yilin Li"
                    },
                    {
                        "name": "Caini Chang"
                    },
                    {
                        "name": "Jiahui Yu"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Zhenzhen Xie"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10991v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10991v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16464v1",
                "updated": "2025-08-22T15:30:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    30,
                    40,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:30:40Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    30,
                    40,
                    4,
                    234,
                    0
                ],
                "title": "What makes an entity salient in discourse?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What makes an entity salient in discourse?"
                },
                "summary": "Entities in discourse vary broadly in salience: main participants, objects\nand locations are noticeable and memorable, while tangential ones are less\nimportant and quickly forgotten, raising questions about how humans signal and\ninfer relative salience. Using a graded operationalization of salience based on\nsummary-worthiness in multiple summaries of a discourse, this paper explores\ndata from 24 spoken and written genres of English to extract a multifactorial\ncomplex of overt and implicit linguistic cues, such as recurring subjecthood or\ndefiniteness, discourse relations and hierarchy across utterances, as well as\npragmatic functional inferences based on genre and communicative intent.\nTackling the question 'how is the degree of salience expressed for each and\nevery entity mentioned?' our results show that while previous approaches to\nsalience all correlate with our salience scores to some extent, no single\ngeneralization is without exceptions, and the phenomenon cuts across all levels\nof linguistic representation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entities in discourse vary broadly in salience: main participants, objects\nand locations are noticeable and memorable, while tangential ones are less\nimportant and quickly forgotten, raising questions about how humans signal and\ninfer relative salience. Using a graded operationalization of salience based on\nsummary-worthiness in multiple summaries of a discourse, this paper explores\ndata from 24 spoken and written genres of English to extract a multifactorial\ncomplex of overt and implicit linguistic cues, such as recurring subjecthood or\ndefiniteness, discourse relations and hierarchy across utterances, as well as\npragmatic functional inferences based on genre and communicative intent.\nTackling the question 'how is the degree of salience expressed for each and\nevery entity mentioned?' our results show that while previous approaches to\nsalience all correlate with our salience scores to some extent, no single\ngeneralization is without exceptions, and the phenomenon cuts across all levels\nof linguistic representation."
                },
                "authors": [
                    {
                        "name": "Amir Zeldes"
                    },
                    {
                        "name": "Jessica Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jessica Lin"
                },
                "author": "Jessica Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16463v1",
                "updated": "2025-08-22T15:25:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    25,
                    40,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:25:40Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    25,
                    40,
                    4,
                    234,
                    0
                ],
                "title": "Modular Embedding Recomposition for Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Embedding Recomposition for Incremental Learning"
                },
                "summary": "The advent of pre-trained Vision-Language Models (VLMs) has significantly\ntransformed Continual Learning (CL), mainly due to their zero-shot\nclassification abilities. Such proficiency makes VLMs well-suited for\nreal-world applications, enabling robust performance on novel unseen classes\nwithout requiring adaptation. However, fine-tuning remains essential when\ndownstream tasks deviate significantly from the pre-training domain. Prior CL\napproaches primarily focus on preserving the zero-shot capabilities of VLMs\nduring incremental fine-tuning on a downstream task. We take a step further by\ndevising an approach that transforms preservation into enhancement of the\nzero-shot capabilities of VLMs. Our approach, named MoDular Embedding\nRecomposition (MoDER), introduces a modular framework that trains multiple\ntextual experts, each specialized in a single seen class, and stores them in a\nfoundational hub. At inference time, for each unseen class, we query the hub\nand compose the retrieved experts to synthesize a refined prototype that\nimproves classification. We show the effectiveness of our method across two\npopular zero-shot incremental protocols, Class-IL and MTIL, comprising a total\nof 14 datasets. The codebase is available at\nhttps://github.com/aimagelab/mammoth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of pre-trained Vision-Language Models (VLMs) has significantly\ntransformed Continual Learning (CL), mainly due to their zero-shot\nclassification abilities. Such proficiency makes VLMs well-suited for\nreal-world applications, enabling robust performance on novel unseen classes\nwithout requiring adaptation. However, fine-tuning remains essential when\ndownstream tasks deviate significantly from the pre-training domain. Prior CL\napproaches primarily focus on preserving the zero-shot capabilities of VLMs\nduring incremental fine-tuning on a downstream task. We take a step further by\ndevising an approach that transforms preservation into enhancement of the\nzero-shot capabilities of VLMs. Our approach, named MoDular Embedding\nRecomposition (MoDER), introduces a modular framework that trains multiple\ntextual experts, each specialized in a single seen class, and stores them in a\nfoundational hub. At inference time, for each unseen class, we query the hub\nand compose the retrieved experts to synthesize a refined prototype that\nimproves classification. We show the effectiveness of our method across two\npopular zero-shot incremental protocols, Class-IL and MTIL, comprising a total\nof 14 datasets. The codebase is available at\nhttps://github.com/aimagelab/mammoth."
                },
                "authors": [
                    {
                        "name": "Aniello Panariello"
                    },
                    {
                        "name": "Emanuele Frascaroli"
                    },
                    {
                        "name": "Pietro Buzzega"
                    },
                    {
                        "name": "Lorenzo Bonicelli"
                    },
                    {
                        "name": "Angelo Porrello"
                    },
                    {
                        "name": "Simone Calderara"
                    }
                ],
                "author_detail": {
                    "name": "Simone Calderara"
                },
                "author": "Simone Calderara",
                "arxiv_comment": "Accepted to the 36th British Machine Vision Conference (BMVC 2025),\n  Sheffield, UK",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16459v1",
                "updated": "2025-08-22T15:20:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    20,
                    43,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:20:43Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    20,
                    43,
                    4,
                    234,
                    0
                ],
                "title": "GPL-SLAM: A Laser SLAM Framework with Gaussian Process Based Extended\n  Landmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPL-SLAM: A Laser SLAM Framework with Gaussian Process Based Extended\n  Landmarks"
                },
                "summary": "We present a novel Simultaneous Localization and Mapping (SLAM) method that\nemploys Gaussian Process (GP) based landmark (object) representations. Instead\nof conventional grid maps or point cloud registration, we model the environment\non a per object basis using GP based contour representations. These contours\nare updated online through a recursive scheme, enabling efficient memory usage.\nThe SLAM problem is formulated within a fully Bayesian framework, allowing\njoint inference over the robot pose and object based map. This representation\nprovides semantic information such as the number of objects and their areas,\nwhile also supporting probabilistic measurement to object associations.\nFurthermore, the GP based contours yield confidence bounds on object shapes,\noffering valuable information for downstream tasks like safe navigation and\nexploration. We validate our method on synthetic and real world experiments,\nand show that it delivers accurate localization and mapping performance across\ndiverse structured environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel Simultaneous Localization and Mapping (SLAM) method that\nemploys Gaussian Process (GP) based landmark (object) representations. Instead\nof conventional grid maps or point cloud registration, we model the environment\non a per object basis using GP based contour representations. These contours\nare updated online through a recursive scheme, enabling efficient memory usage.\nThe SLAM problem is formulated within a fully Bayesian framework, allowing\njoint inference over the robot pose and object based map. This representation\nprovides semantic information such as the number of objects and their areas,\nwhile also supporting probabilistic measurement to object associations.\nFurthermore, the GP based contours yield confidence bounds on object shapes,\noffering valuable information for downstream tasks like safe navigation and\nexploration. We validate our method on synthetic and real world experiments,\nand show that it delivers accurate localization and mapping performance across\ndiverse structured environments."
                },
                "authors": [
                    {
                        "name": "Ali Emre Balc"
                    },
                    {
                        "name": "Erhan Ege Keyvan"
                    },
                    {
                        "name": "Emre zkan"
                    }
                ],
                "author_detail": {
                    "name": "Emre zkan"
                },
                "arxiv_affiliation": "Middle East Technical University",
                "author": "Emre zkan",
                "arxiv_comment": "Authors Ali Emre Balc{\\i} and Erhan Ege Keyvan contributed equally to\n  this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16456v1",
                "updated": "2025-08-22T15:15:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    15,
                    38,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:15:38Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    15,
                    38,
                    4,
                    234,
                    0
                ],
                "title": "A Probabilistic Inference Scaling Theory for LLM Self-Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Probabilistic Inference Scaling Theory for LLM Self-Correction"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the capability to refine their\ngenerated answers through self-correction, enabling continuous performance\nimprovement over multiple rounds. However, the mechanisms underlying how and\nwhy accuracy evolves during this iterative process remain unexplored. To fill\nthis gap, we propose a probabilistic theory to model the dynamics of accuracy\nchange and explain the performance improvements observed in multi-round\nself-correction. Through mathematical derivation, we establish that the\naccuracy after the $t^{th}$ round of self-correction is given by: $Acc_t = Upp\n- \\alpha^t(Upp - Acc_0),$ where $Acc_0$ denotes the initial accuracy, $Upp$\nrepresents the upper bound of accuracy convergence, and $\\alpha$ determines the\nrate of convergence. Based on our theory, these parameters can be calculated\nand the predicted accuracy curve then can be obtained through only a single\nround of self-correction. Extensive experiments across diverse models and\ndatasets demonstrate that our theoretical predictions align closely with\nempirical accuracy curves, validating the effectiveness of the theory. Our work\nprovides a theoretical foundation for understanding LLM self-correction, thus\npaving the way for further explorations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the capability to refine their\ngenerated answers through self-correction, enabling continuous performance\nimprovement over multiple rounds. However, the mechanisms underlying how and\nwhy accuracy evolves during this iterative process remain unexplored. To fill\nthis gap, we propose a probabilistic theory to model the dynamics of accuracy\nchange and explain the performance improvements observed in multi-round\nself-correction. Through mathematical derivation, we establish that the\naccuracy after the $t^{th}$ round of self-correction is given by: $Acc_t = Upp\n- \\alpha^t(Upp - Acc_0),$ where $Acc_0$ denotes the initial accuracy, $Upp$\nrepresents the upper bound of accuracy convergence, and $\\alpha$ determines the\nrate of convergence. Based on our theory, these parameters can be calculated\nand the predicted accuracy curve then can be obtained through only a single\nround of self-correction. Extensive experiments across diverse models and\ndatasets demonstrate that our theoretical predictions align closely with\nempirical accuracy curves, validating the effectiveness of the theory. Our work\nprovides a theoretical foundation for understanding LLM self-correction, thus\npaving the way for further explorations."
                },
                "authors": [
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Yudong Wang"
                    },
                    {
                        "name": "Ziyao Xu"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16449v1",
                "updated": "2025-08-22T15:08:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    8,
                    34,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:08:34Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    8,
                    34,
                    4,
                    234,
                    0
                ],
                "title": "GreenLLM: SLO-Aware Dynamic Frequency Scaling for Energy-Efficient LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GreenLLM: SLO-Aware Dynamic Frequency Scaling for Energy-Efficient LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are becoming the backbone of modern cloud\nservices, yet their inference costs are dominated by GPU energy. Unlike\ntraditional GPU workloads, LLM inference has two stages with different\ncharacteristics: the prefill phase, which is latency sensitive and scales\nquadratically with prompt length, and the decode phase, which progresses token\nby token with unpredictable length. Current GPU power governors (for example,\nNVIDIA's default) overlook this asymmetry and treat both stages uniformly. The\nresult is mismatched voltage and frequency settings, head-of-line blocking, and\nexcessive energy use.\n  We introduce GreenLLM, an SLO-aware serving framework that minimizes GPU\nenergy by explicitly separating prefill and decode control. At ingress,\nrequests are routed into length-based queues so short prompts avoid\nhead-of-line blocking and TTFT improves. For prefill, GreenLLM collects short\ntraces on a GPU node, fits compact latency-power models over SM frequency, and\nsolves a queueing-aware optimization to select energy-minimal clocks per class.\nDuring decode, a lightweight dual-loop controller tracks throughput (tokens per\nsecond) and adjusts frequency with hysteretic, fine-grained steps to hold tail\nTBT within target bounds. Across Alibaba and Azure trace replays, GreenLLM\nreduces total energy by up to 34 percent versus the default DVFS baseline, with\nno loss of throughput and with less than 3.5 percent additional SLO violations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming the backbone of modern cloud\nservices, yet their inference costs are dominated by GPU energy. Unlike\ntraditional GPU workloads, LLM inference has two stages with different\ncharacteristics: the prefill phase, which is latency sensitive and scales\nquadratically with prompt length, and the decode phase, which progresses token\nby token with unpredictable length. Current GPU power governors (for example,\nNVIDIA's default) overlook this asymmetry and treat both stages uniformly. The\nresult is mismatched voltage and frequency settings, head-of-line blocking, and\nexcessive energy use.\n  We introduce GreenLLM, an SLO-aware serving framework that minimizes GPU\nenergy by explicitly separating prefill and decode control. At ingress,\nrequests are routed into length-based queues so short prompts avoid\nhead-of-line blocking and TTFT improves. For prefill, GreenLLM collects short\ntraces on a GPU node, fits compact latency-power models over SM frequency, and\nsolves a queueing-aware optimization to select energy-minimal clocks per class.\nDuring decode, a lightweight dual-loop controller tracks throughput (tokens per\nsecond) and adjusts frequency with hysteretic, fine-grained steps to hold tail\nTBT within target bounds. Across Alibaba and Azure trace replays, GreenLLM\nreduces total energy by up to 34 percent versus the default DVFS baseline, with\nno loss of throughput and with less than 3.5 percent additional SLO violations."
                },
                "authors": [
                    {
                        "name": "Qunyou Liu"
                    },
                    {
                        "name": "Darong Huang"
                    },
                    {
                        "name": "Marina Zapater"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16447v1",
                "updated": "2025-08-22T15:02:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    2,
                    7,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:02:07Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    2,
                    7,
                    4,
                    234,
                    0
                ],
                "title": "Boardwalk: Towards a Framework for Creating Board Games with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boardwalk: Towards a Framework for Creating Board Games with LLMs"
                },
                "summary": "Implementing board games in code can be a time-consuming task. However, Large\nLanguage Models (LLMs) have been proven effective at generating code for\ndomain-specific tasks with simple contextual information. We aim to investigate\nwhether LLMs can implement digital versions of board games from rules described\nin natural language. This would be a step towards an LLM-assisted framework for\nquick board game code generation. We expect to determine the main challenges\nfor LLMs to implement the board games, and how different approaches and models\ncompare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek\nand ChatGPT) with coding a selection of 12 popular and obscure games in\nfree-form and within Boardwalk, our proposed General Game Playing API. We\nanonymize the games and components to avoid evoking pre-trained LLM knowledge.\nThe implementations are tested for playability and rule compliance. We evaluate\nsuccess rate and common errors across LLMs and game popularity. Our approach\nproves viable, with the best performing model, Claude 3.7 Sonnet, yielding\n55.6\\% of games without any errors. While compliance with the API increases\nerror frequency, the severity of errors is more significantly dependent on the\nLLM. We outline future steps for creating a framework to integrate this\nprocess, making the elaboration of board games more accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementing board games in code can be a time-consuming task. However, Large\nLanguage Models (LLMs) have been proven effective at generating code for\ndomain-specific tasks with simple contextual information. We aim to investigate\nwhether LLMs can implement digital versions of board games from rules described\nin natural language. This would be a step towards an LLM-assisted framework for\nquick board game code generation. We expect to determine the main challenges\nfor LLMs to implement the board games, and how different approaches and models\ncompare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek\nand ChatGPT) with coding a selection of 12 popular and obscure games in\nfree-form and within Boardwalk, our proposed General Game Playing API. We\nanonymize the games and components to avoid evoking pre-trained LLM knowledge.\nThe implementations are tested for playability and rule compliance. We evaluate\nsuccess rate and common errors across LLMs and game popularity. Our approach\nproves viable, with the best performing model, Claude 3.7 Sonnet, yielding\n55.6\\% of games without any errors. While compliance with the API increases\nerror frequency, the severity of errors is more significantly dependent on the\nLLM. We outline future steps for creating a framework to integrate this\nprocess, making the elaboration of board games more accessible."
                },
                "authors": [
                    {
                        "name": "lvaro Guglielmin Becker"
                    },
                    {
                        "name": "Gabriel Bauer de Oliveira"
                    },
                    {
                        "name": "Lana Bertoldo Rossato"
                    },
                    {
                        "name": "Anderson Rocha Tavares"
                    }
                ],
                "author_detail": {
                    "name": "Anderson Rocha Tavares"
                },
                "author": "Anderson Rocha Tavares",
                "arxiv_comment": "Accepted at SBGames 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07767v2",
                "updated": "2025-08-22T15:01:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    1,
                    14,
                    4,
                    234,
                    0
                ],
                "published": "2025-07-10T13:50:07Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    13,
                    50,
                    7,
                    3,
                    191,
                    0
                ],
                "title": "Structured Prompts, Better Outcomes? Exploring the Effects of a\n  Structured Interface with ChatGPT in a Graduate Robotics Course",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Prompts, Better Outcomes? Exploring the Effects of a\n  Structured Interface with ChatGPT in a Graduate Robotics Course"
                },
                "summary": "Prior research shows that how students engage with Large Language Models\n(LLMs) influences their problem-solving and understanding, reinforcing the need\nto support productive LLM-uses that promote learning. This study evaluates the\nimpact of a structured GPT platform designed to promote 'good' prompting\nbehavior with data from 58 students in a graduate-level robotics course. The\nstudents were assigned to either an intervention group using the structured\nplatform or a control group using ChatGPT freely for two practice lab sessions,\nbefore a third session where all students could freely use ChatGPT. We analyzed\nstudent perception (pre-post surveys), prompting behavior (logs), performance\n(task scores), and learning (pre-post tests). Although we found no differences\nin performance or learning between groups, we identified prompting behaviors -\nsuch as having clear prompts focused on understanding code - that were linked\nwith higher learning gains and were more prominent when students used the\nstructured platform. However, such behaviors did not transfer once students\nwere no longer constrained to use the structured platform. Qualitative survey\ndata showed mixed perceptions: some students perceived the value of the\nstructured platform, but most did not perceive its relevance and resisted\nchanging their habits. These findings contribute to ongoing efforts to identify\neffective strategies for integrating LLMs into learning and question the\neffectiveness of bottom-up approaches that temporarily alter user interfaces to\ninfluence students' interaction. Future research could instead explore top-down\nstrategies that address students' motivations and explicitly demonstrate how\ncertain interaction patterns support learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior research shows that how students engage with Large Language Models\n(LLMs) influences their problem-solving and understanding, reinforcing the need\nto support productive LLM-uses that promote learning. This study evaluates the\nimpact of a structured GPT platform designed to promote 'good' prompting\nbehavior with data from 58 students in a graduate-level robotics course. The\nstudents were assigned to either an intervention group using the structured\nplatform or a control group using ChatGPT freely for two practice lab sessions,\nbefore a third session where all students could freely use ChatGPT. We analyzed\nstudent perception (pre-post surveys), prompting behavior (logs), performance\n(task scores), and learning (pre-post tests). Although we found no differences\nin performance or learning between groups, we identified prompting behaviors -\nsuch as having clear prompts focused on understanding code - that were linked\nwith higher learning gains and were more prominent when students used the\nstructured platform. However, such behaviors did not transfer once students\nwere no longer constrained to use the structured platform. Qualitative survey\ndata showed mixed perceptions: some students perceived the value of the\nstructured platform, but most did not perceive its relevance and resisted\nchanging their habits. These findings contribute to ongoing efforts to identify\neffective strategies for integrating LLMs into learning and question the\neffectiveness of bottom-up approaches that temporarily alter user interfaces to\ninfluence students' interaction. Future research could instead explore top-down\nstrategies that address students' motivations and explicitly demonstrate how\ncertain interaction patterns support learning."
                },
                "authors": [
                    {
                        "name": "Jerome Brender"
                    },
                    {
                        "name": "Laila El-Hamamsy"
                    },
                    {
                        "name": "Kim Uittenhove"
                    },
                    {
                        "name": "Francesco Mondada"
                    },
                    {
                        "name": "Engin Bumbacher"
                    }
                ],
                "author_detail": {
                    "name": "Engin Bumbacher"
                },
                "author": "Engin Bumbacher",
                "arxiv_comment": "Accepted, to appear in the proceedings of the EC-TEL 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16446v1",
                "updated": "2025-08-22T15:01:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    1,
                    3,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:01:03Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    1,
                    3,
                    4,
                    234,
                    0
                ],
                "title": "Scalable Bayesian inference on high-dimensional multivariate linear\n  regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Bayesian inference on high-dimensional multivariate linear\n  regression"
                },
                "summary": "We consider jointly estimating the coefficient matrix and the error precision\nmatrix in high-dimensional multivariate linear regression models. Bayesian\nmethods in this context often face computational challenges, leading to\nprevious approaches that either utilize a generalized likelihood without\nensuring the positive definiteness of the precision matrix or rely on\nmaximization algorithms targeting only the posterior mode, thus failing to\naddress uncertainty. In this work, we propose two Bayesian methods: an exact\nmethod and an approximate two-step method. We first propose an exact method\nbased on spike and slab priors for the coefficient matrix and DAG-Wishart prior\nfor the error precision matrix, whose computational complexity is comparable to\nthe state-of-the-art generalized likelihood-based Bayesian method. To further\nenhance scalability, a two-step approach is developed by ignoring the\ndependency structure among response variables. This method estimates the\ncoefficient matrix first, followed by the calculation of the posterior of the\nerror precision matrix based on the estimated errors. We validate the two-step\nmethod by demonstrating (i) selection consistency and posterior convergence\nrates for the coefficient matrix and (ii) selection consistency for the\ndirected acyclic graph (DAG) of errors. We demonstrate the practical\nperformance of proposed methods through synthetic and real data analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider jointly estimating the coefficient matrix and the error precision\nmatrix in high-dimensional multivariate linear regression models. Bayesian\nmethods in this context often face computational challenges, leading to\nprevious approaches that either utilize a generalized likelihood without\nensuring the positive definiteness of the precision matrix or rely on\nmaximization algorithms targeting only the posterior mode, thus failing to\naddress uncertainty. In this work, we propose two Bayesian methods: an exact\nmethod and an approximate two-step method. We first propose an exact method\nbased on spike and slab priors for the coefficient matrix and DAG-Wishart prior\nfor the error precision matrix, whose computational complexity is comparable to\nthe state-of-the-art generalized likelihood-based Bayesian method. To further\nenhance scalability, a two-step approach is developed by ignoring the\ndependency structure among response variables. This method estimates the\ncoefficient matrix first, followed by the calculation of the posterior of the\nerror precision matrix based on the estimated errors. We validate the two-step\nmethod by demonstrating (i) selection consistency and posterior convergence\nrates for the coefficient matrix and (ii) selection consistency for the\ndirected acyclic graph (DAG) of errors. We demonstrate the practical\nperformance of proposed methods through synthetic and real data analysis."
                },
                "authors": [
                    {
                        "name": "Xuan Cao"
                    },
                    {
                        "name": "Kyoungjae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyoungjae Lee"
                },
                "author": "Kyoungjae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16445v1",
                "updated": "2025-08-22T14:59:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    59,
                    35,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T14:59:35Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    59,
                    35,
                    4,
                    234,
                    0
                ],
                "title": "Using LLMs and Essence to Support Software Practice Adoption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs and Essence to Support Software Practice Adoption"
                },
                "summary": "Recent advancements in natural language processing (NLP) have enabled the\ndevelopment of automated tools that support various domains, including software\nengineering. However, while NLP and artificial intelligence (AI) research has\nextensively focused on tasks such as code generation, less attention has been\ngiven to automating support for the adoption of best practices, the evolution\nof ways of working, and the monitoring of process health. This study addresses\nthis gap by exploring the integration of Essence, a standard and thinking\nframework for managing software engineering practices, with large language\nmodels (LLMs). To this end, a specialised chatbot was developed to assist\nstudents and professionals in understanding and applying Essence. The chatbot\nemploys a retrieval-augmented generation (RAG) system to retrieve relevant\ncontextual information from a curated knowledge base. Four different LLMs were\nused to create multiple chatbot configurations, each evaluated both as a base\nmodel and augmented with the RAG system. The system performance was evaluated\nthrough both the relevance of retrieved context and the quality of generated\nresponses. Comparative analysis against the general-purpose LLMs demonstrated\nthat the proposed system consistently outperforms its baseline counterpart in\ndomain-specific tasks. By facilitating access to structured software\nengineering knowledge, this work contributes to bridging the gap between\ntheoretical frameworks and practical application, potentially improving process\nmanagement and the adoption of software development practices. While further\nvalidation through user studies is required, these findings highlight the\npotential of LLM-based automation to enhance learning and decision-making in\nsoftware engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in natural language processing (NLP) have enabled the\ndevelopment of automated tools that support various domains, including software\nengineering. However, while NLP and artificial intelligence (AI) research has\nextensively focused on tasks such as code generation, less attention has been\ngiven to automating support for the adoption of best practices, the evolution\nof ways of working, and the monitoring of process health. This study addresses\nthis gap by exploring the integration of Essence, a standard and thinking\nframework for managing software engineering practices, with large language\nmodels (LLMs). To this end, a specialised chatbot was developed to assist\nstudents and professionals in understanding and applying Essence. The chatbot\nemploys a retrieval-augmented generation (RAG) system to retrieve relevant\ncontextual information from a curated knowledge base. Four different LLMs were\nused to create multiple chatbot configurations, each evaluated both as a base\nmodel and augmented with the RAG system. The system performance was evaluated\nthrough both the relevance of retrieved context and the quality of generated\nresponses. Comparative analysis against the general-purpose LLMs demonstrated\nthat the proposed system consistently outperforms its baseline counterpart in\ndomain-specific tasks. By facilitating access to structured software\nengineering knowledge, this work contributes to bridging the gap between\ntheoretical frameworks and practical application, potentially improving process\nmanagement and the adoption of software development practices. While further\nvalidation through user studies is required, these findings highlight the\npotential of LLM-based automation to enhance learning and decision-making in\nsoftware engineering."
                },
                "authors": [
                    {
                        "name": "Sonia Nicoletti"
                    },
                    {
                        "name": "Paolo Ciancarini"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Ciancarini"
                },
                "author": "Paolo Ciancarini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04916v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04916v4",
                "updated": "2025-08-22T14:52:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    52,
                    2,
                    4,
                    234,
                    0
                ],
                "published": "2025-02-07T13:33:40Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    33,
                    40,
                    4,
                    38,
                    0
                ],
                "title": "Classification or Prompting: A Case Study on Legal Requirements\n  Traceability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification or Prompting: A Case Study on Legal Requirements\n  Traceability"
                },
                "summary": "New regulations are introduced to ensure software development aligns with\nethical concerns and protects public safety. Showing compliance requires\ntracing requirements to legal provisions. Requirements traceability is a key\ntask where engineers must analyze technical requirements against target\nartifacts, often within limited time. Manually analyzing complex systems with\nhundreds of requirements is infeasible. The legal dimension adds challenges\nthat increase effort. In this paper, we investigate two automated solutions\nbased on language models, including large ones (LLMs). The first solution,\nKashif, is a classifier that leverages sentence transformers and semantic\nsimilarity. The second solution, RICE_LRT, prompts a recent generative LLM\nbased on RICE, a prompt engineering framework. On a benchmark dataset, we\nempirically evaluate Kashif and compare it against five different baseline\nclassifiers from the literature. Kashif can identify trace links with a recall\nof 67%, precision of 50%, and F2 score of 63%, outperforming the best baseline\nby a substantial margin of 41 percentage points (pp) in F2. However, on unseen,\nmore complex requirements documents traced to the European General Data\nProtection Regulation (GDPR), Kashif performs poorly, yielding an average\nrecall of 15%, an average precision of 10%, and an average F2 score of 13.5%.\nOn the same documents, however, our RICE solution yields an average recall of\n84%, an average precision of 30%, and an average F2 score of 61%. RICE achieved\na remarkable improvement of 47.5 pp over Kashif in terms of F2 score. Our\nresults suggest that requirements traceability in the legal context cannot be\nsimply addressed by building classifiers, as such solutions do not generalize\nand fail to perform well on complex regulations and requirements. Resorting to\ngenerative LLMs, with careful prompt engineering, is thus a more promising\nalternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New regulations are introduced to ensure software development aligns with\nethical concerns and protects public safety. Showing compliance requires\ntracing requirements to legal provisions. Requirements traceability is a key\ntask where engineers must analyze technical requirements against target\nartifacts, often within limited time. Manually analyzing complex systems with\nhundreds of requirements is infeasible. The legal dimension adds challenges\nthat increase effort. In this paper, we investigate two automated solutions\nbased on language models, including large ones (LLMs). The first solution,\nKashif, is a classifier that leverages sentence transformers and semantic\nsimilarity. The second solution, RICE_LRT, prompts a recent generative LLM\nbased on RICE, a prompt engineering framework. On a benchmark dataset, we\nempirically evaluate Kashif and compare it against five different baseline\nclassifiers from the literature. Kashif can identify trace links with a recall\nof 67%, precision of 50%, and F2 score of 63%, outperforming the best baseline\nby a substantial margin of 41 percentage points (pp) in F2. However, on unseen,\nmore complex requirements documents traced to the European General Data\nProtection Regulation (GDPR), Kashif performs poorly, yielding an average\nrecall of 15%, an average precision of 10%, and an average F2 score of 13.5%.\nOn the same documents, however, our RICE solution yields an average recall of\n84%, an average precision of 30%, and an average F2 score of 61%. RICE achieved\na remarkable improvement of 47.5 pp over Kashif in terms of F2 score. Our\nresults suggest that requirements traceability in the legal context cannot be\nsimply addressed by building classifiers, as such solutions do not generalize\nand fail to perform well on complex regulations and requirements. Resorting to\ngenerative LLMs, with careful prompt engineering, is thus a more promising\nalternative."
                },
                "authors": [
                    {
                        "name": "Romina Etezadi"
                    },
                    {
                        "name": "Sallam Abualhaija"
                    },
                    {
                        "name": "Chetan Arora"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "arxiv_comment": "38 pages, 4 figues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04916v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04916v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16439v1",
                "updated": "2025-08-22T14:50:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    50,
                    55,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T14:50:55Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    50,
                    55,
                    4,
                    234,
                    0
                ],
                "title": "PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark"
                },
                "summary": "Large language models (LLMs) and vision-augmented LLMs (VLMs) have\nsignificantly advanced medical informatics, diagnostics, and decision support.\nHowever, these models exhibit systematic biases, particularly age bias,\ncompromising their reliability and equity. This is evident in their poorer\nperformance on pediatric-focused text and visual question-answering tasks. This\nbias reflects a broader imbalance in medical research, where pediatric studies\nreceive less funding and representation despite the significant disease burden\nin children. To address these issues, a new comprehensive multi-modal pediatric\nquestion-answering benchmark, PediatricsMQA, has been introduced. It consists\nof 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric\ntopics across seven developmental stages (prenatal to adolescent) and 2,067\nvision-based MCQs using 634 pediatric images from 67 imaging modalities and 256\nanatomical regions. The dataset was developed using a hybrid manual-automatic\npipeline, incorporating peer-reviewed pediatric literature, validated question\nbanks, existing benchmarks, and existing QA resources. Evaluating\nstate-of-the-art open models, we find dramatic performance drops in younger\ncohorts, highlighting the need for age-aware methods to ensure equitable AI\nsupport in pediatric care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and vision-augmented LLMs (VLMs) have\nsignificantly advanced medical informatics, diagnostics, and decision support.\nHowever, these models exhibit systematic biases, particularly age bias,\ncompromising their reliability and equity. This is evident in their poorer\nperformance on pediatric-focused text and visual question-answering tasks. This\nbias reflects a broader imbalance in medical research, where pediatric studies\nreceive less funding and representation despite the significant disease burden\nin children. To address these issues, a new comprehensive multi-modal pediatric\nquestion-answering benchmark, PediatricsMQA, has been introduced. It consists\nof 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric\ntopics across seven developmental stages (prenatal to adolescent) and 2,067\nvision-based MCQs using 634 pediatric images from 67 imaging modalities and 256\nanatomical regions. The dataset was developed using a hybrid manual-automatic\npipeline, incorporating peer-reviewed pediatric literature, validated question\nbanks, existing benchmarks, and existing QA resources. Evaluating\nstate-of-the-art open models, we find dramatic performance drops in younger\ncohorts, highlighting the need for age-aware methods to ensure equitable AI\nsupport in pediatric care."
                },
                "authors": [
                    {
                        "name": "Adil Bahaj"
                    },
                    {
                        "name": "Mounir Ghogho"
                    }
                ],
                "author_detail": {
                    "name": "Mounir Ghogho"
                },
                "author": "Mounir Ghogho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16438v1",
                "updated": "2025-08-22T14:50:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    50,
                    26,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T14:50:26Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    50,
                    26,
                    4,
                    234,
                    0
                ],
                "title": "OPERA: A Reinforcement Learning--Enhanced Orchestrated Planner-Executor\n  Architecture for Reasoning-Oriented Multi-Hop Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPERA: A Reinforcement Learning--Enhanced Orchestrated Planner-Executor\n  Architecture for Reasoning-Oriented Multi-Hop Retrieval"
                },
                "summary": "Recent advances in large language models (LLMs) and dense retrievers have\ndriven significant progress in retrieval-augmented generation (RAG). However,\nexisting approaches face significant challenges in complex reasoning-oriented\nmulti-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Prior\nmethods struggle to generate robust multi-step plans for complex queries, as\nrule-based decomposers perform poorly on out-of-template questions. 2)\nSuboptimal reasoning-driven retrieval: Related methods employ limited query\nreformulation, leading to iterative retrieval loops that often fail to locate\ngolden documents. 3) Insufficient reasoning-guided filtering: Prevailing\nmethods lack the fine-grained reasoning to effectively filter salient\ninformation from noisy results, hindering utilization of retrieved knowledge.\nFundamentally, these limitations all stem from the weak coupling between\nretrieval and reasoning in current RAG architectures. We introduce the\nOrchestrated Planner-Executor Reasoning Architecture (OPERA), a novel\nreasoning-driven retrieval framework. OPERA's Goal Planning Module (GPM)\ndecomposes questions into sub-goals, which are executed by a Reason-Execute\nModule (REM) with specialized components for precise reasoning and effective\nretrieval. To train OPERA, we propose Multi-Agents Progressive Group Relative\nPolicy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complex\nmulti-hop benchmarks show OPERA's superior performance, validating both the\nMAPGRPO method and OPERA's design. Code is available at\nhttps://github.com/Ameame1/OPERA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) and dense retrievers have\ndriven significant progress in retrieval-augmented generation (RAG). However,\nexisting approaches face significant challenges in complex reasoning-oriented\nmulti-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Prior\nmethods struggle to generate robust multi-step plans for complex queries, as\nrule-based decomposers perform poorly on out-of-template questions. 2)\nSuboptimal reasoning-driven retrieval: Related methods employ limited query\nreformulation, leading to iterative retrieval loops that often fail to locate\ngolden documents. 3) Insufficient reasoning-guided filtering: Prevailing\nmethods lack the fine-grained reasoning to effectively filter salient\ninformation from noisy results, hindering utilization of retrieved knowledge.\nFundamentally, these limitations all stem from the weak coupling between\nretrieval and reasoning in current RAG architectures. We introduce the\nOrchestrated Planner-Executor Reasoning Architecture (OPERA), a novel\nreasoning-driven retrieval framework. OPERA's Goal Planning Module (GPM)\ndecomposes questions into sub-goals, which are executed by a Reason-Execute\nModule (REM) with specialized components for precise reasoning and effective\nretrieval. To train OPERA, we propose Multi-Agents Progressive Group Relative\nPolicy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complex\nmulti-hop benchmarks show OPERA's superior performance, validating both the\nMAPGRPO method and OPERA's design. Code is available at\nhttps://github.com/Ameame1/OPERA."
                },
                "authors": [
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Yanbing Liu"
                    },
                    {
                        "name": "Fangfang Yuan"
                    },
                    {
                        "name": "Cong Cao"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Kun Peng"
                    },
                    {
                        "name": "WeiZhuo Chen"
                    },
                    {
                        "name": "Jianjun Li"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Ma"
                },
                "author": "Zhiyuan Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05617v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05617v2",
                "updated": "2025-08-22T14:49:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    49,
                    24,
                    4,
                    234,
                    0
                ],
                "published": "2025-02-08T15:51:31Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    15,
                    51,
                    31,
                    5,
                    39,
                    0
                ],
                "title": "Classical post-processing approach for quantum amplitude estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical post-processing approach for quantum amplitude estimation"
                },
                "summary": "We propose an approach for quantum amplitude estimation (QAE) designed to\nenhance computational efficiency while minimizing the reliance on quantum\nresources. Our method leverages quantum computers to generate a sequence of\nsignals, from which the quantum amplitude is inferred through classical\npost-processing techniques. Unlike traditional methods that use quantum phase\nestimation (QPE), which requires numerous controlled unitary operations and the\nquantum Fourier transform, our method avoids these complex and\nresource-demanding steps. By integrating quantum computing with classical\npost-processing techniques, our method significantly reduces the need for\nquantum gates and qubits, thus optimizing the utilization of quantum hardware.\nWe present numerical simulations to validate the effectiveness of our method\nand provide a comprehensive analysis of its computational complexity and error.\nThis hybrid strategy not only improves the practicality of QAE but also\nbroadens its applicability in quantum computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an approach for quantum amplitude estimation (QAE) designed to\nenhance computational efficiency while minimizing the reliance on quantum\nresources. Our method leverages quantum computers to generate a sequence of\nsignals, from which the quantum amplitude is inferred through classical\npost-processing techniques. Unlike traditional methods that use quantum phase\nestimation (QPE), which requires numerous controlled unitary operations and the\nquantum Fourier transform, our method avoids these complex and\nresource-demanding steps. By integrating quantum computing with classical\npost-processing techniques, our method significantly reduces the need for\nquantum gates and qubits, thus optimizing the utilization of quantum hardware.\nWe present numerical simulations to validate the effectiveness of our method\nand provide a comprehensive analysis of its computational complexity and error.\nThis hybrid strategy not only improves the practicality of QAE but also\nbroadens its applicability in quantum computing."
                },
                "authors": [
                    {
                        "name": "Yongdan Yang"
                    },
                    {
                        "name": "Ruyu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ruyu Yang"
                },
                "author": "Ruyu Yang",
                "arxiv_comment": "8 pages, 6 figures",
                "arxiv_journal_ref": "Physical Review A 112, 012625 (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05617v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05617v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13622v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13622v4",
                "updated": "2025-08-22T14:48:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    48,
                    22,
                    4,
                    234,
                    0
                ],
                "published": "2025-01-23T12:44:45Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    44,
                    45,
                    3,
                    23,
                    0
                ],
                "title": "Coarse-to-Fine Process Reward Modeling for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-to-Fine Process Reward Modeling for Mathematical Reasoning"
                },
                "summary": "The Process Reward Model (PRM) plays a crucial role in mathematical reasoning\ntasks, requiring high-quality supervised process data. However, we observe that\nreasoning steps generated by Large Language Models (LLMs) often fail to exhibit\nstrictly incremental information, leading to redundancy that can hinder\neffective reasoning. To address this issue, we propose CFPRM, a simple yet\neffective coarse-to-fine strategy. Instead of focusing on the detection of\nredundant steps, our approach first establishes a coarse-grained window to\nmerge adjacent reasoning steps into unified, holistic steps. The window size is\nthen progressively reduced to extract fine-grained reasoning steps, enabling\ndata collection at multiple granularities for training. By leveraging this\nhierarchical refinement process, CFPRM mitigates redundancy while preserving\nessential fine-grained knowledge. Extensive experiments on two reasoning\ndatasets across three loss criteria validate the CFPRM's effectiveness and\nversatility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Process Reward Model (PRM) plays a crucial role in mathematical reasoning\ntasks, requiring high-quality supervised process data. However, we observe that\nreasoning steps generated by Large Language Models (LLMs) often fail to exhibit\nstrictly incremental information, leading to redundancy that can hinder\neffective reasoning. To address this issue, we propose CFPRM, a simple yet\neffective coarse-to-fine strategy. Instead of focusing on the detection of\nredundant steps, our approach first establishes a coarse-grained window to\nmerge adjacent reasoning steps into unified, holistic steps. The window size is\nthen progressively reduced to extract fine-grained reasoning steps, enabling\ndata collection at multiple granularities for training. By leveraging this\nhierarchical refinement process, CFPRM mitigates redundancy while preserving\nessential fine-grained knowledge. Extensive experiments on two reasoning\ndatasets across three loss criteria validate the CFPRM's effectiveness and\nversatility."
                },
                "authors": [
                    {
                        "name": "Yulan Hu"
                    },
                    {
                        "name": "Sheng Ouyang"
                    },
                    {
                        "name": "Jinman Zhao"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13622v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13622v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16431v1",
                "updated": "2025-08-22T14:42:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    42,
                    50,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T14:42:50Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    42,
                    50,
                    4,
                    234,
                    0
                ],
                "title": "Cetvel: A Unified Benchmark for Evaluating Language Understanding,\n  Generation and Cultural Capacity of LLMs for Turkish",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cetvel: A Unified Benchmark for Evaluating Language Understanding,\n  Generation and Cultural Capacity of LLMs for Turkish"
                },
                "summary": "We introduce Cetvel, a comprehensive benchmark designed to evaluate large\nlanguage models (LLMs) in Turkish. Existing Turkish benchmarks often lack\neither task diversity or culturally relevant content, or both. Cetvel addresses\nthese gaps by combining a broad range of both discriminative and generative\ntasks ensuring content that reflects the linguistic and cultural richness of\nTurkish language. Cetvel covers 23 tasks grouped into seven categories,\nincluding tasks such as grammatical error correction, machine translation, and\nquestion answering rooted in Turkish history and idiomatic language. We\nevaluate 33 open-weight LLMs (up to 70B parameters) covering different model\nfamilies and instruction paradigms. Our experiments reveal that Turkish-centric\ninstruction-tuned models generally underperform relative to multilingual or\ngeneral-purpose models (e.g. Llama 3 and Mistral), despite being tailored for\nthe language. Moreover, we show that tasks such as grammatical error correction\nand extractive question answering are particularly discriminative in\ndifferentiating model capabilities. Cetvel offers a comprehensive and\nculturally grounded evaluation suite for advancing the development and\nassessment of LLMs in Turkish.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Cetvel, a comprehensive benchmark designed to evaluate large\nlanguage models (LLMs) in Turkish. Existing Turkish benchmarks often lack\neither task diversity or culturally relevant content, or both. Cetvel addresses\nthese gaps by combining a broad range of both discriminative and generative\ntasks ensuring content that reflects the linguistic and cultural richness of\nTurkish language. Cetvel covers 23 tasks grouped into seven categories,\nincluding tasks such as grammatical error correction, machine translation, and\nquestion answering rooted in Turkish history and idiomatic language. We\nevaluate 33 open-weight LLMs (up to 70B parameters) covering different model\nfamilies and instruction paradigms. Our experiments reveal that Turkish-centric\ninstruction-tuned models generally underperform relative to multilingual or\ngeneral-purpose models (e.g. Llama 3 and Mistral), despite being tailored for\nthe language. Moreover, we show that tasks such as grammatical error correction\nand extractive question answering are particularly discriminative in\ndifferentiating model capabilities. Cetvel offers a comprehensive and\nculturally grounded evaluation suite for advancing the development and\nassessment of LLMs in Turkish."
                },
                "authors": [
                    {
                        "name": "Yakup Abrek Er"
                    },
                    {
                        "name": "Ilker Kesen"
                    },
                    {
                        "name": "Gzde Gl ahin"
                    },
                    {
                        "name": "Aykut Erdem"
                    }
                ],
                "author_detail": {
                    "name": "Aykut Erdem"
                },
                "author": "Aykut Erdem",
                "arxiv_comment": "31 pages, 2 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00417v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00417v3",
                "updated": "2025-08-22T14:31:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    31,
                    34,
                    4,
                    234,
                    0
                ],
                "published": "2023-12-01T08:41:25Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    8,
                    41,
                    25,
                    4,
                    335,
                    0
                ],
                "title": "Geodesic slice sampling on Riemannian manifolds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geodesic slice sampling on Riemannian manifolds"
                },
                "summary": "We propose a theoretically justified and practically applicable slice\nsampling based Markov chain Monte Carlo (MCMC) method for approximate sampling\nfrom probability measures on Riemannian manifolds. The latter naturally arise\nas posterior distributions in Bayesian inference of matrix-valued parameters,\nfor example belonging to either the Stiefel or the Grassmann manifold. Our\nmethod, called geodesic slice sampling, is reversible with respect to the\ndistribution of interest, and generalizes Hit-and-run slice sampling on\n$\\mathbb{R}^{d}$ to Riemannian manifolds by using geodesics instead of straight\nlines. We demonstrate the robustness of our sampler's performance compared to\nother MCMC methods dealing with manifold valued distributions through extensive\nnumerical experiments, on both synthetic and real data. In particular, we\nillustrate its remarkable ability to cope with anisotropic target densities,\nwithout using gradient information and preconditioning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a theoretically justified and practically applicable slice\nsampling based Markov chain Monte Carlo (MCMC) method for approximate sampling\nfrom probability measures on Riemannian manifolds. The latter naturally arise\nas posterior distributions in Bayesian inference of matrix-valued parameters,\nfor example belonging to either the Stiefel or the Grassmann manifold. Our\nmethod, called geodesic slice sampling, is reversible with respect to the\ndistribution of interest, and generalizes Hit-and-run slice sampling on\n$\\mathbb{R}^{d}$ to Riemannian manifolds by using geodesics instead of straight\nlines. We demonstrate the robustness of our sampler's performance compared to\nother MCMC methods dealing with manifold valued distributions through extensive\nnumerical experiments, on both synthetic and real data. In particular, we\nillustrate its remarkable ability to cope with anisotropic target densities,\nwithout using gradient information and preconditioning."
                },
                "authors": [
                    {
                        "name": "Alain Durmus"
                    },
                    {
                        "name": "Samuel Gruffaz"
                    },
                    {
                        "name": "Mareike Hasenpflug"
                    },
                    {
                        "name": "Daniel Rudolf"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Rudolf"
                },
                "author": "Daniel Rudolf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00417v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00417v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60-08",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16421v1",
                "updated": "2025-08-22T14:31:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    31,
                    23,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T14:31:23Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    31,
                    23,
                    4,
                    234,
                    0
                ],
                "title": "Terrestrial Exoplanet Internal Structure Constraints Enabled by\n  Comprehensive Host Star Characterization Reveal that Terrestrial Planets in\n  Mean-motion Resonances are Water Rich",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terrestrial Exoplanet Internal Structure Constraints Enabled by\n  Comprehensive Host Star Characterization Reveal that Terrestrial Planets in\n  Mean-motion Resonances are Water Rich"
                },
                "summary": "Exoplanet mass and radius inferences fundamentally rely on host star mass and\nradius inferences. Despite the importance of host star mass, radius, and\nelemental abundance inferences for the derivation of exoplanet internal\nstructure constraints, published constraints have often been based on\ninferences that are not self-consistent. For 24 dwarf stars hosting terrestrial\nexoplanets, we use astrometric and photometric data plus high-resolution\nspectroscopy to infer accurate, precise, homogeneous, and physically\nself-consistent photospheric and fundamental stellar parameters as well as\nelemental abundances. We infer updated planetary masses and radii using these\ndata plus Doppler and transit observables, then use the complete data set to\nderive constraints on the core-mass fractions of these terrestrial exoplanets.\nWe find that the population of resonant or likely formerly resonant terrestrial\nexoplanets represented by Kepler-36 b and Kepler-105 c has a significantly\nlower mean core-mass fraction than the rest of the terrestrial exoplanets in\nour sample. Their resonant configurations suggest that they migrated inwards\nfrom more distant formation locations, and we attribute their low densities to\nthe incorporation and retention of significant amounts of water during their\nformation. We confirm that the ultra-short-period exoplanets 55 Cnc e and\nWASP-47 e have densities inconsistent with pure-rock compositions. We propose\nthat they are both the stripped cores of mini-Neptunes and associate their low\ndensities with the presence of significant amounts of hydrogen, helium, water,\nand/or other volatiles in their interiors. We verify that our results are\nindependent of stellar parameter and elemental abundance inference approach and\ntherefore robust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exoplanet mass and radius inferences fundamentally rely on host star mass and\nradius inferences. Despite the importance of host star mass, radius, and\nelemental abundance inferences for the derivation of exoplanet internal\nstructure constraints, published constraints have often been based on\ninferences that are not self-consistent. For 24 dwarf stars hosting terrestrial\nexoplanets, we use astrometric and photometric data plus high-resolution\nspectroscopy to infer accurate, precise, homogeneous, and physically\nself-consistent photospheric and fundamental stellar parameters as well as\nelemental abundances. We infer updated planetary masses and radii using these\ndata plus Doppler and transit observables, then use the complete data set to\nderive constraints on the core-mass fractions of these terrestrial exoplanets.\nWe find that the population of resonant or likely formerly resonant terrestrial\nexoplanets represented by Kepler-36 b and Kepler-105 c has a significantly\nlower mean core-mass fraction than the rest of the terrestrial exoplanets in\nour sample. Their resonant configurations suggest that they migrated inwards\nfrom more distant formation locations, and we attribute their low densities to\nthe incorporation and retention of significant amounts of water during their\nformation. We confirm that the ultra-short-period exoplanets 55 Cnc e and\nWASP-47 e have densities inconsistent with pure-rock compositions. We propose\nthat they are both the stripped cores of mini-Neptunes and associate their low\ndensities with the presence of significant amounts of hydrogen, helium, water,\nand/or other volatiles in their interiors. We verify that our results are\nindependent of stellar parameter and elemental abundance inference approach and\ntherefore robust."
                },
                "authors": [
                    {
                        "name": "Alejandra Ross"
                    },
                    {
                        "name": "Henrique Reggiani"
                    },
                    {
                        "name": "Kevin C. Schlaufman"
                    },
                    {
                        "name": "Mykhaylo Plotnykov"
                    },
                    {
                        "name": "Diana Valencia"
                    }
                ],
                "author_detail": {
                    "name": "Diana Valencia"
                },
                "author": "Diana Valencia",
                "arxiv_comment": "Accepted for Publication at ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16419v1",
                "updated": "2025-08-22T14:30:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    30,
                    24,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T14:30:24Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    30,
                    24,
                    4,
                    234,
                    0
                ],
                "title": "LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and\n  Security Vulnerabilities in C++ and Python",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and\n  Security Vulnerabilities in C++ and Python"
                },
                "summary": "Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are\nincreasingly embedded in software/application development, supporting tasks\nfrom code generation to debugging. Yet, their real-world effectiveness in\ndetecting diverse software bugs, particularly complex, security-relevant\nvulnerabilities, remains underexplored. This study presents a systematic,\nempirical evaluation of these three leading LLMs using a benchmark of\nfoundational programming errors, classic security flaws, and advanced,\nproduction-grade bugs in C++ and Python. The dataset integrates real code from\nSEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated\nthrough local compilation and testing pipelines. A novel multi-stage,\ncontext-aware prompting protocol simulates realistic debugging scenarios, while\na graded rubric measures detection accuracy, reasoning depth, and remediation\nquality. Our results show that all models excel at identifying syntactic and\nsemantic issues in well-scoped code, making them promising for educational use\nand as first-pass reviewers in automated code auditing. Performance diminishes\nin scenarios involving complex security vulnerabilities and large-scale\nproduction code, with ChatGPT-4 and Claude 3 generally providing more nuanced\ncontextual analyses than LLaMA 4. This highlights both the promise and the\npresent constraints of LLMs in serving as reliable code analysis tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are\nincreasingly embedded in software/application development, supporting tasks\nfrom code generation to debugging. Yet, their real-world effectiveness in\ndetecting diverse software bugs, particularly complex, security-relevant\nvulnerabilities, remains underexplored. This study presents a systematic,\nempirical evaluation of these three leading LLMs using a benchmark of\nfoundational programming errors, classic security flaws, and advanced,\nproduction-grade bugs in C++ and Python. The dataset integrates real code from\nSEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated\nthrough local compilation and testing pipelines. A novel multi-stage,\ncontext-aware prompting protocol simulates realistic debugging scenarios, while\na graded rubric measures detection accuracy, reasoning depth, and remediation\nquality. Our results show that all models excel at identifying syntactic and\nsemantic issues in well-scoped code, making them promising for educational use\nand as first-pass reviewers in automated code auditing. Performance diminishes\nin scenarios involving complex security vulnerabilities and large-scale\nproduction code, with ChatGPT-4 and Claude 3 generally providing more nuanced\ncontextual analyses than LLaMA 4. This highlights both the promise and the\npresent constraints of LLMs in serving as reliable code analysis tools."
                },
                "authors": [
                    {
                        "name": "Akshay Mhatre"
                    },
                    {
                        "name": "Noujoud Nader"
                    },
                    {
                        "name": "Patrick Diehl"
                    },
                    {
                        "name": "Deepti Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Deepti Gupta"
                },
                "author": "Deepti Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09457v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09457v2",
                "updated": "2025-08-22T14:16:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    16,
                    43,
                    4,
                    234,
                    0
                ],
                "published": "2025-06-11T07:02:18Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    7,
                    2,
                    18,
                    2,
                    162,
                    0
                ],
                "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Bridging the Reward-Generation Gap in Direct Alignment\n  Algorithms"
                },
                "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we adopt\na token-level MDP perspective of DAAs to analyze its limitations and introduce\na simple yet effective approach called Prefix-Oriented Equal-length Training\n(POET), which truncates both preferred and dispreferred responses to match the\nshorter one's length. Training with \\mname, where both responses in each sample\nare truncated to equal length, resulting in diverse truncated lengths across\nsamples, the optimization of DAAs objective is implicitly constrained to\nconverge across all timesteps of token-level MDP, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we adopt\na token-level MDP perspective of DAAs to analyze its limitations and introduce\na simple yet effective approach called Prefix-Oriented Equal-length Training\n(POET), which truncates both preferred and dispreferred responses to match the\nshorter one's length. Training with \\mname, where both responses in each sample\nare truncated to equal length, resulting in diverse truncated lengths across\nsamples, the optimization of DAAs objective is implicitly constrained to\nconverge across all timesteps of token-level MDP, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs."
                },
                "authors": [
                    {
                        "name": "Zeguan Xiao"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Ke Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Tang"
                },
                "author": "Ke Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09457v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16406v1",
                "updated": "2025-08-22T14:13:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    13,
                    16,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T14:13:16Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    13,
                    16,
                    4,
                    234,
                    0
                ],
                "title": "Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak\n  Prevention for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak\n  Prevention for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which\nattempt to elicit harmful responses from LLMs. The evolving nature and\ndiversity of these attacks pose many challenges for defense systems, including\n(1) adaptation to counter emerging attack strategies without costly retraining,\nand (2) control of the trade-off between safety and utility. To address these\nchallenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for\njailbreak detection that incorporates a database of known attack examples into\nRetrieval-Augmented Generation, which is used to infer the underlying,\nmalicious user query and jailbreak strategy used to attack the system. RAD\nenables training-free updates for newly discovered jailbreak strategies and\nprovides a mechanism to balance safety and utility. Experiments on StrongREJECT\nshow that RAD substantially reduces the effectiveness of strong jailbreak\nattacks such as PAP and PAIR while maintaining low rejection rates for benign\nqueries. We propose a novel evaluation scheme and show that RAD achieves a\nrobust safety-utility trade-off across a range of operating points in a\ncontrollable manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which\nattempt to elicit harmful responses from LLMs. The evolving nature and\ndiversity of these attacks pose many challenges for defense systems, including\n(1) adaptation to counter emerging attack strategies without costly retraining,\nand (2) control of the trade-off between safety and utility. To address these\nchallenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for\njailbreak detection that incorporates a database of known attack examples into\nRetrieval-Augmented Generation, which is used to infer the underlying,\nmalicious user query and jailbreak strategy used to attack the system. RAD\nenables training-free updates for newly discovered jailbreak strategies and\nprovides a mechanism to balance safety and utility. Experiments on StrongREJECT\nshow that RAD substantially reduces the effectiveness of strong jailbreak\nattacks such as PAP and PAIR while maintaining low rejection rates for benign\nqueries. We propose a novel evaluation scheme and show that RAD achieves a\nrobust safety-utility trade-off across a range of operating points in a\ncontrollable manner."
                },
                "authors": [
                    {
                        "name": "Guangyu Yang"
                    },
                    {
                        "name": "Jinghong Chen"
                    },
                    {
                        "name": "Jingbiao Mei"
                    },
                    {
                        "name": "Weizhe Lin"
                    },
                    {
                        "name": "Bill Byrne"
                    }
                ],
                "author_detail": {
                    "name": "Bill Byrne"
                },
                "author": "Bill Byrne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02768v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02768v3",
                "updated": "2025-08-22T14:08:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    8,
                    58,
                    4,
                    234,
                    0
                ],
                "published": "2025-04-03T17:05:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    5,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal\n  Pairs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal\n  Pairs"
                },
                "summary": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages and 2 types of subject-verb agreement,\ncontaining more than 128,000 minimal pairs. Our minimal pairs are created using\na fully automated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages and 2 types of subject-verb agreement,\ncontaining more than 128,000 minimal pairs. Our minimal pairs are created using\na fully automated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages."
                },
                "authors": [
                    {
                        "name": "Jaap Jumelet"
                    },
                    {
                        "name": "Leonie Weissweiler"
                    },
                    {
                        "name": "Joakim Nivre"
                    },
                    {
                        "name": "Arianna Bisazza"
                    }
                ],
                "author_detail": {
                    "name": "Arianna Bisazza"
                },
                "author": "Arianna Bisazza",
                "arxiv_comment": "Published in TACL, MIT Press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02768v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02768v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16402v1",
                "updated": "2025-08-22T14:04:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    4,
                    55,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T14:04:55Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    4,
                    55,
                    4,
                    234,
                    0
                ],
                "title": "AetherCode: Evaluating LLMs' Ability to Win In Premier Programming\n  Competitions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AetherCode: Evaluating LLMs' Ability to Win In Premier Programming\n  Competitions"
                },
                "summary": "Competitive programming has emerged as a critical benchmark for evaluating\nthe reasoning and coding capabilities of Large Language Models (LLMs). Despite\nimpressive progress on existing benchmarks, we argue that current evaluations\noverstate model proficiency, masking a substantial gap between LLMs and elite\nhuman programmers. This gap arises from two key limitations: insufficient\ndifficulty and scope of benchmark problems, and evaluation bias from\nlow-quality test cases. To address these shortcomings, we present AetherCode, a\nnew benchmark that draws problems from premier programming competitions such as\nIOI and ICPC, offering broader coverage and higher difficulty. AetherCode\nfurther incorporates comprehensive, expert-validated test suites built through\na hybrid of automated generation and human curation, ensuring rigorous and\nreliable assessment. By combining challenging problem design with robust\nevaluation, AetherCode provides a more faithful measure of LLM capabilities and\nsets a new standard for future research in code reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive programming has emerged as a critical benchmark for evaluating\nthe reasoning and coding capabilities of Large Language Models (LLMs). Despite\nimpressive progress on existing benchmarks, we argue that current evaluations\noverstate model proficiency, masking a substantial gap between LLMs and elite\nhuman programmers. This gap arises from two key limitations: insufficient\ndifficulty and scope of benchmark problems, and evaluation bias from\nlow-quality test cases. To address these shortcomings, we present AetherCode, a\nnew benchmark that draws problems from premier programming competitions such as\nIOI and ICPC, offering broader coverage and higher difficulty. AetherCode\nfurther incorporates comprehensive, expert-validated test suites built through\na hybrid of automated generation and human curation, ensuring rigorous and\nreliable assessment. By combining challenging problem design with robust\nevaluation, AetherCode provides a more faithful measure of LLM capabilities and\nsets a new standard for future research in code reasoning."
                },
                "authors": [
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Jiaze Chen"
                    },
                    {
                        "name": "Zhicheng Liu"
                    },
                    {
                        "name": "Markus Mak"
                    },
                    {
                        "name": "Yidi Du"
                    },
                    {
                        "name": "Geonsik Moon"
                    },
                    {
                        "name": "Luoqi Xu"
                    },
                    {
                        "name": "Aaron Tua"
                    },
                    {
                        "name": "Kunshuo Peng"
                    },
                    {
                        "name": "Jiayi Lu"
                    },
                    {
                        "name": "Mingfei Xia"
                    },
                    {
                        "name": "Boqian Zou"
                    },
                    {
                        "name": "Chenyang Ran"
                    },
                    {
                        "name": "Guang Tian"
                    },
                    {
                        "name": "Shoutai Zhu"
                    },
                    {
                        "name": "Yeheng Duan"
                    },
                    {
                        "name": "Zhenghui Kang"
                    },
                    {
                        "name": "Zhenxing Lin"
                    },
                    {
                        "name": "Shangshu Li"
                    },
                    {
                        "name": "Qiang Luo"
                    },
                    {
                        "name": "Qingshen Long"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Yihan Xiao"
                    },
                    {
                        "name": "Yurong Wu"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Yuyi Fu"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Ming Ding"
                    }
                ],
                "author_detail": {
                    "name": "Ming Ding"
                },
                "author": "Ming Ding",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16399v1",
                "updated": "2025-08-22T14:00:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    0,
                    58,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T14:00:58Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    0,
                    58,
                    4,
                    234,
                    0
                ],
                "title": "Constraints on the extreme mass-ratio inspiral population from LISA data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraints on the extreme mass-ratio inspiral population from LISA data"
                },
                "summary": "Gravitational waves from extreme mass-ratio inspirals (EMRIs), the inspirals\nof stellar-mass compact objects into massive black holes, are predicted to be\nobserved by the Laser Interferometer Space Antenna (LISA). A sufficiently large\nnumber of EMRI observations will provide unique insights into the massive black\nhole population. We have developed a hierarchical Bayesian inference framework\ncapable of constraining the parameters of the EMRI population, accounting for\nselection biases. We leverage the capacity of a feed-forward neural network as\nan emulator, enabling detectability calculations of $\\sim10^5$ EMRIs in a\nfraction of a second, speeding up the likelihood evaluation by $\\gtrsim6$\norders of magnitude. We validate our framework on a phenomenological EMRI\npopulation model. This framework enables studies of how well we can constrain\nEMRI population parameters, such as the slope of both the massive and\nstellar-mass black hole mass spectra and the branching fractions of different\nformation channels, allowing further investigation into the evolution of\nmassive black holes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves from extreme mass-ratio inspirals (EMRIs), the inspirals\nof stellar-mass compact objects into massive black holes, are predicted to be\nobserved by the Laser Interferometer Space Antenna (LISA). A sufficiently large\nnumber of EMRI observations will provide unique insights into the massive black\nhole population. We have developed a hierarchical Bayesian inference framework\ncapable of constraining the parameters of the EMRI population, accounting for\nselection biases. We leverage the capacity of a feed-forward neural network as\nan emulator, enabling detectability calculations of $\\sim10^5$ EMRIs in a\nfraction of a second, speeding up the likelihood evaluation by $\\gtrsim6$\norders of magnitude. We validate our framework on a phenomenological EMRI\npopulation model. This framework enables studies of how well we can constrain\nEMRI population parameters, such as the slope of both the massive and\nstellar-mass black hole mass spectra and the branching fractions of different\nformation channels, allowing further investigation into the evolution of\nmassive black holes."
                },
                "authors": [
                    {
                        "name": "Shashwat Singh"
                    },
                    {
                        "name": "Christian E. A. Chapman-Bird"
                    },
                    {
                        "name": "Christopher P L Berry"
                    },
                    {
                        "name": "John Veitch"
                    }
                ],
                "author_detail": {
                    "name": "John Veitch"
                },
                "author": "John Veitch",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16397v1",
                "updated": "2025-08-22T13:58:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    58,
                    35,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T13:58:35Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    58,
                    35,
                    4,
                    234,
                    0
                ],
                "title": "A Lightweight Group Multiscale Bidirectional Interactive Network for\n  Real-Time Steel Surface Defect Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Lightweight Group Multiscale Bidirectional Interactive Network for\n  Real-Time Steel Surface Defect Detection"
                },
                "summary": "Real-time surface defect detection is critical for maintaining product\nquality and production efficiency in the steel manufacturing industry. Despite\npromising accuracy, existing deep learning methods often suffer from high\ncomputational complexity and slow inference speeds, which limit their\ndeployment in resource-constrained industrial environments. Recent lightweight\napproaches adopt multibranch architectures based on depthwise separable\nconvolution (DSConv) to capture multiscale contextual information. However,\nthese methods often suffer from increased computational overhead and lack\neffective cross-scale feature interaction, limiting their ability to fully\nleverage multiscale representations. To address these challenges, we propose\nGMBINet, a lightweight framework that enhances multiscale feature extraction\nand interaction through novel Group Multiscale Bidirectional Interactive (GMBI)\nmodules. The GMBI adopts a group-wise strategy for multiscale feature\nextraction, ensuring scale-agnostic computational complexity. It further\nintegrates a Bidirectional Progressive Feature Interactor (BPFI) and a\nparameter-free Element-Wise Multiplication-Summation (EWMS) operation to\nenhance cross-scale interaction without introducing additional computational\noverhead. Experiments on SD-Saliency-900 and NRSD-MN datasets demonstrate that\nGMBINet delivers competitive accuracy with real-time speeds of 1048 FPS on GPU\nand 16.53 FPS on CPU at 512 resolution, using only 0.19 M parameters.\nAdditional evaluations on the NEU-CLS defect classification dataset further\nconfirm the strong generalization ability of our method, demonstrating its\npotential for broader industrial vision applications beyond surface defect\ndetection. The dataset and code are publicly available at:\nhttps://github.com/zhangyongcode/GMBINet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time surface defect detection is critical for maintaining product\nquality and production efficiency in the steel manufacturing industry. Despite\npromising accuracy, existing deep learning methods often suffer from high\ncomputational complexity and slow inference speeds, which limit their\ndeployment in resource-constrained industrial environments. Recent lightweight\napproaches adopt multibranch architectures based on depthwise separable\nconvolution (DSConv) to capture multiscale contextual information. However,\nthese methods often suffer from increased computational overhead and lack\neffective cross-scale feature interaction, limiting their ability to fully\nleverage multiscale representations. To address these challenges, we propose\nGMBINet, a lightweight framework that enhances multiscale feature extraction\nand interaction through novel Group Multiscale Bidirectional Interactive (GMBI)\nmodules. The GMBI adopts a group-wise strategy for multiscale feature\nextraction, ensuring scale-agnostic computational complexity. It further\nintegrates a Bidirectional Progressive Feature Interactor (BPFI) and a\nparameter-free Element-Wise Multiplication-Summation (EWMS) operation to\nenhance cross-scale interaction without introducing additional computational\noverhead. Experiments on SD-Saliency-900 and NRSD-MN datasets demonstrate that\nGMBINet delivers competitive accuracy with real-time speeds of 1048 FPS on GPU\nand 16.53 FPS on CPU at 512 resolution, using only 0.19 M parameters.\nAdditional evaluations on the NEU-CLS defect classification dataset further\nconfirm the strong generalization ability of our method, demonstrating its\npotential for broader industrial vision applications beyond surface defect\ndetection. The dataset and code are publicly available at:\nhttps://github.com/zhangyongcode/GMBINet."
                },
                "authors": [
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Cunjian Chen"
                    },
                    {
                        "name": "Qiang Gao"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Bin Fang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Fang"
                },
                "author": "Bin Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19954v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19954v2",
                "updated": "2025-08-22T13:55:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    55,
                    7,
                    4,
                    234,
                    0
                ],
                "published": "2025-02-27T10:30:50Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    10,
                    30,
                    50,
                    3,
                    58,
                    0
                ],
                "title": "Collaborative Stance Detection via Small-Large Language Model\n  Consistency Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Stance Detection via Small-Large Language Model\n  Consistency Verification"
                },
                "summary": "Stance detection on social media aims to identify attitudes expressed in\ntweets towards specific targets. Current studies prioritize Large Language\nModels (LLMs) over Small Language Models (SLMs) due to the overwhelming\nperformance improving provided by LLMs. However, heavily relying on LLMs for\nstance detection, regardless of the cost, is impractical for real-world social\nmedia monitoring systems that require vast data analysis. To this end, we\npropose \\textbf{\\underline{Co}}llaborative Stance Detection via Small-Large\nLanguage Model Consistency \\textbf{\\underline{Ver}}ification (\\textbf{CoVer})\nframework, which enhances LLM utilization via context-shared batch reasoning\nand logical verification between LLM and SLM. Specifically, instead of\nprocessing each text individually, CoVer processes texts batch-by-batch,\nobtaining stance predictions and corresponding explanations via LLM reasoning\nin a shared context. Then, to exclude the bias caused by context noises, CoVer\nintroduces the SLM for logical consistency verification. Finally, texts that\nrepeatedly exhibit low logical consistency are classified using\nconsistency-weighted aggregation of prior LLM stance predictions. Our\nexperiments show that CoVer outperforms state-of-the-art methods across\nmultiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per\ntweet while significantly enhancing performance. Our CoVer offers a more\npractical solution for LLM deploying for social media stance detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stance detection on social media aims to identify attitudes expressed in\ntweets towards specific targets. Current studies prioritize Large Language\nModels (LLMs) over Small Language Models (SLMs) due to the overwhelming\nperformance improving provided by LLMs. However, heavily relying on LLMs for\nstance detection, regardless of the cost, is impractical for real-world social\nmedia monitoring systems that require vast data analysis. To this end, we\npropose \\textbf{\\underline{Co}}llaborative Stance Detection via Small-Large\nLanguage Model Consistency \\textbf{\\underline{Ver}}ification (\\textbf{CoVer})\nframework, which enhances LLM utilization via context-shared batch reasoning\nand logical verification between LLM and SLM. Specifically, instead of\nprocessing each text individually, CoVer processes texts batch-by-batch,\nobtaining stance predictions and corresponding explanations via LLM reasoning\nin a shared context. Then, to exclude the bias caused by context noises, CoVer\nintroduces the SLM for logical consistency verification. Finally, texts that\nrepeatedly exhibit low logical consistency are classified using\nconsistency-weighted aggregation of prior LLM stance predictions. Our\nexperiments show that CoVer outperforms state-of-the-art methods across\nmultiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per\ntweet while significantly enhancing performance. Our CoVer offers a more\npractical solution for LLM deploying for social media stance detection."
                },
                "authors": [
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zixiang Tang"
                    },
                    {
                        "name": "Teli Liu"
                    },
                    {
                        "name": "Min Liu"
                    }
                ],
                "author_detail": {
                    "name": "Min Liu"
                },
                "author": "Min Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19954v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19954v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16394v1",
                "updated": "2025-08-22T13:54:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    54,
                    1,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T13:54:01Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    54,
                    1,
                    4,
                    234,
                    0
                ],
                "title": "What do we mean by stellar mass? The impact of the pre-main sequence on\n  the mass to light ratio of young and intermediate age stellar populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What do we mean by stellar mass? The impact of the pre-main sequence on\n  the mass to light ratio of young and intermediate age stellar populations"
                },
                "summary": "Stellar population synthesis models are an essential tool with which galaxy\nphysical parameters are extracted from observations. However they are built on\nassumptions designed for use in the local Universe, and not always appropriate\nto high redshift galaxies. Here we consider the impact of including the\nhitherto-neglected stellar pre-main sequence delay timescale on the\ninterpretation of composite stellar populations at ages of <1 Gyr. We find that\ndoing so has an impact on the optical luminosity of very young stellar\npopulations of up to ~10 per cent, although smaller changes in observed light\n(<5 per cent) are expected in most use cases. However the impact on the\ninferred stellar mass and mass-to-light ratios is significant (a factor of 2 or\nmore), depending on how those properties are defined. We find that the short\ntime scales for star formation in the distant Universe require a clearer\ndefinition for the stellar mass in a population, and will impact assumptions\nabout the inferred shape of the stellar initial mass function from\nobservations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stellar population synthesis models are an essential tool with which galaxy\nphysical parameters are extracted from observations. However they are built on\nassumptions designed for use in the local Universe, and not always appropriate\nto high redshift galaxies. Here we consider the impact of including the\nhitherto-neglected stellar pre-main sequence delay timescale on the\ninterpretation of composite stellar populations at ages of <1 Gyr. We find that\ndoing so has an impact on the optical luminosity of very young stellar\npopulations of up to ~10 per cent, although smaller changes in observed light\n(<5 per cent) are expected in most use cases. However the impact on the\ninferred stellar mass and mass-to-light ratios is significant (a factor of 2 or\nmore), depending on how those properties are defined. We find that the short\ntime scales for star formation in the distant Universe require a clearer\ndefinition for the stellar mass in a population, and will impact assumptions\nabout the inferred shape of the stellar initial mass function from\nobservations."
                },
                "authors": [
                    {
                        "name": "Elizabeth R. Stanway"
                    },
                    {
                        "name": "Conor M. Byrne"
                    },
                    {
                        "name": "Ankur Upadhyaya"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Upadhyaya"
                },
                "arxiv_affiliation": "University of Warwick, UK",
                "author": "Ankur Upadhyaya",
                "arxiv_comment": "12 pages, 15 figures. Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16390v1",
                "updated": "2025-08-22T13:48:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    48,
                    37,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T13:48:37Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    48,
                    37,
                    4,
                    234,
                    0
                ],
                "title": "RoMedQA: The First Benchmark for Romanian Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoMedQA: The First Benchmark for Romanian Medical Question Answering"
                },
                "summary": "Question answering (QA) is an actively studied topic, being a core natural\nlanguage processing (NLP) task that needs to be addressed before achieving\nArtificial General Intelligence (AGI). However, the lack of QA datasets in\nspecific domains and languages hinders the development of robust AI models able\nto generalize across various domains and languages. To this end, we introduce\nRoMedQA, the first Romanian QA benchmark for the medical domain, alongside a\ncomprehensive evaluation of state-of-the-art large language models (LLMs). We\nconstruct a high-quality and large-scale dataset comprising 102,646 QA pairs\nrelated to cancer patients. The questions regard medical case summaries of\n1,011 patients, requiring either keyword extraction or reasoning to be answered\ncorrectly. RoMedQA is the result of a time-consuming manual annotation process\ncarried out by seven physicians specialized in oncology or radiotherapy, who\nspent a total of about 2,100 work hours to generate the QA pairs. We experiment\nwith four LLMs from distinct families of models on RoMedQA. Each model is\nemployed in two scenarios, namely one based on zero-shot prompting and one\nbased on supervised fine-tuning. Our results show that fine-tuned models\nsignificantly outperform their zero-shot counterparts, clearly indicating that\npretrained models fail to generalize on RoMedQA. Our findings demonstrate the\nimportance of both domain-specific and language-specific fine-tuning for\nreliable clinical QA in Romanian. We publicly release our dataset and code at\nhttps://github.com/ana-rogoz/RoMedQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering (QA) is an actively studied topic, being a core natural\nlanguage processing (NLP) task that needs to be addressed before achieving\nArtificial General Intelligence (AGI). However, the lack of QA datasets in\nspecific domains and languages hinders the development of robust AI models able\nto generalize across various domains and languages. To this end, we introduce\nRoMedQA, the first Romanian QA benchmark for the medical domain, alongside a\ncomprehensive evaluation of state-of-the-art large language models (LLMs). We\nconstruct a high-quality and large-scale dataset comprising 102,646 QA pairs\nrelated to cancer patients. The questions regard medical case summaries of\n1,011 patients, requiring either keyword extraction or reasoning to be answered\ncorrectly. RoMedQA is the result of a time-consuming manual annotation process\ncarried out by seven physicians specialized in oncology or radiotherapy, who\nspent a total of about 2,100 work hours to generate the QA pairs. We experiment\nwith four LLMs from distinct families of models on RoMedQA. Each model is\nemployed in two scenarios, namely one based on zero-shot prompting and one\nbased on supervised fine-tuning. Our results show that fine-tuned models\nsignificantly outperform their zero-shot counterparts, clearly indicating that\npretrained models fail to generalize on RoMedQA. Our findings demonstrate the\nimportance of both domain-specific and language-specific fine-tuning for\nreliable clinical QA in Romanian. We publicly release our dataset and code at\nhttps://github.com/ana-rogoz/RoMedQA."
                },
                "authors": [
                    {
                        "name": "Ana-Cristina Rogoz"
                    },
                    {
                        "name": "Radu Tudor Ionescu"
                    },
                    {
                        "name": "Alexandra-Valentina Anghel"
                    },
                    {
                        "name": "Ionut-Lucian Antone-Iordache"
                    },
                    {
                        "name": "Simona Coniac"
                    },
                    {
                        "name": "Andreea Iuliana Ionescu"
                    }
                ],
                "author_detail": {
                    "name": "Andreea Iuliana Ionescu"
                },
                "author": "Andreea Iuliana Ionescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01832v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01832v2",
                "updated": "2025-08-22T13:41:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    41,
                    57,
                    4,
                    234,
                    0
                ],
                "published": "2025-03-03T18:55:09Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    55,
                    9,
                    0,
                    62,
                    0
                ],
                "title": "Rotary Offset Features in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotary Offset Features in Large Language Models"
                },
                "summary": "Transformer-based Large Language Models (LLMs) rely on positional encodings\nto provide sequence position information to their attention mechanism. Rotary\nPositional Encodings (RoPE), which encode relative position by rotating queries\nand keys, have become widely used in modern LLMs. We study the features and\npatterns that emerge in queries and keys when using rotary embeddings and\nintroduce the concept of rotary offset features. Our analysis reveals that\nthese features, which frequently exhibit large activations and are often\ninterpreted as outliers, arise consistently across layers, attention heads, and\nmodel architectures. We derive bounds predicting which rotary frequencies give\nrise to rotary offset features and the minimum angle between the query-key\npairs for these features. We verify our predictions empirically across models\nof different sizes and architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) rely on positional encodings\nto provide sequence position information to their attention mechanism. Rotary\nPositional Encodings (RoPE), which encode relative position by rotating queries\nand keys, have become widely used in modern LLMs. We study the features and\npatterns that emerge in queries and keys when using rotary embeddings and\nintroduce the concept of rotary offset features. Our analysis reveals that\nthese features, which frequently exhibit large activations and are often\ninterpreted as outliers, arise consistently across layers, attention heads, and\nmodel architectures. We derive bounds predicting which rotary frequencies give\nrise to rotary offset features and the minimum angle between the query-key\npairs for these features. We verify our predictions empirically across models\nof different sizes and architectures."
                },
                "authors": [
                    {
                        "name": "Andr Jonasson"
                    }
                ],
                "author_detail": {
                    "name": "Andr Jonasson"
                },
                "author": "Andr Jonasson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01832v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01832v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14599v2",
                "updated": "2025-08-22T13:39:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    39,
                    45,
                    4,
                    234,
                    0
                ],
                "published": "2024-10-18T16:53:49Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    53,
                    49,
                    4,
                    292,
                    0
                ],
                "title": "Bayesian Multi-wavelength Imaging of the LMC SN1987A with SRG/eROSITA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Multi-wavelength Imaging of the LMC SN1987A with SRG/eROSITA"
                },
                "summary": "The eROSITA Early Data Release (EDR) and eROSITA All-Sky Survey (eRASS1) data\nhave already revealed a remarkable number of undiscovered X-ray sources. Using\nBayesian inference and generative modeling techniques for X-ray imaging, we aim\nto increase the sensitivity and scientific value of these observations by\ndenoising, deconvolving, and decomposing the X-ray sky. Leveraging information\nfield theory, we can exploit the spatial and spectral correlation structures of\nthe different physical components of the sky with non-parametric priors to\nenhance the image reconstruction. By incorporating instrumental effects into\nthe forward model, we develop a comprehensive Bayesian imaging algorithm for\neROSITA pointing observations. Finally, we apply the developed algorithm to EDR\ndata of the Large Magellanic Cloud (LMC) SN1987A, fusing data sets from\nobservations made by five different telescope modules. The final result is a\ndenoised, deconvolved, and decomposed view of the LMC, which enables the\nanalysis of its fine-scale structures, the identification of point sources in\nthis region, and enhanced calibration for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The eROSITA Early Data Release (EDR) and eROSITA All-Sky Survey (eRASS1) data\nhave already revealed a remarkable number of undiscovered X-ray sources. Using\nBayesian inference and generative modeling techniques for X-ray imaging, we aim\nto increase the sensitivity and scientific value of these observations by\ndenoising, deconvolving, and decomposing the X-ray sky. Leveraging information\nfield theory, we can exploit the spatial and spectral correlation structures of\nthe different physical components of the sky with non-parametric priors to\nenhance the image reconstruction. By incorporating instrumental effects into\nthe forward model, we develop a comprehensive Bayesian imaging algorithm for\neROSITA pointing observations. Finally, we apply the developed algorithm to EDR\ndata of the Large Magellanic Cloud (LMC) SN1987A, fusing data sets from\nobservations made by five different telescope modules. The final result is a\ndenoised, deconvolved, and decomposed view of the LMC, which enables the\nanalysis of its fine-scale structures, the identification of point sources in\nthis region, and enhanced calibration for future work."
                },
                "authors": [
                    {
                        "name": "Vincent Eberle"
                    },
                    {
                        "name": "Matteo Guardiani"
                    },
                    {
                        "name": "Margret Westerkamp"
                    },
                    {
                        "name": "Philipp Frank"
                    },
                    {
                        "name": "Michael Freyberg"
                    },
                    {
                        "name": "Mara Salvato"
                    },
                    {
                        "name": "Torsten Enlin"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Enlin"
                },
                "author": "Torsten Enlin",
                "arxiv_doi": "10.5281/zenodo.16918521",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5281/zenodo.16918521",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.14599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16383v1",
                "updated": "2025-08-22T13:38:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    38,
                    12,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T13:38:12Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    38,
                    12,
                    4,
                    234,
                    0
                ],
                "title": "GLARE: Agentic Reasoning for Legal Judgment Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLARE: Agentic Reasoning for Legal Judgment Prediction"
                },
                "summary": "Legal judgment prediction (LJP) has become increasingly important in the\nlegal field. In this paper, we identify that existing large language models\n(LLMs) have significant problems of insufficient reasoning due to a lack of\nlegal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning\nframework that dynamically acquires key legal knowledge by invoking different\nmodules, thereby improving the breadth and depth of reasoning. Experiments\nconducted on the real-world dataset verify the effectiveness of our method.\nFurthermore, the reasoning chain generated during the analysis process can\nincrease interpretability and provide the possibility for practical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal judgment prediction (LJP) has become increasingly important in the\nlegal field. In this paper, we identify that existing large language models\n(LLMs) have significant problems of insufficient reasoning due to a lack of\nlegal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning\nframework that dynamically acquires key legal knowledge by invoking different\nmodules, thereby improving the breadth and depth of reasoning. Experiments\nconducted on the real-world dataset verify the effectiveness of our method.\nFurthermore, the reasoning chain generated during the analysis process can\nincrease interpretability and provide the possibility for practical\napplications."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16379v1",
                "updated": "2025-08-22T13:34:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    34,
                    49,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T13:34:49Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    34,
                    49,
                    4,
                    234,
                    0
                ],
                "title": "Agentic AI Empowered Multi-UAV Trajectory Optimization in Low-Altitude\n  Economy Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI Empowered Multi-UAV Trajectory Optimization in Low-Altitude\n  Economy Networks"
                },
                "summary": "This paper proposes a novel Agentic Retrieval-augmented generation with\nMamba-Attention Integrated Transformer (ARMAIT) framework for multi-Unmanned\nAerial Vehicle (UAV) trajectory optimization. The framework is built upon Large\nLanguage Models (LLMs), incorporating Retrieval-Augmented Generation (RAG)\nempowered by Agentic AI and integrated with a UAV-specific knowledge base.\nThrough the Agentic RAG, the LLM autonomously interprets high-level task\nrequirements and identifies the key components necessary for trajectory\noptimization, including model inputs and outputs, network architecture, reward\nfunctions, and task constraints. To support efficient modeling across different\nsystem scales, we introduce the Mamba-Attention Integrated Transformer (MAIT),\na hybrid neural architecture that combines the long-range dependency modeling\ncapability of attention mechanisms with the efficient temporal dynamic\nrepresentation of Mamba. Furthermore, a Trajectory-Group Relative Policy\nOptimization (T-GRPO) method is proposed to achieve unified policy gradient\noptimization in both discrete and continuous trajectory spaces for MAIT\ntraining. Extensive experimental results validate the feasibility and\neffectiveness of the proposed ARMAIT framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel Agentic Retrieval-augmented generation with\nMamba-Attention Integrated Transformer (ARMAIT) framework for multi-Unmanned\nAerial Vehicle (UAV) trajectory optimization. The framework is built upon Large\nLanguage Models (LLMs), incorporating Retrieval-Augmented Generation (RAG)\nempowered by Agentic AI and integrated with a UAV-specific knowledge base.\nThrough the Agentic RAG, the LLM autonomously interprets high-level task\nrequirements and identifies the key components necessary for trajectory\noptimization, including model inputs and outputs, network architecture, reward\nfunctions, and task constraints. To support efficient modeling across different\nsystem scales, we introduce the Mamba-Attention Integrated Transformer (MAIT),\na hybrid neural architecture that combines the long-range dependency modeling\ncapability of attention mechanisms with the efficient temporal dynamic\nrepresentation of Mamba. Furthermore, a Trajectory-Group Relative Policy\nOptimization (T-GRPO) method is proposed to achieve unified policy gradient\noptimization in both discrete and continuous trajectory spaces for MAIT\ntraining. Extensive experimental results validate the feasibility and\neffectiveness of the proposed ARMAIT framework."
                },
                "authors": [
                    {
                        "name": "Feibo Jiang"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Xitao Pan"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Cunhua Pan"
                    }
                ],
                "author_detail": {
                    "name": "Cunhua Pan"
                },
                "author": "Cunhua Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16371v1",
                "updated": "2025-08-22T13:25:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    25,
                    0,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T13:25:00Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    25,
                    0,
                    4,
                    234,
                    0
                ],
                "title": "The Mediomatix Corpus: Parallel Data for Romansh Idioms via Comparable\n  Schoolbooks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mediomatix Corpus: Parallel Data for Romansh Idioms via Comparable\n  Schoolbooks"
                },
                "summary": "The five idioms (i.e., varieties) of the Romansh language are largely\nstandardized and are taught in the schools of the respective communities in\nSwitzerland. In this paper, we present the first parallel corpus of Romansh\nidioms. The corpus is based on 291 schoolbook volumes, which are comparable in\ncontent for the five idioms. We use automatic alignment methods to extract 207k\nmulti-parallel segments from the books, with more than 2M tokens in total. A\nsmall-scale human evaluation confirms that the segments are highly parallel,\nmaking the dataset suitable for NLP applications such as machine translation\nbetween Romansh idioms. We release the parallel and unaligned versions of the\ndataset under a CC-BY-NC-SA license and demonstrate its utility for machine\ntranslation by training and evaluating an LLM on a sample of the dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The five idioms (i.e., varieties) of the Romansh language are largely\nstandardized and are taught in the schools of the respective communities in\nSwitzerland. In this paper, we present the first parallel corpus of Romansh\nidioms. The corpus is based on 291 schoolbook volumes, which are comparable in\ncontent for the five idioms. We use automatic alignment methods to extract 207k\nmulti-parallel segments from the books, with more than 2M tokens in total. A\nsmall-scale human evaluation confirms that the segments are highly parallel,\nmaking the dataset suitable for NLP applications such as machine translation\nbetween Romansh idioms. We release the parallel and unaligned versions of the\ndataset under a CC-BY-NC-SA license and demonstrate its utility for machine\ntranslation by training and evaluating an LLM on a sample of the dataset."
                },
                "authors": [
                    {
                        "name": "Zachary Hopton"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Andrin Bchler"
                    },
                    {
                        "name": "Anna Rutkiewicz"
                    },
                    {
                        "name": "Rico Cathomas"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05220v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05220v3",
                "updated": "2025-08-22T13:24:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    24,
                    48,
                    4,
                    234,
                    0
                ],
                "published": "2025-04-07T16:05:52Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    5,
                    52,
                    0,
                    97,
                    0
                ],
                "title": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort\n  for Retrieval and RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort\n  for Retrieval and RAG"
                },
                "summary": "Retrieval models typically rely on costly human-labeled query-document\nrelevance annotations for training and evaluation. To reduce this cost and\nleverage the potential of Large Language Models (LLMs) in relevance judgments,\nwe aim to explore whether LLM-generated annotations can effectively replace\nhuman annotations in training retrieval models. Retrieval usually emphasizes\nrelevance, which indicates \"topic-relatedness\" of a document to a query, while\nin RAG, the value of a document (or utility) depends on how it contributes to\nanswer generation. Recognizing this mismatch, some researchers use LLM\nperformance on downstream tasks with documents as labels, but this approach\nrequires manual answers for specific tasks, leading to high costs and limited\ngeneralization. In another line of work, prompting LLMs to select useful\ndocuments as RAG references eliminates the need for human annotation and is not\ntask-specific. If we leverage LLMs' utility judgments to annotate retrieval\ndata, we may retain cross-task generalization without human annotation in\nlarge-scale corpora. Therefore, we investigate utility-focused annotation via\nLLMs for large-scale retriever training data across both in-domain and\nout-of-domain settings on the retrieval and RAG tasks. To reduce the impact of\nlow-quality positives labeled by LLMs, we design a novel loss function, i.e.,\nDisj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on\nutility-focused annotations significantly outperform those trained on human\nannotations in the out-of-domain setting on both tasks, demonstrating superior\ngeneralization capabilities. (2) LLM annotation does not replace human\nannotation in the in-domain setting. However, incorporating just 20%\nhuman-annotated data enables retrievers trained with utility-focused\nannotations to match the performance of models trained entirely with human\nannotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval models typically rely on costly human-labeled query-document\nrelevance annotations for training and evaluation. To reduce this cost and\nleverage the potential of Large Language Models (LLMs) in relevance judgments,\nwe aim to explore whether LLM-generated annotations can effectively replace\nhuman annotations in training retrieval models. Retrieval usually emphasizes\nrelevance, which indicates \"topic-relatedness\" of a document to a query, while\nin RAG, the value of a document (or utility) depends on how it contributes to\nanswer generation. Recognizing this mismatch, some researchers use LLM\nperformance on downstream tasks with documents as labels, but this approach\nrequires manual answers for specific tasks, leading to high costs and limited\ngeneralization. In another line of work, prompting LLMs to select useful\ndocuments as RAG references eliminates the need for human annotation and is not\ntask-specific. If we leverage LLMs' utility judgments to annotate retrieval\ndata, we may retain cross-task generalization without human annotation in\nlarge-scale corpora. Therefore, we investigate utility-focused annotation via\nLLMs for large-scale retriever training data across both in-domain and\nout-of-domain settings on the retrieval and RAG tasks. To reduce the impact of\nlow-quality positives labeled by LLMs, we design a novel loss function, i.e.,\nDisj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on\nutility-focused annotations significantly outperform those trained on human\nannotations in the out-of-domain setting on both tasks, demonstrating superior\ngeneralization capabilities. (2) LLM annotation does not replace human\nannotation in the in-domain setting. However, incorporating just 20%\nhuman-annotated data enables retrievers trained with utility-focused\nannotations to match the performance of models trained entirely with human\nannotations."
                },
                "authors": [
                    {
                        "name": "Hengran Zhang"
                    },
                    {
                        "name": "Minghao Tang"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Shihao Liu"
                    },
                    {
                        "name": "Daiting Shi"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Accepted by the EMNLP25 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05220v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05220v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.14501v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.14501v3",
                "updated": "2025-08-22T13:16:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    16,
                    50,
                    4,
                    234,
                    0
                ],
                "published": "2022-06-29T09:40:48Z",
                "published_parsed": [
                    2022,
                    6,
                    29,
                    9,
                    40,
                    48,
                    2,
                    180,
                    0
                ],
                "title": "From chambers to echo chambers: Quantifying polarization with a\n  second-neighbor approach applied to Twitter's climate discussion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From chambers to echo chambers: Quantifying polarization with a\n  second-neighbor approach applied to Twitter's climate discussion"
                },
                "summary": "Social media platforms often foster environments where users primarily engage\nwith content that aligns with their existing beliefs, thereby reinforcing their\nviews and limiting exposure to opposing viewpoints. In this paper, we analyze X\n(formerly Twitter) discussions on climate change throughout 2019, using an\nunsupervised method centered on chambers--second-order information sources--to\nuncover ideological patterns at scale. Beyond direct connections, chambers\ncapture shared sources of influence, revealing polarization dynamics\nefficiently and effectively. Analyzing retweet patterns, we identify echo\nchambers of climate believers and skeptics, revealing strong chamber overlap\nwithin ideological groups and minimal overlap between them, resulting in a\nrobust bimodal structure that characterizes polarization. Our method enables us\nto infer the stance of high-impact users based on their audience's chamber\nalignment, allowing for the classification of over half the retweeting\npopulation with minimal cross-group interaction, in what we term augmented echo\nchamber classification. We benchmark our approach against manual labeling and a\nstate-of-the-art latent ideology model, finding comparable performance but with\nnearly four times greater coverage. Moreover, we find that echo chamber\nstructures remain stable over time, even as their members change significantly,\nsuggesting that these structures are a persistent and emergent property of the\nsystem. Notably, polarization decreases and climate skepticism rises during the\n#FridaysForFuture strikes in September 2019. This chamber-based analysis offers\nvaluable insights into the persistence and fluidity of ideological polarization\non social media.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social media platforms often foster environments where users primarily engage\nwith content that aligns with their existing beliefs, thereby reinforcing their\nviews and limiting exposure to opposing viewpoints. In this paper, we analyze X\n(formerly Twitter) discussions on climate change throughout 2019, using an\nunsupervised method centered on chambers--second-order information sources--to\nuncover ideological patterns at scale. Beyond direct connections, chambers\ncapture shared sources of influence, revealing polarization dynamics\nefficiently and effectively. Analyzing retweet patterns, we identify echo\nchambers of climate believers and skeptics, revealing strong chamber overlap\nwithin ideological groups and minimal overlap between them, resulting in a\nrobust bimodal structure that characterizes polarization. Our method enables us\nto infer the stance of high-impact users based on their audience's chamber\nalignment, allowing for the classification of over half the retweeting\npopulation with minimal cross-group interaction, in what we term augmented echo\nchamber classification. We benchmark our approach against manual labeling and a\nstate-of-the-art latent ideology model, finding comparable performance but with\nnearly four times greater coverage. Moreover, we find that echo chamber\nstructures remain stable over time, even as their members change significantly,\nsuggesting that these structures are a persistent and emergent property of the\nsystem. Notably, polarization decreases and climate skepticism rises during the\n#FridaysForFuture strikes in September 2019. This chamber-based analysis offers\nvaluable insights into the persistence and fluidity of ideological polarization\non social media."
                },
                "authors": [
                    {
                        "name": "Blas Kolic"
                    },
                    {
                        "name": "Fabin Aguirre-Lpez"
                    },
                    {
                        "name": "Sergio Hernndez-Williams"
                    },
                    {
                        "name": "Guillermo Garduo-Hernndez"
                    }
                ],
                "author_detail": {
                    "name": "Guillermo Garduo-Hernndez"
                },
                "author": "Guillermo Garduo-Hernndez",
                "arxiv_doi": "10.1093/comnet/cnaf020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/comnet/cnaf020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2206.14501v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.14501v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "34 pages (22 of main text + 12 of appendices), 13 figures, 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62P25, 91C20, 91D30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21054v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21054v5",
                "updated": "2025-08-22T13:16:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    16,
                    29,
                    4,
                    234,
                    0
                ],
                "published": "2024-07-24T12:07:54Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    7,
                    54,
                    2,
                    206,
                    0
                ],
                "title": "Sentiment Reasoning for Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment Reasoning for Healthcare"
                },
                "summary": "Transparency in AI healthcare decision-making is crucial. By incorporating\nrationales to explain reason for each predicted label, users could understand\nLarge Language Models (LLMs)'s reasoning to make better decision. In this work,\nwe introduce a new task - Sentiment Reasoning - for both speech and text\nmodalities, and our proposed multimodal multitask framework and the world's\nlargest multimodal sentiment analysis dataset. Sentiment Reasoning is an\nauxiliary task in sentiment analysis where the model predicts both the\nsentiment label and generates the rationale behind it based on the input\ntranscript. Our study conducted on both human transcripts and Automatic Speech\nRecognition (ASR) transcripts shows that Sentiment Reasoning helps improve\nmodel transparency by providing rationale for model prediction with quality\nsemantically comparable to humans while also improving model's classification\nperformance (+2% increase in both accuracy and macro-F1) via\nrationale-augmented fine-tuning. Also, no significant difference in the\nsemantic quality of generated rationales between human and ASR transcripts. All\ncode, data (five languages - Vietnamese, English, Chinese, German, and French)\nand models are published online:\nhttps://github.com/leduckhai/Sentiment-Reasoning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transparency in AI healthcare decision-making is crucial. By incorporating\nrationales to explain reason for each predicted label, users could understand\nLarge Language Models (LLMs)'s reasoning to make better decision. In this work,\nwe introduce a new task - Sentiment Reasoning - for both speech and text\nmodalities, and our proposed multimodal multitask framework and the world's\nlargest multimodal sentiment analysis dataset. Sentiment Reasoning is an\nauxiliary task in sentiment analysis where the model predicts both the\nsentiment label and generates the rationale behind it based on the input\ntranscript. Our study conducted on both human transcripts and Automatic Speech\nRecognition (ASR) transcripts shows that Sentiment Reasoning helps improve\nmodel transparency by providing rationale for model prediction with quality\nsemantically comparable to humans while also improving model's classification\nperformance (+2% increase in both accuracy and macro-F1) via\nrationale-augmented fine-tuning. Also, no significant difference in the\nsemantic quality of generated rationales between human and ASR transcripts. All\ncode, data (five languages - Vietnamese, English, Chinese, German, and French)\nand models are published online:\nhttps://github.com/leduckhai/Sentiment-Reasoning"
                },
                "authors": [
                    {
                        "name": "Khai-Nguyen Nguyen"
                    },
                    {
                        "name": "Khai Le-Duc"
                    },
                    {
                        "name": "Bach Phan Tat"
                    },
                    {
                        "name": "Duy Le"
                    },
                    {
                        "name": "Long Vo-Dang"
                    },
                    {
                        "name": "Truong-Son Hy"
                    }
                ],
                "author_detail": {
                    "name": "Truong-Son Hy"
                },
                "author": "Truong-Son Hy",
                "arxiv_comment": "ACL 2025 Industry Track (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21054v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21054v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16357v1",
                "updated": "2025-08-22T13:04:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    4,
                    43,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T13:04:43Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    4,
                    43,
                    4,
                    234,
                    0
                ],
                "title": "MizanQA: Benchmarking Large Language Models on Moroccan Legal Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MizanQA: Benchmarking Large Language Models on Moroccan Legal Question\n  Answering"
                },
                "summary": "The rapid advancement of large language models (LLMs) has significantly\npropelled progress in natural language processing (NLP). However, their\neffectiveness in specialized, low-resource domains-such as Arabic legal\ncontexts-remains limited. This paper introduces MizanQA (pronounced Mizan,\nmeaning \"scale\" in Arabic, a universal symbol of justice), a benchmark designed\nto evaluate LLMs on Moroccan legal question answering (QA) tasks, characterised\nby rich linguistic and legal complexity. The dataset draws on Modern Standard\nArabic, Islamic Maliki jurisprudence, Moroccan customary law, and French legal\ninfluences. Comprising over 1,700 multiple-choice questions, including\nmulti-answer formats, MizanQA captures the nuances of authentic legal\nreasoning. Benchmarking experiments with multilingual and Arabic-focused LLMs\nreveal substantial performance gaps, highlighting the need for tailored\nevaluation metrics and culturally grounded, domain-specific LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has significantly\npropelled progress in natural language processing (NLP). However, their\neffectiveness in specialized, low-resource domains-such as Arabic legal\ncontexts-remains limited. This paper introduces MizanQA (pronounced Mizan,\nmeaning \"scale\" in Arabic, a universal symbol of justice), a benchmark designed\nto evaluate LLMs on Moroccan legal question answering (QA) tasks, characterised\nby rich linguistic and legal complexity. The dataset draws on Modern Standard\nArabic, Islamic Maliki jurisprudence, Moroccan customary law, and French legal\ninfluences. Comprising over 1,700 multiple-choice questions, including\nmulti-answer formats, MizanQA captures the nuances of authentic legal\nreasoning. Benchmarking experiments with multilingual and Arabic-focused LLMs\nreveal substantial performance gaps, highlighting the need for tailored\nevaluation metrics and culturally grounded, domain-specific LLM development."
                },
                "authors": [
                    {
                        "name": "Adil Bahaj"
                    },
                    {
                        "name": "Mounir Ghogho"
                    }
                ],
                "author_detail": {
                    "name": "Mounir Ghogho"
                },
                "author": "Mounir Ghogho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13408v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13408v2",
                "updated": "2025-08-22T13:02:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    2,
                    19,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-19T00:04:48Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    0,
                    4,
                    48,
                    1,
                    231,
                    0
                ],
                "title": "NovoMolGen: Rethinking Molecular Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NovoMolGen: Rethinking Molecular Language Model Pretraining"
                },
                "summary": "Designing de-novo molecules with desired property profiles requires efficient\nexploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$\npossible synthesizable candidates. While various deep generative models have\nbeen developed to design small molecules using diverse input representations,\nMolecular Large Language Models (Mol-LLMs) based on string representations have\nemerged as a scalable approach capable of exploring billions of molecules.\nHowever, there remains limited understanding regarding how standard language\nmodeling practices such as textual representations, tokenization strategies,\nmodel size, and dataset scale impact molecular generation performance. In this\nwork, we systematically investigate these critical aspects by introducing\nNovoMolGen, a family of transformer-based foundation models pretrained on 1.5\nbillion molecules for de-novo molecule generation. Through extensive empirical\nanalyses, we identify a weak correlation between performance metrics measured\nduring pretraining and actual downstream performance, revealing important\ndistinctions between molecular and general NLP training dynamics. NovoMolGen\nestablishes new state-of-the-art results, substantially outperforming prior\nMol-LLMs and specialized generative models in both unconstrained and\ngoal-directed molecular generation tasks, thus providing a robust foundation\nfor advancing efficient and effective molecular modeling strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing de-novo molecules with desired property profiles requires efficient\nexploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$\npossible synthesizable candidates. While various deep generative models have\nbeen developed to design small molecules using diverse input representations,\nMolecular Large Language Models (Mol-LLMs) based on string representations have\nemerged as a scalable approach capable of exploring billions of molecules.\nHowever, there remains limited understanding regarding how standard language\nmodeling practices such as textual representations, tokenization strategies,\nmodel size, and dataset scale impact molecular generation performance. In this\nwork, we systematically investigate these critical aspects by introducing\nNovoMolGen, a family of transformer-based foundation models pretrained on 1.5\nbillion molecules for de-novo molecule generation. Through extensive empirical\nanalyses, we identify a weak correlation between performance metrics measured\nduring pretraining and actual downstream performance, revealing important\ndistinctions between molecular and general NLP training dynamics. NovoMolGen\nestablishes new state-of-the-art results, substantially outperforming prior\nMol-LLMs and specialized generative models in both unconstrained and\ngoal-directed molecular generation tasks, thus providing a robust foundation\nfor advancing efficient and effective molecular modeling strategies."
                },
                "authors": [
                    {
                        "name": "Kamran Chitsaz"
                    },
                    {
                        "name": "Roshan Balaji"
                    },
                    {
                        "name": "Quentin Fournier"
                    },
                    {
                        "name": "Nirav Pravinbhai Bhatt"
                    },
                    {
                        "name": "Sarath Chandar"
                    }
                ],
                "author_detail": {
                    "name": "Sarath Chandar"
                },
                "author": "Sarath Chandar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13408v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07495v2",
                "updated": "2025-08-22T13:02:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    2,
                    16,
                    4,
                    234,
                    0
                ],
                "published": "2024-10-10T00:13:59Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    0,
                    13,
                    59,
                    3,
                    284,
                    0
                ],
                "title": "PublicHearingBR: A Brazilian Portuguese Dataset of Public Hearing\n  Transcripts for Summarization of Long Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PublicHearingBR: A Brazilian Portuguese Dataset of Public Hearing\n  Transcripts for Summarization of Long Documents"
                },
                "summary": "This paper introduces PublicHearingBR, a Brazilian Portuguese dataset\ndesigned for summarizing long documents. The dataset consists of transcripts of\npublic hearings held by the Brazilian Chamber of Deputies, paired with news\narticles and structured summaries containing the individuals participating in\nthe hearing and their statements or opinions. The dataset supports the\ndevelopment and evaluation of long document summarization systems in\nPortuguese. Our contributions include the dataset, a hybrid summarization\nsystem to establish a baseline for future studies, and a discussion of\nevaluation metrics for summarization involving large language models,\naddressing the challenge of hallucination in the generated summaries. As a\nresult of this discussion, the dataset also includes annotated data to evaluate\nnatural language inference tasks in Portuguese.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PublicHearingBR, a Brazilian Portuguese dataset\ndesigned for summarizing long documents. The dataset consists of transcripts of\npublic hearings held by the Brazilian Chamber of Deputies, paired with news\narticles and structured summaries containing the individuals participating in\nthe hearing and their statements or opinions. The dataset supports the\ndevelopment and evaluation of long document summarization systems in\nPortuguese. Our contributions include the dataset, a hybrid summarization\nsystem to establish a baseline for future studies, and a discussion of\nevaluation metrics for summarization involving large language models,\naddressing the challenge of hallucination in the generated summaries. As a\nresult of this discussion, the dataset also includes annotated data to evaluate\nnatural language inference tasks in Portuguese."
                },
                "authors": [
                    {
                        "name": "Leandro Carsio Fernandes"
                    },
                    {
                        "name": "Guilherme Zeferino Rodrigues Dobins"
                    },
                    {
                        "name": "Roberto Lotufo"
                    },
                    {
                        "name": "Jayr Alencar Pereira"
                    }
                ],
                "author_detail": {
                    "name": "Jayr Alencar Pereira"
                },
                "author": "Jayr Alencar Pereira",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21184v2",
                "updated": "2025-08-22T12:57:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    57,
                    8,
                    4,
                    234,
                    0
                ],
                "published": "2025-05-27T13:33:57Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    33,
                    57,
                    1,
                    147,
                    0
                ],
                "title": "PoisonSwarm: Universal Harmful Information Synthesis via Model\n  Crowdsourcing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoisonSwarm: Universal Harmful Information Synthesis via Model\n  Crowdsourcing"
                },
                "summary": "To construct responsible and secure AI applications, harmful information data\nis widely utilized for adversarial testing and the development of safeguards.\nExisting studies mainly leverage Large Language Models (LLMs) to synthesize\ndata to obtain high-quality task datasets at scale, thereby avoiding costly\nhuman annotation. However, limited by the safety alignment mechanisms of LLMs,\nthe synthesis of harmful data still faces challenges in generation reliability\nand content diversity. In this study, we propose a novel harmful information\nsynthesis framework, PoisonSwarm, which applies the model crowdsourcing\nstrategy to generate diverse harmful data while maintaining a high success\nrate. Specifically, we generate abundant benign data as the based templates in\na counterfactual manner. Subsequently, we decompose each based template into\nmultiple semantic units and perform unit-by-unit toxification and final\nrefinement through dynamic model switching, thus ensuring the success of\nsynthesis. Experimental results demonstrate that PoisonSwarm achieves\nstate-of-the-art performance in synthesizing different categories of harmful\ndata with high scalability and diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To construct responsible and secure AI applications, harmful information data\nis widely utilized for adversarial testing and the development of safeguards.\nExisting studies mainly leverage Large Language Models (LLMs) to synthesize\ndata to obtain high-quality task datasets at scale, thereby avoiding costly\nhuman annotation. However, limited by the safety alignment mechanisms of LLMs,\nthe synthesis of harmful data still faces challenges in generation reliability\nand content diversity. In this study, we propose a novel harmful information\nsynthesis framework, PoisonSwarm, which applies the model crowdsourcing\nstrategy to generate diverse harmful data while maintaining a high success\nrate. Specifically, we generate abundant benign data as the based templates in\na counterfactual manner. Subsequently, we decompose each based template into\nmultiple semantic units and perform unit-by-unit toxification and final\nrefinement through dynamic model switching, thus ensuring the success of\nsynthesis. Experimental results demonstrate that PoisonSwarm achieves\nstate-of-the-art performance in synthesizing different categories of harmful\ndata with high scalability and diversity."
                },
                "authors": [
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zhifei Zheng"
                    },
                    {
                        "name": "Ziji Hao"
                    },
                    {
                        "name": "Teli Liu"
                    },
                    {
                        "name": "Min Liu"
                    }
                ],
                "author_detail": {
                    "name": "Min Liu"
                },
                "author": "Min Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00132v2",
                "updated": "2025-08-22T12:52:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    52,
                    38,
                    4,
                    234,
                    0
                ],
                "published": "2025-03-31T18:33:55Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    18,
                    33,
                    55,
                    0,
                    90,
                    0
                ],
                "title": "Contextualize-then-Aggregate: Circuits for In-Context Learning in\n  Gemma-2 2B",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualize-then-Aggregate: Circuits for In-Context Learning in\n  Gemma-2 2B"
                },
                "summary": "In-Context Learning (ICL) is an intriguing ability of large language models\n(LLMs). Despite a substantial amount of work on its behavioral aspects and how\nit emerges in miniature setups, it remains unclear which mechanism assembles\ntask information from the individual examples in a fewshot prompt. We use\ncausal interventions to identify information flow in Gemma-2 2B for five\nnaturalistic ICL tasks. We find that the model infers task information using a\ntwo-step strategy we call contextualize-then-aggregate: In the lower layers,\nthe model builds up representations of individual fewshot examples, which are\ncontextualized by preceding examples through connections between fewshot input\nand output tokens across the sequence. In the higher layers, these\nrepresentations are aggregated to identify the task and prepare prediction of\nthe next output. The importance of the contextualization step differs between\ntasks, and it may become more important in the presence of ambiguous examples.\nOverall, by providing rigorous causal analysis, our results shed light on the\nmechanisms through which ICL happens in language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) is an intriguing ability of large language models\n(LLMs). Despite a substantial amount of work on its behavioral aspects and how\nit emerges in miniature setups, it remains unclear which mechanism assembles\ntask information from the individual examples in a fewshot prompt. We use\ncausal interventions to identify information flow in Gemma-2 2B for five\nnaturalistic ICL tasks. We find that the model infers task information using a\ntwo-step strategy we call contextualize-then-aggregate: In the lower layers,\nthe model builds up representations of individual fewshot examples, which are\ncontextualized by preceding examples through connections between fewshot input\nand output tokens across the sequence. In the higher layers, these\nrepresentations are aggregated to identify the task and prepare prediction of\nthe next output. The importance of the contextualization step differs between\ntasks, and it may become more important in the presence of ambiguous examples.\nOverall, by providing rigorous causal analysis, our results shed light on the\nmechanisms through which ICL happens in language models."
                },
                "authors": [
                    {
                        "name": "Aleksandra Bakalova"
                    },
                    {
                        "name": "Yana Veitsman"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Michael Hahn"
                    }
                ],
                "author_detail": {
                    "name": "Michael Hahn"
                },
                "author": "Michael Hahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16349v1",
                "updated": "2025-08-22T12:41:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    41,
                    55,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T12:41:55Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    41,
                    55,
                    4,
                    234,
                    0
                ],
                "title": "Identification of Nonlinear Damping of Transverse Loop Oscillations by\n  KHI-induced Turbulence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identification of Nonlinear Damping of Transverse Loop Oscillations by\n  KHI-induced Turbulence"
                },
                "summary": "Kink oscillations in coronal loops have been extensively studied for their\npotential contributions to coronal heating and their role in plasma diagnostics\nthrough coronal seismology. A key focus is the strong damping of\nlarge-amplitude kink oscillations, which observational evidence suggests is\nnonlinear. However, directly identifying the nonlinearity is a challenge. This\nwork presents an analytic formula describing nonlinear standing kink\noscillations dissipated by turbulence, characterised by a time-varying damping\nrate and period drift. We investigate how the damping behaviour depends on the\ndriving amplitude and loop properties, showing that the initial damping time\n$\\tau$ is inversely proportional to the velocity disturbance over the loop\nradius, $V_i/R$. Using MCMC fitting with Bayesian inference, the nonlinear\nfunction better fits an observed decaying kink oscillation than traditional\nlinear models, including exponential damping, suggesting its nonlinear nature.\nBy applying a Bayesian model comparison, we establish regimes in which\nnonlinear and linear resonant absorption mechanisms dominate based on the\nrelationship between the damping rate $\\tau/P$ and $V_i/R$. Additionally,\nanalysis of two specific events reveals that while one favours the nonlinear\nmodel, the other is better explained by the linear model. Our results suggest\nthat this analytical approximation of nonlinear damping due to turbulence\nprovides a valid and reliable description of large-amplitude decaying kink\noscillations in coronal loops.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kink oscillations in coronal loops have been extensively studied for their\npotential contributions to coronal heating and their role in plasma diagnostics\nthrough coronal seismology. A key focus is the strong damping of\nlarge-amplitude kink oscillations, which observational evidence suggests is\nnonlinear. However, directly identifying the nonlinearity is a challenge. This\nwork presents an analytic formula describing nonlinear standing kink\noscillations dissipated by turbulence, characterised by a time-varying damping\nrate and period drift. We investigate how the damping behaviour depends on the\ndriving amplitude and loop properties, showing that the initial damping time\n$\\tau$ is inversely proportional to the velocity disturbance over the loop\nradius, $V_i/R$. Using MCMC fitting with Bayesian inference, the nonlinear\nfunction better fits an observed decaying kink oscillation than traditional\nlinear models, including exponential damping, suggesting its nonlinear nature.\nBy applying a Bayesian model comparison, we establish regimes in which\nnonlinear and linear resonant absorption mechanisms dominate based on the\nrelationship between the damping rate $\\tau/P$ and $V_i/R$. Additionally,\nanalysis of two specific events reveals that while one favours the nonlinear\nmodel, the other is better explained by the linear model. Our results suggest\nthat this analytical approximation of nonlinear damping due to turbulence\nprovides a valid and reliable description of large-amplitude decaying kink\noscillations in coronal loops."
                },
                "authors": [
                    {
                        "name": "Sihui Zhong"
                    },
                    {
                        "name": "Andrew Hillier"
                    },
                    {
                        "name": "Iigo Arregui"
                    }
                ],
                "author_detail": {
                    "name": "Iigo Arregui"
                },
                "author": "Iigo Arregui",
                "arxiv_doi": "10.3847/1538-4357/adfd5c",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/adfd5c",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.16349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 9 figures, 3 tables, accepted by ApJ",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16347v1",
                "updated": "2025-08-22T12:41:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    41,
                    26,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T12:41:26Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    41,
                    26,
                    4,
                    234,
                    0
                ],
                "title": "Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and\n  Investigating the Real Misuse Threat of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and\n  Investigating the Real Misuse Threat of LLMs"
                },
                "summary": "With the development of Large Language Models (LLMs), numerous efforts have\nrevealed their vulnerabilities to jailbreak attacks. Although these studies\nhave driven the progress in LLMs' safety alignment, it remains unclear whether\nLLMs have internalized authentic knowledge to deal with real-world crimes, or\nare merely forced to simulate toxic language patterns. This ambiguity raises\nconcerns that jailbreak success is often attributable to a hallucination loop\nbetween jailbroken LLM and judger LLM. By decoupling the use of jailbreak\ntechniques, we construct knowledge-intensive Q\\&A to investigate the misuse\nthreats of LLMs in terms of dangerous knowledge possession, harmful task\nplanning utility, and harmfulness judgment robustness. Experiments reveal a\nmismatch between jailbreak success rates and harmful knowledge possession in\nLLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness\njudgments on toxic language patterns. Our study reveals a gap between existing\nLLM safety assessments and real-world threat potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of Large Language Models (LLMs), numerous efforts have\nrevealed their vulnerabilities to jailbreak attacks. Although these studies\nhave driven the progress in LLMs' safety alignment, it remains unclear whether\nLLMs have internalized authentic knowledge to deal with real-world crimes, or\nare merely forced to simulate toxic language patterns. This ambiguity raises\nconcerns that jailbreak success is often attributable to a hallucination loop\nbetween jailbroken LLM and judger LLM. By decoupling the use of jailbreak\ntechniques, we construct knowledge-intensive Q\\&A to investigate the misuse\nthreats of LLMs in terms of dangerous knowledge possession, harmful task\nplanning utility, and harmfulness judgment robustness. Experiments reveal a\nmismatch between jailbreak success rates and harmful knowledge possession in\nLLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness\njudgments on toxic language patterns. Our study reveals a gap between existing\nLLM safety assessments and real-world threat potential."
                },
                "authors": [
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Yijun Lin"
                    },
                    {
                        "name": "Zenghao Duan"
                    },
                    {
                        "name": "zhifei zheng"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Zhiyi yin"
                    },
                    {
                        "name": "Jianping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jianping Zhang"
                },
                "author": "Jianping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00038v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00038v4",
                "updated": "2025-08-22T12:35:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    35,
                    34,
                    4,
                    234,
                    0
                ],
                "published": "2025-02-25T08:41:25Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    8,
                    41,
                    25,
                    1,
                    56,
                    0
                ],
                "title": "from Benign import Toxic: Jailbreaking the Language Model via\n  Adversarial Metaphors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "from Benign import Toxic: Jailbreaking the Language Model via\n  Adversarial Metaphors"
                },
                "summary": "Current studies have exposed the risk of Large Language Models (LLMs)\ngenerating harmful content by jailbreak attacks. However, they overlook that\nthe direct generation of harmful content from scratch is more difficult than\ninducing LLM to calibrate benign content into harmful forms. In our study, we\nintroduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR)\nto induce the LLM to calibrate malicious metaphors for jailbreaking.\nSpecifically, to answer harmful queries, AVATAR adaptively identifies a set of\nbenign but logically related metaphors as the initial seed. Then, driven by\nthese metaphors, the target LLM is induced to reason and calibrate about the\nmetaphorical content, thus jailbroken by either directly outputting harmful\nresponses or calibrating residuals between metaphorical and professional\nharmful content. Experimental results demonstrate that AVATAR can effectively\nand transferable jailbreak LLMs and achieve a state-of-the-art attack success\nrate across multiple advanced LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current studies have exposed the risk of Large Language Models (LLMs)\ngenerating harmful content by jailbreak attacks. However, they overlook that\nthe direct generation of harmful content from scratch is more difficult than\ninducing LLM to calibrate benign content into harmful forms. In our study, we\nintroduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR)\nto induce the LLM to calibrate malicious metaphors for jailbreaking.\nSpecifically, to answer harmful queries, AVATAR adaptively identifies a set of\nbenign but logically related metaphors as the initial seed. Then, driven by\nthese metaphors, the target LLM is induced to reason and calibrate about the\nmetaphorical content, thus jailbroken by either directly outputting harmful\nresponses or calibrating residuals between metaphorical and professional\nharmful content. Experimental results demonstrate that AVATAR can effectively\nand transferable jailbreak LLMs and achieve a state-of-the-art attack success\nrate across multiple advanced LLMs."
                },
                "authors": [
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zenghao Duan"
                    },
                    {
                        "name": "Teli Liu"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Zhiyi Yin"
                    },
                    {
                        "name": "Jiangyu Lei"
                    },
                    {
                        "name": "Qi Li"
                    }
                ],
                "author_detail": {
                    "name": "Qi Li"
                },
                "author": "Qi Li",
                "arxiv_doi": "10.18653/v1/2025.acl-long.238",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.238",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.00038v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00038v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2412.12145",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06360v2",
                "updated": "2025-08-22T12:32:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    32,
                    59,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-08T14:46:05Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    46,
                    5,
                    4,
                    220,
                    0
                ],
                "title": "Cyberbullying Detection via Aggression-Enhanced Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyberbullying Detection via Aggression-Enhanced Prompting"
                },
                "summary": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks."
                },
                "authors": [
                    {
                        "name": "Aisha Saeid"
                    },
                    {
                        "name": "Anu Sabu"
                    },
                    {
                        "name": "Girish A. Koushik"
                    },
                    {
                        "name": "Ferrante Neri"
                    },
                    {
                        "name": "Diptesh Kanojia"
                    }
                ],
                "author_detail": {
                    "name": "Diptesh Kanojia"
                },
                "author": "Diptesh Kanojia",
                "arxiv_comment": "Accepted to RANLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02954v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02954v4",
                "updated": "2025-08-22T12:30:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    30,
                    44,
                    4,
                    234,
                    0
                ],
                "published": "2025-06-03T14:47:22Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    47,
                    22,
                    1,
                    154,
                    0
                ],
                "title": "Mutation-Guided Unit Test Generation with a Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-Guided Unit Test Generation with a Large Language Model"
                },
                "summary": "Unit tests play a vital role in uncovering potential faults in software.\nWhile tools like EvoSuite focus on maximizing code coverage, recent advances in\nlarge language models (LLMs) have shifted attention toward LLM-based test\ngeneration. However, code coverage metrics -- such as line and branch coverage\n-- remain overly emphasized in reported research, despite being weak indicators\nof a test suite's fault-detection capability. In contrast, mutation score\noffers a more reliable and stringent measure, as demonstrated in our findings\nwhere some test suites achieve 100% coverage but only 4% mutation score.\nAlthough a few studies consider mutation score, the effectiveness of LLMs in\nkilling mutants remains underexplored. In this paper, we propose MUTGEN, a\nmutation-guided, LLM-based test generation approach that incorporates mutation\nfeedback directly into the prompt. Evaluated on 204 subjects from two\nbenchmarks, MUTGEN significantly outperforms both EvoSuite and vanilla\nprompt-based strategies in terms of mutation score. Furthermore, MUTGEN\nintroduces an iterative generation mechanism that pushes the limits of LLMs in\nkilling additional mutants. Our study also provide insights into the\nlimitations of LLM-based generation, analyzing the reasons for live and\nuncovered mutants, and the impact of different mutation operators on generation\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit tests play a vital role in uncovering potential faults in software.\nWhile tools like EvoSuite focus on maximizing code coverage, recent advances in\nlarge language models (LLMs) have shifted attention toward LLM-based test\ngeneration. However, code coverage metrics -- such as line and branch coverage\n-- remain overly emphasized in reported research, despite being weak indicators\nof a test suite's fault-detection capability. In contrast, mutation score\noffers a more reliable and stringent measure, as demonstrated in our findings\nwhere some test suites achieve 100% coverage but only 4% mutation score.\nAlthough a few studies consider mutation score, the effectiveness of LLMs in\nkilling mutants remains underexplored. In this paper, we propose MUTGEN, a\nmutation-guided, LLM-based test generation approach that incorporates mutation\nfeedback directly into the prompt. Evaluated on 204 subjects from two\nbenchmarks, MUTGEN significantly outperforms both EvoSuite and vanilla\nprompt-based strategies in terms of mutation score. Furthermore, MUTGEN\nintroduces an iterative generation mechanism that pushes the limits of LLMs in\nkilling additional mutants. Our study also provide insights into the\nlimitations of LLM-based generation, analyzing the reasons for live and\nuncovered mutants, and the impact of different mutation operators on generation\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Guancheng Wang"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Lionel C. Briand"
                    },
                    {
                        "name": "Kui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kui Liu"
                },
                "author": "Kui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02954v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02954v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16334v1",
                "updated": "2025-08-22T12:21:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    21,
                    37,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T12:21:37Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    21,
                    37,
                    4,
                    234,
                    0
                ],
                "title": "From Linear to Hierarchical: Evolving Tree-structured Thoughts for\n  Efficient Alpha Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Linear to Hierarchical: Evolving Tree-structured Thoughts for\n  Efficient Alpha Mining"
                },
                "summary": "Alpha mining, which discovers signals that predict asset returns, has long\nbeen attractive for automatic quantitative investment. This problem is\ntypically formulated as a tree-based symbolic regression with handcrafted\nmarket data features and arithmetic operators. Unfortunately, existing symbolic\nmethods are concerned with computational inefficiency and dependence on prior\nknowledge. Recent implementation of Large Language Models (LLMs) show that they\ncan automatically generate executable codes for various tasks efficiently, thus\ncan be considered as a new promising way for alpha mining. Specifically,\nLLMs-driven methods evolve a set of heuristics, including thoughts and codes,\nwhere the thoughts are usually represented as plain-text prompts of codes.\nUnfortunately, trivially adopting them in alpha mining ignores the fact that\nalphas are with hierarchical tree structures. This paper introduces\nTree-structured thought Evolution (TreEvo), which evolves hierarchical\nreasoning ideas solely at the thought level. Experiments on four real-market\ndatasets demonstrate that TreEvo can obtain better alphas with much less\ncomputational time and human expert efforts. And this superiority hardly holds\nwithout the tree-structured thoughts and the compatible evolutionary operators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alpha mining, which discovers signals that predict asset returns, has long\nbeen attractive for automatic quantitative investment. This problem is\ntypically formulated as a tree-based symbolic regression with handcrafted\nmarket data features and arithmetic operators. Unfortunately, existing symbolic\nmethods are concerned with computational inefficiency and dependence on prior\nknowledge. Recent implementation of Large Language Models (LLMs) show that they\ncan automatically generate executable codes for various tasks efficiently, thus\ncan be considered as a new promising way for alpha mining. Specifically,\nLLMs-driven methods evolve a set of heuristics, including thoughts and codes,\nwhere the thoughts are usually represented as plain-text prompts of codes.\nUnfortunately, trivially adopting them in alpha mining ignores the fact that\nalphas are with hierarchical tree structures. This paper introduces\nTree-structured thought Evolution (TreEvo), which evolves hierarchical\nreasoning ideas solely at the thought level. Experiments on four real-market\ndatasets demonstrate that TreEvo can obtain better alphas with much less\ncomputational time and human expert efforts. And this superiority hardly holds\nwithout the tree-structured thoughts and the compatible evolutionary operators."
                },
                "authors": [
                    {
                        "name": "Junji Ren"
                    },
                    {
                        "name": "Junjie Zhao"
                    },
                    {
                        "name": "Shengcai Liu"
                    },
                    {
                        "name": "Peng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Yang"
                },
                "author": "Peng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11244v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11244v2",
                "updated": "2025-08-22T12:17:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    17,
                    17,
                    4,
                    234,
                    0
                ],
                "published": "2025-02-16T19:44:01Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    19,
                    44,
                    1,
                    6,
                    47,
                    0
                ],
                "title": "Soteria: Language-Specific Functional Parameter Steering for\n  Multilingual Safety Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soteria: Language-Specific Functional Parameter Steering for\n  Multilingual Safety Alignment"
                },
                "summary": "Ensuring consistent safety across multiple languages remains a significant\nchallenge for large language models (LLMs). We introduce Soteria, a lightweight\nyet powerful strategy that locates and minimally adjusts the \"functional heads\"\nmost responsible for harmful content generation in each language. By altering\nonly a fraction of parameters, Soteria drastically reduces policy violations\nwithout sacrificing overall model performance, even in low-resource settings.\nTo rigorously evaluate our approach, we also present XThreatBench, a\nspecialized multilingual dataset capturing fine-grained harmful behaviors drawn\nfrom real policy guidelines. Experiments with leading open-source LLMs (e.g.,\nLlama, Qwen, Mistral) show that Soteria consistently improves safety metrics\nacross high-, mid-, and low-resource languages. These findings highlight a\npromising path toward scalable, linguistically attuned, and ethically aligned\nLLMs worldwide.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring consistent safety across multiple languages remains a significant\nchallenge for large language models (LLMs). We introduce Soteria, a lightweight\nyet powerful strategy that locates and minimally adjusts the \"functional heads\"\nmost responsible for harmful content generation in each language. By altering\nonly a fraction of parameters, Soteria drastically reduces policy violations\nwithout sacrificing overall model performance, even in low-resource settings.\nTo rigorously evaluate our approach, we also present XThreatBench, a\nspecialized multilingual dataset capturing fine-grained harmful behaviors drawn\nfrom real policy guidelines. Experiments with leading open-source LLMs (e.g.,\nLlama, Qwen, Mistral) show that Soteria consistently improves safety metrics\nacross high-, mid-, and low-resource languages. These findings highlight a\npromising path toward scalable, linguistically attuned, and ethically aligned\nLLMs worldwide."
                },
                "authors": [
                    {
                        "name": "Somnath Banerjee"
                    },
                    {
                        "name": "Sayan Layek"
                    },
                    {
                        "name": "Pratyush Chatterjee"
                    },
                    {
                        "name": "Animesh Mukherjee"
                    },
                    {
                        "name": "Rima Hazra"
                    }
                ],
                "author_detail": {
                    "name": "Rima Hazra"
                },
                "author": "Rima Hazra",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11244v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11244v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16329v1",
                "updated": "2025-08-22T12:15:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    15,
                    22,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T12:15:22Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    15,
                    22,
                    4,
                    234,
                    0
                ],
                "title": "Simulation based inference of the ionization history from the 2D 21 cm\n  power spectrum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation based inference of the ionization history from the 2D 21 cm\n  power spectrum"
                },
                "summary": "The 21 cm signal contains a wealth of information about the formation of the\nfirst stars and the reionization of the intergalactic medium during the Cosmic\nDawn (CD) and Epoch of Reionization (EoR). The timing of these important\nmilestones has only roughly been constrained through indirect measurements,\nsuch as from the cosmic microwave background (CMB) optical depth, and\nLyman-$\\alpha$ forest. Therefore, inferring the neutral fraction over cosmic\ntime is a goal of upcoming 21 cm experiments, such as the Square Kilometer\nArray (SKA). We contrast two approaches to infer astrophysical parameters and\nionization history from 21 cm 2D power spectra (2DPS). We develop an emulator\nof the 21 cm 2DPS, trained on 21cmFAST simulations, taking into account the\nexpected instrumental noise from the SKA and sample variance. We then perform\nsimulation based inference (SBI) using neural posterior estimation (NPE). We\ncompare training on datasets of noisy 2DPS obtained from 21cmFAST simulations\nand an emulator, to infer astrophysical parameters of interest. Using an\nemulator of the ionization history, which has been trained on simulations from\nthe same astrophysical parameters, we then obtain posterior distributions of\nthe ionization history over the redshift range z $\\sim$ 5-12. We demonstrate\nthat both methods are capable of accurately recovering the ionization history\nand astrophysical parameters. However, coverage tests indicate that adding\nemulated samples does not improve predictions. This work suggests that due to\nthe stochastic nature of the 2DPS, using an emulator of this summary statistic\nmay result in poorer inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 21 cm signal contains a wealth of information about the formation of the\nfirst stars and the reionization of the intergalactic medium during the Cosmic\nDawn (CD) and Epoch of Reionization (EoR). The timing of these important\nmilestones has only roughly been constrained through indirect measurements,\nsuch as from the cosmic microwave background (CMB) optical depth, and\nLyman-$\\alpha$ forest. Therefore, inferring the neutral fraction over cosmic\ntime is a goal of upcoming 21 cm experiments, such as the Square Kilometer\nArray (SKA). We contrast two approaches to infer astrophysical parameters and\nionization history from 21 cm 2D power spectra (2DPS). We develop an emulator\nof the 21 cm 2DPS, trained on 21cmFAST simulations, taking into account the\nexpected instrumental noise from the SKA and sample variance. We then perform\nsimulation based inference (SBI) using neural posterior estimation (NPE). We\ncompare training on datasets of noisy 2DPS obtained from 21cmFAST simulations\nand an emulator, to infer astrophysical parameters of interest. Using an\nemulator of the ionization history, which has been trained on simulations from\nthe same astrophysical parameters, we then obtain posterior distributions of\nthe ionization history over the redshift range z $\\sim$ 5-12. We demonstrate\nthat both methods are capable of accurately recovering the ionization history\nand astrophysical parameters. However, coverage tests indicate that adding\nemulated samples does not improve predictions. This work suggests that due to\nthe stochastic nature of the 2DPS, using an emulator of this summary statistic\nmay result in poorer inference."
                },
                "authors": [
                    {
                        "name": "Nadia Cooper"
                    },
                    {
                        "name": "Carina Norregaard"
                    },
                    {
                        "name": "Romain Meriot"
                    },
                    {
                        "name": "Jonathan R. Pritchard"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan R. Pritchard"
                },
                "author": "Jonathan R. Pritchard",
                "arxiv_comment": "16 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16326v1",
                "updated": "2025-08-22T12:13:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    13,
                    49,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T12:13:49Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    13,
                    49,
                    4,
                    234,
                    0
                ],
                "title": "Heterogeneous Quantile Treatment Effect Estimation for Longitudinal Data\n  with High-Dimensional Confounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Quantile Treatment Effect Estimation for Longitudinal Data\n  with High-Dimensional Confounding"
                },
                "summary": "Causal inference plays a fundamental role in various real-world applications.\nHowever, in the motivating non-small cell lung cancer (NSCLC) study, it is\nchallenging to estimate the treatment effect of chemotherapy on circulating\ntumor DNA (ctDNA). First, the heterogeneous treatment effects vary across\npatient subgroups defined by baseline characteristics. Second, there exists a\nbroad set of demographic, clinical and molecular variables act as potential\nconfounders. Third, ctDNA trajectories over time show heavy-tailed non-Gaussian\nbehavior. Finally, repeated measurements within subjects introduce unknown\ncorrelation. Combining convolution-smoothed quantile regression and orthogonal\nrandom forest, we propose a framework to estimate heterogeneous quantile\ntreatment effects in the presence of high-dimensional confounding, which not\nonly captures effect heterogeneity across covariates, but also behaves robustly\nto nuisance parameter estimation error. We establish the theoretical properties\nof the proposed estimator and demonstrate its finite-sample performance through\ncomprehensive simulations. We illustrate its practical utility in the motivated\nNSCLC study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference plays a fundamental role in various real-world applications.\nHowever, in the motivating non-small cell lung cancer (NSCLC) study, it is\nchallenging to estimate the treatment effect of chemotherapy on circulating\ntumor DNA (ctDNA). First, the heterogeneous treatment effects vary across\npatient subgroups defined by baseline characteristics. Second, there exists a\nbroad set of demographic, clinical and molecular variables act as potential\nconfounders. Third, ctDNA trajectories over time show heavy-tailed non-Gaussian\nbehavior. Finally, repeated measurements within subjects introduce unknown\ncorrelation. Combining convolution-smoothed quantile regression and orthogonal\nrandom forest, we propose a framework to estimate heterogeneous quantile\ntreatment effects in the presence of high-dimensional confounding, which not\nonly captures effect heterogeneity across covariates, but also behaves robustly\nto nuisance parameter estimation error. We establish the theoretical properties\nof the proposed estimator and demonstrate its finite-sample performance through\ncomprehensive simulations. We illustrate its practical utility in the motivated\nNSCLC study."
                },
                "authors": [
                    {
                        "name": "Zhixin Qiu"
                    },
                    {
                        "name": "Huichen Zhu"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yanlin Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yanlin Tang"
                },
                "author": "Yanlin Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16325v1",
                "updated": "2025-08-22T12:13:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    13,
                    38,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T12:13:38Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    13,
                    38,
                    4,
                    234,
                    0
                ],
                "title": "LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging\n  Interpretable Jailbreak Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging\n  Interpretable Jailbreak Concepts"
                },
                "summary": "Large Language Models have found success in a variety of applications;\nhowever, their safety remains a matter of concern due to the existence of\nvarious types of jailbreaking methods. Despite significant efforts, alignment\nand safety fine-tuning only provide a certain degree of robustness against\njailbreak attacks that covertly mislead LLMs towards the generation of harmful\ncontent. This leaves them prone to a number of vulnerabilities, ranging from\ntargeted misuse to accidental profiling of users. This work introduces\n\\textbf{LLMSymGuard}, a novel framework that leverages Sparse Autoencoders\n(SAEs) to identify interpretable concepts within LLM internals associated with\ndifferent jailbreak themes. By extracting semantically meaningful internal\nrepresentations, LLMSymGuard enables building symbolic, logical safety\nguardrails -- offering transparent and robust defenses without sacrificing\nmodel capabilities or requiring further fine-tuning. Leveraging advances in\nmechanistic interpretability of LLMs, our approach demonstrates that LLMs learn\nhuman-interpretable concepts from jailbreaks, and provides a foundation for\ndesigning more interpretable and logical safeguard measures against attackers.\nCode will be released upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have found success in a variety of applications;\nhowever, their safety remains a matter of concern due to the existence of\nvarious types of jailbreaking methods. Despite significant efforts, alignment\nand safety fine-tuning only provide a certain degree of robustness against\njailbreak attacks that covertly mislead LLMs towards the generation of harmful\ncontent. This leaves them prone to a number of vulnerabilities, ranging from\ntargeted misuse to accidental profiling of users. This work introduces\n\\textbf{LLMSymGuard}, a novel framework that leverages Sparse Autoencoders\n(SAEs) to identify interpretable concepts within LLM internals associated with\ndifferent jailbreak themes. By extracting semantically meaningful internal\nrepresentations, LLMSymGuard enables building symbolic, logical safety\nguardrails -- offering transparent and robust defenses without sacrificing\nmodel capabilities or requiring further fine-tuning. Leveraging advances in\nmechanistic interpretability of LLMs, our approach demonstrates that LLMs learn\nhuman-interpretable concepts from jailbreaks, and provides a foundation for\ndesigning more interpretable and logical safeguard measures against attackers.\nCode will be released upon publication."
                },
                "authors": [
                    {
                        "name": "Darpan Aswal"
                    },
                    {
                        "name": "Cline Hudelot"
                    }
                ],
                "author_detail": {
                    "name": "Cline Hudelot"
                },
                "author": "Cline Hudelot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14540v2",
                "updated": "2025-08-22T12:13:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    13,
                    32,
                    4,
                    234,
                    0
                ],
                "published": "2025-01-24T14:45:21Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    45,
                    21,
                    4,
                    24,
                    0
                ],
                "title": "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning"
                },
                "summary": "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems"
                },
                "authors": [
                    {
                        "name": "Benjamin Callewaert"
                    },
                    {
                        "name": "Simon Vandevelde"
                    },
                    {
                        "name": "Joost Vennekens"
                    }
                ],
                "author_detail": {
                    "name": "Joost Vennekens"
                },
                "author": "Joost Vennekens",
                "arxiv_comment": "Accepted at ICLP 2025, part of ECPTS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13824v2",
                "updated": "2025-08-22T12:12:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    12,
                    9,
                    4,
                    234,
                    0
                ],
                "published": "2025-01-23T16:45:51Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    45,
                    51,
                    3,
                    23,
                    0
                ],
                "title": "Can Hallucinations Help? Boosting LLMs for Drug Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Hallucinations Help? Boosting LLMs for Drug Discovery"
                },
                "summary": "Hallucinations in large language models (LLMs), plausible but factually\ninaccurate text, are often viewed as undesirable. However, recent work suggests\nthat such outputs may hold creative potential. In this paper, we investigate\nwhether hallucinations can improve LLMs on molecule property prediction, a key\ntask in early-stage drug discovery. We prompt LLMs to generate natural language\ndescriptions from molecular SMILES strings and incorporate these often\nhallucinated descriptions into downstream classification tasks. Evaluating\nseven instruction-tuned LLMs across five datasets, we find that hallucinations\nsignificantly improve predictive accuracy for some models. Notably,\nFalcon3-Mamba-7B outperforms all baselines when hallucinated text is included,\nwhile hallucinations generated by GPT-4o consistently yield the greatest gains\nbetween models. We further identify and categorize over 18,000 beneficial\nhallucinations, with structural misdescriptions emerging as the most impactful\ntype, suggesting that hallucinated statements about molecular structure may\nincrease model confidence. Ablation studies show that larger models benefit\nmore from hallucinations, while temperature has a limited effect. Our findings\nchallenge conventional views of hallucination as purely problematic and suggest\nnew directions for leveraging hallucinations as a useful signal in scientific\nmodeling tasks like drug discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in large language models (LLMs), plausible but factually\ninaccurate text, are often viewed as undesirable. However, recent work suggests\nthat such outputs may hold creative potential. In this paper, we investigate\nwhether hallucinations can improve LLMs on molecule property prediction, a key\ntask in early-stage drug discovery. We prompt LLMs to generate natural language\ndescriptions from molecular SMILES strings and incorporate these often\nhallucinated descriptions into downstream classification tasks. Evaluating\nseven instruction-tuned LLMs across five datasets, we find that hallucinations\nsignificantly improve predictive accuracy for some models. Notably,\nFalcon3-Mamba-7B outperforms all baselines when hallucinated text is included,\nwhile hallucinations generated by GPT-4o consistently yield the greatest gains\nbetween models. We further identify and categorize over 18,000 beneficial\nhallucinations, with structural misdescriptions emerging as the most impactful\ntype, suggesting that hallucinated statements about molecular structure may\nincrease model confidence. Ablation studies show that larger models benefit\nmore from hallucinations, while temperature has a limited effect. Our findings\nchallenge conventional views of hallucination as purely problematic and suggest\nnew directions for leveraging hallucinations as a useful signal in scientific\nmodeling tasks like drug discovery."
                },
                "authors": [
                    {
                        "name": "Shuzhou Yuan"
                    },
                    {
                        "name": "Zhan Qu"
                    },
                    {
                        "name": "Ashish Yashwanth Kangen"
                    },
                    {
                        "name": "Michael Frber"
                    }
                ],
                "author_detail": {
                    "name": "Michael Frber"
                },
                "author": "Michael Frber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03427v2",
                "updated": "2025-08-22T11:58:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    58,
                    27,
                    4,
                    234,
                    0
                ],
                "published": "2025-05-06T11:07:26Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    7,
                    26,
                    1,
                    126,
                    0
                ],
                "title": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare."
                },
                "authors": [
                    {
                        "name": "Mouath Abu Daoud"
                    },
                    {
                        "name": "Chaimae Abouzahir"
                    },
                    {
                        "name": "Leen Kharouf"
                    },
                    {
                        "name": "Walid Al-Eisawi"
                    },
                    {
                        "name": "Nizar Habash"
                    },
                    {
                        "name": "Farah E. Shamout"
                    }
                ],
                "author_detail": {
                    "name": "Farah E. Shamout"
                },
                "author": "Farah E. Shamout",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16318v1",
                "updated": "2025-08-22T11:57:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    57,
                    55,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T11:57:55Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    57,
                    55,
                    4,
                    234,
                    0
                ],
                "title": "SATORI: Static Test Oracle Generation for REST APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SATORI: Static Test Oracle Generation for REST APIs"
                },
                "summary": "REST API test case generation tools are evolving rapidly, with growing\ncapabilities for the automated generation of complex tests. However, despite\ntheir strengths in test data generation, these tools are constrained by the\ntypes of test oracles they support, often limited to crashes, regressions, and\nnoncompliance with API specifications or design standards. This paper\nintroduces SATORI (Static API Test ORacle Inference), a black-box approach for\ngenerating test oracles for REST APIs by analyzing their OpenAPI Specification.\nSATORI uses large language models to infer the expected behavior of an API by\nanalyzing the properties of the response fields of its operations, such as\ntheir name and descriptions. To foster its adoption, we extended the\nPostmanAssertify tool to automatically convert the test oracles reported by\nSATORI into executable assertions. Evaluation results on 17 operations from 12\nindustrial APIs show that SATORI can automatically generate up to hundreds of\nvalid test oracles per operation. SATORI achieved an F1-score of 74.3%,\noutperforming the state-of-the-art dynamic approach AGORA+ (69.3%)-which\nrequires executing the API-when generating comparable oracle types. Moreover,\nour findings show that static and dynamic oracle inference methods are\ncomplementary: together, SATORI and AGORA+ found 90% of the oracles in our\nannotated ground-truth dataset. Notably, SATORI uncovered 18 bugs in popular\nAPIs (Amadeus Hotel, Deutschebahn, FDIC, GitLab, Marvel, OMDb and Vimeo)\nleading to documentation updates by the API maintainers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REST API test case generation tools are evolving rapidly, with growing\ncapabilities for the automated generation of complex tests. However, despite\ntheir strengths in test data generation, these tools are constrained by the\ntypes of test oracles they support, often limited to crashes, regressions, and\nnoncompliance with API specifications or design standards. This paper\nintroduces SATORI (Static API Test ORacle Inference), a black-box approach for\ngenerating test oracles for REST APIs by analyzing their OpenAPI Specification.\nSATORI uses large language models to infer the expected behavior of an API by\nanalyzing the properties of the response fields of its operations, such as\ntheir name and descriptions. To foster its adoption, we extended the\nPostmanAssertify tool to automatically convert the test oracles reported by\nSATORI into executable assertions. Evaluation results on 17 operations from 12\nindustrial APIs show that SATORI can automatically generate up to hundreds of\nvalid test oracles per operation. SATORI achieved an F1-score of 74.3%,\noutperforming the state-of-the-art dynamic approach AGORA+ (69.3%)-which\nrequires executing the API-when generating comparable oracle types. Moreover,\nour findings show that static and dynamic oracle inference methods are\ncomplementary: together, SATORI and AGORA+ found 90% of the oracles in our\nannotated ground-truth dataset. Notably, SATORI uncovered 18 bugs in popular\nAPIs (Amadeus Hotel, Deutschebahn, FDIC, GitLab, Marvel, OMDb and Vimeo)\nleading to documentation updates by the API maintainers."
                },
                "authors": [
                    {
                        "name": "Juan C. Alonso"
                    },
                    {
                        "name": "Alberto Martin-Lopez"
                    },
                    {
                        "name": "Sergio Segura"
                    },
                    {
                        "name": "Gabriele Bavota"
                    },
                    {
                        "name": "Antonio Ruiz-Corts"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Ruiz-Corts"
                },
                "author": "Antonio Ruiz-Corts",
                "arxiv_comment": "Accepted for publication at 40th IEEE/ACM International Conference on\n  Automated Software Engineering, ASE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16315v1",
                "updated": "2025-08-22T11:52:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    52,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T11:52:04Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    52,
                    4,
                    4,
                    234,
                    0
                ],
                "title": "OwkinZero: Accelerating Biological Discovery with AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OwkinZero: Accelerating Biological Discovery with AI"
                },
                "summary": "While large language models (LLMs) are rapidly advancing scientific research,\nthey continue to struggle with core biological reasoning tasks essential for\ntranslational and biomedical discovery. To address this limitation, we created\nand curated eight comprehensive benchmark datasets comprising over 300,000\nverifiable question-and-answer pairs, each targeting critical challenges in\ndrug discovery including target druggability, modality suitability, and drug\nperturbation effects. Using this resource, we developed the OwkinZero models by\npost-training open-source LLMs through a Reinforcement Learning from Verifiable\nRewards strategy. Our results demonstrate that specialized 8-32B OwkinZero\nmodels substantially outperform larger, state-of-the-art commercial LLMs on\nthese biological benchmarks. Remarkably, we uncover evidence of a key aspect of\ngeneralization: specialist models trained on a single task consistently\noutperform their base models on previously unseen tasks. This generalization\neffect is further amplified in our comprehensive OwkinZero models, which were\ntrained on a mixture of datasets and achieve even broader cross-task\nimprovements. This study represents a significant step toward addressing the\nbiological reasoning blind spot in current LLMs, demonstrating that targeted\nreinforcement learning on carefully curated data can unlock generalizable\nperformance in specialized models, thereby accelerating AI-driven biological\ndiscovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) are rapidly advancing scientific research,\nthey continue to struggle with core biological reasoning tasks essential for\ntranslational and biomedical discovery. To address this limitation, we created\nand curated eight comprehensive benchmark datasets comprising over 300,000\nverifiable question-and-answer pairs, each targeting critical challenges in\ndrug discovery including target druggability, modality suitability, and drug\nperturbation effects. Using this resource, we developed the OwkinZero models by\npost-training open-source LLMs through a Reinforcement Learning from Verifiable\nRewards strategy. Our results demonstrate that specialized 8-32B OwkinZero\nmodels substantially outperform larger, state-of-the-art commercial LLMs on\nthese biological benchmarks. Remarkably, we uncover evidence of a key aspect of\ngeneralization: specialist models trained on a single task consistently\noutperform their base models on previously unseen tasks. This generalization\neffect is further amplified in our comprehensive OwkinZero models, which were\ntrained on a mixture of datasets and achieve even broader cross-task\nimprovements. This study represents a significant step toward addressing the\nbiological reasoning blind spot in current LLMs, demonstrating that targeted\nreinforcement learning on carefully curated data can unlock generalizable\nperformance in specialized models, thereby accelerating AI-driven biological\ndiscovery."
                },
                "authors": [
                    {
                        "name": "Nathan Bigaud"
                    },
                    {
                        "name": "Vincent Cabeli"
                    },
                    {
                        "name": "Meltem Gurel"
                    },
                    {
                        "name": "Arthur Pignet"
                    },
                    {
                        "name": "John Klein"
                    },
                    {
                        "name": "Gilles Wainrib"
                    },
                    {
                        "name": "Eric Durand"
                    }
                ],
                "author_detail": {
                    "name": "Eric Durand"
                },
                "author": "Eric Durand",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16313v1",
                "updated": "2025-08-22T11:50:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    50,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T11:50:04Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    50,
                    4,
                    4,
                    234,
                    0
                ],
                "title": "Retrieval Enhanced Feedback via In-context Neural Error-book",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Enhanced Feedback via In-context Neural Error-book"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved reasoning capabilities, with in-context learning (ICL) emerging as a\nkey technique for adaptation without retraining. While previous works have\nfocused on leveraging correct examples, recent research highlights the\nimportance of learning from errors to enhance performance. However, existing\nmethods lack a structured framework for analyzing and mitigating errors,\nparticularly in Multimodal Large Language Models (MLLMs), where integrating\nvisual and textual inputs adds complexity. To address this issue, we propose\nREFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a\nteacher-student framework that systematically structures errors and provides\ntargeted feedback. REFINE introduces three systematic queries to construct\nstructured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance\nmultimodal reasoning by prioritizing relevant visual information, diagnosing\ncritical failure points, and formulating corrective actions. Unlike prior\napproaches that rely on redundant retrievals, REFINE optimizes structured\nfeedback retrieval, improving inference efficiency, token usage, and\nscalability. Our results demonstrate substantial speedup, reduced computational\ncosts, and successful generalization, highlighting REFINE's potential for\nenhancing multimodal reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved reasoning capabilities, with in-context learning (ICL) emerging as a\nkey technique for adaptation without retraining. While previous works have\nfocused on leveraging correct examples, recent research highlights the\nimportance of learning from errors to enhance performance. However, existing\nmethods lack a structured framework for analyzing and mitigating errors,\nparticularly in Multimodal Large Language Models (MLLMs), where integrating\nvisual and textual inputs adds complexity. To address this issue, we propose\nREFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a\nteacher-student framework that systematically structures errors and provides\ntargeted feedback. REFINE introduces three systematic queries to construct\nstructured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance\nmultimodal reasoning by prioritizing relevant visual information, diagnosing\ncritical failure points, and formulating corrective actions. Unlike prior\napproaches that rely on redundant retrievals, REFINE optimizes structured\nfeedback retrieval, improving inference efficiency, token usage, and\nscalability. Our results demonstrate substantial speedup, reduced computational\ncosts, and successful generalization, highlighting REFINE's potential for\nenhancing multimodal reasoning."
                },
                "authors": [
                    {
                        "name": "Jongyeop Hyun"
                    },
                    {
                        "name": "Bumsoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Bumsoo Kim"
                },
                "author": "Bumsoo Kim",
                "arxiv_comment": "Accepted at EMNLP 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16312v1",
                "updated": "2025-08-22T11:48:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    48,
                    24,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T11:48:24Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    48,
                    24,
                    4,
                    234,
                    0
                ],
                "title": "Tree-based methods for length-biased survival data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree-based methods for length-biased survival data"
                },
                "summary": "Left-truncated survival data commonly arise in prevalent cohort studies,\nwhere only individuals who have experienced disease onset and survived until\nenrollment in the study. When the onset process follows a stationary Poisson\nprocess, the resulting data are length-biased. This sampling mechanism induces\na selection bias towards longer survival individuals, and nonparametric and\nsemiparametric methods for traditional survival data are not directly\napplicable. While tree-based methods developed for left-truncated data can be\napplied, they may be inefficient for length-biased data, as they do not account\nfor the distribution of truncation times. To address this, we propose new\nsurvival trees and forests for length-biased right-censored data within the\nconditional inference framework. Our approach uses a score function derived\nfrom the full likelihood to construct permutation test statistics for unbiased\nvariable selection. For survival prediction, we consider two estimators of the\nunbiased survival function, differing in statistical efficiency and\ncomputational complexity. These elements enhance efficiency in tree\nconstruction and improve accuracy of survival prediction in ensemble settings.\nSimulation studies demonstrate efficiency gains in both tree recovery and\nsurvival prediction, often exceeding the gains from ensembling alone. We\nfurther illustrate the utility of the proposed methods using lung cancer data\nfrom the Cancer Public Library Database.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Left-truncated survival data commonly arise in prevalent cohort studies,\nwhere only individuals who have experienced disease onset and survived until\nenrollment in the study. When the onset process follows a stationary Poisson\nprocess, the resulting data are length-biased. This sampling mechanism induces\na selection bias towards longer survival individuals, and nonparametric and\nsemiparametric methods for traditional survival data are not directly\napplicable. While tree-based methods developed for left-truncated data can be\napplied, they may be inefficient for length-biased data, as they do not account\nfor the distribution of truncation times. To address this, we propose new\nsurvival trees and forests for length-biased right-censored data within the\nconditional inference framework. Our approach uses a score function derived\nfrom the full likelihood to construct permutation test statistics for unbiased\nvariable selection. For survival prediction, we consider two estimators of the\nunbiased survival function, differing in statistical efficiency and\ncomputational complexity. These elements enhance efficiency in tree\nconstruction and improve accuracy of survival prediction in ensemble settings.\nSimulation studies demonstrate efficiency gains in both tree recovery and\nsurvival prediction, often exceeding the gains from ensembling alone. We\nfurther illustrate the utility of the proposed methods using lung cancer data\nfrom the Cancer Public Library Database."
                },
                "authors": [
                    {
                        "name": "Jinwoo Lee"
                    },
                    {
                        "name": "Jiyu Sun"
                    },
                    {
                        "name": "Hyunwoo Lee"
                    },
                    {
                        "name": "Donghwan Lee"
                    }
                ],
                "author_detail": {
                    "name": "Donghwan Lee"
                },
                "author": "Donghwan Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16311v1",
                "updated": "2025-08-22T11:43:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    43,
                    39,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T11:43:39Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    43,
                    39,
                    4,
                    234,
                    0
                ],
                "title": "Exploiting Information Redundancy in Attention Maps for Extreme\n  Quantization of Vision Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Information Redundancy in Attention Maps for Extreme\n  Quantization of Vision Transformers"
                },
                "summary": "Transformer models rely on Multi-Head Self-Attention (MHSA) mechanisms, where\neach attention head contributes to the final representation. However, their\ncomputational complexity and high memory demands due to MHSA hinders their\ndeployment at the edge. In this work, we analyze and exploit information\nredundancy in attention maps to accelerate model inference. By quantifying the\ninformation captured by each attention head using Shannon entropy, our analysis\nreveals that attention heads with lower entropy, i.e., exhibiting more\ndeterministic behavior, tend to contribute less information, motivating\ntargeted compression strategies. Relying on these insights, we propose Entropy\nAttention Maps (EAM), a model that freezes the weights of low-entropy attention\nmaps and quantizes these values to low precision to avoid redundant\nre-computation. Empirical validation on ImageNet-1k shows that EAM achieves\nsimilar or higher accuracy at $\\leq$20\\% sparsity in attention maps and\ncompetitive performance beyond this level for the DeiT and Swin Transformer\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models rely on Multi-Head Self-Attention (MHSA) mechanisms, where\neach attention head contributes to the final representation. However, their\ncomputational complexity and high memory demands due to MHSA hinders their\ndeployment at the edge. In this work, we analyze and exploit information\nredundancy in attention maps to accelerate model inference. By quantifying the\ninformation captured by each attention head using Shannon entropy, our analysis\nreveals that attention heads with lower entropy, i.e., exhibiting more\ndeterministic behavior, tend to contribute less information, motivating\ntargeted compression strategies. Relying on these insights, we propose Entropy\nAttention Maps (EAM), a model that freezes the weights of low-entropy attention\nmaps and quantizes these values to low precision to avoid redundant\nre-computation. Empirical validation on ImageNet-1k shows that EAM achieves\nsimilar or higher accuracy at $\\leq$20\\% sparsity in attention maps and\ncompetitive performance beyond this level for the DeiT and Swin Transformer\nmodels."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Karim Haroun"
                    },
                    {
                        "name": "Tom Pegeot"
                    }
                ],
                "author_detail": {
                    "name": "Tom Pegeot"
                },
                "author": "Tom Pegeot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13227v2",
                "updated": "2025-08-22T11:26:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    26,
                    53,
                    4,
                    234,
                    0
                ],
                "published": "2025-04-17T13:09:38Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    9,
                    38,
                    3,
                    107,
                    0
                ],
                "title": "DIDS: Domain Impact-aware Data Sampling for Large Language Model\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIDS: Domain Impact-aware Data Sampling for Large Language Model\n  Training"
                },
                "summary": "Large language models (LLMs) are commonly trained on multi-domain datasets,\nwhere domain sampling strategies significantly impact model performance due to\nvarying domain importance across downstream tasks. Existing approaches for\noptimizing domain-level sampling strategies struggle with maintaining\nintra-domain consistency and accurately measuring domain impact. In this paper,\nwe present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain\nconsistency, a gradient clustering algorithm is proposed to group training data\nbased on their learning effects, where a proxy language model and\ndimensionality reduction are employed to reduce computational overhead. To\naccurately measure domain impact, we develop a Fisher Information Matrix (FIM)\nguided metric that quantifies how domain-specific parameter updates affect the\nmodel's output distributions on downstream tasks, with theoretical guarantees.\nFurthermore, to determine optimal sampling ratios, DIDS combines both the\nFIM-guided domain impact assessment and loss learning trajectories that\nindicate domain-specific potential, while accounting for diminishing marginal\nreturns. Extensive experiments demonstrate that DIDS achieves 3.4% higher\naverage performance while maintaining comparable training efficiency. The code\nis available at https://github.com/shiweijiezero/DIDS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are commonly trained on multi-domain datasets,\nwhere domain sampling strategies significantly impact model performance due to\nvarying domain importance across downstream tasks. Existing approaches for\noptimizing domain-level sampling strategies struggle with maintaining\nintra-domain consistency and accurately measuring domain impact. In this paper,\nwe present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain\nconsistency, a gradient clustering algorithm is proposed to group training data\nbased on their learning effects, where a proxy language model and\ndimensionality reduction are employed to reduce computational overhead. To\naccurately measure domain impact, we develop a Fisher Information Matrix (FIM)\nguided metric that quantifies how domain-specific parameter updates affect the\nmodel's output distributions on downstream tasks, with theoretical guarantees.\nFurthermore, to determine optimal sampling ratios, DIDS combines both the\nFIM-guided domain impact assessment and loss learning trajectories that\nindicate domain-specific potential, while accounting for diminishing marginal\nreturns. Extensive experiments demonstrate that DIDS achieves 3.4% higher\naverage performance while maintaining comparable training efficiency. The code\nis available at https://github.com/shiweijiezero/DIDS."
                },
                "authors": [
                    {
                        "name": "Weijie Shi"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Yaguang Wu"
                    },
                    {
                        "name": "Jingzhi Fang"
                    },
                    {
                        "name": "Ruiyuan Zhang"
                    },
                    {
                        "name": "Jiajie Xu"
                    },
                    {
                        "name": "Jia Zhu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Sirui Han"
                    },
                    {
                        "name": "Xiaofang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofang Zhou"
                },
                "author": "Xiaofang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14913v2",
                "updated": "2025-08-22T11:14:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    14,
                    44,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-13T20:44:37Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    20,
                    44,
                    37,
                    2,
                    225,
                    0
                ],
                "title": "Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural\n  Localization of Math Word Problems in Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural\n  Localization of Math Word Problems in Low-Resource Languages"
                },
                "summary": "Large language models (LLMs) have demonstrated significant capabilities in\nsolving mathematical problems expressed in natural language. However,\nmultilingual and culturally-grounded mathematical reasoning in low-resource\nlanguages lags behind English due to the scarcity of socio-cultural task\ndatasets that reflect accurate native entities such as person names,\norganization names, and currencies. Existing multilingual benchmarks are\npredominantly produced via translation and typically retain English-centric\nentities, owing to the high cost associated with human annotater-based\nlocalization. Moreover, automated localization tools are limited, and hence,\ntruly localized datasets remain scarce. To bridge this gap, we introduce a\nframework for LLM-driven cultural localization of math word problems that\nautomatically constructs datasets with native names, organizations, and\ncurrencies from existing sources. We find that translated benchmarks can\nobscure true multilingual math ability under appropriate socio-cultural\ncontexts. Through extensive experiments, we also show that our framework can\nhelp mitigate English-centric entity bias and improves robustness when native\nentities are introduced across various languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant capabilities in\nsolving mathematical problems expressed in natural language. However,\nmultilingual and culturally-grounded mathematical reasoning in low-resource\nlanguages lags behind English due to the scarcity of socio-cultural task\ndatasets that reflect accurate native entities such as person names,\norganization names, and currencies. Existing multilingual benchmarks are\npredominantly produced via translation and typically retain English-centric\nentities, owing to the high cost associated with human annotater-based\nlocalization. Moreover, automated localization tools are limited, and hence,\ntruly localized datasets remain scarce. To bridge this gap, we introduce a\nframework for LLM-driven cultural localization of math word problems that\nautomatically constructs datasets with native names, organizations, and\ncurrencies from existing sources. We find that translated benchmarks can\nobscure true multilingual math ability under appropriate socio-cultural\ncontexts. Through extensive experiments, we also show that our framework can\nhelp mitigate English-centric entity bias and improves robustness when native\nentities are introduced across various languages."
                },
                "authors": [
                    {
                        "name": "Israel Abebe Azime"
                    },
                    {
                        "name": "Tadesse Destaw Belay"
                    },
                    {
                        "name": "Dietrich Klakow"
                    },
                    {
                        "name": "Philipp Slusallek"
                    },
                    {
                        "name": "Anshuman Chhabra"
                    }
                ],
                "author_detail": {
                    "name": "Anshuman Chhabra"
                },
                "author": "Anshuman Chhabra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16279v1",
                "updated": "2025-08-22T10:35:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    35,
                    56,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T10:35:56Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    35,
                    56,
                    4,
                    234,
                    0
                ],
                "title": "AgentScope 1.0: A Developer-Centric Framework for Building Agentic\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentScope 1.0: A Developer-Centric Framework for Building Agentic\n  Applications"
                },
                "summary": "Driven by rapid advancements of Large Language Models (LLMs), agents are\nempowered to combine intrinsic knowledge with dynamic tool use, greatly\nenhancing their capacity to address real-world tasks. In line with such an\nevolution, AgentScope introduces major improvements in a new version (1.0),\ntowards comprehensively supporting flexible and efficient tool-based\nagent-environment interactions for building agentic applications. Specifically,\nwe abstract foundational components essential for agentic applications and\nprovide unified interfaces and extensible modules, enabling developers to\neasily leverage the latest progress, such as new models and MCPs. Furthermore,\nwe ground agent behaviors in the ReAct paradigm and offer advanced agent-level\ninfrastructure based on a systematic asynchronous design, which enriches both\nhuman-agent and agent-agent interaction patterns while improving execution\nefficiency. Building on this foundation, we integrate several built-in agents\ntailored to specific practical scenarios. AgentScope also includes robust\nengineering support for developer-friendly experiences. We provide a scalable\nevaluation module with a visual studio interface, making the development of\nlong-trajectory agentic applications more manageable and easier to trace. In\naddition, AgentScope offers a runtime sandbox to ensure safe agent execution\nand facilitates rapid deployment in production environments. With these\nenhancements, AgentScope provides a practical foundation for building scalable,\nadaptive, and effective agentic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driven by rapid advancements of Large Language Models (LLMs), agents are\nempowered to combine intrinsic knowledge with dynamic tool use, greatly\nenhancing their capacity to address real-world tasks. In line with such an\nevolution, AgentScope introduces major improvements in a new version (1.0),\ntowards comprehensively supporting flexible and efficient tool-based\nagent-environment interactions for building agentic applications. Specifically,\nwe abstract foundational components essential for agentic applications and\nprovide unified interfaces and extensible modules, enabling developers to\neasily leverage the latest progress, such as new models and MCPs. Furthermore,\nwe ground agent behaviors in the ReAct paradigm and offer advanced agent-level\ninfrastructure based on a systematic asynchronous design, which enriches both\nhuman-agent and agent-agent interaction patterns while improving execution\nefficiency. Building on this foundation, we integrate several built-in agents\ntailored to specific practical scenarios. AgentScope also includes robust\nengineering support for developer-friendly experiences. We provide a scalable\nevaluation module with a visual studio interface, making the development of\nlong-trajectory agentic applications more manageable and easier to trace. In\naddition, AgentScope offers a runtime sandbox to ensure safe agent execution\nand facilitates rapid deployment in production environments. With these\nenhancements, AgentScope provides a practical foundation for building scalable,\nadaptive, and effective agentic applications."
                },
                "authors": [
                    {
                        "name": "Dawei Gao"
                    },
                    {
                        "name": "Zitao Li"
                    },
                    {
                        "name": "Yuexiang Xie"
                    },
                    {
                        "name": "Weirui Kuang"
                    },
                    {
                        "name": "Liuyi Yao"
                    },
                    {
                        "name": "Bingchen Qian"
                    },
                    {
                        "name": "Zhijian Ma"
                    },
                    {
                        "name": "Yue Cui"
                    },
                    {
                        "name": "Haohao Luo"
                    },
                    {
                        "name": "Shen Li"
                    },
                    {
                        "name": "Lu Yi"
                    },
                    {
                        "name": "Yi Yu"
                    },
                    {
                        "name": "Shiqi He"
                    },
                    {
                        "name": "Zhiling Luo"
                    },
                    {
                        "name": "Wenmeng Zhou"
                    },
                    {
                        "name": "Zhicheng Zhang"
                    },
                    {
                        "name": "Xuguang He"
                    },
                    {
                        "name": "Ziqian Chen"
                    },
                    {
                        "name": "Weikai Liao"
                    },
                    {
                        "name": "Farruh Isakulovich Kushnazarov"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00719v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00719v2",
                "updated": "2025-08-22T10:30:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    30,
                    51,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-01T15:38:21Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    38,
                    21,
                    4,
                    213,
                    0
                ],
                "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and\n  Context-Aware KGQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and\n  Context-Aware KGQA"
                },
                "summary": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yingxu Wang"
                    },
                    {
                        "name": "Shiqi Fan"
                    },
                    {
                        "name": "Mengzhu Wang"
                    },
                    {
                        "name": "Siyang Gao"
                    },
                    {
                        "name": "Siwei Liu"
                    },
                    {
                        "name": "Nan Yin"
                    }
                ],
                "author_detail": {
                    "name": "Nan Yin"
                },
                "author": "Nan Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00719v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00719v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07535v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07535v2",
                "updated": "2025-08-22T10:28:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    28,
                    35,
                    4,
                    234,
                    0
                ],
                "published": "2025-03-10T17:03:07Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    3,
                    7,
                    0,
                    69,
                    0
                ],
                "title": "LBM: Latent Bridge Matching for Fast Image-to-Image Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LBM: Latent Bridge Matching for Fast Image-to-Image Translation"
                },
                "summary": "In this paper, we introduce Latent Bridge Matching (LBM), a new, versatile\nand scalable method that relies on Bridge Matching in a latent space to achieve\nfast image-to-image translation. We show that the method can reach\nstate-of-the-art results for various image-to-image tasks using only a single\ninference step. In addition to its efficiency, we also demonstrate the\nversatility of the method across different image translation tasks such as\nobject removal, normal and depth estimation, and object relighting. We also\nderive a conditional framework of LBM and demonstrate its effectiveness by\ntackling the tasks of controllable image relighting and shadow generation. We\nprovide an implementation at https://github.com/gojasper/LBM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Latent Bridge Matching (LBM), a new, versatile\nand scalable method that relies on Bridge Matching in a latent space to achieve\nfast image-to-image translation. We show that the method can reach\nstate-of-the-art results for various image-to-image tasks using only a single\ninference step. In addition to its efficiency, we also demonstrate the\nversatility of the method across different image translation tasks such as\nobject removal, normal and depth estimation, and object relighting. We also\nderive a conditional framework of LBM and demonstrate its effectiveness by\ntackling the tasks of controllable image relighting and shadow generation. We\nprovide an implementation at https://github.com/gojasper/LBM."
                },
                "authors": [
                    {
                        "name": "Clment Chadebec"
                    },
                    {
                        "name": "Onur Tasar"
                    },
                    {
                        "name": "Sanjeev Sreetharan"
                    },
                    {
                        "name": "Benjamin Aubin"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Aubin"
                },
                "author": "Benjamin Aubin",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07535v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16277v1",
                "updated": "2025-08-22T10:19:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    19,
                    42,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T10:19:42Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    19,
                    42,
                    4,
                    234,
                    0
                ],
                "title": "The next question after Turing's question: Introducing the Grow-AI test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The next question after Turing's question: Introducing the Grow-AI test"
                },
                "summary": "This study aims to extend the framework for assessing artificial\nintelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),\ndesigned to answer the question \"Can machines grow up?\" -- a natural successor\nto the Turing Test. The methodology applied is based on a system of six primary\ncriteria (C1-C6), each assessed through a specific \"game\", divided into four\narenas that explore both the human dimension and its transposition into AI. All\ndecisions and actions of the entity are recorded in a standardized AI Journal,\nthe primary source for calculating composite scores. The assessment uses the\nprior expert method to establish initial weights, and the global score -- Grow\nUp Index -- is calculated as the arithmetic mean of the six scores, with\ninterpretation on maturity thresholds. The results show that the methodology\nallows for a coherent and comparable assessment of the level of \"growth\" of AI\nentities, regardless of their type (robots, software agents, LLMs). The\nmulti-game structure highlights strengths and vulnerable areas, and the use of\na unified journal guarantees traceability and replicability in the evaluation.\nThe originality of the work lies in the conceptual transposition of the process\nof \"growing\" from the human world to that of artificial intelligence, in an\nintegrated testing format that combines perspectives from psychology, robotics,\ncomputer science, and ethics. Through this approach, GROW-AI not only measures\nperformance but also captures the evolutionary path of an AI entity towards\nmaturity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study aims to extend the framework for assessing artificial\nintelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),\ndesigned to answer the question \"Can machines grow up?\" -- a natural successor\nto the Turing Test. The methodology applied is based on a system of six primary\ncriteria (C1-C6), each assessed through a specific \"game\", divided into four\narenas that explore both the human dimension and its transposition into AI. All\ndecisions and actions of the entity are recorded in a standardized AI Journal,\nthe primary source for calculating composite scores. The assessment uses the\nprior expert method to establish initial weights, and the global score -- Grow\nUp Index -- is calculated as the arithmetic mean of the six scores, with\ninterpretation on maturity thresholds. The results show that the methodology\nallows for a coherent and comparable assessment of the level of \"growth\" of AI\nentities, regardless of their type (robots, software agents, LLMs). The\nmulti-game structure highlights strengths and vulnerable areas, and the use of\na unified journal guarantees traceability and replicability in the evaluation.\nThe originality of the work lies in the conceptual transposition of the process\nof \"growing\" from the human world to that of artificial intelligence, in an\nintegrated testing format that combines perspectives from psychology, robotics,\ncomputer science, and ethics. Through this approach, GROW-AI not only measures\nperformance but also captures the evolutionary path of an AI entity towards\nmaturity."
                },
                "authors": [
                    {
                        "name": "Alexandru Tugui"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Tugui"
                },
                "author": "Alexandru Tugui",
                "arxiv_comment": "9th International Conference on Inventive Systems and Control ICISC\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01, 68T05, 68T42, 91A80",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; K.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16270v1",
                "updated": "2025-08-22T10:13:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    13,
                    13,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T10:13:13Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    13,
                    13,
                    4,
                    234,
                    0
                ],
                "title": "LLMs that Understand Processes: Instruction-tuning for Semantics-Aware\n  Process Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs that Understand Processes: Instruction-tuning for Semantics-Aware\n  Process Mining"
                },
                "summary": "Process mining is increasingly using textual information associated with\nevents to tackle tasks such as anomaly detection and process discovery. Such\nsemantics-aware process mining focuses on what behavior should be possible in a\nprocess (i.e., expectations), thus providing an important complement to\ntraditional, frequency-based techniques that focus on recorded behavior (i.e.,\nreality). Large Language Models (LLMs) provide a powerful means for tackling\nsemantics-aware tasks. However, the best performance is so far achieved through\ntask-specific fine-tuning, which is computationally intensive and results in\nmodels that can only handle one specific task. To overcome this lack of\ngeneralization, we use this paper to investigate the potential of\ninstruction-tuning for semantics-aware process mining. The idea of\ninstruction-tuning here is to expose an LLM to prompt-answer pairs for\ndifferent tasks, e.g., anomaly detection and next-activity prediction, making\nit more familiar with process mining, thus allowing it to also perform better\nat unseen tasks, such as process discovery. Our findings demonstrate a varied\nimpact of instruction-tuning: while performance considerably improved on\nprocess discovery and prediction tasks, it varies across models on anomaly\ndetection tasks, highlighting that the selection of tasks for\ninstruction-tuning is critical to achieving desired outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process mining is increasingly using textual information associated with\nevents to tackle tasks such as anomaly detection and process discovery. Such\nsemantics-aware process mining focuses on what behavior should be possible in a\nprocess (i.e., expectations), thus providing an important complement to\ntraditional, frequency-based techniques that focus on recorded behavior (i.e.,\nreality). Large Language Models (LLMs) provide a powerful means for tackling\nsemantics-aware tasks. However, the best performance is so far achieved through\ntask-specific fine-tuning, which is computationally intensive and results in\nmodels that can only handle one specific task. To overcome this lack of\ngeneralization, we use this paper to investigate the potential of\ninstruction-tuning for semantics-aware process mining. The idea of\ninstruction-tuning here is to expose an LLM to prompt-answer pairs for\ndifferent tasks, e.g., anomaly detection and next-activity prediction, making\nit more familiar with process mining, thus allowing it to also perform better\nat unseen tasks, such as process discovery. Our findings demonstrate a varied\nimpact of instruction-tuning: while performance considerably improved on\nprocess discovery and prediction tasks, it varies across models on anomaly\ndetection tasks, highlighting that the selection of tasks for\ninstruction-tuning is critical to achieving desired outcomes."
                },
                "authors": [
                    {
                        "name": "Vira Pyrih"
                    },
                    {
                        "name": "Adrian Rebmann"
                    },
                    {
                        "name": "Han van der Aa"
                    }
                ],
                "author_detail": {
                    "name": "Han van der Aa"
                },
                "author": "Han van der Aa",
                "arxiv_comment": "Accepted at IEEE ICPM 2025, 8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16267v1",
                "updated": "2025-08-22T09:59:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    59,
                    23,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T09:59:23Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    59,
                    23,
                    4,
                    234,
                    0
                ],
                "title": "From Confidence to Collapse in LLM Factual Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Confidence to Collapse in LLM Factual Robustness"
                },
                "summary": "Ensuring the robustness of factual knowledge in LLMs is critical for reliable\napplications in tasks such as question answering and reasoning. However,\nexisting evaluation methods predominantly focus on performance-based metrics,\noften investigating from the perspective of prompt perturbations, which\ncaptures only the externally triggered side of knowledge robustness. To bridge\nthis gap, we introduce a principled approach to measure factual robustness from\nthe perspective of the generation process by analyzing token distribution\nentropy in combination with temperature scaling sensitivity. These two factors\nbuild the Factual Robustness Score (FRS), a novel metric which quantifies the\nstability of a fact against perturbations in decoding conditions, given its\ninitial uncertainty. To validate our approach, we conduct extensive experiments\non 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We\nshow that factual robustness varies significantly -- smaller models report an\nFRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\\%$ under\nincreased uncertainty. These insights demonstrate how entropy and temperature\nscaling impact factual accuracy, and lay a foundation for developing more\nrobust knowledge retention and retrieval in future models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the robustness of factual knowledge in LLMs is critical for reliable\napplications in tasks such as question answering and reasoning. However,\nexisting evaluation methods predominantly focus on performance-based metrics,\noften investigating from the perspective of prompt perturbations, which\ncaptures only the externally triggered side of knowledge robustness. To bridge\nthis gap, we introduce a principled approach to measure factual robustness from\nthe perspective of the generation process by analyzing token distribution\nentropy in combination with temperature scaling sensitivity. These two factors\nbuild the Factual Robustness Score (FRS), a novel metric which quantifies the\nstability of a fact against perturbations in decoding conditions, given its\ninitial uncertainty. To validate our approach, we conduct extensive experiments\non 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We\nshow that factual robustness varies significantly -- smaller models report an\nFRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\\%$ under\nincreased uncertainty. These insights demonstrate how entropy and temperature\nscaling impact factual accuracy, and lay a foundation for developing more\nrobust knowledge retention and retrieval in future models."
                },
                "authors": [
                    {
                        "name": "Alina Fastowski"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16265v1",
                "updated": "2025-08-22T09:57:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    57,
                    40,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T09:57:40Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    57,
                    40,
                    4,
                    234,
                    0
                ],
                "title": "M3TQA: Massively Multilingual Multitask Table Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M3TQA: Massively Multilingual Multitask Table Question Answering"
                },
                "summary": "Tabular data is a fundamental component of real-world information systems,\nyet most research in table understanding remains confined to English, leaving\nmultilingual comprehension significantly underexplored. Existing multilingual\ntable benchmarks suffer from geolinguistic imbalance - overrepresenting certain\nlanguages and lacking sufficient scale for rigorous cross-lingual analysis. To\naddress these limitations, we introduce a comprehensive framework for massively\nmultilingual multitask table question answering, featuring m3TQA-Instruct, a\nlarge-scale benchmark spanning 97 languages across diverse language families,\nincluding underrepresented and low-resource languages. We construct m3TQA by\ncurating 50 real-world tables in Chinese and English, then applying a robust\nsix-step LLM-based translation pipeline powered by DeepSeek and GPT-4o,\nachieving high translation fidelity with a median BLEU score of 60.19 as\nvalidated through back-translation. The benchmark includes 2,916 professionally\nannotated question-answering pairs across four tasks designed to evaluate\nnuanced table reasoning capabilities. Experiments on state-of-the-art LLMs\nreveal critical insights into cross-lingual generalization, demonstrating that\nsynthetically generated, unannotated QA data can significantly boost\nperformance, particularly for low-resource languages. M3T-Bench establishes a\nnew standard for multilingual table understanding, providing both a challenging\nevaluation platform and a scalable methodology for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data is a fundamental component of real-world information systems,\nyet most research in table understanding remains confined to English, leaving\nmultilingual comprehension significantly underexplored. Existing multilingual\ntable benchmarks suffer from geolinguistic imbalance - overrepresenting certain\nlanguages and lacking sufficient scale for rigorous cross-lingual analysis. To\naddress these limitations, we introduce a comprehensive framework for massively\nmultilingual multitask table question answering, featuring m3TQA-Instruct, a\nlarge-scale benchmark spanning 97 languages across diverse language families,\nincluding underrepresented and low-resource languages. We construct m3TQA by\ncurating 50 real-world tables in Chinese and English, then applying a robust\nsix-step LLM-based translation pipeline powered by DeepSeek and GPT-4o,\nachieving high translation fidelity with a median BLEU score of 60.19 as\nvalidated through back-translation. The benchmark includes 2,916 professionally\nannotated question-answering pairs across four tasks designed to evaluate\nnuanced table reasoning capabilities. Experiments on state-of-the-art LLMs\nreveal critical insights into cross-lingual generalization, demonstrating that\nsynthetically generated, unannotated QA data can significantly boost\nperformance, particularly for low-resource languages. M3T-Bench establishes a\nnew standard for multilingual table understanding, providing both a challenging\nevaluation platform and a scalable methodology for future research."
                },
                "authors": [
                    {
                        "name": "Daixin Shu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Zhenhe Wu"
                    },
                    {
                        "name": "Xianjie Wu"
                    },
                    {
                        "name": "Xianfu Cheng"
                    },
                    {
                        "name": "Xiangyuan Guan"
                    },
                    {
                        "name": "Yanghai Wang"
                    },
                    {
                        "name": "Pengfei Wu"
                    },
                    {
                        "name": "Tingyang Yang"
                    },
                    {
                        "name": "Hualei Zhu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16261v1",
                "updated": "2025-08-22T09:52:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    52,
                    31,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T09:52:31Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    52,
                    31,
                    4,
                    234,
                    0
                ],
                "title": "On the Evolution of Federated Post-Training Large Language Models: A\n  Model Accessibility View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Evolution of Federated Post-Training Large Language Models: A\n  Model Accessibility View"
                },
                "summary": "Federated Learning (FL) enables training models across decentralized data\nsilos while preserving client data privacy. Recent research has explored\nefficient methods for post-training large language models (LLMs) within FL to\naddress computational and communication challenges. While existing approaches\noften rely on access to LLMs' internal information, which is frequently\nrestricted in real-world scenarios, an inference-only paradigm (black-box\nFedLLM) has emerged to address these limitations. This paper presents a\ncomprehensive survey on federated tuning for LLMs. We propose a taxonomy\ncategorizing existing studies along two axes: model access-based and parameter\nefficiency-based optimization. We classify FedLLM approaches into white-box,\ngray-box, and black-box techniques, highlighting representative methods within\neach category. We review emerging research treating LLMs as black-box inference\nAPIs and discuss promising directions and open challenges for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables training models across decentralized data\nsilos while preserving client data privacy. Recent research has explored\nefficient methods for post-training large language models (LLMs) within FL to\naddress computational and communication challenges. While existing approaches\noften rely on access to LLMs' internal information, which is frequently\nrestricted in real-world scenarios, an inference-only paradigm (black-box\nFedLLM) has emerged to address these limitations. This paper presents a\ncomprehensive survey on federated tuning for LLMs. We propose a taxonomy\ncategorizing existing studies along two axes: model access-based and parameter\nefficiency-based optimization. We classify FedLLM approaches into white-box,\ngray-box, and black-box techniques, highlighting representative methods within\neach category. We review emerging research treating LLMs as black-box inference\nAPIs and discuss promising directions and open challenges for future research."
                },
                "authors": [
                    {
                        "name": "Tao Guo"
                    },
                    {
                        "name": "Junxiao Wang"
                    },
                    {
                        "name": "Fushuo Huo"
                    },
                    {
                        "name": "Laizhong Cui"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Jie Gui"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16260v1",
                "updated": "2025-08-22T09:47:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    47,
                    53,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T09:47:53Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    47,
                    53,
                    4,
                    234,
                    0
                ],
                "title": "MCPVerse: An Expansive, Real-World Benchmark for Agentic Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCPVerse: An Expansive, Real-World Benchmark for Agentic Tool Use"
                },
                "summary": "Large Language Models (LLMs) are evolving from text generators into reasoning\nagents. This transition makes their ability to use external tools a critical\ncapability. However, evaluating this skill presents a significant challenge.\nExisting benchmarks are often limited by their reliance on synthetic tools and\nseverely constrained action spaces. To address these limitations, we introduce\nMCPVerse, an expansive, real-world benchmark for evaluating agentic tool use.\nMCPVerse integrates more than 550 real-world, executable tools to create an\nunprecedented action space exceeding 140k tokens, and employs outcome-based\nevaluation with real-time ground truth for time-sensitive tasks. We benchmarked\nthe state-of-the-art LLMs across three modes (Oracle, Standard, and Max-Scale),\nrevealing that while most models suffer performance degradation when confronted\nwith larger tool sets, the agentic models, such as Claude-4-Sonnet, can\neffectively leverage expanded exploration spaces to improve accuracy. This\nfinding not only exposes the limitations of state-of-the-art models in complex,\nreal-world scenarios but also establishes MCPVerse as a critical benchmark for\nmeasuring and advancing agentic tool use capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are evolving from text generators into reasoning\nagents. This transition makes their ability to use external tools a critical\ncapability. However, evaluating this skill presents a significant challenge.\nExisting benchmarks are often limited by their reliance on synthetic tools and\nseverely constrained action spaces. To address these limitations, we introduce\nMCPVerse, an expansive, real-world benchmark for evaluating agentic tool use.\nMCPVerse integrates more than 550 real-world, executable tools to create an\nunprecedented action space exceeding 140k tokens, and employs outcome-based\nevaluation with real-time ground truth for time-sensitive tasks. We benchmarked\nthe state-of-the-art LLMs across three modes (Oracle, Standard, and Max-Scale),\nrevealing that while most models suffer performance degradation when confronted\nwith larger tool sets, the agentic models, such as Claude-4-Sonnet, can\neffectively leverage expanded exploration spaces to improve accuracy. This\nfinding not only exposes the limitations of state-of-the-art models in complex,\nreal-world scenarios but also establishes MCPVerse as a critical benchmark for\nmeasuring and advancing agentic tool use capabilities."
                },
                "authors": [
                    {
                        "name": "Fei Lei"
                    },
                    {
                        "name": "Yibo Yang"
                    },
                    {
                        "name": "Wenxiu Sun"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.00998v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00998v2",
                "updated": "2025-08-22T17:56:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    56,
                    26,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-01T18:06:13Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    18,
                    6,
                    13,
                    4,
                    213,
                    0
                ],
                "title": "Are LLM-Powered Social Media Bots Realistic?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLM-Powered Social Media Bots Realistic?"
                },
                "summary": "As Large Language Models (LLMs) become more sophisticated, there is a\npossibility to harness LLMs to power social media bots. This work investigates\nthe realism of generating LLM-Powered social media bot networks. Through a\ncombination of manual effort, network science and LLMs, we create synthetic bot\nagent personas, their tweets and their interactions, thereby simulating social\nmedia networks. We compare the generated networks against empirical bot/human\ndata, observing that both network and linguistic properties of LLM-Powered Bots\ndiffer from Wild Bots/Humans. This has implications towards the detection and\neffectiveness of LLM-Powered Bots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become more sophisticated, there is a\npossibility to harness LLMs to power social media bots. This work investigates\nthe realism of generating LLM-Powered social media bot networks. Through a\ncombination of manual effort, network science and LLMs, we create synthetic bot\nagent personas, their tweets and their interactions, thereby simulating social\nmedia networks. We compare the generated networks against empirical bot/human\ndata, observing that both network and linguistic properties of LLM-Powered Bots\ndiffer from Wild Bots/Humans. This has implications towards the detection and\neffectiveness of LLM-Powered Bots."
                },
                "authors": [
                    {
                        "name": "Lynnette Hui Xian Ng"
                    },
                    {
                        "name": "Kathleen M. Carley"
                    }
                ],
                "author_detail": {
                    "name": "Kathleen M. Carley"
                },
                "author": "Kathleen M. Carley",
                "arxiv_comment": "Accepted into SBP-BRiMS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00998v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00998v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16571v1",
                "updated": "2025-08-22T17:50:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    50,
                    0,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T17:50:00Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    50,
                    0,
                    4,
                    234,
                    0
                ],
                "title": "LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due\n  Diligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due\n  Diligence"
                },
                "summary": "In this paper, we describe and benchmark a competitor-discovery component\nused within an agentic AI system for fast drug asset due diligence. A\ncompetitor-discovery AI agent, given an indication, retrieves all drugs\ncomprising the competitive landscape of that indication and extracts canonical\nattributes for these drugs. The competitor definition is investor-specific, and\ndata is paywalled/licensed, fragmented across registries, ontology-mismatched\nby indication, alias-heavy for drug names, multimodal, and rapidly changing.\nAlthough considered the best tool for this problem, the current LLM-based AI\nsystems aren't capable of reliably retrieving all competing drug names, and\nthere is no accepted public benchmark for this task. To address the lack of\nevaluation, we use LLM-based agents to transform five years of multi-modal,\nunstructured diligence memos from a private biotech VC fund into a structured\nevaluation corpus mapping indications to competitor drugs with normalized\nattributes. We also introduce a competitor validating LLM-as-a-judge agent that\nfilters out false positives from the list of predicted competitors to maximize\nprecision and suppress hallucinations. On this benchmark, our\ncompetitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research\n(65%) and Perplexity Labs (60%). The system is deployed in production with\nenterprise users; in a case study with a biotech VC investment fund, analyst\nturnaround time dropped from 2.5 days to $\\sim$3 hours ($\\sim$20x) for the\ncompetitive analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we describe and benchmark a competitor-discovery component\nused within an agentic AI system for fast drug asset due diligence. A\ncompetitor-discovery AI agent, given an indication, retrieves all drugs\ncomprising the competitive landscape of that indication and extracts canonical\nattributes for these drugs. The competitor definition is investor-specific, and\ndata is paywalled/licensed, fragmented across registries, ontology-mismatched\nby indication, alias-heavy for drug names, multimodal, and rapidly changing.\nAlthough considered the best tool for this problem, the current LLM-based AI\nsystems aren't capable of reliably retrieving all competing drug names, and\nthere is no accepted public benchmark for this task. To address the lack of\nevaluation, we use LLM-based agents to transform five years of multi-modal,\nunstructured diligence memos from a private biotech VC fund into a structured\nevaluation corpus mapping indications to competitor drugs with normalized\nattributes. We also introduce a competitor validating LLM-as-a-judge agent that\nfilters out false positives from the list of predicted competitors to maximize\nprecision and suppress hallucinations. On this benchmark, our\ncompetitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research\n(65%) and Perplexity Labs (60%). The system is deployed in production with\nenterprise users; in a case study with a biotech VC investment fund, analyst\nturnaround time dropped from 2.5 days to $\\sim$3 hours ($\\sim$20x) for the\ncompetitive analysis."
                },
                "authors": [
                    {
                        "name": "Alisa Vinogradova"
                    },
                    {
                        "name": "Vlad Vinogradov"
                    },
                    {
                        "name": "Dmitrii Radkevich"
                    },
                    {
                        "name": "Ilya Yasny"
                    },
                    {
                        "name": "Dmitry Kobyzev"
                    },
                    {
                        "name": "Ivan Izmailov"
                    },
                    {
                        "name": "Katsiaryna Yanchanka"
                    },
                    {
                        "name": "Andrey Doronichev"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Doronichev"
                },
                "arxiv_affiliation": "Optic Inc",
                "author": "Andrey Doronichev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16560v1",
                "updated": "2025-08-22T17:26:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    26,
                    33,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T17:26:33Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    26,
                    33,
                    4,
                    234,
                    0
                ],
                "title": "Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse\n  Autoencoders"
                },
                "summary": "Sparse Autoencoders (SAEs) extract features from LLM internal activations,\nmeant to correspond to single concepts. A core SAE training hyperparameter is\nL0: how many features should fire per token on average. Existing work compares\nSAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a\nfree parameter with no single correct value. In this work we study the effect\nof L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE\nfails to learn the underlying features of the LLM. If L0 is too low, the SAE\nwill mix correlated features to improve reconstruction. If L0 is too high, the\nSAE finds degenerate solutions that also mix features. Further, we demonstrate\na method to determine the correct L0 value for an SAE on a given training\ndistribution, which finds the true L0 in toy models and coincides with peak\nsparse probing performance in LLMs. We find that most commonly used SAEs have\nan L0 that is too low. Our work shows that, to train SAEs with correct\nfeatures, practitioners must set L0 correctly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) extract features from LLM internal activations,\nmeant to correspond to single concepts. A core SAE training hyperparameter is\nL0: how many features should fire per token on average. Existing work compares\nSAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a\nfree parameter with no single correct value. In this work we study the effect\nof L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE\nfails to learn the underlying features of the LLM. If L0 is too low, the SAE\nwill mix correlated features to improve reconstruction. If L0 is too high, the\nSAE finds degenerate solutions that also mix features. Further, we demonstrate\na method to determine the correct L0 value for an SAE on a given training\ndistribution, which finds the true L0 in toy models and coincides with peak\nsparse probing performance in LLMs. We find that most commonly used SAEs have\nan L0 that is too low. Our work shows that, to train SAEs with correct\nfeatures, practitioners must set L0 correctly."
                },
                "authors": [
                    {
                        "name": "David Chanin"
                    },
                    {
                        "name": "Adri Garriga-Alonso"
                    }
                ],
                "author_detail": {
                    "name": "Adri Garriga-Alonso"
                },
                "author": "Adri Garriga-Alonso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16553v1",
                "updated": "2025-08-22T17:21:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    21,
                    56,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T17:21:56Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    21,
                    56,
                    4,
                    234,
                    0
                ],
                "title": "TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a\n  Milling Machine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a\n  Milling Machine"
                },
                "summary": "In the context of industry 4.0, long-serving industrial machines can be\nretrofitted with process monitoring capabilities for future use in a smart\nfactory. One possible approach is the deployment of wireless monitoring\nsystems, which can benefit substantially from the TinyML paradigm. This work\npresents a complete TinyML flow from dataset generation, to machine learning\nmodel development, up to implementation and evaluation of a full preprocessing\nand classification pipeline on a microcontroller. After a short review on\nTinyML in industrial process monitoring, the creation of the novel MillingVibes\ndataset is described. The feasibility of a TinyML system for\nstructure-integrated process quality monitoring could be shown by the\ndevelopment of an 8-bit-quantized convolutional neural network (CNN) model with\n12.59kiB parameter storage. A test accuracy of 100.0% could be reached at\n15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex\nM4F microcontroller, serving as a reference for future TinyML process\nmonitoring solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context of industry 4.0, long-serving industrial machines can be\nretrofitted with process monitoring capabilities for future use in a smart\nfactory. One possible approach is the deployment of wireless monitoring\nsystems, which can benefit substantially from the TinyML paradigm. This work\npresents a complete TinyML flow from dataset generation, to machine learning\nmodel development, up to implementation and evaluation of a full preprocessing\nand classification pipeline on a microcontroller. After a short review on\nTinyML in industrial process monitoring, the creation of the novel MillingVibes\ndataset is described. The feasibility of a TinyML system for\nstructure-integrated process quality monitoring could be shown by the\ndevelopment of an 8-bit-quantized convolutional neural network (CNN) model with\n12.59kiB parameter storage. A test accuracy of 100.0% could be reached at\n15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex\nM4F microcontroller, serving as a reference for future TinyML process\nmonitoring solutions."
                },
                "authors": [
                    {
                        "name": "Tim Langer"
                    },
                    {
                        "name": "Matthias Widra"
                    },
                    {
                        "name": "Volkhard Beyer"
                    }
                ],
                "author_detail": {
                    "name": "Volkhard Beyer"
                },
                "author": "Volkhard Beyer",
                "arxiv_comment": "10 pages, 5 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.5.4; C.5.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16546v1",
                "updated": "2025-08-22T17:10:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    10,
                    37,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T17:10:37Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    10,
                    37,
                    4,
                    234,
                    0
                ],
                "title": "RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs.\n  Reinforcement Learning Fine-Tuning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs.\n  Reinforcement Learning Fine-Tuning for LLMs"
                },
                "summary": "Training large language models (LLMs) from scratch is increasingly\nimpractical, making post-training methods such as supervised fine-tuning (SFT)\nand reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern\npractice. Using an out-of-distribution (OOD) variant of the 24-point card game\nand new spectrum-based diagnostics, we revisit how these two stages reshape\nmodel representation and OOD performance. Our key findings are- (1) RL-FT can\nrestore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to\n15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and\na clear distribution shift, RL-FT cannot fully recover OOD performance. (2)\nDirection shifts of singular vectors matter more than singular value\nmagnitudes. These shifts concentrate on directions linked to the largest and\nsmallest singular values, leaving the bulk spectrum intact. (3) Low-rank and\nshallow recovery is effective: restoring singular vector directions for the top\n20% of values or first 25% of layers recovers 70-80% of OOD performance. (4)\nStronger SFT checkpoints enable better recovery by RL, while overfitted ones\nresist restoration. These results reconcile prior reports of RL superior OOD\nperformance: RL primarily counteracts SFT-induced directional drift rather than\nfinding new solutions. Our spectrum-aware analysis highlights inexpensive\nrecovery knobs low-rank UV merging and shallow-layer resets that practitioners\ncan use before costly RL fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) from scratch is increasingly\nimpractical, making post-training methods such as supervised fine-tuning (SFT)\nand reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern\npractice. Using an out-of-distribution (OOD) variant of the 24-point card game\nand new spectrum-based diagnostics, we revisit how these two stages reshape\nmodel representation and OOD performance. Our key findings are- (1) RL-FT can\nrestore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to\n15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and\na clear distribution shift, RL-FT cannot fully recover OOD performance. (2)\nDirection shifts of singular vectors matter more than singular value\nmagnitudes. These shifts concentrate on directions linked to the largest and\nsmallest singular values, leaving the bulk spectrum intact. (3) Low-rank and\nshallow recovery is effective: restoring singular vector directions for the top\n20% of values or first 25% of layers recovers 70-80% of OOD performance. (4)\nStronger SFT checkpoints enable better recovery by RL, while overfitted ones\nresist restoration. These results reconcile prior reports of RL superior OOD\nperformance: RL primarily counteracts SFT-induced directional drift rather than\nfinding new solutions. Our spectrum-aware analysis highlights inexpensive\nrecovery knobs low-rank UV merging and shallow-layer resets that practitioners\ncan use before costly RL fine-tuning."
                },
                "authors": [
                    {
                        "name": "Hangzhan Jin"
                    },
                    {
                        "name": "Sicheng Lv"
                    },
                    {
                        "name": "Sifan Wu"
                    },
                    {
                        "name": "Mohammad Hamdaqa"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Hamdaqa"
                },
                "author": "Mohammad Hamdaqa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11401v2",
                "updated": "2025-08-22T17:10:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    10,
                    33,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-15T11:10:40Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    10,
                    40,
                    4,
                    227,
                    0
                ],
                "title": "FACET: Teacher-Centred LLM-Based Multi-Agent Systems-Towards\n  Personalized Educational Worksheets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FACET: Teacher-Centred LLM-Based Multi-Agent Systems-Towards\n  Personalized Educational Worksheets"
                },
                "summary": "The increasing heterogeneity of student populations poses significant\nchallenges for teachers, particularly in mathematics education, where\ncognitive, motivational, and emotional differences strongly influence learning\noutcomes. While AI-driven personalization tools have emerged, most remain\nperformance-focused, offering limited support for teachers and neglecting\nbroader pedagogical needs. This paper presents the FACET framework, a\nteacher-facing, large language model (LLM)-based multi-agent system designed to\ngenerate individualized classroom materials that integrate both cognitive and\nmotivational dimensions of learner profiles. The framework comprises three\nspecialized agents: (1) learner agents that simulate diverse profiles\nincorporating topic proficiency and intrinsic motivation, (2) a teacher agent\nthat adapts instructional content according to didactical principles, and (3)\nan evaluator agent that provides automated quality assurance. We tested the\nsystem using authentic grade 8 mathematics curriculum content and evaluated its\nfeasibility through a) automated agent-based assessment of output quality and\nb) exploratory feedback from K-12 in-service teachers. Results from ten\ninternal evaluations highlighted high stability and alignment between generated\nmaterials and learner profiles, and teacher feedback particularly highlighted\nstructure and suitability of tasks. The findings demonstrate the potential of\nmulti-agent LLM architectures to provide scalable, context-aware\npersonalization in heterogeneous classroom settings, and outline directions for\nextending the framework to richer learner profiles and real-world classroom\ntrials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing heterogeneity of student populations poses significant\nchallenges for teachers, particularly in mathematics education, where\ncognitive, motivational, and emotional differences strongly influence learning\noutcomes. While AI-driven personalization tools have emerged, most remain\nperformance-focused, offering limited support for teachers and neglecting\nbroader pedagogical needs. This paper presents the FACET framework, a\nteacher-facing, large language model (LLM)-based multi-agent system designed to\ngenerate individualized classroom materials that integrate both cognitive and\nmotivational dimensions of learner profiles. The framework comprises three\nspecialized agents: (1) learner agents that simulate diverse profiles\nincorporating topic proficiency and intrinsic motivation, (2) a teacher agent\nthat adapts instructional content according to didactical principles, and (3)\nan evaluator agent that provides automated quality assurance. We tested the\nsystem using authentic grade 8 mathematics curriculum content and evaluated its\nfeasibility through a) automated agent-based assessment of output quality and\nb) exploratory feedback from K-12 in-service teachers. Results from ten\ninternal evaluations highlighted high stability and alignment between generated\nmaterials and learner profiles, and teacher feedback particularly highlighted\nstructure and suitability of tasks. The findings demonstrate the potential of\nmulti-agent LLM architectures to provide scalable, context-aware\npersonalization in heterogeneous classroom settings, and outline directions for\nextending the framework to richer learner profiles and real-world classroom\ntrials."
                },
                "authors": [
                    {
                        "name": "Jana Gonnermann-Mller"
                    },
                    {
                        "name": "Jennifer Haase"
                    },
                    {
                        "name": "Konstantin Fackeldey"
                    },
                    {
                        "name": "Sebastian Pokutta"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Pokutta"
                },
                "author": "Sebastian Pokutta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10652v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10652v3",
                "updated": "2025-08-22T17:01:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    17,
                    1,
                    56,
                    4,
                    234,
                    0
                ],
                "published": "2025-03-07T10:37:31Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    10,
                    37,
                    31,
                    4,
                    66,
                    0
                ],
                "title": "Can Large Language Models Simulate Human Responses? A Case Study of\n  Stated Preference Experiments in the Context of Heating-related Choices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Simulate Human Responses? A Case Study of\n  Stated Preference Experiments in the Context of Heating-related Choices"
                },
                "summary": "Stated preference (SP) surveys are a key method to research how individuals\nmake trade-offs in hypothetical, also futuristic, scenarios. In energy context\nthis includes key decarbonisation enablement contexts, such as low-carbon\ntechnologies, distributed renewable energy generation, and demand-side response\n[1,2]. However, they tend to be costly, time-consuming, and can be affected by\nrespondent fatigue and ethical constraints. Large language models (LLMs) have\ndemonstrated remarkable capabilities in generating human-like textual\nresponses, prompting growing interest in their application to survey research.\nThis study investigates the use of LLMs to simulate consumer choices in\nenergy-related SP surveys and explores their integration into data analysis\nworkflows. A series of test scenarios were designed to systematically assess\nthe simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5 and\nDeepSeek-R1) at both individual and aggregated levels, considering contexts\nfactors such as prompt design, in-context learning (ICL), chain-of-thought\n(CoT) reasoning, LLM types, integration with traditional choice models, and\npotential biases. Cloud-based LLMs do not consistently outperform smaller local\nmodels. In this study, the reasoning model DeepSeek-R1 achieves the highest\naverage accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factor\nidentification, and choice distribution alignment. Across models, systematic\nbiases are observed against the gas boiler and no-retrofit options, with a\npreference for more energy-efficient alternatives. The findings suggest that\nprevious SP choices are the most effective input factor, while longer prompts\nwith additional factors and varied formats can cause LLMs to lose focus,\nreducing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stated preference (SP) surveys are a key method to research how individuals\nmake trade-offs in hypothetical, also futuristic, scenarios. In energy context\nthis includes key decarbonisation enablement contexts, such as low-carbon\ntechnologies, distributed renewable energy generation, and demand-side response\n[1,2]. However, they tend to be costly, time-consuming, and can be affected by\nrespondent fatigue and ethical constraints. Large language models (LLMs) have\ndemonstrated remarkable capabilities in generating human-like textual\nresponses, prompting growing interest in their application to survey research.\nThis study investigates the use of LLMs to simulate consumer choices in\nenergy-related SP surveys and explores their integration into data analysis\nworkflows. A series of test scenarios were designed to systematically assess\nthe simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5 and\nDeepSeek-R1) at both individual and aggregated levels, considering contexts\nfactors such as prompt design, in-context learning (ICL), chain-of-thought\n(CoT) reasoning, LLM types, integration with traditional choice models, and\npotential biases. Cloud-based LLMs do not consistently outperform smaller local\nmodels. In this study, the reasoning model DeepSeek-R1 achieves the highest\naverage accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factor\nidentification, and choice distribution alignment. Across models, systematic\nbiases are observed against the gas boiler and no-retrofit options, with a\npreference for more energy-efficient alternatives. The findings suggest that\nprevious SP choices are the most effective input factor, while longer prompts\nwith additional factors and varied formats can cause LLMs to lose focus,\nreducing accuracy."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Jacek Pawlak"
                    },
                    {
                        "name": "Aruna Sivakumar"
                    }
                ],
                "author_detail": {
                    "name": "Aruna Sivakumar"
                },
                "author": "Aruna Sivakumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10652v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10652v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14295v2",
                "updated": "2025-08-22T16:49:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    49,
                    10,
                    4,
                    234,
                    0
                ],
                "published": "2025-07-18T18:07:38Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    18,
                    7,
                    38,
                    4,
                    199,
                    0
                ],
                "title": "A Simple \"Try Again\" Can Elicit Multi-Turn LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple \"Try Again\" Can Elicit Multi-Turn LLM Reasoning"
                },
                "summary": "Multi-turn problem solving is critical yet challenging for Large Reasoning\nModels (LRMs) to reflect on their reasoning and revise from feedback. Existing\nReinforcement Learning (RL) methods train large reasoning models on a\nsingle-turn paradigm with verifiable rewards. However, we observe that models\ntrained with existing RL paradigms often lose their ability to solve problems\nacross multiple turns and struggle to revise answers based on contextual\nfeedback, leading to repetitive responses. We ask: can LRMs learn to reflect\ntheir answers in a multi-turn context? In this work, we find that training\nmodels with multi-turn RL using only unary feedback (e.g., \"Let's try again\")\nafter wrong answers can improve both single-turn performance and multi-turn\nreasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement\nlearning, which uses minimal yet common unary user feedback during iterative\nproblem solving. It can be easily applied to existing single-turn RL training\nsetups. Experimental results show that RL training with UFO keeps single-turn\nperformance and improves multi-turn reasoning accuracy by up to 14%, enabling\nlanguage models to better react to feedback in multi-turn problem solving. To\nfurther minimize the number of turns needed for a correct answer while\nencouraging diverse reasoning when mistakes occur, we design reward structures\nthat guide models to produce careful and deliberate answers in each turn. Code:\nhttps://github.com/lichengliu03/unary-feedback",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn problem solving is critical yet challenging for Large Reasoning\nModels (LRMs) to reflect on their reasoning and revise from feedback. Existing\nReinforcement Learning (RL) methods train large reasoning models on a\nsingle-turn paradigm with verifiable rewards. However, we observe that models\ntrained with existing RL paradigms often lose their ability to solve problems\nacross multiple turns and struggle to revise answers based on contextual\nfeedback, leading to repetitive responses. We ask: can LRMs learn to reflect\ntheir answers in a multi-turn context? In this work, we find that training\nmodels with multi-turn RL using only unary feedback (e.g., \"Let's try again\")\nafter wrong answers can improve both single-turn performance and multi-turn\nreasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement\nlearning, which uses minimal yet common unary user feedback during iterative\nproblem solving. It can be easily applied to existing single-turn RL training\nsetups. Experimental results show that RL training with UFO keeps single-turn\nperformance and improves multi-turn reasoning accuracy by up to 14%, enabling\nlanguage models to better react to feedback in multi-turn problem solving. To\nfurther minimize the number of turns needed for a correct answer while\nencouraging diverse reasoning when mistakes occur, we design reward structures\nthat guide models to produce careful and deliberate answers in each turn. Code:\nhttps://github.com/lichengliu03/unary-feedback"
                },
                "authors": [
                    {
                        "name": "Licheng Liu"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Linjie Li"
                    },
                    {
                        "name": "Chenwei Xu"
                    },
                    {
                        "name": "Yiping Lu"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Avirup Sil"
                    },
                    {
                        "name": "Manling Li"
                    }
                ],
                "author_detail": {
                    "name": "Manling Li"
                },
                "author": "Manling Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16517v1",
                "updated": "2025-08-22T16:40:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    40,
                    17,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T16:40:17Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    40,
                    17,
                    4,
                    234,
                    0
                ],
                "title": "ARSP: Automated Repair of Verilog Designs via Semantic Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARSP: Automated Repair of Verilog Designs via Semantic Partitioning"
                },
                "summary": "Debugging functional Verilog bugs consumes a significant portion of front-end\ndesign time. While Large Language Models (LLMs) have demonstrated great\npotential in mitigating this effort, existing LLM-based automated debugging\nmethods underperform on industrial-scale modules. A major reason for this is\nbug signal dilution in long contexts, where a few bug-relevant tokens are\noverwhelmed by hundreds of unrelated lines, diffusing the model's attention. To\naddress this issue, we introduce ARSP, a two-stage system that mitigates\ndilution via semantics-guided fragmentation. A Partition LLM splits a module\ninto semantically tight fragments; a Repair LLM patches each fragment; edits\nare merged without altering unrelated logic. A synthetic data framework\ngenerates fragment-level training pairs spanning bug types, design styles, and\nscales to supervise both models. Experiments show that ARSP achieves 77.92%\npass@1 and 83.88% pass@5, outperforming mainstream commercial LLMs including\nClaude-3.7 and SOTA automated Verilog debugging tools Strider and MEIC. Also,\nsemantic partitioning improves pass@1 by 11.6% and pass@5 by 10.2% over\nwhole-module debugging, validating the effectiveness of fragment-level scope\nreduction in LLM-based Verilog debugging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging functional Verilog bugs consumes a significant portion of front-end\ndesign time. While Large Language Models (LLMs) have demonstrated great\npotential in mitigating this effort, existing LLM-based automated debugging\nmethods underperform on industrial-scale modules. A major reason for this is\nbug signal dilution in long contexts, where a few bug-relevant tokens are\noverwhelmed by hundreds of unrelated lines, diffusing the model's attention. To\naddress this issue, we introduce ARSP, a two-stage system that mitigates\ndilution via semantics-guided fragmentation. A Partition LLM splits a module\ninto semantically tight fragments; a Repair LLM patches each fragment; edits\nare merged without altering unrelated logic. A synthetic data framework\ngenerates fragment-level training pairs spanning bug types, design styles, and\nscales to supervise both models. Experiments show that ARSP achieves 77.92%\npass@1 and 83.88% pass@5, outperforming mainstream commercial LLMs including\nClaude-3.7 and SOTA automated Verilog debugging tools Strider and MEIC. Also,\nsemantic partitioning improves pass@1 by 11.6% and pass@5 by 10.2% over\nwhole-module debugging, validating the effectiveness of fragment-level scope\nreduction in LLM-based Verilog debugging."
                },
                "authors": [
                    {
                        "name": "Bingkun Yao"
                    },
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Yuxin Du"
                    },
                    {
                        "name": "Yuchen Hu"
                    },
                    {
                        "name": "Hong Gao"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16516v1",
                "updated": "2025-08-22T16:39:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    39,
                    53,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T16:39:53Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    39,
                    53,
                    4,
                    234,
                    0
                ],
                "title": "A Node-Aware Dynamic Quantization Approach for Graph Collaborative\n  Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Node-Aware Dynamic Quantization Approach for Graph Collaborative\n  Filtering"
                },
                "summary": "In the realm of collaborative filtering recommendation systems, Graph Neural\nNetworks (GNNs) have demonstrated remarkable performance but face significant\nchallenges in deployment on resource-constrained edge devices due to their high\nembedding parameter requirements and computational costs. Using common\nquantization method directly on node embeddings may overlooks their graph based\nstructure, causing error accumulation during message passing and degrading the\nquality of quantized embeddings.To address this, we propose Graph based\nNode-Aware Dynamic Quantization training for collaborative filtering (GNAQ), a\nnovel quantization approach that leverages graph structural information to\nenhance the balance between efficiency and accuracy of GNNs for Top-K\nrecommendation. GNAQ introduces a node-aware dynamic quantization strategy that\nadapts quantization scales to individual node embeddings by incorporating graph\ninteraction relationships. Specifically, it initializes quantization intervals\nbased on node-wise feature distributions and dynamically refines them through\nmessage passing in GNN layers. This approach mitigates information loss caused\nby fixed quantization scales and captures hierarchical semantic features in\nuser-item interaction graphs. Additionally, GNAQ employs graph relation-aware\ngradient estimation to replace traditional straight-through estimators,\nensuring more accurate gradient propagation during training. Extensive\nexperiments on four real-world datasets demonstrate that GNAQ outperforms\nstate-of-the-art quantization methods, including BiGeaR and N2UQ, by achieving\naverage improvement in 27.8\\% Recall@10 and 17.6\\% NDCG@10 under 2-bit\nquantization. In particular, GNAQ is capable of maintaining the performance of\nfull-precision models while reducing their model sizes by 8 to 12 times; in\naddition, the training time is twice as fast compared to quantization baseline\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of collaborative filtering recommendation systems, Graph Neural\nNetworks (GNNs) have demonstrated remarkable performance but face significant\nchallenges in deployment on resource-constrained edge devices due to their high\nembedding parameter requirements and computational costs. Using common\nquantization method directly on node embeddings may overlooks their graph based\nstructure, causing error accumulation during message passing and degrading the\nquality of quantized embeddings.To address this, we propose Graph based\nNode-Aware Dynamic Quantization training for collaborative filtering (GNAQ), a\nnovel quantization approach that leverages graph structural information to\nenhance the balance between efficiency and accuracy of GNNs for Top-K\nrecommendation. GNAQ introduces a node-aware dynamic quantization strategy that\nadapts quantization scales to individual node embeddings by incorporating graph\ninteraction relationships. Specifically, it initializes quantization intervals\nbased on node-wise feature distributions and dynamically refines them through\nmessage passing in GNN layers. This approach mitigates information loss caused\nby fixed quantization scales and captures hierarchical semantic features in\nuser-item interaction graphs. Additionally, GNAQ employs graph relation-aware\ngradient estimation to replace traditional straight-through estimators,\nensuring more accurate gradient propagation during training. Extensive\nexperiments on four real-world datasets demonstrate that GNAQ outperforms\nstate-of-the-art quantization methods, including BiGeaR and N2UQ, by achieving\naverage improvement in 27.8\\% Recall@10 and 17.6\\% NDCG@10 under 2-bit\nquantization. In particular, GNAQ is capable of maintaining the performance of\nfull-precision models while reducing their model sizes by 8 to 12 times; in\naddition, the training time is twice as fast compared to quantization baseline\nmethods."
                },
                "authors": [
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Chunyang Li"
                    },
                    {
                        "name": "Yu Yin"
                    },
                    {
                        "name": "Xiaohui Tao"
                    },
                    {
                        "name": "Jianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Zhang"
                },
                "author": "Jianwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16514v1",
                "updated": "2025-08-22T16:37:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    37,
                    40,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T16:37:40Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    37,
                    40,
                    4,
                    234,
                    0
                ],
                "title": "FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the\n  Data Synthesis Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the\n  Data Synthesis Pipeline"
                },
                "summary": "Recent works improving LLM math reasoning with synthetic data have used\nunique setups, making comparison of data synthesis strategies impractical. This\nleaves many unanswered questions about the roles of different factors in the\nsynthetic data pipeline, such as the impact of filtering low-quality problems.\nTo address this gap, we introduce FLAMES, a Framework for LLM Assessment of\nMath rEasoning Data Synthesis, and perform a systematic study of 10 existing\ndata synthesis strategies and multiple other factors impacting the performance\nof synthetic math reasoning data. Our FLAMES experiments provide several\nvaluable insights about the optimal balance of difficulty and diversity of\nsynthetic data. First, data agents designed to increase problem complexity lead\nto best improvements on most math metrics. Second, with a fixed data generation\nbudget, keeping higher problem coverage is more important than keeping only\nproblems with reliable solutions. Third, GSM8K- and MATH-based synthetic data\ncan lead to improvements on competition-level benchmarks, showcasing\neasy-to-hard generalization. Leveraging insights from our FLAMES experiments,\nwe design two novel data synthesis strategies for improving out-of-domain\ngeneralization and robustness. Further, we develop the FLAMES dataset, an\neffective blend of our novel and existing data synthesis strategies,\noutperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5),\nGSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES\ndataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and\nClaude 3.5 Sonnet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works improving LLM math reasoning with synthetic data have used\nunique setups, making comparison of data synthesis strategies impractical. This\nleaves many unanswered questions about the roles of different factors in the\nsynthetic data pipeline, such as the impact of filtering low-quality problems.\nTo address this gap, we introduce FLAMES, a Framework for LLM Assessment of\nMath rEasoning Data Synthesis, and perform a systematic study of 10 existing\ndata synthesis strategies and multiple other factors impacting the performance\nof synthetic math reasoning data. Our FLAMES experiments provide several\nvaluable insights about the optimal balance of difficulty and diversity of\nsynthetic data. First, data agents designed to increase problem complexity lead\nto best improvements on most math metrics. Second, with a fixed data generation\nbudget, keeping higher problem coverage is more important than keeping only\nproblems with reliable solutions. Third, GSM8K- and MATH-based synthetic data\ncan lead to improvements on competition-level benchmarks, showcasing\neasy-to-hard generalization. Leveraging insights from our FLAMES experiments,\nwe design two novel data synthesis strategies for improving out-of-domain\ngeneralization and robustness. Further, we develop the FLAMES dataset, an\neffective blend of our novel and existing data synthesis strategies,\noutperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5),\nGSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES\ndataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and\nClaude 3.5 Sonnet."
                },
                "authors": [
                    {
                        "name": "Parker Seegmiller"
                    },
                    {
                        "name": "Kartik Mehta"
                    },
                    {
                        "name": "Soumya Saha"
                    },
                    {
                        "name": "Chenyang Tao"
                    },
                    {
                        "name": "Shereen Oraby"
                    },
                    {
                        "name": "Arpit Gupta"
                    },
                    {
                        "name": "Tagyoung Chung"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "arxiv_comment": "To appear at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16499v1",
                "updated": "2025-08-22T16:25:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    25,
                    8,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T16:25:08Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    25,
                    8,
                    4,
                    234,
                    0
                ],
                "title": "How Small is Enough? Empirical Evidence of Quantized Small Language\n  Models for Automated Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Small is Enough? Empirical Evidence of Quantized Small Language\n  Models for Automated Program Repair"
                },
                "summary": "Background: Large language models (LLMs) have greatly improved the accuracy\nof automated program repair (APR) methods. However, LLMs are constrained by\nhigh computational resource requirements. Aims: We focus on small language\nmodels (SLMs), which perform well even with limited computational resources\ncompared to LLMs. We aim to evaluate whether SLMs can achieve competitive\nperformance in APR tasks. Method: We conducted experiments on the QuixBugs\nbenchmark to compare the bug-fixing accuracy of SLMs and LLMs. We also analyzed\nthe impact of int8 quantization on APR performance. Results: The latest SLMs\ncan fix bugs as accurately as--or even more accurately than--LLMs. Also, int8\nquantization had minimal effect on APR accuracy while significantly reducing\nmemory requirements. Conclusions: SLMs present a viable alternative to LLMs for\nAPR, offering competitive accuracy with lower computational costs, and\nquantization can further enhance their efficiency without compromising\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Large language models (LLMs) have greatly improved the accuracy\nof automated program repair (APR) methods. However, LLMs are constrained by\nhigh computational resource requirements. Aims: We focus on small language\nmodels (SLMs), which perform well even with limited computational resources\ncompared to LLMs. We aim to evaluate whether SLMs can achieve competitive\nperformance in APR tasks. Method: We conducted experiments on the QuixBugs\nbenchmark to compare the bug-fixing accuracy of SLMs and LLMs. We also analyzed\nthe impact of int8 quantization on APR performance. Results: The latest SLMs\ncan fix bugs as accurately as--or even more accurately than--LLMs. Also, int8\nquantization had minimal effect on APR accuracy while significantly reducing\nmemory requirements. Conclusions: SLMs present a viable alternative to LLMs for\nAPR, offering competitive accuracy with lower computational costs, and\nquantization can further enhance their efficiency without compromising\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Kazuki Kusama"
                    },
                    {
                        "name": "Honglin Shu"
                    },
                    {
                        "name": "Masanari Kondo"
                    },
                    {
                        "name": "Yasutaka Kamei"
                    }
                ],
                "author_detail": {
                    "name": "Yasutaka Kamei"
                },
                "author": "Yasutaka Kamei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07908v2",
                "updated": "2025-08-22T16:20:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    20,
                    32,
                    4,
                    234,
                    0
                ],
                "published": "2025-07-10T16:39:49Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    16,
                    39,
                    49,
                    3,
                    191,
                    0
                ],
                "title": "Not Only Consistency: Enhance Test-Time Adaptation with Spatio-temporal\n  Inconsistency for Remote Physiological Measurement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not Only Consistency: Enhance Test-Time Adaptation with Spatio-temporal\n  Inconsistency for Remote Physiological Measurement"
                },
                "summary": "Remote physiological measurement (RPM) has emerged as a promising\nnon-invasive method for monitoring physiological signals using the non-contact\ndevice. Although various domain adaptation and generalization methods were\nproposed to promote the adaptability of deep-based RPM models in unseen\ndeployment environments, considerations in aspects such as privacy concerns and\nreal-time adaptation restrict their application in real-world deployment. Thus,\nwe aim to propose a novel fully Test-Time Adaptation (TTA) strategy tailored\nfor RPM tasks in this work. Specifically, based on prior knowledge in\nphysiology and our observations, we noticed not only there is spatio-temporal\nconsistency in the frequency domain of BVP signals, but also that inconsistency\nin the time domain was significant. Given this, by leveraging both consistency\nand inconsistency priors, we introduce an innovative expert knowledge-based\nself-supervised\n\\textbf{C}onsistency-\\textbf{i}n\\textbf{C}onsistency-\\textbf{i}ntegration\n(\\textbf{CiCi}) framework to enhances model adaptation during inference.\nBesides, our approach further incorporates a gradient dynamic control mechanism\nto mitigate potential conflicts between priors, ensuring stable adaptation\nacross instances. Through extensive experiments on five diverse datasets under\nthe TTA protocol, our method consistently outperforms existing techniques,\npresenting state-of-the-art performance in real-time self-supervised adaptation\nwithout accessing source data. The code will be released later.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote physiological measurement (RPM) has emerged as a promising\nnon-invasive method for monitoring physiological signals using the non-contact\ndevice. Although various domain adaptation and generalization methods were\nproposed to promote the adaptability of deep-based RPM models in unseen\ndeployment environments, considerations in aspects such as privacy concerns and\nreal-time adaptation restrict their application in real-world deployment. Thus,\nwe aim to propose a novel fully Test-Time Adaptation (TTA) strategy tailored\nfor RPM tasks in this work. Specifically, based on prior knowledge in\nphysiology and our observations, we noticed not only there is spatio-temporal\nconsistency in the frequency domain of BVP signals, but also that inconsistency\nin the time domain was significant. Given this, by leveraging both consistency\nand inconsistency priors, we introduce an innovative expert knowledge-based\nself-supervised\n\\textbf{C}onsistency-\\textbf{i}n\\textbf{C}onsistency-\\textbf{i}ntegration\n(\\textbf{CiCi}) framework to enhances model adaptation during inference.\nBesides, our approach further incorporates a gradient dynamic control mechanism\nto mitigate potential conflicts between priors, ensuring stable adaptation\nacross instances. Through extensive experiments on five diverse datasets under\nthe TTA protocol, our method consistently outperforms existing techniques,\npresenting state-of-the-art performance in real-time self-supervised adaptation\nwithout accessing source data. The code will be released later."
                },
                "authors": [
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Jiyao Wang"
                    },
                    {
                        "name": "Yuxuan Fan"
                    },
                    {
                        "name": "Can Liu"
                    },
                    {
                        "name": "Houcheng Su"
                    },
                    {
                        "name": "Weichen Guo"
                    },
                    {
                        "name": "Zitong Yu"
                    },
                    {
                        "name": "Dengbo He"
                    },
                    {
                        "name": "Kaishun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kaishun Wu"
                },
                "author": "Kaishun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16495v1",
                "updated": "2025-08-22T16:17:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    17,
                    31,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T16:17:31Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    17,
                    31,
                    4,
                    234,
                    0
                ],
                "title": "Post Hoc Regression Refinement via Pairwise Rankings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post Hoc Regression Refinement via Pairwise Rankings"
                },
                "summary": "Accurate prediction of continuous properties is essential to many scientific\nand engineering tasks. Although deep-learning regressors excel with abundant\nlabels, their accuracy deteriorates in data-scarce regimes. We introduce\nRankRefine, a model-agnostic, plug-and-play post hoc method that refines\nregression with expert knowledge coming from pairwise rankings. Given a query\nitem and a small reference set with known properties, RankRefine combines the\nbase regressor's output with a rank-based estimate via inverse variance\nweighting, requiring no retraining. In molecular property prediction task,\nRankRefine achieves up to 10% relative reduction in mean absolute error using\nonly 20 pairwise comparisons obtained through a general-purpose large language\nmodel (LLM) with no finetuning. As rankings provided by human experts or\ngeneral-purpose LLMs are sufficient for improving regression across diverse\ndomains, RankRefine offers practicality and broad applicability, especially in\nlow-data settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate prediction of continuous properties is essential to many scientific\nand engineering tasks. Although deep-learning regressors excel with abundant\nlabels, their accuracy deteriorates in data-scarce regimes. We introduce\nRankRefine, a model-agnostic, plug-and-play post hoc method that refines\nregression with expert knowledge coming from pairwise rankings. Given a query\nitem and a small reference set with known properties, RankRefine combines the\nbase regressor's output with a rank-based estimate via inverse variance\nweighting, requiring no retraining. In molecular property prediction task,\nRankRefine achieves up to 10% relative reduction in mean absolute error using\nonly 20 pairwise comparisons obtained through a general-purpose large language\nmodel (LLM) with no finetuning. As rankings provided by human experts or\ngeneral-purpose LLMs are sufficient for improving regression across diverse\ndomains, RankRefine offers practicality and broad applicability, especially in\nlow-data settings."
                },
                "authors": [
                    {
                        "name": "Kevin Tirta Wijaya"
                    },
                    {
                        "name": "Michael Sun"
                    },
                    {
                        "name": "Minghao Guo"
                    },
                    {
                        "name": "Hans-Peter Seidel"
                    },
                    {
                        "name": "Wojciech Matusik"
                    },
                    {
                        "name": "Vahid Babaei"
                    }
                ],
                "author_detail": {
                    "name": "Vahid Babaei"
                },
                "author": "Vahid Babaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13082v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13082v3",
                "updated": "2025-08-22T16:12:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    12,
                    45,
                    4,
                    234,
                    0
                ],
                "published": "2024-09-19T20:40:52Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    20,
                    40,
                    52,
                    3,
                    263,
                    0
                ],
                "title": "AutoVerus: Automated Proof Generation for Rust Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoVerus: Automated Proof Generation for Rust Code"
                },
                "summary": "Generative AI has shown its values for many software engineering tasks. Still\nin its infancy, large language model (LLM)-based proof generation lags behind\nLLM-based code generation. In this paper, we present AutoVerus. AutoVerus uses\nLLMs to automatically generate correctness proof for Rust code. AutoVerus is\ndesigned to match the unique features of Verus, a verification tool that can\nprove the correctness of Rust code using proofs and specifications also written\nin Rust. AutoVerus consists of a network of LLM agents that are crafted and\norchestrated to mimic human experts' three phases of proof construction:\npreliminary proof generation, proof refinement guided by generic tips, and\nproof debugging guided by verification errors. To thoroughly evaluate AutoVerus\nand help foster future research in this direction, we have built a benchmark\nsuite of 150 non-trivial proof tasks, based on existing code-generation\nbenchmarks and verification benchmarks. Our evaluation shows that AutoVerus can\nautomatically generate correct proof for more than 90% of them, with more than\nhalf of them tackled in less than 30 seconds or 3 LLM calls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI has shown its values for many software engineering tasks. Still\nin its infancy, large language model (LLM)-based proof generation lags behind\nLLM-based code generation. In this paper, we present AutoVerus. AutoVerus uses\nLLMs to automatically generate correctness proof for Rust code. AutoVerus is\ndesigned to match the unique features of Verus, a verification tool that can\nprove the correctness of Rust code using proofs and specifications also written\nin Rust. AutoVerus consists of a network of LLM agents that are crafted and\norchestrated to mimic human experts' three phases of proof construction:\npreliminary proof generation, proof refinement guided by generic tips, and\nproof debugging guided by verification errors. To thoroughly evaluate AutoVerus\nand help foster future research in this direction, we have built a benchmark\nsuite of 150 non-trivial proof tasks, based on existing code-generation\nbenchmarks and verification benchmarks. Our evaluation shows that AutoVerus can\nautomatically generate correct proof for more than 90% of them, with more than\nhalf of them tackled in less than 30 seconds or 3 LLM calls."
                },
                "authors": [
                    {
                        "name": "Chenyuan Yang"
                    },
                    {
                        "name": "Xuheng Li"
                    },
                    {
                        "name": "Md Rakib Hossain Misu"
                    },
                    {
                        "name": "Jianan Yao"
                    },
                    {
                        "name": "Weidong Cui"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Chris Hawblitzel"
                    },
                    {
                        "name": "Shuvendu Lahiri"
                    },
                    {
                        "name": "Jacob R. Lorch"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Ziqiao Zhou"
                    },
                    {
                        "name": "Shan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shan Lu"
                },
                "author": "Shan Lu",
                "arxiv_doi": "10.1145/3763174",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3763174",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.13082v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13082v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "OOPSLA 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14879v2",
                "updated": "2025-08-22T16:12:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    12,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-20T17:50:15Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    50,
                    15,
                    2,
                    232,
                    0
                ],
                "title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds"
                },
                "summary": "Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding. The project homepage is\navailable at \\href{https://daibingquan.github.io/MeshCoder}{this link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding. The project homepage is\navailable at \\href{https://daibingquan.github.io/MeshCoder}{this link}."
                },
                "authors": [
                    {
                        "name": "Bingquan Dai"
                    },
                    {
                        "name": "Li Ray Luo"
                    },
                    {
                        "name": "Qihong Tang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xinyu Lian"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Minghan Qin"
                    },
                    {
                        "name": "Xudong Xu"
                    },
                    {
                        "name": "Bo Dai"
                    },
                    {
                        "name": "Haoqian Wang"
                    },
                    {
                        "name": "Zhaoyang Lyu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10969v2",
                "updated": "2025-08-22T16:11:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    11,
                    58,
                    4,
                    234,
                    0
                ],
                "published": "2024-09-17T08:11:07Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    11,
                    7,
                    1,
                    261,
                    0
                ],
                "title": "Enhancing Code-switched Text-to-Speech Synthesis Capability in Large\n  Language Models with only Monolingual Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Code-switched Text-to-Speech Synthesis Capability in Large\n  Language Models with only Monolingual Corpora"
                },
                "summary": "While Large Language Models (LLMs) have shown potential in speech generation\nand recognition, their applications are mainly confined to monolingual\nscenarios, with limited explorations in code-switched (CS) contexts. In this\npaper, we propose a Code-Switched Large Language Model (CS-LLM) to enhance the\ncode-switched text-to-speech synthesis (CS TTS) capability in LLMs with only\nmonolingual corpora. Specifically, we begin by enhancing the multilingual\nspeech processing ability of LLMs through multilingual speech recognition and\nsynthesis tasks. Then, we develop an effective code-switched (CS) data\nconstruction strategy that splits and concatenates words from different\nmonolingual speech corpora to equip LLMs with improved CS TTS ability.\nExperiments show that our approach outperforms baselines in CS TTS in terms of\nnaturalness, speaker consistency and similarity even with limited data.\nAdditionally, the constructed CS data further improves multilingual speech\nsynthesis and recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have shown potential in speech generation\nand recognition, their applications are mainly confined to monolingual\nscenarios, with limited explorations in code-switched (CS) contexts. In this\npaper, we propose a Code-Switched Large Language Model (CS-LLM) to enhance the\ncode-switched text-to-speech synthesis (CS TTS) capability in LLMs with only\nmonolingual corpora. Specifically, we begin by enhancing the multilingual\nspeech processing ability of LLMs through multilingual speech recognition and\nsynthesis tasks. Then, we develop an effective code-switched (CS) data\nconstruction strategy that splits and concatenates words from different\nmonolingual speech corpora to equip LLMs with improved CS TTS ability.\nExperiments show that our approach outperforms baselines in CS TTS in terms of\nnaturalness, speaker consistency and similarity even with limited data.\nAdditionally, the constructed CS data further improves multilingual speech\nsynthesis and recognition."
                },
                "authors": [
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Daxin Tan"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Xiao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Chen"
                },
                "author": "Xiao Chen",
                "arxiv_comment": "Accepted to ASRU2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10848v2",
                "updated": "2025-08-22T16:11:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    11,
                    37,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-14T17:18:35Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    18,
                    35,
                    3,
                    226,
                    0
                ],
                "title": "Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy,\n  Expertise, and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy,\n  Expertise, and Reasoning"
                },
                "summary": "Amidst a shortage of qualified mental health professionals, the integration\nof large language models (LLMs) into psychological applications offers a\npromising way to alleviate the growing burden of mental health disorders.\nRecent reasoning-augmented LLMs have achieved remarkable performance in\nmathematics and programming, while research in the psychological domain has\npredominantly emphasized emotional support and empathetic dialogue, with\nlimited attention to reasoning mechanisms that are beneficial to generating\nreliable responses. Therefore, in this paper, we propose Psyche-R1, the first\nChinese psychological LLM that jointly integrates empathy, psychological\nexpertise, and reasoning, built upon a novel data curation pipeline.\nSpecifically, we design a comprehensive data synthesis pipeline that produces\nover 75k high-quality psychological questions paired with detailed rationales,\ngenerated through chain-of-thought (CoT) reasoning and iterative\nprompt-rationale optimization, along with 73k empathetic dialogues.\nSubsequently, we employ a hybrid training strategy wherein challenging samples\nare identified through a multi-LLM cross-selection strategy for group relative\npolicy optimization (GRPO) to improve reasoning ability, while the remaining\ndata is used for supervised fine-tuning (SFT) to enhance empathetic response\ngeneration and psychological domain knowledge. Extensive experiment results\ndemonstrate the effectiveness of the Psyche-R1 across several psychological\nbenchmarks, where our 7B Psyche-R1 achieves comparable results to 671B\nDeepSeek-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amidst a shortage of qualified mental health professionals, the integration\nof large language models (LLMs) into psychological applications offers a\npromising way to alleviate the growing burden of mental health disorders.\nRecent reasoning-augmented LLMs have achieved remarkable performance in\nmathematics and programming, while research in the psychological domain has\npredominantly emphasized emotional support and empathetic dialogue, with\nlimited attention to reasoning mechanisms that are beneficial to generating\nreliable responses. Therefore, in this paper, we propose Psyche-R1, the first\nChinese psychological LLM that jointly integrates empathy, psychological\nexpertise, and reasoning, built upon a novel data curation pipeline.\nSpecifically, we design a comprehensive data synthesis pipeline that produces\nover 75k high-quality psychological questions paired with detailed rationales,\ngenerated through chain-of-thought (CoT) reasoning and iterative\nprompt-rationale optimization, along with 73k empathetic dialogues.\nSubsequently, we employ a hybrid training strategy wherein challenging samples\nare identified through a multi-LLM cross-selection strategy for group relative\npolicy optimization (GRPO) to improve reasoning ability, while the remaining\ndata is used for supervised fine-tuning (SFT) to enhance empathetic response\ngeneration and psychological domain knowledge. Extensive experiment results\ndemonstrate the effectiveness of the Psyche-R1 across several psychological\nbenchmarks, where our 7B Psyche-R1 achieves comparable results to 671B\nDeepSeek-R1."
                },
                "authors": [
                    {
                        "name": "Chongyuan Dai"
                    },
                    {
                        "name": "Jinpeng Hu"
                    },
                    {
                        "name": "Hongchang Shi"
                    },
                    {
                        "name": "Zhuo Li"
                    },
                    {
                        "name": "Xun Yang"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16488v1",
                "updated": "2025-08-22T16:07:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    7,
                    29,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T16:07:29Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    7,
                    29,
                    4,
                    234,
                    0
                ],
                "title": "SafeSpace: An Integrated Web Application for Digital Safety and\n  Emotional Well-being",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeSpace: An Integrated Web Application for Digital Safety and\n  Emotional Well-being"
                },
                "summary": "In the digital era, individuals are increasingly exposed to online harms such\nas toxicity, manipulation, and grooming, which often pose emotional and safety\nrisks. Existing systems for detecting abusive content or issuing safety alerts\noperate in isolation and rarely combine digital safety with emotional\nwell-being. In this paper, we present SafeSpace, a unified web application that\nintegrates three modules: (1) toxicity detection in chats and screenshots using\nNLP models and Google's Perspective API, (2) a configurable safety ping system\nthat issues emergency alerts with the user's live location (longitude and\nlatitude) via SMTP-based emails when check-ins are missed or SOS alerts are\nmanually triggered, and (3) a reflective questionnaire that evaluates\nrelationship health and emotional resilience. The system employs Firebase for\nalert management and a modular architecture designed for usability, privacy,\nand scalability. The experimental evaluation shows 93% precision in toxicity\ndetection, 100% reliability in safety alerts under emulator tests, and 92%\nalignment between automated and manual questionnaire scoring. SafeSpace,\nimplemented as a web application, demonstrates the feasibility of integrating\ndetection, protection, and reflection within a single platform, with future\ndeployment envisioned as a mobile application for broader accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the digital era, individuals are increasingly exposed to online harms such\nas toxicity, manipulation, and grooming, which often pose emotional and safety\nrisks. Existing systems for detecting abusive content or issuing safety alerts\noperate in isolation and rarely combine digital safety with emotional\nwell-being. In this paper, we present SafeSpace, a unified web application that\nintegrates three modules: (1) toxicity detection in chats and screenshots using\nNLP models and Google's Perspective API, (2) a configurable safety ping system\nthat issues emergency alerts with the user's live location (longitude and\nlatitude) via SMTP-based emails when check-ins are missed or SOS alerts are\nmanually triggered, and (3) a reflective questionnaire that evaluates\nrelationship health and emotional resilience. The system employs Firebase for\nalert management and a modular architecture designed for usability, privacy,\nand scalability. The experimental evaluation shows 93% precision in toxicity\ndetection, 100% reliability in safety alerts under emulator tests, and 92%\nalignment between automated and manual questionnaire scoring. SafeSpace,\nimplemented as a web application, demonstrates the feasibility of integrating\ndetection, protection, and reflection within a single platform, with future\ndeployment envisioned as a mobile application for broader accessibility."
                },
                "authors": [
                    {
                        "name": "Kayenat Fatmi"
                    },
                    {
                        "name": "Mohammad Abbas"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Abbas"
                },
                "author": "Mohammad Abbas",
                "arxiv_comment": "5 pages, 2 figures, 1 table. Preprint submitted to arXiv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09919v2",
                "updated": "2025-08-22T16:04:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    16,
                    4,
                    28,
                    4,
                    234,
                    0
                ],
                "published": "2025-05-15T03:07:54Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    7,
                    54,
                    3,
                    135,
                    0
                ],
                "title": "Hyper Yoshimura: How a slight tweak on a classical folding pattern\n  unleashes meta-stability for deployable robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyper Yoshimura: How a slight tweak on a classical folding pattern\n  unleashes meta-stability for deployable robots"
                },
                "summary": "Deployable structures inspired by origami have provided lightweight, compact,\nand reconfigurable solutions for various robotic and architectural\napplications. However, creating an integrated structural system that can\neffectively balance the competing requirements of high packing efficiency,\nsimple deployment, and precise morphing into multiple load-bearing\nconfigurations remains a significant challenge. This study introduces a new\nclass of hyper-Yoshimura origami, which exhibits a wide range of kinematically\nadmissible and locally metastable states, including newly discovered symmetric\n\"self-packing\" and asymmetric \"pop-out\" states. This metastability is achieved\nby breaking a design rule of Yoshimura origami that has been in place for many\ndecades. To this end, this study derives a new set of mathematically rigorous\ndesign rules and geometric formulations. Based on this, forward and inverse\nkinematic strategies are developed to stack hyper-Yoshimura modules into\ndeployable booms that can approximate complex 3D shapes. Finally, this study\nshowcases the potential of hyper-Yoshimura with a meter-scale pop-up cellphone\ncharging station deployed at our university's bus transit station, along with a\n3D-printed, scaled prototype of a space crane that can function as an object\nmanipulator, solar tracking device, or high-load-bearing structure. These\nresults establish hyper-Yoshimura as a promising platform for deployable and\nadaptable robotic systems in both terrestrial and space environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployable structures inspired by origami have provided lightweight, compact,\nand reconfigurable solutions for various robotic and architectural\napplications. However, creating an integrated structural system that can\neffectively balance the competing requirements of high packing efficiency,\nsimple deployment, and precise morphing into multiple load-bearing\nconfigurations remains a significant challenge. This study introduces a new\nclass of hyper-Yoshimura origami, which exhibits a wide range of kinematically\nadmissible and locally metastable states, including newly discovered symmetric\n\"self-packing\" and asymmetric \"pop-out\" states. This metastability is achieved\nby breaking a design rule of Yoshimura origami that has been in place for many\ndecades. To this end, this study derives a new set of mathematically rigorous\ndesign rules and geometric formulations. Based on this, forward and inverse\nkinematic strategies are developed to stack hyper-Yoshimura modules into\ndeployable booms that can approximate complex 3D shapes. Finally, this study\nshowcases the potential of hyper-Yoshimura with a meter-scale pop-up cellphone\ncharging station deployed at our university's bus transit station, along with a\n3D-printed, scaled prototype of a space crane that can function as an object\nmanipulator, solar tracking device, or high-load-bearing structure. These\nresults establish hyper-Yoshimura as a promising platform for deployable and\nadaptable robotic systems in both terrestrial and space environments."
                },
                "authors": [
                    {
                        "name": "Ziyang Zhou"
                    },
                    {
                        "name": "Yogesh Phalak"
                    },
                    {
                        "name": "Vishrut Deshpande"
                    },
                    {
                        "name": "Ethan O'Brien"
                    },
                    {
                        "name": "Ian Walker"
                    },
                    {
                        "name": "Suyi Li"
                    }
                ],
                "author_detail": {
                    "name": "Suyi Li"
                },
                "author": "Suyi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24102v3",
                "updated": "2025-08-22T15:58:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    58,
                    31,
                    4,
                    234,
                    0
                ],
                "published": "2025-03-31T13:56:03Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    56,
                    3,
                    0,
                    90,
                    0
                ],
                "title": "Is Small Language Model the Silver Bullet to Low-Resource Languages\n  Machine Translation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Small Language Model the Silver Bullet to Low-Resource Languages\n  Machine Translation?"
                },
                "summary": "Low-resource languages (LRLs) lack sufficient linguistic resources and are\nunderrepresented in benchmark datasets, resulting in persistently lower\ntranslation quality than high-resource languages, especially in\nprivacy-sensitive and resource-limited contexts. Firstly, this study\nsystematically evaluates state-of-the-art smaller Large Language Models in 200\nlanguages using the FLORES-200 benchmark, highlighting persistent deficiencies\nand disparities in the translation of LRLs. To mitigate these limitations, we\ninvestigate knowledge distillation from large pre-trained teacher models to\nSmall Language Models (SLMs) through supervised fine-tuning. The results show\nsubstantial improvements; for example, the translation performance of English\nto Luxembourgish (EN to LB), measured by the LLM-as-a-Judge score, increases\nfrom 0.36 to 0.89 in the validation set for Llama-3.2-3B. We further\ninvestigate various fine-tuning configurations and tasks to clarify the\ntrade-offs between data scale and training efficiency, verify that the model\nretains its general capabilities without significant catastrophic forgetting\nafter training, and explore the distillation benefits to other LRLs on SLMs\n(Khasi, Assamese, and Ukrainian). In general, this work exposes the limitations\nand fairness issues of current SLMs in LRL translation and systematically\nexplores the potential of using the distillation of knowledge from large to\nsmall models, offering practical, empirically grounded recommendations to\nimprove LRL translation systems",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-resource languages (LRLs) lack sufficient linguistic resources and are\nunderrepresented in benchmark datasets, resulting in persistently lower\ntranslation quality than high-resource languages, especially in\nprivacy-sensitive and resource-limited contexts. Firstly, this study\nsystematically evaluates state-of-the-art smaller Large Language Models in 200\nlanguages using the FLORES-200 benchmark, highlighting persistent deficiencies\nand disparities in the translation of LRLs. To mitigate these limitations, we\ninvestigate knowledge distillation from large pre-trained teacher models to\nSmall Language Models (SLMs) through supervised fine-tuning. The results show\nsubstantial improvements; for example, the translation performance of English\nto Luxembourgish (EN to LB), measured by the LLM-as-a-Judge score, increases\nfrom 0.36 to 0.89 in the validation set for Llama-3.2-3B. We further\ninvestigate various fine-tuning configurations and tasks to clarify the\ntrade-offs between data scale and training efficiency, verify that the model\nretains its general capabilities without significant catastrophic forgetting\nafter training, and explore the distillation benefits to other LRLs on SLMs\n(Khasi, Assamese, and Ukrainian). In general, this work exposes the limitations\nand fairness issues of current SLMs in LRL translation and systematically\nexplores the potential of using the distillation of knowledge from large to\nsmall models, offering practical, empirically grounded recommendations to\nimprove LRL translation systems"
                },
                "authors": [
                    {
                        "name": "Yewei Song"
                    },
                    {
                        "name": "Lujun Li"
                    },
                    {
                        "name": "Cedric Lothritz"
                    },
                    {
                        "name": "Saad Ezzini"
                    },
                    {
                        "name": "Lama Sleem"
                    },
                    {
                        "name": "Niccolo Gentile"
                    },
                    {
                        "name": "Radu State"
                    },
                    {
                        "name": "Tegawend F. Bissyand"
                    },
                    {
                        "name": "Jacques Klein"
                    }
                ],
                "author_detail": {
                    "name": "Jacques Klein"
                },
                "author": "Jacques Klein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16484v1",
                "updated": "2025-08-22T15:57:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    57,
                    57,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:57:57Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    57,
                    57,
                    4,
                    234,
                    0
                ],
                "title": "HAMSA: Hijacking Aligned Compact Models via Stealthy Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAMSA: Hijacking Aligned Compact Models via Stealthy Automation"
                },
                "summary": "Large Language Models (LLMs), especially their compact efficiency-oriented\nvariants, remain susceptible to jailbreak attacks that can elicit harmful\noutputs despite extensive alignment efforts. Existing adversarial prompt\ngeneration techniques often rely on manual engineering or rudimentary\nobfuscation, producing low-quality or incoherent text that is easily flagged by\nperplexity-based filters. We present an automated red-teaming framework that\nevolves semantically meaningful and stealthy jailbreak prompts for aligned\ncompact LLMs. The approach employs a multi-stage evolutionary search, where\ncandidate prompts are iteratively refined using a population-based strategy\naugmented with temperature-controlled variability to balance exploration and\ncoherence preservation. This enables the systematic discovery of prompts\ncapable of bypassing alignment safeguards while maintaining natural language\nfluency. We evaluate our method on benchmarks in English (In-The-Wild Jailbreak\nPrompts on LLMs), and a newly curated Arabic one derived from In-The-Wild\nJailbreak Prompts on LLMs and annotated by native Arabic linguists, enabling\nmultilingual assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), especially their compact efficiency-oriented\nvariants, remain susceptible to jailbreak attacks that can elicit harmful\noutputs despite extensive alignment efforts. Existing adversarial prompt\ngeneration techniques often rely on manual engineering or rudimentary\nobfuscation, producing low-quality or incoherent text that is easily flagged by\nperplexity-based filters. We present an automated red-teaming framework that\nevolves semantically meaningful and stealthy jailbreak prompts for aligned\ncompact LLMs. The approach employs a multi-stage evolutionary search, where\ncandidate prompts are iteratively refined using a population-based strategy\naugmented with temperature-controlled variability to balance exploration and\ncoherence preservation. This enables the systematic discovery of prompts\ncapable of bypassing alignment safeguards while maintaining natural language\nfluency. We evaluate our method on benchmarks in English (In-The-Wild Jailbreak\nPrompts on LLMs), and a newly curated Arabic one derived from In-The-Wild\nJailbreak Prompts on LLMs and annotated by native Arabic linguists, enabling\nmultilingual assessment."
                },
                "authors": [
                    {
                        "name": "Alexey Krylov"
                    },
                    {
                        "name": "Iskander Vagizov"
                    },
                    {
                        "name": "Dmitrii Korzh"
                    },
                    {
                        "name": "Maryam Douiba"
                    },
                    {
                        "name": "Azidine Guezzaz"
                    },
                    {
                        "name": "Vladimir Kokh"
                    },
                    {
                        "name": "Sergey D. Erokhin"
                    },
                    {
                        "name": "Elena V. Tutubalina"
                    },
                    {
                        "name": "Oleg Y. Rogov"
                    }
                ],
                "author_detail": {
                    "name": "Oleg Y. Rogov"
                },
                "author": "Oleg Y. Rogov",
                "arxiv_comment": "9 pages, 1 figure; article under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16481v1",
                "updated": "2025-08-22T15:53:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    53,
                    22,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:53:22Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    53,
                    22,
                    4,
                    234,
                    0
                ],
                "title": "Benchmarking the Robustness of Agentic Systems to Adversarially-Induced\n  Harms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking the Robustness of Agentic Systems to Adversarially-Induced\n  Harms"
                },
                "summary": "Ensuring the safe use of agentic systems requires a thorough understanding of\nthe range of malicious behaviors these systems may exhibit when under attack.\nIn this paper, we evaluate the robustness of LLM-based agentic systems against\nattacks that aim to elicit harmful actions from agents. To this end, we propose\na novel taxonomy of harms for agentic systems and a novel benchmark, BAD-ACTS,\nfor studying the security of agentic systems with respect to a wide range of\nharmful actions. BAD-ACTS consists of 4 implementations of agentic systems in\ndistinct application environments, as well as a dataset of 188 high-quality\nexamples of harmful actions. This enables a comprehensive study of the\nrobustness of agentic systems across a wide range of categories of harmful\nbehaviors, available tools, and inter-agent communication structures. Using\nthis benchmark, we analyze the robustness of agentic systems against an\nattacker that controls one of the agents in the system and aims to manipulate\nother agents to execute a harmful target action. Our results show that the\nattack has a high success rate, demonstrating that even a single adversarial\nagent within the system can have a significant impact on the security. This\nattack remains effective even when agents use a simple prompting-based defense\nstrategy. However, we additionally propose a more effective defense based on\nmessage monitoring. We believe that this benchmark provides a diverse testbed\nfor the security research of agentic systems. The benchmark can be found at\ngithub.com/JNoether/BAD-ACTS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safe use of agentic systems requires a thorough understanding of\nthe range of malicious behaviors these systems may exhibit when under attack.\nIn this paper, we evaluate the robustness of LLM-based agentic systems against\nattacks that aim to elicit harmful actions from agents. To this end, we propose\na novel taxonomy of harms for agentic systems and a novel benchmark, BAD-ACTS,\nfor studying the security of agentic systems with respect to a wide range of\nharmful actions. BAD-ACTS consists of 4 implementations of agentic systems in\ndistinct application environments, as well as a dataset of 188 high-quality\nexamples of harmful actions. This enables a comprehensive study of the\nrobustness of agentic systems across a wide range of categories of harmful\nbehaviors, available tools, and inter-agent communication structures. Using\nthis benchmark, we analyze the robustness of agentic systems against an\nattacker that controls one of the agents in the system and aims to manipulate\nother agents to execute a harmful target action. Our results show that the\nattack has a high success rate, demonstrating that even a single adversarial\nagent within the system can have a significant impact on the security. This\nattack remains effective even when agents use a simple prompting-based defense\nstrategy. However, we additionally propose a more effective defense based on\nmessage monitoring. We believe that this benchmark provides a diverse testbed\nfor the security research of agentic systems. The benchmark can be found at\ngithub.com/JNoether/BAD-ACTS"
                },
                "authors": [
                    {
                        "name": "Jonathan Nther"
                    },
                    {
                        "name": "Adish Singla"
                    },
                    {
                        "name": "Goran Radanovic"
                    }
                ],
                "author_detail": {
                    "name": "Goran Radanovic"
                },
                "author": "Goran Radanovic",
                "arxiv_comment": "52 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16478v1",
                "updated": "2025-08-22T15:47:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    47,
                    17,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:47:17Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    47,
                    17,
                    4,
                    234,
                    0
                ],
                "title": "LLM-as-classifier: Semi-Supervised, Iterative Framework for Hierarchical\n  Text Classification using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-classifier: Semi-Supervised, Iterative Framework for Hierarchical\n  Text Classification using Large Language Models"
                },
                "summary": "The advent of Large Language Models (LLMs) has provided unprecedented\ncapabilities for analyzing unstructured text data. However, deploying these\nmodels as reliable, robust, and scalable classifiers in production environments\npresents significant methodological challenges. Standard fine-tuning approaches\ncan be resource-intensive and often struggle with the dynamic nature of\nreal-world data distributions, which is common in the industry. In this paper,\nwe propose a comprehensive, semi-supervised framework that leverages the zero-\nand few-shot capabilities of LLMs for building hierarchical text classifiers as\na framework for a solution to these industry-wide challenges. Our methodology\nemphasizes an iterative, human-in-the-loop process that begins with domain\nknowledge elicitation and progresses through prompt refinement, hierarchical\nexpansion, and multi-faceted validation. We introduce techniques for assessing\nand mitigating sequence-based biases and outline a protocol for continuous\nmonitoring and adaptation. This framework is designed to bridge the gap between\nthe raw power of LLMs and the practical need for accurate, interpretable, and\nmaintainable classification systems in industry applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has provided unprecedented\ncapabilities for analyzing unstructured text data. However, deploying these\nmodels as reliable, robust, and scalable classifiers in production environments\npresents significant methodological challenges. Standard fine-tuning approaches\ncan be resource-intensive and often struggle with the dynamic nature of\nreal-world data distributions, which is common in the industry. In this paper,\nwe propose a comprehensive, semi-supervised framework that leverages the zero-\nand few-shot capabilities of LLMs for building hierarchical text classifiers as\na framework for a solution to these industry-wide challenges. Our methodology\nemphasizes an iterative, human-in-the-loop process that begins with domain\nknowledge elicitation and progresses through prompt refinement, hierarchical\nexpansion, and multi-faceted validation. We introduce techniques for assessing\nand mitigating sequence-based biases and outline a protocol for continuous\nmonitoring and adaptation. This framework is designed to bridge the gap between\nthe raw power of LLMs and the practical need for accurate, interpretable, and\nmaintainable classification systems in industry applications."
                },
                "authors": [
                    {
                        "name": "Doohee You"
                    },
                    {
                        "name": "Andy Parisi"
                    },
                    {
                        "name": "Zach Vander Velden"
                    },
                    {
                        "name": "Lara Dantas Inojosa"
                    }
                ],
                "author_detail": {
                    "name": "Lara Dantas Inojosa"
                },
                "author": "Lara Dantas Inojosa",
                "arxiv_comment": "20 pages excluding reference list, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14313v2",
                "updated": "2025-08-22T15:37:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    37,
                    12,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-19T23:41:15Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    23,
                    41,
                    15,
                    1,
                    231,
                    0
                ],
                "title": "Your Reward Function for RL is Your Best PRM for Search: Unifying RL and\n  Search-Based TTS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Reward Function for RL is Your Best PRM for Search: Unifying RL and\n  Search-Based TTS"
                },
                "summary": "Test-time scaling (TTS) for large language models (LLMs) has thus far fallen\ninto two largely separate paradigms: (1) reinforcement learning (RL) methods\nthat optimize sparse outcome-based rewards, yet suffer from instability and low\nsample efficiency; and (2) search-based techniques guided by independently\ntrained, static process reward models (PRMs), which require expensive human- or\nLLM-generated labels and often degrade under distribution shifts. In this\npaper, we introduce AIRL-S, the first natural unification of RL-based and\nsearch-based TTS. Central to AIRL-S is the insight that the reward function\nlearned during RL training inherently represents the ideal PRM for guiding\ndownstream search. Specifically, we leverage adversarial inverse reinforcement\nlearning (AIRL) combined with group relative policy optimization (GRPO) to\nlearn a dense, dynamic PRM directly from correct reasoning traces, entirely\neliminating the need for labeled intermediate process data. At inference, the\nresulting PRM simultaneously serves as the critic for RL rollouts and as a\nheuristic to effectively guide search procedures, facilitating robust reasoning\nchain extension, mitigating reward hacking, and enhancing cross-task\ngeneralization. Experimental results across eight benchmarks, including\nmathematics, scientific reasoning, and code generation, demonstrate that our\nunified approach improves performance by 9 % on average over the base model,\nmatching GPT-4o. Furthermore, when integrated into multiple search algorithms,\nour PRM consistently outperforms all baseline PRMs trained with labeled data.\nThese results underscore that, indeed, your reward function for RL is your best\nPRM for search, providing a robust and cost-effective solution to complex\nreasoning tasks in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) for large language models (LLMs) has thus far fallen\ninto two largely separate paradigms: (1) reinforcement learning (RL) methods\nthat optimize sparse outcome-based rewards, yet suffer from instability and low\nsample efficiency; and (2) search-based techniques guided by independently\ntrained, static process reward models (PRMs), which require expensive human- or\nLLM-generated labels and often degrade under distribution shifts. In this\npaper, we introduce AIRL-S, the first natural unification of RL-based and\nsearch-based TTS. Central to AIRL-S is the insight that the reward function\nlearned during RL training inherently represents the ideal PRM for guiding\ndownstream search. Specifically, we leverage adversarial inverse reinforcement\nlearning (AIRL) combined with group relative policy optimization (GRPO) to\nlearn a dense, dynamic PRM directly from correct reasoning traces, entirely\neliminating the need for labeled intermediate process data. At inference, the\nresulting PRM simultaneously serves as the critic for RL rollouts and as a\nheuristic to effectively guide search procedures, facilitating robust reasoning\nchain extension, mitigating reward hacking, and enhancing cross-task\ngeneralization. Experimental results across eight benchmarks, including\nmathematics, scientific reasoning, and code generation, demonstrate that our\nunified approach improves performance by 9 % on average over the base model,\nmatching GPT-4o. Furthermore, when integrated into multiple search algorithms,\nour PRM consistently outperforms all baseline PRMs trained with labeled data.\nThese results underscore that, indeed, your reward function for RL is your best\nPRM for search, providing a robust and cost-effective solution to complex\nreasoning tasks in LLMs."
                },
                "authors": [
                    {
                        "name": "Can Jin"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Qixin Zhang"
                    },
                    {
                        "name": "Hongwu Peng"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Marco Pavone"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Zhang-Wei Hong"
                    },
                    {
                        "name": "Tong Che"
                    },
                    {
                        "name": "Dimitris N. Metaxas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris N. Metaxas"
                },
                "author": "Dimitris N. Metaxas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10991v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10991v2",
                "updated": "2025-08-22T15:35:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    35,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-14T18:00:25Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    18,
                    0,
                    25,
                    3,
                    226,
                    0
                ],
                "title": "MCP-Guard: A Defense Framework for Model Context Protocol Integrity in\n  Large Language Model Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCP-Guard: A Defense Framework for Model Context Protocol Integrity in\n  Large Language Model Applications"
                },
                "summary": "The integration of Large Language Models (LLMs) with external tools via\nprotocols such as the Model Context Protocol (MCP) introduces critical security\nvulnerabilities, including prompt injection, data exfiltration, and other\nthreats. To counter these challenges, we propose MCP-Guard, a robust, layered\ndefense architecture designed for LLM--tool interactions. MCP-Guard employs a\nthree-stage detection pipeline that balances efficiency with accuracy: it\nprogresses from lightweight static scanning for overt threats and a deep neural\ndetector for semantic attacks, to our fine-tuned E5-based model achieves\n(96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM\narbitrator synthesizes these signals to deliver the final decision while\nminimizing false positives. To facilitate rigorous training and evaluation, we\nalso introduce MCP-AttackBench, a comprehensive benchmark of over 70,000\nsamples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench\nsimulates diverse, real-world attack vectors in the MCP format, providing a\nfoundation for future research into securing LLM-tool ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) with external tools via\nprotocols such as the Model Context Protocol (MCP) introduces critical security\nvulnerabilities, including prompt injection, data exfiltration, and other\nthreats. To counter these challenges, we propose MCP-Guard, a robust, layered\ndefense architecture designed for LLM--tool interactions. MCP-Guard employs a\nthree-stage detection pipeline that balances efficiency with accuracy: it\nprogresses from lightweight static scanning for overt threats and a deep neural\ndetector for semantic attacks, to our fine-tuned E5-based model achieves\n(96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM\narbitrator synthesizes these signals to deliver the final decision while\nminimizing false positives. To facilitate rigorous training and evaluation, we\nalso introduce MCP-AttackBench, a comprehensive benchmark of over 70,000\nsamples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench\nsimulates diverse, real-world attack vectors in the MCP format, providing a\nfoundation for future research into securing LLM-tool ecosystems."
                },
                "authors": [
                    {
                        "name": "Wenpeng Xing"
                    },
                    {
                        "name": "Zhonghao Qi"
                    },
                    {
                        "name": "Yupeng Qin"
                    },
                    {
                        "name": "Yilin Li"
                    },
                    {
                        "name": "Caini Chang"
                    },
                    {
                        "name": "Jiahui Yu"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Zhenzhen Xie"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10991v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10991v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16457v1",
                "updated": "2025-08-22T15:18:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    18,
                    50,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:18:50Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    18,
                    50,
                    4,
                    234,
                    0
                ],
                "title": "Wide-Area Power System Oscillations from Large-Scale AI Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wide-Area Power System Oscillations from Large-Scale AI Workloads"
                },
                "summary": "This paper develops a new dynamic power profiling approach for modeling\nAI-centric datacenter loads and analyzing their impact on grid operations,\nparticularly their potential to induce wide-area grid oscillations. We\ncharacterize the periodic stochastic power fluctuations inherent to large-scale\nAI workloads during both the training and fine-tuning stages, driven by the\nstate-of-the-art GPU computing architecture designs. These sustained, large\npower fluctuations, unlike conventional load ramping, act as persistent forcing\ninputs capable of interacting with and amplifying local and inter-area\noscillation modes. Using the WECC 179-bus system as a test case, we examine the\namplitude and variability of oscillatory responses under different factors,\nranging from system strength, penetration level, fluctuation frequency range,\nindividual datacenter size, to geographical deployment. Simulation results show\nthat, notably, narrower fluctuation bands, larger single-site capacities, or\ndispersed siting can intensify oscillations across multiple modes. Our models\nand numerical studies provide a quantitative basis for integrating AI-dominant\nelectricity demands into grid oscillation studies, and further support the\ndevelopment of new planning and operational measures to power the continuous AI\nload growth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper develops a new dynamic power profiling approach for modeling\nAI-centric datacenter loads and analyzing their impact on grid operations,\nparticularly their potential to induce wide-area grid oscillations. We\ncharacterize the periodic stochastic power fluctuations inherent to large-scale\nAI workloads during both the training and fine-tuning stages, driven by the\nstate-of-the-art GPU computing architecture designs. These sustained, large\npower fluctuations, unlike conventional load ramping, act as persistent forcing\ninputs capable of interacting with and amplifying local and inter-area\noscillation modes. Using the WECC 179-bus system as a test case, we examine the\namplitude and variability of oscillatory responses under different factors,\nranging from system strength, penetration level, fluctuation frequency range,\nindividual datacenter size, to geographical deployment. Simulation results show\nthat, notably, narrower fluctuation bands, larger single-site capacities, or\ndispersed siting can intensify oscillations across multiple modes. Our models\nand numerical studies provide a quantitative basis for integrating AI-dominant\nelectricity demands into grid oscillation studies, and further support the\ndevelopment of new planning and operational measures to power the continuous AI\nload growth."
                },
                "authors": [
                    {
                        "name": "Min-Seung Ko"
                    },
                    {
                        "name": "Hao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhu"
                },
                "author": "Hao Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16456v1",
                "updated": "2025-08-22T15:15:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    15,
                    38,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:15:38Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    15,
                    38,
                    4,
                    234,
                    0
                ],
                "title": "A Probabilistic Inference Scaling Theory for LLM Self-Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Probabilistic Inference Scaling Theory for LLM Self-Correction"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the capability to refine their\ngenerated answers through self-correction, enabling continuous performance\nimprovement over multiple rounds. However, the mechanisms underlying how and\nwhy accuracy evolves during this iterative process remain unexplored. To fill\nthis gap, we propose a probabilistic theory to model the dynamics of accuracy\nchange and explain the performance improvements observed in multi-round\nself-correction. Through mathematical derivation, we establish that the\naccuracy after the $t^{th}$ round of self-correction is given by: $Acc_t = Upp\n- \\alpha^t(Upp - Acc_0),$ where $Acc_0$ denotes the initial accuracy, $Upp$\nrepresents the upper bound of accuracy convergence, and $\\alpha$ determines the\nrate of convergence. Based on our theory, these parameters can be calculated\nand the predicted accuracy curve then can be obtained through only a single\nround of self-correction. Extensive experiments across diverse models and\ndatasets demonstrate that our theoretical predictions align closely with\nempirical accuracy curves, validating the effectiveness of the theory. Our work\nprovides a theoretical foundation for understanding LLM self-correction, thus\npaving the way for further explorations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the capability to refine their\ngenerated answers through self-correction, enabling continuous performance\nimprovement over multiple rounds. However, the mechanisms underlying how and\nwhy accuracy evolves during this iterative process remain unexplored. To fill\nthis gap, we propose a probabilistic theory to model the dynamics of accuracy\nchange and explain the performance improvements observed in multi-round\nself-correction. Through mathematical derivation, we establish that the\naccuracy after the $t^{th}$ round of self-correction is given by: $Acc_t = Upp\n- \\alpha^t(Upp - Acc_0),$ where $Acc_0$ denotes the initial accuracy, $Upp$\nrepresents the upper bound of accuracy convergence, and $\\alpha$ determines the\nrate of convergence. Based on our theory, these parameters can be calculated\nand the predicted accuracy curve then can be obtained through only a single\nround of self-correction. Extensive experiments across diverse models and\ndatasets demonstrate that our theoretical predictions align closely with\nempirical accuracy curves, validating the effectiveness of the theory. Our work\nprovides a theoretical foundation for understanding LLM self-correction, thus\npaving the way for further explorations."
                },
                "authors": [
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Yudong Wang"
                    },
                    {
                        "name": "Ziyao Xu"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16449v1",
                "updated": "2025-08-22T15:08:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    8,
                    34,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:08:34Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    8,
                    34,
                    4,
                    234,
                    0
                ],
                "title": "GreenLLM: SLO-Aware Dynamic Frequency Scaling for Energy-Efficient LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GreenLLM: SLO-Aware Dynamic Frequency Scaling for Energy-Efficient LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are becoming the backbone of modern cloud\nservices, yet their inference costs are dominated by GPU energy. Unlike\ntraditional GPU workloads, LLM inference has two stages with different\ncharacteristics: the prefill phase, which is latency sensitive and scales\nquadratically with prompt length, and the decode phase, which progresses token\nby token with unpredictable length. Current GPU power governors (for example,\nNVIDIA's default) overlook this asymmetry and treat both stages uniformly. The\nresult is mismatched voltage and frequency settings, head-of-line blocking, and\nexcessive energy use.\n  We introduce GreenLLM, an SLO-aware serving framework that minimizes GPU\nenergy by explicitly separating prefill and decode control. At ingress,\nrequests are routed into length-based queues so short prompts avoid\nhead-of-line blocking and TTFT improves. For prefill, GreenLLM collects short\ntraces on a GPU node, fits compact latency-power models over SM frequency, and\nsolves a queueing-aware optimization to select energy-minimal clocks per class.\nDuring decode, a lightweight dual-loop controller tracks throughput (tokens per\nsecond) and adjusts frequency with hysteretic, fine-grained steps to hold tail\nTBT within target bounds. Across Alibaba and Azure trace replays, GreenLLM\nreduces total energy by up to 34 percent versus the default DVFS baseline, with\nno loss of throughput and with less than 3.5 percent additional SLO violations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming the backbone of modern cloud\nservices, yet their inference costs are dominated by GPU energy. Unlike\ntraditional GPU workloads, LLM inference has two stages with different\ncharacteristics: the prefill phase, which is latency sensitive and scales\nquadratically with prompt length, and the decode phase, which progresses token\nby token with unpredictable length. Current GPU power governors (for example,\nNVIDIA's default) overlook this asymmetry and treat both stages uniformly. The\nresult is mismatched voltage and frequency settings, head-of-line blocking, and\nexcessive energy use.\n  We introduce GreenLLM, an SLO-aware serving framework that minimizes GPU\nenergy by explicitly separating prefill and decode control. At ingress,\nrequests are routed into length-based queues so short prompts avoid\nhead-of-line blocking and TTFT improves. For prefill, GreenLLM collects short\ntraces on a GPU node, fits compact latency-power models over SM frequency, and\nsolves a queueing-aware optimization to select energy-minimal clocks per class.\nDuring decode, a lightweight dual-loop controller tracks throughput (tokens per\nsecond) and adjusts frequency with hysteretic, fine-grained steps to hold tail\nTBT within target bounds. Across Alibaba and Azure trace replays, GreenLLM\nreduces total energy by up to 34 percent versus the default DVFS baseline, with\nno loss of throughput and with less than 3.5 percent additional SLO violations."
                },
                "authors": [
                    {
                        "name": "Qunyou Liu"
                    },
                    {
                        "name": "Darong Huang"
                    },
                    {
                        "name": "Marina Zapater"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16447v1",
                "updated": "2025-08-22T15:02:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    2,
                    7,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T15:02:07Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    2,
                    7,
                    4,
                    234,
                    0
                ],
                "title": "Boardwalk: Towards a Framework for Creating Board Games with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boardwalk: Towards a Framework for Creating Board Games with LLMs"
                },
                "summary": "Implementing board games in code can be a time-consuming task. However, Large\nLanguage Models (LLMs) have been proven effective at generating code for\ndomain-specific tasks with simple contextual information. We aim to investigate\nwhether LLMs can implement digital versions of board games from rules described\nin natural language. This would be a step towards an LLM-assisted framework for\nquick board game code generation. We expect to determine the main challenges\nfor LLMs to implement the board games, and how different approaches and models\ncompare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek\nand ChatGPT) with coding a selection of 12 popular and obscure games in\nfree-form and within Boardwalk, our proposed General Game Playing API. We\nanonymize the games and components to avoid evoking pre-trained LLM knowledge.\nThe implementations are tested for playability and rule compliance. We evaluate\nsuccess rate and common errors across LLMs and game popularity. Our approach\nproves viable, with the best performing model, Claude 3.7 Sonnet, yielding\n55.6\\% of games without any errors. While compliance with the API increases\nerror frequency, the severity of errors is more significantly dependent on the\nLLM. We outline future steps for creating a framework to integrate this\nprocess, making the elaboration of board games more accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementing board games in code can be a time-consuming task. However, Large\nLanguage Models (LLMs) have been proven effective at generating code for\ndomain-specific tasks with simple contextual information. We aim to investigate\nwhether LLMs can implement digital versions of board games from rules described\nin natural language. This would be a step towards an LLM-assisted framework for\nquick board game code generation. We expect to determine the main challenges\nfor LLMs to implement the board games, and how different approaches and models\ncompare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek\nand ChatGPT) with coding a selection of 12 popular and obscure games in\nfree-form and within Boardwalk, our proposed General Game Playing API. We\nanonymize the games and components to avoid evoking pre-trained LLM knowledge.\nThe implementations are tested for playability and rule compliance. We evaluate\nsuccess rate and common errors across LLMs and game popularity. Our approach\nproves viable, with the best performing model, Claude 3.7 Sonnet, yielding\n55.6\\% of games without any errors. While compliance with the API increases\nerror frequency, the severity of errors is more significantly dependent on the\nLLM. We outline future steps for creating a framework to integrate this\nprocess, making the elaboration of board games more accessible."
                },
                "authors": [
                    {
                        "name": "lvaro Guglielmin Becker"
                    },
                    {
                        "name": "Gabriel Bauer de Oliveira"
                    },
                    {
                        "name": "Lana Bertoldo Rossato"
                    },
                    {
                        "name": "Anderson Rocha Tavares"
                    }
                ],
                "author_detail": {
                    "name": "Anderson Rocha Tavares"
                },
                "author": "Anderson Rocha Tavares",
                "arxiv_comment": "Accepted at SBGames 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07767v2",
                "updated": "2025-08-22T15:01:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    1,
                    14,
                    4,
                    234,
                    0
                ],
                "published": "2025-07-10T13:50:07Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    13,
                    50,
                    7,
                    3,
                    191,
                    0
                ],
                "title": "Structured Prompts, Better Outcomes? Exploring the Effects of a\n  Structured Interface with ChatGPT in a Graduate Robotics Course",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Prompts, Better Outcomes? Exploring the Effects of a\n  Structured Interface with ChatGPT in a Graduate Robotics Course"
                },
                "summary": "Prior research shows that how students engage with Large Language Models\n(LLMs) influences their problem-solving and understanding, reinforcing the need\nto support productive LLM-uses that promote learning. This study evaluates the\nimpact of a structured GPT platform designed to promote 'good' prompting\nbehavior with data from 58 students in a graduate-level robotics course. The\nstudents were assigned to either an intervention group using the structured\nplatform or a control group using ChatGPT freely for two practice lab sessions,\nbefore a third session where all students could freely use ChatGPT. We analyzed\nstudent perception (pre-post surveys), prompting behavior (logs), performance\n(task scores), and learning (pre-post tests). Although we found no differences\nin performance or learning between groups, we identified prompting behaviors -\nsuch as having clear prompts focused on understanding code - that were linked\nwith higher learning gains and were more prominent when students used the\nstructured platform. However, such behaviors did not transfer once students\nwere no longer constrained to use the structured platform. Qualitative survey\ndata showed mixed perceptions: some students perceived the value of the\nstructured platform, but most did not perceive its relevance and resisted\nchanging their habits. These findings contribute to ongoing efforts to identify\neffective strategies for integrating LLMs into learning and question the\neffectiveness of bottom-up approaches that temporarily alter user interfaces to\ninfluence students' interaction. Future research could instead explore top-down\nstrategies that address students' motivations and explicitly demonstrate how\ncertain interaction patterns support learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior research shows that how students engage with Large Language Models\n(LLMs) influences their problem-solving and understanding, reinforcing the need\nto support productive LLM-uses that promote learning. This study evaluates the\nimpact of a structured GPT platform designed to promote 'good' prompting\nbehavior with data from 58 students in a graduate-level robotics course. The\nstudents were assigned to either an intervention group using the structured\nplatform or a control group using ChatGPT freely for two practice lab sessions,\nbefore a third session where all students could freely use ChatGPT. We analyzed\nstudent perception (pre-post surveys), prompting behavior (logs), performance\n(task scores), and learning (pre-post tests). Although we found no differences\nin performance or learning between groups, we identified prompting behaviors -\nsuch as having clear prompts focused on understanding code - that were linked\nwith higher learning gains and were more prominent when students used the\nstructured platform. However, such behaviors did not transfer once students\nwere no longer constrained to use the structured platform. Qualitative survey\ndata showed mixed perceptions: some students perceived the value of the\nstructured platform, but most did not perceive its relevance and resisted\nchanging their habits. These findings contribute to ongoing efforts to identify\neffective strategies for integrating LLMs into learning and question the\neffectiveness of bottom-up approaches that temporarily alter user interfaces to\ninfluence students' interaction. Future research could instead explore top-down\nstrategies that address students' motivations and explicitly demonstrate how\ncertain interaction patterns support learning."
                },
                "authors": [
                    {
                        "name": "Jerome Brender"
                    },
                    {
                        "name": "Laila El-Hamamsy"
                    },
                    {
                        "name": "Kim Uittenhove"
                    },
                    {
                        "name": "Francesco Mondada"
                    },
                    {
                        "name": "Engin Bumbacher"
                    }
                ],
                "author_detail": {
                    "name": "Engin Bumbacher"
                },
                "author": "Engin Bumbacher",
                "arxiv_comment": "Accepted, to appear in the proceedings of the EC-TEL 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16445v1",
                "updated": "2025-08-22T14:59:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    59,
                    35,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T14:59:35Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    59,
                    35,
                    4,
                    234,
                    0
                ],
                "title": "Using LLMs and Essence to Support Software Practice Adoption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs and Essence to Support Software Practice Adoption"
                },
                "summary": "Recent advancements in natural language processing (NLP) have enabled the\ndevelopment of automated tools that support various domains, including software\nengineering. However, while NLP and artificial intelligence (AI) research has\nextensively focused on tasks such as code generation, less attention has been\ngiven to automating support for the adoption of best practices, the evolution\nof ways of working, and the monitoring of process health. This study addresses\nthis gap by exploring the integration of Essence, a standard and thinking\nframework for managing software engineering practices, with large language\nmodels (LLMs). To this end, a specialised chatbot was developed to assist\nstudents and professionals in understanding and applying Essence. The chatbot\nemploys a retrieval-augmented generation (RAG) system to retrieve relevant\ncontextual information from a curated knowledge base. Four different LLMs were\nused to create multiple chatbot configurations, each evaluated both as a base\nmodel and augmented with the RAG system. The system performance was evaluated\nthrough both the relevance of retrieved context and the quality of generated\nresponses. Comparative analysis against the general-purpose LLMs demonstrated\nthat the proposed system consistently outperforms its baseline counterpart in\ndomain-specific tasks. By facilitating access to structured software\nengineering knowledge, this work contributes to bridging the gap between\ntheoretical frameworks and practical application, potentially improving process\nmanagement and the adoption of software development practices. While further\nvalidation through user studies is required, these findings highlight the\npotential of LLM-based automation to enhance learning and decision-making in\nsoftware engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in natural language processing (NLP) have enabled the\ndevelopment of automated tools that support various domains, including software\nengineering. However, while NLP and artificial intelligence (AI) research has\nextensively focused on tasks such as code generation, less attention has been\ngiven to automating support for the adoption of best practices, the evolution\nof ways of working, and the monitoring of process health. This study addresses\nthis gap by exploring the integration of Essence, a standard and thinking\nframework for managing software engineering practices, with large language\nmodels (LLMs). To this end, a specialised chatbot was developed to assist\nstudents and professionals in understanding and applying Essence. The chatbot\nemploys a retrieval-augmented generation (RAG) system to retrieve relevant\ncontextual information from a curated knowledge base. Four different LLMs were\nused to create multiple chatbot configurations, each evaluated both as a base\nmodel and augmented with the RAG system. The system performance was evaluated\nthrough both the relevance of retrieved context and the quality of generated\nresponses. Comparative analysis against the general-purpose LLMs demonstrated\nthat the proposed system consistently outperforms its baseline counterpart in\ndomain-specific tasks. By facilitating access to structured software\nengineering knowledge, this work contributes to bridging the gap between\ntheoretical frameworks and practical application, potentially improving process\nmanagement and the adoption of software development practices. While further\nvalidation through user studies is required, these findings highlight the\npotential of LLM-based automation to enhance learning and decision-making in\nsoftware engineering."
                },
                "authors": [
                    {
                        "name": "Sonia Nicoletti"
                    },
                    {
                        "name": "Paolo Ciancarini"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Ciancarini"
                },
                "author": "Paolo Ciancarini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04916v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04916v4",
                "updated": "2025-08-22T14:52:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    52,
                    2,
                    4,
                    234,
                    0
                ],
                "published": "2025-02-07T13:33:40Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    33,
                    40,
                    4,
                    38,
                    0
                ],
                "title": "Classification or Prompting: A Case Study on Legal Requirements\n  Traceability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification or Prompting: A Case Study on Legal Requirements\n  Traceability"
                },
                "summary": "New regulations are introduced to ensure software development aligns with\nethical concerns and protects public safety. Showing compliance requires\ntracing requirements to legal provisions. Requirements traceability is a key\ntask where engineers must analyze technical requirements against target\nartifacts, often within limited time. Manually analyzing complex systems with\nhundreds of requirements is infeasible. The legal dimension adds challenges\nthat increase effort. In this paper, we investigate two automated solutions\nbased on language models, including large ones (LLMs). The first solution,\nKashif, is a classifier that leverages sentence transformers and semantic\nsimilarity. The second solution, RICE_LRT, prompts a recent generative LLM\nbased on RICE, a prompt engineering framework. On a benchmark dataset, we\nempirically evaluate Kashif and compare it against five different baseline\nclassifiers from the literature. Kashif can identify trace links with a recall\nof 67%, precision of 50%, and F2 score of 63%, outperforming the best baseline\nby a substantial margin of 41 percentage points (pp) in F2. However, on unseen,\nmore complex requirements documents traced to the European General Data\nProtection Regulation (GDPR), Kashif performs poorly, yielding an average\nrecall of 15%, an average precision of 10%, and an average F2 score of 13.5%.\nOn the same documents, however, our RICE solution yields an average recall of\n84%, an average precision of 30%, and an average F2 score of 61%. RICE achieved\na remarkable improvement of 47.5 pp over Kashif in terms of F2 score. Our\nresults suggest that requirements traceability in the legal context cannot be\nsimply addressed by building classifiers, as such solutions do not generalize\nand fail to perform well on complex regulations and requirements. Resorting to\ngenerative LLMs, with careful prompt engineering, is thus a more promising\nalternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New regulations are introduced to ensure software development aligns with\nethical concerns and protects public safety. Showing compliance requires\ntracing requirements to legal provisions. Requirements traceability is a key\ntask where engineers must analyze technical requirements against target\nartifacts, often within limited time. Manually analyzing complex systems with\nhundreds of requirements is infeasible. The legal dimension adds challenges\nthat increase effort. In this paper, we investigate two automated solutions\nbased on language models, including large ones (LLMs). The first solution,\nKashif, is a classifier that leverages sentence transformers and semantic\nsimilarity. The second solution, RICE_LRT, prompts a recent generative LLM\nbased on RICE, a prompt engineering framework. On a benchmark dataset, we\nempirically evaluate Kashif and compare it against five different baseline\nclassifiers from the literature. Kashif can identify trace links with a recall\nof 67%, precision of 50%, and F2 score of 63%, outperforming the best baseline\nby a substantial margin of 41 percentage points (pp) in F2. However, on unseen,\nmore complex requirements documents traced to the European General Data\nProtection Regulation (GDPR), Kashif performs poorly, yielding an average\nrecall of 15%, an average precision of 10%, and an average F2 score of 13.5%.\nOn the same documents, however, our RICE solution yields an average recall of\n84%, an average precision of 30%, and an average F2 score of 61%. RICE achieved\na remarkable improvement of 47.5 pp over Kashif in terms of F2 score. Our\nresults suggest that requirements traceability in the legal context cannot be\nsimply addressed by building classifiers, as such solutions do not generalize\nand fail to perform well on complex regulations and requirements. Resorting to\ngenerative LLMs, with careful prompt engineering, is thus a more promising\nalternative."
                },
                "authors": [
                    {
                        "name": "Romina Etezadi"
                    },
                    {
                        "name": "Sallam Abualhaija"
                    },
                    {
                        "name": "Chetan Arora"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "arxiv_comment": "38 pages, 4 figues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04916v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04916v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16439v1",
                "updated": "2025-08-22T14:50:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    50,
                    55,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T14:50:55Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    50,
                    55,
                    4,
                    234,
                    0
                ],
                "title": "PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark"
                },
                "summary": "Large language models (LLMs) and vision-augmented LLMs (VLMs) have\nsignificantly advanced medical informatics, diagnostics, and decision support.\nHowever, these models exhibit systematic biases, particularly age bias,\ncompromising their reliability and equity. This is evident in their poorer\nperformance on pediatric-focused text and visual question-answering tasks. This\nbias reflects a broader imbalance in medical research, where pediatric studies\nreceive less funding and representation despite the significant disease burden\nin children. To address these issues, a new comprehensive multi-modal pediatric\nquestion-answering benchmark, PediatricsMQA, has been introduced. It consists\nof 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric\ntopics across seven developmental stages (prenatal to adolescent) and 2,067\nvision-based MCQs using 634 pediatric images from 67 imaging modalities and 256\nanatomical regions. The dataset was developed using a hybrid manual-automatic\npipeline, incorporating peer-reviewed pediatric literature, validated question\nbanks, existing benchmarks, and existing QA resources. Evaluating\nstate-of-the-art open models, we find dramatic performance drops in younger\ncohorts, highlighting the need for age-aware methods to ensure equitable AI\nsupport in pediatric care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and vision-augmented LLMs (VLMs) have\nsignificantly advanced medical informatics, diagnostics, and decision support.\nHowever, these models exhibit systematic biases, particularly age bias,\ncompromising their reliability and equity. This is evident in their poorer\nperformance on pediatric-focused text and visual question-answering tasks. This\nbias reflects a broader imbalance in medical research, where pediatric studies\nreceive less funding and representation despite the significant disease burden\nin children. To address these issues, a new comprehensive multi-modal pediatric\nquestion-answering benchmark, PediatricsMQA, has been introduced. It consists\nof 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric\ntopics across seven developmental stages (prenatal to adolescent) and 2,067\nvision-based MCQs using 634 pediatric images from 67 imaging modalities and 256\nanatomical regions. The dataset was developed using a hybrid manual-automatic\npipeline, incorporating peer-reviewed pediatric literature, validated question\nbanks, existing benchmarks, and existing QA resources. Evaluating\nstate-of-the-art open models, we find dramatic performance drops in younger\ncohorts, highlighting the need for age-aware methods to ensure equitable AI\nsupport in pediatric care."
                },
                "authors": [
                    {
                        "name": "Adil Bahaj"
                    },
                    {
                        "name": "Mounir Ghogho"
                    }
                ],
                "author_detail": {
                    "name": "Mounir Ghogho"
                },
                "author": "Mounir Ghogho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16438v1",
                "updated": "2025-08-22T14:50:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    50,
                    26,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T14:50:26Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    50,
                    26,
                    4,
                    234,
                    0
                ],
                "title": "OPERA: A Reinforcement Learning--Enhanced Orchestrated Planner-Executor\n  Architecture for Reasoning-Oriented Multi-Hop Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPERA: A Reinforcement Learning--Enhanced Orchestrated Planner-Executor\n  Architecture for Reasoning-Oriented Multi-Hop Retrieval"
                },
                "summary": "Recent advances in large language models (LLMs) and dense retrievers have\ndriven significant progress in retrieval-augmented generation (RAG). However,\nexisting approaches face significant challenges in complex reasoning-oriented\nmulti-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Prior\nmethods struggle to generate robust multi-step plans for complex queries, as\nrule-based decomposers perform poorly on out-of-template questions. 2)\nSuboptimal reasoning-driven retrieval: Related methods employ limited query\nreformulation, leading to iterative retrieval loops that often fail to locate\ngolden documents. 3) Insufficient reasoning-guided filtering: Prevailing\nmethods lack the fine-grained reasoning to effectively filter salient\ninformation from noisy results, hindering utilization of retrieved knowledge.\nFundamentally, these limitations all stem from the weak coupling between\nretrieval and reasoning in current RAG architectures. We introduce the\nOrchestrated Planner-Executor Reasoning Architecture (OPERA), a novel\nreasoning-driven retrieval framework. OPERA's Goal Planning Module (GPM)\ndecomposes questions into sub-goals, which are executed by a Reason-Execute\nModule (REM) with specialized components for precise reasoning and effective\nretrieval. To train OPERA, we propose Multi-Agents Progressive Group Relative\nPolicy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complex\nmulti-hop benchmarks show OPERA's superior performance, validating both the\nMAPGRPO method and OPERA's design. Code is available at\nhttps://github.com/Ameame1/OPERA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) and dense retrievers have\ndriven significant progress in retrieval-augmented generation (RAG). However,\nexisting approaches face significant challenges in complex reasoning-oriented\nmulti-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Prior\nmethods struggle to generate robust multi-step plans for complex queries, as\nrule-based decomposers perform poorly on out-of-template questions. 2)\nSuboptimal reasoning-driven retrieval: Related methods employ limited query\nreformulation, leading to iterative retrieval loops that often fail to locate\ngolden documents. 3) Insufficient reasoning-guided filtering: Prevailing\nmethods lack the fine-grained reasoning to effectively filter salient\ninformation from noisy results, hindering utilization of retrieved knowledge.\nFundamentally, these limitations all stem from the weak coupling between\nretrieval and reasoning in current RAG architectures. We introduce the\nOrchestrated Planner-Executor Reasoning Architecture (OPERA), a novel\nreasoning-driven retrieval framework. OPERA's Goal Planning Module (GPM)\ndecomposes questions into sub-goals, which are executed by a Reason-Execute\nModule (REM) with specialized components for precise reasoning and effective\nretrieval. To train OPERA, we propose Multi-Agents Progressive Group Relative\nPolicy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complex\nmulti-hop benchmarks show OPERA's superior performance, validating both the\nMAPGRPO method and OPERA's design. Code is available at\nhttps://github.com/Ameame1/OPERA."
                },
                "authors": [
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Yanbing Liu"
                    },
                    {
                        "name": "Fangfang Yuan"
                    },
                    {
                        "name": "Cong Cao"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Kun Peng"
                    },
                    {
                        "name": "WeiZhuo Chen"
                    },
                    {
                        "name": "Jianjun Li"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Ma"
                },
                "author": "Zhiyuan Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13622v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13622v4",
                "updated": "2025-08-22T14:48:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    48,
                    22,
                    4,
                    234,
                    0
                ],
                "published": "2025-01-23T12:44:45Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    44,
                    45,
                    3,
                    23,
                    0
                ],
                "title": "Coarse-to-Fine Process Reward Modeling for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-to-Fine Process Reward Modeling for Mathematical Reasoning"
                },
                "summary": "The Process Reward Model (PRM) plays a crucial role in mathematical reasoning\ntasks, requiring high-quality supervised process data. However, we observe that\nreasoning steps generated by Large Language Models (LLMs) often fail to exhibit\nstrictly incremental information, leading to redundancy that can hinder\neffective reasoning. To address this issue, we propose CFPRM, a simple yet\neffective coarse-to-fine strategy. Instead of focusing on the detection of\nredundant steps, our approach first establishes a coarse-grained window to\nmerge adjacent reasoning steps into unified, holistic steps. The window size is\nthen progressively reduced to extract fine-grained reasoning steps, enabling\ndata collection at multiple granularities for training. By leveraging this\nhierarchical refinement process, CFPRM mitigates redundancy while preserving\nessential fine-grained knowledge. Extensive experiments on two reasoning\ndatasets across three loss criteria validate the CFPRM's effectiveness and\nversatility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Process Reward Model (PRM) plays a crucial role in mathematical reasoning\ntasks, requiring high-quality supervised process data. However, we observe that\nreasoning steps generated by Large Language Models (LLMs) often fail to exhibit\nstrictly incremental information, leading to redundancy that can hinder\neffective reasoning. To address this issue, we propose CFPRM, a simple yet\neffective coarse-to-fine strategy. Instead of focusing on the detection of\nredundant steps, our approach first establishes a coarse-grained window to\nmerge adjacent reasoning steps into unified, holistic steps. The window size is\nthen progressively reduced to extract fine-grained reasoning steps, enabling\ndata collection at multiple granularities for training. By leveraging this\nhierarchical refinement process, CFPRM mitigates redundancy while preserving\nessential fine-grained knowledge. Extensive experiments on two reasoning\ndatasets across three loss criteria validate the CFPRM's effectiveness and\nversatility."
                },
                "authors": [
                    {
                        "name": "Yulan Hu"
                    },
                    {
                        "name": "Sheng Ouyang"
                    },
                    {
                        "name": "Jinman Zhao"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13622v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13622v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16431v1",
                "updated": "2025-08-22T14:42:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    42,
                    50,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T14:42:50Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    42,
                    50,
                    4,
                    234,
                    0
                ],
                "title": "Cetvel: A Unified Benchmark for Evaluating Language Understanding,\n  Generation and Cultural Capacity of LLMs for Turkish",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cetvel: A Unified Benchmark for Evaluating Language Understanding,\n  Generation and Cultural Capacity of LLMs for Turkish"
                },
                "summary": "We introduce Cetvel, a comprehensive benchmark designed to evaluate large\nlanguage models (LLMs) in Turkish. Existing Turkish benchmarks often lack\neither task diversity or culturally relevant content, or both. Cetvel addresses\nthese gaps by combining a broad range of both discriminative and generative\ntasks ensuring content that reflects the linguistic and cultural richness of\nTurkish language. Cetvel covers 23 tasks grouped into seven categories,\nincluding tasks such as grammatical error correction, machine translation, and\nquestion answering rooted in Turkish history and idiomatic language. We\nevaluate 33 open-weight LLMs (up to 70B parameters) covering different model\nfamilies and instruction paradigms. Our experiments reveal that Turkish-centric\ninstruction-tuned models generally underperform relative to multilingual or\ngeneral-purpose models (e.g. Llama 3 and Mistral), despite being tailored for\nthe language. Moreover, we show that tasks such as grammatical error correction\nand extractive question answering are particularly discriminative in\ndifferentiating model capabilities. Cetvel offers a comprehensive and\nculturally grounded evaluation suite for advancing the development and\nassessment of LLMs in Turkish.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Cetvel, a comprehensive benchmark designed to evaluate large\nlanguage models (LLMs) in Turkish. Existing Turkish benchmarks often lack\neither task diversity or culturally relevant content, or both. Cetvel addresses\nthese gaps by combining a broad range of both discriminative and generative\ntasks ensuring content that reflects the linguistic and cultural richness of\nTurkish language. Cetvel covers 23 tasks grouped into seven categories,\nincluding tasks such as grammatical error correction, machine translation, and\nquestion answering rooted in Turkish history and idiomatic language. We\nevaluate 33 open-weight LLMs (up to 70B parameters) covering different model\nfamilies and instruction paradigms. Our experiments reveal that Turkish-centric\ninstruction-tuned models generally underperform relative to multilingual or\ngeneral-purpose models (e.g. Llama 3 and Mistral), despite being tailored for\nthe language. Moreover, we show that tasks such as grammatical error correction\nand extractive question answering are particularly discriminative in\ndifferentiating model capabilities. Cetvel offers a comprehensive and\nculturally grounded evaluation suite for advancing the development and\nassessment of LLMs in Turkish."
                },
                "authors": [
                    {
                        "name": "Yakup Abrek Er"
                    },
                    {
                        "name": "Ilker Kesen"
                    },
                    {
                        "name": "Gzde Gl ahin"
                    },
                    {
                        "name": "Aykut Erdem"
                    }
                ],
                "author_detail": {
                    "name": "Aykut Erdem"
                },
                "author": "Aykut Erdem",
                "arxiv_comment": "31 pages, 2 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16419v1",
                "updated": "2025-08-22T14:30:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    30,
                    24,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T14:30:24Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    30,
                    24,
                    4,
                    234,
                    0
                ],
                "title": "LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and\n  Security Vulnerabilities in C++ and Python",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and\n  Security Vulnerabilities in C++ and Python"
                },
                "summary": "Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are\nincreasingly embedded in software/application development, supporting tasks\nfrom code generation to debugging. Yet, their real-world effectiveness in\ndetecting diverse software bugs, particularly complex, security-relevant\nvulnerabilities, remains underexplored. This study presents a systematic,\nempirical evaluation of these three leading LLMs using a benchmark of\nfoundational programming errors, classic security flaws, and advanced,\nproduction-grade bugs in C++ and Python. The dataset integrates real code from\nSEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated\nthrough local compilation and testing pipelines. A novel multi-stage,\ncontext-aware prompting protocol simulates realistic debugging scenarios, while\na graded rubric measures detection accuracy, reasoning depth, and remediation\nquality. Our results show that all models excel at identifying syntactic and\nsemantic issues in well-scoped code, making them promising for educational use\nand as first-pass reviewers in automated code auditing. Performance diminishes\nin scenarios involving complex security vulnerabilities and large-scale\nproduction code, with ChatGPT-4 and Claude 3 generally providing more nuanced\ncontextual analyses than LLaMA 4. This highlights both the promise and the\npresent constraints of LLMs in serving as reliable code analysis tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are\nincreasingly embedded in software/application development, supporting tasks\nfrom code generation to debugging. Yet, their real-world effectiveness in\ndetecting diverse software bugs, particularly complex, security-relevant\nvulnerabilities, remains underexplored. This study presents a systematic,\nempirical evaluation of these three leading LLMs using a benchmark of\nfoundational programming errors, classic security flaws, and advanced,\nproduction-grade bugs in C++ and Python. The dataset integrates real code from\nSEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated\nthrough local compilation and testing pipelines. A novel multi-stage,\ncontext-aware prompting protocol simulates realistic debugging scenarios, while\na graded rubric measures detection accuracy, reasoning depth, and remediation\nquality. Our results show that all models excel at identifying syntactic and\nsemantic issues in well-scoped code, making them promising for educational use\nand as first-pass reviewers in automated code auditing. Performance diminishes\nin scenarios involving complex security vulnerabilities and large-scale\nproduction code, with ChatGPT-4 and Claude 3 generally providing more nuanced\ncontextual analyses than LLaMA 4. This highlights both the promise and the\npresent constraints of LLMs in serving as reliable code analysis tools."
                },
                "authors": [
                    {
                        "name": "Akshay Mhatre"
                    },
                    {
                        "name": "Noujoud Nader"
                    },
                    {
                        "name": "Patrick Diehl"
                    },
                    {
                        "name": "Deepti Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Deepti Gupta"
                },
                "author": "Deepti Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09457v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09457v2",
                "updated": "2025-08-22T14:16:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    16,
                    43,
                    4,
                    234,
                    0
                ],
                "published": "2025-06-11T07:02:18Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    7,
                    2,
                    18,
                    2,
                    162,
                    0
                ],
                "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Bridging the Reward-Generation Gap in Direct Alignment\n  Algorithms"
                },
                "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we adopt\na token-level MDP perspective of DAAs to analyze its limitations and introduce\na simple yet effective approach called Prefix-Oriented Equal-length Training\n(POET), which truncates both preferred and dispreferred responses to match the\nshorter one's length. Training with \\mname, where both responses in each sample\nare truncated to equal length, resulting in diverse truncated lengths across\nsamples, the optimization of DAAs objective is implicitly constrained to\nconverge across all timesteps of token-level MDP, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we adopt\na token-level MDP perspective of DAAs to analyze its limitations and introduce\na simple yet effective approach called Prefix-Oriented Equal-length Training\n(POET), which truncates both preferred and dispreferred responses to match the\nshorter one's length. Training with \\mname, where both responses in each sample\nare truncated to equal length, resulting in diverse truncated lengths across\nsamples, the optimization of DAAs objective is implicitly constrained to\nconverge across all timesteps of token-level MDP, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs."
                },
                "authors": [
                    {
                        "name": "Zeguan Xiao"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Ke Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Tang"
                },
                "author": "Ke Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09457v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16406v1",
                "updated": "2025-08-22T14:13:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    13,
                    16,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T14:13:16Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    13,
                    16,
                    4,
                    234,
                    0
                ],
                "title": "Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak\n  Prevention for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak\n  Prevention for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which\nattempt to elicit harmful responses from LLMs. The evolving nature and\ndiversity of these attacks pose many challenges for defense systems, including\n(1) adaptation to counter emerging attack strategies without costly retraining,\nand (2) control of the trade-off between safety and utility. To address these\nchallenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for\njailbreak detection that incorporates a database of known attack examples into\nRetrieval-Augmented Generation, which is used to infer the underlying,\nmalicious user query and jailbreak strategy used to attack the system. RAD\nenables training-free updates for newly discovered jailbreak strategies and\nprovides a mechanism to balance safety and utility. Experiments on StrongREJECT\nshow that RAD substantially reduces the effectiveness of strong jailbreak\nattacks such as PAP and PAIR while maintaining low rejection rates for benign\nqueries. We propose a novel evaluation scheme and show that RAD achieves a\nrobust safety-utility trade-off across a range of operating points in a\ncontrollable manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which\nattempt to elicit harmful responses from LLMs. The evolving nature and\ndiversity of these attacks pose many challenges for defense systems, including\n(1) adaptation to counter emerging attack strategies without costly retraining,\nand (2) control of the trade-off between safety and utility. To address these\nchallenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for\njailbreak detection that incorporates a database of known attack examples into\nRetrieval-Augmented Generation, which is used to infer the underlying,\nmalicious user query and jailbreak strategy used to attack the system. RAD\nenables training-free updates for newly discovered jailbreak strategies and\nprovides a mechanism to balance safety and utility. Experiments on StrongREJECT\nshow that RAD substantially reduces the effectiveness of strong jailbreak\nattacks such as PAP and PAIR while maintaining low rejection rates for benign\nqueries. We propose a novel evaluation scheme and show that RAD achieves a\nrobust safety-utility trade-off across a range of operating points in a\ncontrollable manner."
                },
                "authors": [
                    {
                        "name": "Guangyu Yang"
                    },
                    {
                        "name": "Jinghong Chen"
                    },
                    {
                        "name": "Jingbiao Mei"
                    },
                    {
                        "name": "Weizhe Lin"
                    },
                    {
                        "name": "Bill Byrne"
                    }
                ],
                "author_detail": {
                    "name": "Bill Byrne"
                },
                "author": "Bill Byrne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02768v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02768v3",
                "updated": "2025-08-22T14:08:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    8,
                    58,
                    4,
                    234,
                    0
                ],
                "published": "2025-04-03T17:05:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    5,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal\n  Pairs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal\n  Pairs"
                },
                "summary": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages and 2 types of subject-verb agreement,\ncontaining more than 128,000 minimal pairs. Our minimal pairs are created using\na fully automated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages and 2 types of subject-verb agreement,\ncontaining more than 128,000 minimal pairs. Our minimal pairs are created using\na fully automated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages."
                },
                "authors": [
                    {
                        "name": "Jaap Jumelet"
                    },
                    {
                        "name": "Leonie Weissweiler"
                    },
                    {
                        "name": "Joakim Nivre"
                    },
                    {
                        "name": "Arianna Bisazza"
                    }
                ],
                "author_detail": {
                    "name": "Arianna Bisazza"
                },
                "author": "Arianna Bisazza",
                "arxiv_comment": "Published in TACL, MIT Press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02768v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02768v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16402v1",
                "updated": "2025-08-22T14:04:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    4,
                    55,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T14:04:55Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    4,
                    55,
                    4,
                    234,
                    0
                ],
                "title": "AetherCode: Evaluating LLMs' Ability to Win In Premier Programming\n  Competitions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AetherCode: Evaluating LLMs' Ability to Win In Premier Programming\n  Competitions"
                },
                "summary": "Competitive programming has emerged as a critical benchmark for evaluating\nthe reasoning and coding capabilities of Large Language Models (LLMs). Despite\nimpressive progress on existing benchmarks, we argue that current evaluations\noverstate model proficiency, masking a substantial gap between LLMs and elite\nhuman programmers. This gap arises from two key limitations: insufficient\ndifficulty and scope of benchmark problems, and evaluation bias from\nlow-quality test cases. To address these shortcomings, we present AetherCode, a\nnew benchmark that draws problems from premier programming competitions such as\nIOI and ICPC, offering broader coverage and higher difficulty. AetherCode\nfurther incorporates comprehensive, expert-validated test suites built through\na hybrid of automated generation and human curation, ensuring rigorous and\nreliable assessment. By combining challenging problem design with robust\nevaluation, AetherCode provides a more faithful measure of LLM capabilities and\nsets a new standard for future research in code reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive programming has emerged as a critical benchmark for evaluating\nthe reasoning and coding capabilities of Large Language Models (LLMs). Despite\nimpressive progress on existing benchmarks, we argue that current evaluations\noverstate model proficiency, masking a substantial gap between LLMs and elite\nhuman programmers. This gap arises from two key limitations: insufficient\ndifficulty and scope of benchmark problems, and evaluation bias from\nlow-quality test cases. To address these shortcomings, we present AetherCode, a\nnew benchmark that draws problems from premier programming competitions such as\nIOI and ICPC, offering broader coverage and higher difficulty. AetherCode\nfurther incorporates comprehensive, expert-validated test suites built through\na hybrid of automated generation and human curation, ensuring rigorous and\nreliable assessment. By combining challenging problem design with robust\nevaluation, AetherCode provides a more faithful measure of LLM capabilities and\nsets a new standard for future research in code reasoning."
                },
                "authors": [
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Jiaze Chen"
                    },
                    {
                        "name": "Zhicheng Liu"
                    },
                    {
                        "name": "Markus Mak"
                    },
                    {
                        "name": "Yidi Du"
                    },
                    {
                        "name": "Geonsik Moon"
                    },
                    {
                        "name": "Luoqi Xu"
                    },
                    {
                        "name": "Aaron Tua"
                    },
                    {
                        "name": "Kunshuo Peng"
                    },
                    {
                        "name": "Jiayi Lu"
                    },
                    {
                        "name": "Mingfei Xia"
                    },
                    {
                        "name": "Boqian Zou"
                    },
                    {
                        "name": "Chenyang Ran"
                    },
                    {
                        "name": "Guang Tian"
                    },
                    {
                        "name": "Shoutai Zhu"
                    },
                    {
                        "name": "Yeheng Duan"
                    },
                    {
                        "name": "Zhenghui Kang"
                    },
                    {
                        "name": "Zhenxing Lin"
                    },
                    {
                        "name": "Shangshu Li"
                    },
                    {
                        "name": "Qiang Luo"
                    },
                    {
                        "name": "Qingshen Long"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Yihan Xiao"
                    },
                    {
                        "name": "Yurong Wu"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Yuyi Fu"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Ming Ding"
                    }
                ],
                "author_detail": {
                    "name": "Ming Ding"
                },
                "author": "Ming Ding",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16397v1",
                "updated": "2025-08-22T13:58:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    58,
                    35,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T13:58:35Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    58,
                    35,
                    4,
                    234,
                    0
                ],
                "title": "A Lightweight Group Multiscale Bidirectional Interactive Network for\n  Real-Time Steel Surface Defect Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Lightweight Group Multiscale Bidirectional Interactive Network for\n  Real-Time Steel Surface Defect Detection"
                },
                "summary": "Real-time surface defect detection is critical for maintaining product\nquality and production efficiency in the steel manufacturing industry. Despite\npromising accuracy, existing deep learning methods often suffer from high\ncomputational complexity and slow inference speeds, which limit their\ndeployment in resource-constrained industrial environments. Recent lightweight\napproaches adopt multibranch architectures based on depthwise separable\nconvolution (DSConv) to capture multiscale contextual information. However,\nthese methods often suffer from increased computational overhead and lack\neffective cross-scale feature interaction, limiting their ability to fully\nleverage multiscale representations. To address these challenges, we propose\nGMBINet, a lightweight framework that enhances multiscale feature extraction\nand interaction through novel Group Multiscale Bidirectional Interactive (GMBI)\nmodules. The GMBI adopts a group-wise strategy for multiscale feature\nextraction, ensuring scale-agnostic computational complexity. It further\nintegrates a Bidirectional Progressive Feature Interactor (BPFI) and a\nparameter-free Element-Wise Multiplication-Summation (EWMS) operation to\nenhance cross-scale interaction without introducing additional computational\noverhead. Experiments on SD-Saliency-900 and NRSD-MN datasets demonstrate that\nGMBINet delivers competitive accuracy with real-time speeds of 1048 FPS on GPU\nand 16.53 FPS on CPU at 512 resolution, using only 0.19 M parameters.\nAdditional evaluations on the NEU-CLS defect classification dataset further\nconfirm the strong generalization ability of our method, demonstrating its\npotential for broader industrial vision applications beyond surface defect\ndetection. The dataset and code are publicly available at:\nhttps://github.com/zhangyongcode/GMBINet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time surface defect detection is critical for maintaining product\nquality and production efficiency in the steel manufacturing industry. Despite\npromising accuracy, existing deep learning methods often suffer from high\ncomputational complexity and slow inference speeds, which limit their\ndeployment in resource-constrained industrial environments. Recent lightweight\napproaches adopt multibranch architectures based on depthwise separable\nconvolution (DSConv) to capture multiscale contextual information. However,\nthese methods often suffer from increased computational overhead and lack\neffective cross-scale feature interaction, limiting their ability to fully\nleverage multiscale representations. To address these challenges, we propose\nGMBINet, a lightweight framework that enhances multiscale feature extraction\nand interaction through novel Group Multiscale Bidirectional Interactive (GMBI)\nmodules. The GMBI adopts a group-wise strategy for multiscale feature\nextraction, ensuring scale-agnostic computational complexity. It further\nintegrates a Bidirectional Progressive Feature Interactor (BPFI) and a\nparameter-free Element-Wise Multiplication-Summation (EWMS) operation to\nenhance cross-scale interaction without introducing additional computational\noverhead. Experiments on SD-Saliency-900 and NRSD-MN datasets demonstrate that\nGMBINet delivers competitive accuracy with real-time speeds of 1048 FPS on GPU\nand 16.53 FPS on CPU at 512 resolution, using only 0.19 M parameters.\nAdditional evaluations on the NEU-CLS defect classification dataset further\nconfirm the strong generalization ability of our method, demonstrating its\npotential for broader industrial vision applications beyond surface defect\ndetection. The dataset and code are publicly available at:\nhttps://github.com/zhangyongcode/GMBINet."
                },
                "authors": [
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Cunjian Chen"
                    },
                    {
                        "name": "Qiang Gao"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Bin Fang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Fang"
                },
                "author": "Bin Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19954v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19954v2",
                "updated": "2025-08-22T13:55:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    55,
                    7,
                    4,
                    234,
                    0
                ],
                "published": "2025-02-27T10:30:50Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    10,
                    30,
                    50,
                    3,
                    58,
                    0
                ],
                "title": "Collaborative Stance Detection via Small-Large Language Model\n  Consistency Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Stance Detection via Small-Large Language Model\n  Consistency Verification"
                },
                "summary": "Stance detection on social media aims to identify attitudes expressed in\ntweets towards specific targets. Current studies prioritize Large Language\nModels (LLMs) over Small Language Models (SLMs) due to the overwhelming\nperformance improving provided by LLMs. However, heavily relying on LLMs for\nstance detection, regardless of the cost, is impractical for real-world social\nmedia monitoring systems that require vast data analysis. To this end, we\npropose \\textbf{\\underline{Co}}llaborative Stance Detection via Small-Large\nLanguage Model Consistency \\textbf{\\underline{Ver}}ification (\\textbf{CoVer})\nframework, which enhances LLM utilization via context-shared batch reasoning\nand logical verification between LLM and SLM. Specifically, instead of\nprocessing each text individually, CoVer processes texts batch-by-batch,\nobtaining stance predictions and corresponding explanations via LLM reasoning\nin a shared context. Then, to exclude the bias caused by context noises, CoVer\nintroduces the SLM for logical consistency verification. Finally, texts that\nrepeatedly exhibit low logical consistency are classified using\nconsistency-weighted aggregation of prior LLM stance predictions. Our\nexperiments show that CoVer outperforms state-of-the-art methods across\nmultiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per\ntweet while significantly enhancing performance. Our CoVer offers a more\npractical solution for LLM deploying for social media stance detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stance detection on social media aims to identify attitudes expressed in\ntweets towards specific targets. Current studies prioritize Large Language\nModels (LLMs) over Small Language Models (SLMs) due to the overwhelming\nperformance improving provided by LLMs. However, heavily relying on LLMs for\nstance detection, regardless of the cost, is impractical for real-world social\nmedia monitoring systems that require vast data analysis. To this end, we\npropose \\textbf{\\underline{Co}}llaborative Stance Detection via Small-Large\nLanguage Model Consistency \\textbf{\\underline{Ver}}ification (\\textbf{CoVer})\nframework, which enhances LLM utilization via context-shared batch reasoning\nand logical verification between LLM and SLM. Specifically, instead of\nprocessing each text individually, CoVer processes texts batch-by-batch,\nobtaining stance predictions and corresponding explanations via LLM reasoning\nin a shared context. Then, to exclude the bias caused by context noises, CoVer\nintroduces the SLM for logical consistency verification. Finally, texts that\nrepeatedly exhibit low logical consistency are classified using\nconsistency-weighted aggregation of prior LLM stance predictions. Our\nexperiments show that CoVer outperforms state-of-the-art methods across\nmultiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per\ntweet while significantly enhancing performance. Our CoVer offers a more\npractical solution for LLM deploying for social media stance detection."
                },
                "authors": [
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zixiang Tang"
                    },
                    {
                        "name": "Teli Liu"
                    },
                    {
                        "name": "Min Liu"
                    }
                ],
                "author_detail": {
                    "name": "Min Liu"
                },
                "author": "Min Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19954v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19954v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16390v1",
                "updated": "2025-08-22T13:48:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    48,
                    37,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T13:48:37Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    48,
                    37,
                    4,
                    234,
                    0
                ],
                "title": "RoMedQA: The First Benchmark for Romanian Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoMedQA: The First Benchmark for Romanian Medical Question Answering"
                },
                "summary": "Question answering (QA) is an actively studied topic, being a core natural\nlanguage processing (NLP) task that needs to be addressed before achieving\nArtificial General Intelligence (AGI). However, the lack of QA datasets in\nspecific domains and languages hinders the development of robust AI models able\nto generalize across various domains and languages. To this end, we introduce\nRoMedQA, the first Romanian QA benchmark for the medical domain, alongside a\ncomprehensive evaluation of state-of-the-art large language models (LLMs). We\nconstruct a high-quality and large-scale dataset comprising 102,646 QA pairs\nrelated to cancer patients. The questions regard medical case summaries of\n1,011 patients, requiring either keyword extraction or reasoning to be answered\ncorrectly. RoMedQA is the result of a time-consuming manual annotation process\ncarried out by seven physicians specialized in oncology or radiotherapy, who\nspent a total of about 2,100 work hours to generate the QA pairs. We experiment\nwith four LLMs from distinct families of models on RoMedQA. Each model is\nemployed in two scenarios, namely one based on zero-shot prompting and one\nbased on supervised fine-tuning. Our results show that fine-tuned models\nsignificantly outperform their zero-shot counterparts, clearly indicating that\npretrained models fail to generalize on RoMedQA. Our findings demonstrate the\nimportance of both domain-specific and language-specific fine-tuning for\nreliable clinical QA in Romanian. We publicly release our dataset and code at\nhttps://github.com/ana-rogoz/RoMedQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering (QA) is an actively studied topic, being a core natural\nlanguage processing (NLP) task that needs to be addressed before achieving\nArtificial General Intelligence (AGI). However, the lack of QA datasets in\nspecific domains and languages hinders the development of robust AI models able\nto generalize across various domains and languages. To this end, we introduce\nRoMedQA, the first Romanian QA benchmark for the medical domain, alongside a\ncomprehensive evaluation of state-of-the-art large language models (LLMs). We\nconstruct a high-quality and large-scale dataset comprising 102,646 QA pairs\nrelated to cancer patients. The questions regard medical case summaries of\n1,011 patients, requiring either keyword extraction or reasoning to be answered\ncorrectly. RoMedQA is the result of a time-consuming manual annotation process\ncarried out by seven physicians specialized in oncology or radiotherapy, who\nspent a total of about 2,100 work hours to generate the QA pairs. We experiment\nwith four LLMs from distinct families of models on RoMedQA. Each model is\nemployed in two scenarios, namely one based on zero-shot prompting and one\nbased on supervised fine-tuning. Our results show that fine-tuned models\nsignificantly outperform their zero-shot counterparts, clearly indicating that\npretrained models fail to generalize on RoMedQA. Our findings demonstrate the\nimportance of both domain-specific and language-specific fine-tuning for\nreliable clinical QA in Romanian. We publicly release our dataset and code at\nhttps://github.com/ana-rogoz/RoMedQA."
                },
                "authors": [
                    {
                        "name": "Ana-Cristina Rogoz"
                    },
                    {
                        "name": "Radu Tudor Ionescu"
                    },
                    {
                        "name": "Alexandra-Valentina Anghel"
                    },
                    {
                        "name": "Ionut-Lucian Antone-Iordache"
                    },
                    {
                        "name": "Simona Coniac"
                    },
                    {
                        "name": "Andreea Iuliana Ionescu"
                    }
                ],
                "author_detail": {
                    "name": "Andreea Iuliana Ionescu"
                },
                "author": "Andreea Iuliana Ionescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01832v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01832v2",
                "updated": "2025-08-22T13:41:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    41,
                    57,
                    4,
                    234,
                    0
                ],
                "published": "2025-03-03T18:55:09Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    55,
                    9,
                    0,
                    62,
                    0
                ],
                "title": "Rotary Offset Features in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotary Offset Features in Large Language Models"
                },
                "summary": "Transformer-based Large Language Models (LLMs) rely on positional encodings\nto provide sequence position information to their attention mechanism. Rotary\nPositional Encodings (RoPE), which encode relative position by rotating queries\nand keys, have become widely used in modern LLMs. We study the features and\npatterns that emerge in queries and keys when using rotary embeddings and\nintroduce the concept of rotary offset features. Our analysis reveals that\nthese features, which frequently exhibit large activations and are often\ninterpreted as outliers, arise consistently across layers, attention heads, and\nmodel architectures. We derive bounds predicting which rotary frequencies give\nrise to rotary offset features and the minimum angle between the query-key\npairs for these features. We verify our predictions empirically across models\nof different sizes and architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) rely on positional encodings\nto provide sequence position information to their attention mechanism. Rotary\nPositional Encodings (RoPE), which encode relative position by rotating queries\nand keys, have become widely used in modern LLMs. We study the features and\npatterns that emerge in queries and keys when using rotary embeddings and\nintroduce the concept of rotary offset features. Our analysis reveals that\nthese features, which frequently exhibit large activations and are often\ninterpreted as outliers, arise consistently across layers, attention heads, and\nmodel architectures. We derive bounds predicting which rotary frequencies give\nrise to rotary offset features and the minimum angle between the query-key\npairs for these features. We verify our predictions empirically across models\nof different sizes and architectures."
                },
                "authors": [
                    {
                        "name": "Andr Jonasson"
                    }
                ],
                "author_detail": {
                    "name": "Andr Jonasson"
                },
                "author": "Andr Jonasson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01832v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01832v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16383v1",
                "updated": "2025-08-22T13:38:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    38,
                    12,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T13:38:12Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    38,
                    12,
                    4,
                    234,
                    0
                ],
                "title": "GLARE: Agentic Reasoning for Legal Judgment Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLARE: Agentic Reasoning for Legal Judgment Prediction"
                },
                "summary": "Legal judgment prediction (LJP) has become increasingly important in the\nlegal field. In this paper, we identify that existing large language models\n(LLMs) have significant problems of insufficient reasoning due to a lack of\nlegal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning\nframework that dynamically acquires key legal knowledge by invoking different\nmodules, thereby improving the breadth and depth of reasoning. Experiments\nconducted on the real-world dataset verify the effectiveness of our method.\nFurthermore, the reasoning chain generated during the analysis process can\nincrease interpretability and provide the possibility for practical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal judgment prediction (LJP) has become increasingly important in the\nlegal field. In this paper, we identify that existing large language models\n(LLMs) have significant problems of insufficient reasoning due to a lack of\nlegal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning\nframework that dynamically acquires key legal knowledge by invoking different\nmodules, thereby improving the breadth and depth of reasoning. Experiments\nconducted on the real-world dataset verify the effectiveness of our method.\nFurthermore, the reasoning chain generated during the analysis process can\nincrease interpretability and provide the possibility for practical\napplications."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16379v1",
                "updated": "2025-08-22T13:34:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    34,
                    49,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T13:34:49Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    34,
                    49,
                    4,
                    234,
                    0
                ],
                "title": "Agentic AI Empowered Multi-UAV Trajectory Optimization in Low-Altitude\n  Economy Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI Empowered Multi-UAV Trajectory Optimization in Low-Altitude\n  Economy Networks"
                },
                "summary": "This paper proposes a novel Agentic Retrieval-augmented generation with\nMamba-Attention Integrated Transformer (ARMAIT) framework for multi-Unmanned\nAerial Vehicle (UAV) trajectory optimization. The framework is built upon Large\nLanguage Models (LLMs), incorporating Retrieval-Augmented Generation (RAG)\nempowered by Agentic AI and integrated with a UAV-specific knowledge base.\nThrough the Agentic RAG, the LLM autonomously interprets high-level task\nrequirements and identifies the key components necessary for trajectory\noptimization, including model inputs and outputs, network architecture, reward\nfunctions, and task constraints. To support efficient modeling across different\nsystem scales, we introduce the Mamba-Attention Integrated Transformer (MAIT),\na hybrid neural architecture that combines the long-range dependency modeling\ncapability of attention mechanisms with the efficient temporal dynamic\nrepresentation of Mamba. Furthermore, a Trajectory-Group Relative Policy\nOptimization (T-GRPO) method is proposed to achieve unified policy gradient\noptimization in both discrete and continuous trajectory spaces for MAIT\ntraining. Extensive experimental results validate the feasibility and\neffectiveness of the proposed ARMAIT framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel Agentic Retrieval-augmented generation with\nMamba-Attention Integrated Transformer (ARMAIT) framework for multi-Unmanned\nAerial Vehicle (UAV) trajectory optimization. The framework is built upon Large\nLanguage Models (LLMs), incorporating Retrieval-Augmented Generation (RAG)\nempowered by Agentic AI and integrated with a UAV-specific knowledge base.\nThrough the Agentic RAG, the LLM autonomously interprets high-level task\nrequirements and identifies the key components necessary for trajectory\noptimization, including model inputs and outputs, network architecture, reward\nfunctions, and task constraints. To support efficient modeling across different\nsystem scales, we introduce the Mamba-Attention Integrated Transformer (MAIT),\na hybrid neural architecture that combines the long-range dependency modeling\ncapability of attention mechanisms with the efficient temporal dynamic\nrepresentation of Mamba. Furthermore, a Trajectory-Group Relative Policy\nOptimization (T-GRPO) method is proposed to achieve unified policy gradient\noptimization in both discrete and continuous trajectory spaces for MAIT\ntraining. Extensive experimental results validate the feasibility and\neffectiveness of the proposed ARMAIT framework."
                },
                "authors": [
                    {
                        "name": "Feibo Jiang"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Xitao Pan"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Cunhua Pan"
                    }
                ],
                "author_detail": {
                    "name": "Cunhua Pan"
                },
                "author": "Cunhua Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08306v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08306v3",
                "updated": "2025-08-22T13:29:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    29,
                    52,
                    4,
                    234,
                    0
                ],
                "published": "2025-01-14T18:44:35Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    44,
                    35,
                    1,
                    14,
                    0
                ],
                "title": "Environmental Feature Engineering and Statistical Validation for\n  ML-Based Path Loss Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Environmental Feature Engineering and Statistical Validation for\n  ML-Based Path Loss Prediction"
                },
                "summary": "Wireless communications rely on path loss modeling, which is most effective\nwhen it includes the physical details of the propagation environment. Acquiring\nthis data has historically been challenging, but geographic information systems\ndata is becoming increasingly available with higher resolution and accuracy.\nAccess to such details enables propagation models to more accurately predict\ncoverage and account for interference in wireless deployments. Machine\nlearning-based modeling can significantly support this effort, with feature\nbased approaches allowing for accurate, efficient, and scalable propagation\nmodeling. Building on previous work, we introduce an extended set of features\nthat improves prediction accuracy while, most importantly, proving model\ngeneralization through rigorous statistical assessment and the use of test set\nholdouts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communications rely on path loss modeling, which is most effective\nwhen it includes the physical details of the propagation environment. Acquiring\nthis data has historically been challenging, but geographic information systems\ndata is becoming increasingly available with higher resolution and accuracy.\nAccess to such details enables propagation models to more accurately predict\ncoverage and account for interference in wireless deployments. Machine\nlearning-based modeling can significantly support this effort, with feature\nbased approaches allowing for accurate, efficient, and scalable propagation\nmodeling. Building on previous work, we introduce an extended set of features\nthat improves prediction accuracy while, most importantly, proving model\ngeneralization through rigorous statistical assessment and the use of test set\nholdouts."
                },
                "authors": [
                    {
                        "name": "Jonathan Ethier"
                    },
                    {
                        "name": "Mathieu Chateauvert"
                    },
                    {
                        "name": "Ryan G. Dempsey"
                    },
                    {
                        "name": "Alexis Bose"
                    }
                ],
                "author_detail": {
                    "name": "Alexis Bose"
                },
                "author": "Alexis Bose",
                "arxiv_comment": "4 pages, 4 figures, Submitted to IEEE AWPL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08306v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08306v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16371v1",
                "updated": "2025-08-22T13:25:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    25,
                    0,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T13:25:00Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    25,
                    0,
                    4,
                    234,
                    0
                ],
                "title": "The Mediomatix Corpus: Parallel Data for Romansh Idioms via Comparable\n  Schoolbooks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mediomatix Corpus: Parallel Data for Romansh Idioms via Comparable\n  Schoolbooks"
                },
                "summary": "The five idioms (i.e., varieties) of the Romansh language are largely\nstandardized and are taught in the schools of the respective communities in\nSwitzerland. In this paper, we present the first parallel corpus of Romansh\nidioms. The corpus is based on 291 schoolbook volumes, which are comparable in\ncontent for the five idioms. We use automatic alignment methods to extract 207k\nmulti-parallel segments from the books, with more than 2M tokens in total. A\nsmall-scale human evaluation confirms that the segments are highly parallel,\nmaking the dataset suitable for NLP applications such as machine translation\nbetween Romansh idioms. We release the parallel and unaligned versions of the\ndataset under a CC-BY-NC-SA license and demonstrate its utility for machine\ntranslation by training and evaluating an LLM on a sample of the dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The five idioms (i.e., varieties) of the Romansh language are largely\nstandardized and are taught in the schools of the respective communities in\nSwitzerland. In this paper, we present the first parallel corpus of Romansh\nidioms. The corpus is based on 291 schoolbook volumes, which are comparable in\ncontent for the five idioms. We use automatic alignment methods to extract 207k\nmulti-parallel segments from the books, with more than 2M tokens in total. A\nsmall-scale human evaluation confirms that the segments are highly parallel,\nmaking the dataset suitable for NLP applications such as machine translation\nbetween Romansh idioms. We release the parallel and unaligned versions of the\ndataset under a CC-BY-NC-SA license and demonstrate its utility for machine\ntranslation by training and evaluating an LLM on a sample of the dataset."
                },
                "authors": [
                    {
                        "name": "Zachary Hopton"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Andrin Bchler"
                    },
                    {
                        "name": "Anna Rutkiewicz"
                    },
                    {
                        "name": "Rico Cathomas"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05220v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05220v3",
                "updated": "2025-08-22T13:24:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    24,
                    48,
                    4,
                    234,
                    0
                ],
                "published": "2025-04-07T16:05:52Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    5,
                    52,
                    0,
                    97,
                    0
                ],
                "title": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort\n  for Retrieval and RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort\n  for Retrieval and RAG"
                },
                "summary": "Retrieval models typically rely on costly human-labeled query-document\nrelevance annotations for training and evaluation. To reduce this cost and\nleverage the potential of Large Language Models (LLMs) in relevance judgments,\nwe aim to explore whether LLM-generated annotations can effectively replace\nhuman annotations in training retrieval models. Retrieval usually emphasizes\nrelevance, which indicates \"topic-relatedness\" of a document to a query, while\nin RAG, the value of a document (or utility) depends on how it contributes to\nanswer generation. Recognizing this mismatch, some researchers use LLM\nperformance on downstream tasks with documents as labels, but this approach\nrequires manual answers for specific tasks, leading to high costs and limited\ngeneralization. In another line of work, prompting LLMs to select useful\ndocuments as RAG references eliminates the need for human annotation and is not\ntask-specific. If we leverage LLMs' utility judgments to annotate retrieval\ndata, we may retain cross-task generalization without human annotation in\nlarge-scale corpora. Therefore, we investigate utility-focused annotation via\nLLMs for large-scale retriever training data across both in-domain and\nout-of-domain settings on the retrieval and RAG tasks. To reduce the impact of\nlow-quality positives labeled by LLMs, we design a novel loss function, i.e.,\nDisj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on\nutility-focused annotations significantly outperform those trained on human\nannotations in the out-of-domain setting on both tasks, demonstrating superior\ngeneralization capabilities. (2) LLM annotation does not replace human\nannotation in the in-domain setting. However, incorporating just 20%\nhuman-annotated data enables retrievers trained with utility-focused\nannotations to match the performance of models trained entirely with human\nannotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval models typically rely on costly human-labeled query-document\nrelevance annotations for training and evaluation. To reduce this cost and\nleverage the potential of Large Language Models (LLMs) in relevance judgments,\nwe aim to explore whether LLM-generated annotations can effectively replace\nhuman annotations in training retrieval models. Retrieval usually emphasizes\nrelevance, which indicates \"topic-relatedness\" of a document to a query, while\nin RAG, the value of a document (or utility) depends on how it contributes to\nanswer generation. Recognizing this mismatch, some researchers use LLM\nperformance on downstream tasks with documents as labels, but this approach\nrequires manual answers for specific tasks, leading to high costs and limited\ngeneralization. In another line of work, prompting LLMs to select useful\ndocuments as RAG references eliminates the need for human annotation and is not\ntask-specific. If we leverage LLMs' utility judgments to annotate retrieval\ndata, we may retain cross-task generalization without human annotation in\nlarge-scale corpora. Therefore, we investigate utility-focused annotation via\nLLMs for large-scale retriever training data across both in-domain and\nout-of-domain settings on the retrieval and RAG tasks. To reduce the impact of\nlow-quality positives labeled by LLMs, we design a novel loss function, i.e.,\nDisj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on\nutility-focused annotations significantly outperform those trained on human\nannotations in the out-of-domain setting on both tasks, demonstrating superior\ngeneralization capabilities. (2) LLM annotation does not replace human\nannotation in the in-domain setting. However, incorporating just 20%\nhuman-annotated data enables retrievers trained with utility-focused\nannotations to match the performance of models trained entirely with human\nannotations."
                },
                "authors": [
                    {
                        "name": "Hengran Zhang"
                    },
                    {
                        "name": "Minghao Tang"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Shihao Liu"
                    },
                    {
                        "name": "Daiting Shi"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Accepted by the EMNLP25 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05220v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05220v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21054v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21054v5",
                "updated": "2025-08-22T13:16:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    16,
                    29,
                    4,
                    234,
                    0
                ],
                "published": "2024-07-24T12:07:54Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    7,
                    54,
                    2,
                    206,
                    0
                ],
                "title": "Sentiment Reasoning for Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment Reasoning for Healthcare"
                },
                "summary": "Transparency in AI healthcare decision-making is crucial. By incorporating\nrationales to explain reason for each predicted label, users could understand\nLarge Language Models (LLMs)'s reasoning to make better decision. In this work,\nwe introduce a new task - Sentiment Reasoning - for both speech and text\nmodalities, and our proposed multimodal multitask framework and the world's\nlargest multimodal sentiment analysis dataset. Sentiment Reasoning is an\nauxiliary task in sentiment analysis where the model predicts both the\nsentiment label and generates the rationale behind it based on the input\ntranscript. Our study conducted on both human transcripts and Automatic Speech\nRecognition (ASR) transcripts shows that Sentiment Reasoning helps improve\nmodel transparency by providing rationale for model prediction with quality\nsemantically comparable to humans while also improving model's classification\nperformance (+2% increase in both accuracy and macro-F1) via\nrationale-augmented fine-tuning. Also, no significant difference in the\nsemantic quality of generated rationales between human and ASR transcripts. All\ncode, data (five languages - Vietnamese, English, Chinese, German, and French)\nand models are published online:\nhttps://github.com/leduckhai/Sentiment-Reasoning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transparency in AI healthcare decision-making is crucial. By incorporating\nrationales to explain reason for each predicted label, users could understand\nLarge Language Models (LLMs)'s reasoning to make better decision. In this work,\nwe introduce a new task - Sentiment Reasoning - for both speech and text\nmodalities, and our proposed multimodal multitask framework and the world's\nlargest multimodal sentiment analysis dataset. Sentiment Reasoning is an\nauxiliary task in sentiment analysis where the model predicts both the\nsentiment label and generates the rationale behind it based on the input\ntranscript. Our study conducted on both human transcripts and Automatic Speech\nRecognition (ASR) transcripts shows that Sentiment Reasoning helps improve\nmodel transparency by providing rationale for model prediction with quality\nsemantically comparable to humans while also improving model's classification\nperformance (+2% increase in both accuracy and macro-F1) via\nrationale-augmented fine-tuning. Also, no significant difference in the\nsemantic quality of generated rationales between human and ASR transcripts. All\ncode, data (five languages - Vietnamese, English, Chinese, German, and French)\nand models are published online:\nhttps://github.com/leduckhai/Sentiment-Reasoning"
                },
                "authors": [
                    {
                        "name": "Khai-Nguyen Nguyen"
                    },
                    {
                        "name": "Khai Le-Duc"
                    },
                    {
                        "name": "Bach Phan Tat"
                    },
                    {
                        "name": "Duy Le"
                    },
                    {
                        "name": "Long Vo-Dang"
                    },
                    {
                        "name": "Truong-Son Hy"
                    }
                ],
                "author_detail": {
                    "name": "Truong-Son Hy"
                },
                "author": "Truong-Son Hy",
                "arxiv_comment": "ACL 2025 Industry Track (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21054v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21054v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16357v1",
                "updated": "2025-08-22T13:04:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    4,
                    43,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T13:04:43Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    4,
                    43,
                    4,
                    234,
                    0
                ],
                "title": "MizanQA: Benchmarking Large Language Models on Moroccan Legal Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MizanQA: Benchmarking Large Language Models on Moroccan Legal Question\n  Answering"
                },
                "summary": "The rapid advancement of large language models (LLMs) has significantly\npropelled progress in natural language processing (NLP). However, their\neffectiveness in specialized, low-resource domains-such as Arabic legal\ncontexts-remains limited. This paper introduces MizanQA (pronounced Mizan,\nmeaning \"scale\" in Arabic, a universal symbol of justice), a benchmark designed\nto evaluate LLMs on Moroccan legal question answering (QA) tasks, characterised\nby rich linguistic and legal complexity. The dataset draws on Modern Standard\nArabic, Islamic Maliki jurisprudence, Moroccan customary law, and French legal\ninfluences. Comprising over 1,700 multiple-choice questions, including\nmulti-answer formats, MizanQA captures the nuances of authentic legal\nreasoning. Benchmarking experiments with multilingual and Arabic-focused LLMs\nreveal substantial performance gaps, highlighting the need for tailored\nevaluation metrics and culturally grounded, domain-specific LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has significantly\npropelled progress in natural language processing (NLP). However, their\neffectiveness in specialized, low-resource domains-such as Arabic legal\ncontexts-remains limited. This paper introduces MizanQA (pronounced Mizan,\nmeaning \"scale\" in Arabic, a universal symbol of justice), a benchmark designed\nto evaluate LLMs on Moroccan legal question answering (QA) tasks, characterised\nby rich linguistic and legal complexity. The dataset draws on Modern Standard\nArabic, Islamic Maliki jurisprudence, Moroccan customary law, and French legal\ninfluences. Comprising over 1,700 multiple-choice questions, including\nmulti-answer formats, MizanQA captures the nuances of authentic legal\nreasoning. Benchmarking experiments with multilingual and Arabic-focused LLMs\nreveal substantial performance gaps, highlighting the need for tailored\nevaluation metrics and culturally grounded, domain-specific LLM development."
                },
                "authors": [
                    {
                        "name": "Adil Bahaj"
                    },
                    {
                        "name": "Mounir Ghogho"
                    }
                ],
                "author_detail": {
                    "name": "Mounir Ghogho"
                },
                "author": "Mounir Ghogho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13408v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13408v2",
                "updated": "2025-08-22T13:02:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    13,
                    2,
                    19,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-19T00:04:48Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    0,
                    4,
                    48,
                    1,
                    231,
                    0
                ],
                "title": "NovoMolGen: Rethinking Molecular Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NovoMolGen: Rethinking Molecular Language Model Pretraining"
                },
                "summary": "Designing de-novo molecules with desired property profiles requires efficient\nexploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$\npossible synthesizable candidates. While various deep generative models have\nbeen developed to design small molecules using diverse input representations,\nMolecular Large Language Models (Mol-LLMs) based on string representations have\nemerged as a scalable approach capable of exploring billions of molecules.\nHowever, there remains limited understanding regarding how standard language\nmodeling practices such as textual representations, tokenization strategies,\nmodel size, and dataset scale impact molecular generation performance. In this\nwork, we systematically investigate these critical aspects by introducing\nNovoMolGen, a family of transformer-based foundation models pretrained on 1.5\nbillion molecules for de-novo molecule generation. Through extensive empirical\nanalyses, we identify a weak correlation between performance metrics measured\nduring pretraining and actual downstream performance, revealing important\ndistinctions between molecular and general NLP training dynamics. NovoMolGen\nestablishes new state-of-the-art results, substantially outperforming prior\nMol-LLMs and specialized generative models in both unconstrained and\ngoal-directed molecular generation tasks, thus providing a robust foundation\nfor advancing efficient and effective molecular modeling strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing de-novo molecules with desired property profiles requires efficient\nexploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$\npossible synthesizable candidates. While various deep generative models have\nbeen developed to design small molecules using diverse input representations,\nMolecular Large Language Models (Mol-LLMs) based on string representations have\nemerged as a scalable approach capable of exploring billions of molecules.\nHowever, there remains limited understanding regarding how standard language\nmodeling practices such as textual representations, tokenization strategies,\nmodel size, and dataset scale impact molecular generation performance. In this\nwork, we systematically investigate these critical aspects by introducing\nNovoMolGen, a family of transformer-based foundation models pretrained on 1.5\nbillion molecules for de-novo molecule generation. Through extensive empirical\nanalyses, we identify a weak correlation between performance metrics measured\nduring pretraining and actual downstream performance, revealing important\ndistinctions between molecular and general NLP training dynamics. NovoMolGen\nestablishes new state-of-the-art results, substantially outperforming prior\nMol-LLMs and specialized generative models in both unconstrained and\ngoal-directed molecular generation tasks, thus providing a robust foundation\nfor advancing efficient and effective molecular modeling strategies."
                },
                "authors": [
                    {
                        "name": "Kamran Chitsaz"
                    },
                    {
                        "name": "Roshan Balaji"
                    },
                    {
                        "name": "Quentin Fournier"
                    },
                    {
                        "name": "Nirav Pravinbhai Bhatt"
                    },
                    {
                        "name": "Sarath Chandar"
                    }
                ],
                "author_detail": {
                    "name": "Sarath Chandar"
                },
                "author": "Sarath Chandar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13408v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21184v2",
                "updated": "2025-08-22T12:57:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    57,
                    8,
                    4,
                    234,
                    0
                ],
                "published": "2025-05-27T13:33:57Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    13,
                    33,
                    57,
                    1,
                    147,
                    0
                ],
                "title": "PoisonSwarm: Universal Harmful Information Synthesis via Model\n  Crowdsourcing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoisonSwarm: Universal Harmful Information Synthesis via Model\n  Crowdsourcing"
                },
                "summary": "To construct responsible and secure AI applications, harmful information data\nis widely utilized for adversarial testing and the development of safeguards.\nExisting studies mainly leverage Large Language Models (LLMs) to synthesize\ndata to obtain high-quality task datasets at scale, thereby avoiding costly\nhuman annotation. However, limited by the safety alignment mechanisms of LLMs,\nthe synthesis of harmful data still faces challenges in generation reliability\nand content diversity. In this study, we propose a novel harmful information\nsynthesis framework, PoisonSwarm, which applies the model crowdsourcing\nstrategy to generate diverse harmful data while maintaining a high success\nrate. Specifically, we generate abundant benign data as the based templates in\na counterfactual manner. Subsequently, we decompose each based template into\nmultiple semantic units and perform unit-by-unit toxification and final\nrefinement through dynamic model switching, thus ensuring the success of\nsynthesis. Experimental results demonstrate that PoisonSwarm achieves\nstate-of-the-art performance in synthesizing different categories of harmful\ndata with high scalability and diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To construct responsible and secure AI applications, harmful information data\nis widely utilized for adversarial testing and the development of safeguards.\nExisting studies mainly leverage Large Language Models (LLMs) to synthesize\ndata to obtain high-quality task datasets at scale, thereby avoiding costly\nhuman annotation. However, limited by the safety alignment mechanisms of LLMs,\nthe synthesis of harmful data still faces challenges in generation reliability\nand content diversity. In this study, we propose a novel harmful information\nsynthesis framework, PoisonSwarm, which applies the model crowdsourcing\nstrategy to generate diverse harmful data while maintaining a high success\nrate. Specifically, we generate abundant benign data as the based templates in\na counterfactual manner. Subsequently, we decompose each based template into\nmultiple semantic units and perform unit-by-unit toxification and final\nrefinement through dynamic model switching, thus ensuring the success of\nsynthesis. Experimental results demonstrate that PoisonSwarm achieves\nstate-of-the-art performance in synthesizing different categories of harmful\ndata with high scalability and diversity."
                },
                "authors": [
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zhifei Zheng"
                    },
                    {
                        "name": "Ziji Hao"
                    },
                    {
                        "name": "Teli Liu"
                    },
                    {
                        "name": "Min Liu"
                    }
                ],
                "author_detail": {
                    "name": "Min Liu"
                },
                "author": "Min Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00132v2",
                "updated": "2025-08-22T12:52:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    52,
                    38,
                    4,
                    234,
                    0
                ],
                "published": "2025-03-31T18:33:55Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    18,
                    33,
                    55,
                    0,
                    90,
                    0
                ],
                "title": "Contextualize-then-Aggregate: Circuits for In-Context Learning in\n  Gemma-2 2B",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualize-then-Aggregate: Circuits for In-Context Learning in\n  Gemma-2 2B"
                },
                "summary": "In-Context Learning (ICL) is an intriguing ability of large language models\n(LLMs). Despite a substantial amount of work on its behavioral aspects and how\nit emerges in miniature setups, it remains unclear which mechanism assembles\ntask information from the individual examples in a fewshot prompt. We use\ncausal interventions to identify information flow in Gemma-2 2B for five\nnaturalistic ICL tasks. We find that the model infers task information using a\ntwo-step strategy we call contextualize-then-aggregate: In the lower layers,\nthe model builds up representations of individual fewshot examples, which are\ncontextualized by preceding examples through connections between fewshot input\nand output tokens across the sequence. In the higher layers, these\nrepresentations are aggregated to identify the task and prepare prediction of\nthe next output. The importance of the contextualization step differs between\ntasks, and it may become more important in the presence of ambiguous examples.\nOverall, by providing rigorous causal analysis, our results shed light on the\nmechanisms through which ICL happens in language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) is an intriguing ability of large language models\n(LLMs). Despite a substantial amount of work on its behavioral aspects and how\nit emerges in miniature setups, it remains unclear which mechanism assembles\ntask information from the individual examples in a fewshot prompt. We use\ncausal interventions to identify information flow in Gemma-2 2B for five\nnaturalistic ICL tasks. We find that the model infers task information using a\ntwo-step strategy we call contextualize-then-aggregate: In the lower layers,\nthe model builds up representations of individual fewshot examples, which are\ncontextualized by preceding examples through connections between fewshot input\nand output tokens across the sequence. In the higher layers, these\nrepresentations are aggregated to identify the task and prepare prediction of\nthe next output. The importance of the contextualization step differs between\ntasks, and it may become more important in the presence of ambiguous examples.\nOverall, by providing rigorous causal analysis, our results shed light on the\nmechanisms through which ICL happens in language models."
                },
                "authors": [
                    {
                        "name": "Aleksandra Bakalova"
                    },
                    {
                        "name": "Yana Veitsman"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Michael Hahn"
                    }
                ],
                "author_detail": {
                    "name": "Michael Hahn"
                },
                "author": "Michael Hahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16347v1",
                "updated": "2025-08-22T12:41:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    41,
                    26,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T12:41:26Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    41,
                    26,
                    4,
                    234,
                    0
                ],
                "title": "Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and\n  Investigating the Real Misuse Threat of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and\n  Investigating the Real Misuse Threat of LLMs"
                },
                "summary": "With the development of Large Language Models (LLMs), numerous efforts have\nrevealed their vulnerabilities to jailbreak attacks. Although these studies\nhave driven the progress in LLMs' safety alignment, it remains unclear whether\nLLMs have internalized authentic knowledge to deal with real-world crimes, or\nare merely forced to simulate toxic language patterns. This ambiguity raises\nconcerns that jailbreak success is often attributable to a hallucination loop\nbetween jailbroken LLM and judger LLM. By decoupling the use of jailbreak\ntechniques, we construct knowledge-intensive Q\\&A to investigate the misuse\nthreats of LLMs in terms of dangerous knowledge possession, harmful task\nplanning utility, and harmfulness judgment robustness. Experiments reveal a\nmismatch between jailbreak success rates and harmful knowledge possession in\nLLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness\njudgments on toxic language patterns. Our study reveals a gap between existing\nLLM safety assessments and real-world threat potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of Large Language Models (LLMs), numerous efforts have\nrevealed their vulnerabilities to jailbreak attacks. Although these studies\nhave driven the progress in LLMs' safety alignment, it remains unclear whether\nLLMs have internalized authentic knowledge to deal with real-world crimes, or\nare merely forced to simulate toxic language patterns. This ambiguity raises\nconcerns that jailbreak success is often attributable to a hallucination loop\nbetween jailbroken LLM and judger LLM. By decoupling the use of jailbreak\ntechniques, we construct knowledge-intensive Q\\&A to investigate the misuse\nthreats of LLMs in terms of dangerous knowledge possession, harmful task\nplanning utility, and harmfulness judgment robustness. Experiments reveal a\nmismatch between jailbreak success rates and harmful knowledge possession in\nLLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness\njudgments on toxic language patterns. Our study reveals a gap between existing\nLLM safety assessments and real-world threat potential."
                },
                "authors": [
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Yijun Lin"
                    },
                    {
                        "name": "Zenghao Duan"
                    },
                    {
                        "name": "zhifei zheng"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Zhiyi yin"
                    },
                    {
                        "name": "Jianping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jianping Zhang"
                },
                "author": "Jianping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12719v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12719v2",
                "updated": "2025-08-22T12:38:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    38,
                    13,
                    4,
                    234,
                    0
                ],
                "published": "2025-04-17T07:48:50Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    7,
                    48,
                    50,
                    3,
                    107,
                    0
                ],
                "title": "B*: Efficient and Optimal Base Placement for Fixed-Base Manipulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B*: Efficient and Optimal Base Placement for Fixed-Base Manipulators"
                },
                "summary": "B* is a novel optimization framework that addresses a critical challenge in\nfixed-base manipulator robotics: optimal base placement. Current methods rely\non pre-computed kinematics databases generated through sampling to search for\nsolutions. However, they face an inherent trade-off between solution optimality\nand computational efficiency when determining sampling resolution. To address\nthese limitations, B* unifies multiple objectives without database dependence.\nThe framework employs a two-layer hierarchical approach. The outer layer\nsystematically manages terminal constraints through progressive tightening,\nparticularly for base mobility, enabling feasible initialization and broad\nsolution exploration. The inner layer addresses non-convexities in each\nouter-layer subproblem through sequential local linearization, converting the\noriginal problem into tractable sequential linear programming (SLP). Testing\nacross multiple robot platforms demonstrates B*'s effectiveness. The framework\nachieves solution optimality five orders of magnitude better than\nsampling-based approaches while maintaining perfect success rates and reduced\ncomputational overhead. Operating directly in configuration space, B* enables\nsimultaneous path planning with customizable optimization criteria. B* serves\nas a crucial initialization tool that bridges the gap between theoretical\nmotion planning and practical deployment, where feasible trajectory existence\nis fundamental.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B* is a novel optimization framework that addresses a critical challenge in\nfixed-base manipulator robotics: optimal base placement. Current methods rely\non pre-computed kinematics databases generated through sampling to search for\nsolutions. However, they face an inherent trade-off between solution optimality\nand computational efficiency when determining sampling resolution. To address\nthese limitations, B* unifies multiple objectives without database dependence.\nThe framework employs a two-layer hierarchical approach. The outer layer\nsystematically manages terminal constraints through progressive tightening,\nparticularly for base mobility, enabling feasible initialization and broad\nsolution exploration. The inner layer addresses non-convexities in each\nouter-layer subproblem through sequential local linearization, converting the\noriginal problem into tractable sequential linear programming (SLP). Testing\nacross multiple robot platforms demonstrates B*'s effectiveness. The framework\nachieves solution optimality five orders of magnitude better than\nsampling-based approaches while maintaining perfect success rates and reduced\ncomputational overhead. Operating directly in configuration space, B* enables\nsimultaneous path planning with customizable optimization criteria. B* serves\nas a crucial initialization tool that bridges the gap between theoretical\nmotion planning and practical deployment, where feasible trajectory existence\nis fundamental."
                },
                "authors": [
                    {
                        "name": "Zihang Zhao"
                    },
                    {
                        "name": "Leiyao Cui"
                    },
                    {
                        "name": "Sirui Xie"
                    },
                    {
                        "name": "Saiyao Zhang"
                    },
                    {
                        "name": "Zhi Han"
                    },
                    {
                        "name": "Lecheng Ruan"
                    },
                    {
                        "name": "Yixin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yixin Zhu"
                },
                "author": "Yixin Zhu",
                "arxiv_comment": "accepted for publication in the IEEE Robotics and Automation Letters\n  (RA-L)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12719v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12719v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00038v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00038v4",
                "updated": "2025-08-22T12:35:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    35,
                    34,
                    4,
                    234,
                    0
                ],
                "published": "2025-02-25T08:41:25Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    8,
                    41,
                    25,
                    1,
                    56,
                    0
                ],
                "title": "from Benign import Toxic: Jailbreaking the Language Model via\n  Adversarial Metaphors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "from Benign import Toxic: Jailbreaking the Language Model via\n  Adversarial Metaphors"
                },
                "summary": "Current studies have exposed the risk of Large Language Models (LLMs)\ngenerating harmful content by jailbreak attacks. However, they overlook that\nthe direct generation of harmful content from scratch is more difficult than\ninducing LLM to calibrate benign content into harmful forms. In our study, we\nintroduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR)\nto induce the LLM to calibrate malicious metaphors for jailbreaking.\nSpecifically, to answer harmful queries, AVATAR adaptively identifies a set of\nbenign but logically related metaphors as the initial seed. Then, driven by\nthese metaphors, the target LLM is induced to reason and calibrate about the\nmetaphorical content, thus jailbroken by either directly outputting harmful\nresponses or calibrating residuals between metaphorical and professional\nharmful content. Experimental results demonstrate that AVATAR can effectively\nand transferable jailbreak LLMs and achieve a state-of-the-art attack success\nrate across multiple advanced LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current studies have exposed the risk of Large Language Models (LLMs)\ngenerating harmful content by jailbreak attacks. However, they overlook that\nthe direct generation of harmful content from scratch is more difficult than\ninducing LLM to calibrate benign content into harmful forms. In our study, we\nintroduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR)\nto induce the LLM to calibrate malicious metaphors for jailbreaking.\nSpecifically, to answer harmful queries, AVATAR adaptively identifies a set of\nbenign but logically related metaphors as the initial seed. Then, driven by\nthese metaphors, the target LLM is induced to reason and calibrate about the\nmetaphorical content, thus jailbroken by either directly outputting harmful\nresponses or calibrating residuals between metaphorical and professional\nharmful content. Experimental results demonstrate that AVATAR can effectively\nand transferable jailbreak LLMs and achieve a state-of-the-art attack success\nrate across multiple advanced LLMs."
                },
                "authors": [
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zenghao Duan"
                    },
                    {
                        "name": "Teli Liu"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Zhiyi Yin"
                    },
                    {
                        "name": "Jiangyu Lei"
                    },
                    {
                        "name": "Qi Li"
                    }
                ],
                "author_detail": {
                    "name": "Qi Li"
                },
                "author": "Qi Li",
                "arxiv_doi": "10.18653/v1/2025.acl-long.238",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.238",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.00038v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00038v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2412.12145",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06360v2",
                "updated": "2025-08-22T12:32:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    32,
                    59,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-08T14:46:05Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    46,
                    5,
                    4,
                    220,
                    0
                ],
                "title": "Cyberbullying Detection via Aggression-Enhanced Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyberbullying Detection via Aggression-Enhanced Prompting"
                },
                "summary": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks."
                },
                "authors": [
                    {
                        "name": "Aisha Saeid"
                    },
                    {
                        "name": "Anu Sabu"
                    },
                    {
                        "name": "Girish A. Koushik"
                    },
                    {
                        "name": "Ferrante Neri"
                    },
                    {
                        "name": "Diptesh Kanojia"
                    }
                ],
                "author_detail": {
                    "name": "Diptesh Kanojia"
                },
                "author": "Diptesh Kanojia",
                "arxiv_comment": "Accepted to RANLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02954v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02954v4",
                "updated": "2025-08-22T12:30:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    30,
                    44,
                    4,
                    234,
                    0
                ],
                "published": "2025-06-03T14:47:22Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    47,
                    22,
                    1,
                    154,
                    0
                ],
                "title": "Mutation-Guided Unit Test Generation with a Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-Guided Unit Test Generation with a Large Language Model"
                },
                "summary": "Unit tests play a vital role in uncovering potential faults in software.\nWhile tools like EvoSuite focus on maximizing code coverage, recent advances in\nlarge language models (LLMs) have shifted attention toward LLM-based test\ngeneration. However, code coverage metrics -- such as line and branch coverage\n-- remain overly emphasized in reported research, despite being weak indicators\nof a test suite's fault-detection capability. In contrast, mutation score\noffers a more reliable and stringent measure, as demonstrated in our findings\nwhere some test suites achieve 100% coverage but only 4% mutation score.\nAlthough a few studies consider mutation score, the effectiveness of LLMs in\nkilling mutants remains underexplored. In this paper, we propose MUTGEN, a\nmutation-guided, LLM-based test generation approach that incorporates mutation\nfeedback directly into the prompt. Evaluated on 204 subjects from two\nbenchmarks, MUTGEN significantly outperforms both EvoSuite and vanilla\nprompt-based strategies in terms of mutation score. Furthermore, MUTGEN\nintroduces an iterative generation mechanism that pushes the limits of LLMs in\nkilling additional mutants. Our study also provide insights into the\nlimitations of LLM-based generation, analyzing the reasons for live and\nuncovered mutants, and the impact of different mutation operators on generation\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit tests play a vital role in uncovering potential faults in software.\nWhile tools like EvoSuite focus on maximizing code coverage, recent advances in\nlarge language models (LLMs) have shifted attention toward LLM-based test\ngeneration. However, code coverage metrics -- such as line and branch coverage\n-- remain overly emphasized in reported research, despite being weak indicators\nof a test suite's fault-detection capability. In contrast, mutation score\noffers a more reliable and stringent measure, as demonstrated in our findings\nwhere some test suites achieve 100% coverage but only 4% mutation score.\nAlthough a few studies consider mutation score, the effectiveness of LLMs in\nkilling mutants remains underexplored. In this paper, we propose MUTGEN, a\nmutation-guided, LLM-based test generation approach that incorporates mutation\nfeedback directly into the prompt. Evaluated on 204 subjects from two\nbenchmarks, MUTGEN significantly outperforms both EvoSuite and vanilla\nprompt-based strategies in terms of mutation score. Furthermore, MUTGEN\nintroduces an iterative generation mechanism that pushes the limits of LLMs in\nkilling additional mutants. Our study also provide insights into the\nlimitations of LLM-based generation, analyzing the reasons for live and\nuncovered mutants, and the impact of different mutation operators on generation\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Guancheng Wang"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Lionel C. Briand"
                    },
                    {
                        "name": "Kui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kui Liu"
                },
                "author": "Kui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02954v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02954v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16334v1",
                "updated": "2025-08-22T12:21:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    21,
                    37,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T12:21:37Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    21,
                    37,
                    4,
                    234,
                    0
                ],
                "title": "From Linear to Hierarchical: Evolving Tree-structured Thoughts for\n  Efficient Alpha Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Linear to Hierarchical: Evolving Tree-structured Thoughts for\n  Efficient Alpha Mining"
                },
                "summary": "Alpha mining, which discovers signals that predict asset returns, has long\nbeen attractive for automatic quantitative investment. This problem is\ntypically formulated as a tree-based symbolic regression with handcrafted\nmarket data features and arithmetic operators. Unfortunately, existing symbolic\nmethods are concerned with computational inefficiency and dependence on prior\nknowledge. Recent implementation of Large Language Models (LLMs) show that they\ncan automatically generate executable codes for various tasks efficiently, thus\ncan be considered as a new promising way for alpha mining. Specifically,\nLLMs-driven methods evolve a set of heuristics, including thoughts and codes,\nwhere the thoughts are usually represented as plain-text prompts of codes.\nUnfortunately, trivially adopting them in alpha mining ignores the fact that\nalphas are with hierarchical tree structures. This paper introduces\nTree-structured thought Evolution (TreEvo), which evolves hierarchical\nreasoning ideas solely at the thought level. Experiments on four real-market\ndatasets demonstrate that TreEvo can obtain better alphas with much less\ncomputational time and human expert efforts. And this superiority hardly holds\nwithout the tree-structured thoughts and the compatible evolutionary operators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alpha mining, which discovers signals that predict asset returns, has long\nbeen attractive for automatic quantitative investment. This problem is\ntypically formulated as a tree-based symbolic regression with handcrafted\nmarket data features and arithmetic operators. Unfortunately, existing symbolic\nmethods are concerned with computational inefficiency and dependence on prior\nknowledge. Recent implementation of Large Language Models (LLMs) show that they\ncan automatically generate executable codes for various tasks efficiently, thus\ncan be considered as a new promising way for alpha mining. Specifically,\nLLMs-driven methods evolve a set of heuristics, including thoughts and codes,\nwhere the thoughts are usually represented as plain-text prompts of codes.\nUnfortunately, trivially adopting them in alpha mining ignores the fact that\nalphas are with hierarchical tree structures. This paper introduces\nTree-structured thought Evolution (TreEvo), which evolves hierarchical\nreasoning ideas solely at the thought level. Experiments on four real-market\ndatasets demonstrate that TreEvo can obtain better alphas with much less\ncomputational time and human expert efforts. And this superiority hardly holds\nwithout the tree-structured thoughts and the compatible evolutionary operators."
                },
                "authors": [
                    {
                        "name": "Junji Ren"
                    },
                    {
                        "name": "Junjie Zhao"
                    },
                    {
                        "name": "Shengcai Liu"
                    },
                    {
                        "name": "Peng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Yang"
                },
                "author": "Peng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11244v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11244v2",
                "updated": "2025-08-22T12:17:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    17,
                    17,
                    4,
                    234,
                    0
                ],
                "published": "2025-02-16T19:44:01Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    19,
                    44,
                    1,
                    6,
                    47,
                    0
                ],
                "title": "Soteria: Language-Specific Functional Parameter Steering for\n  Multilingual Safety Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soteria: Language-Specific Functional Parameter Steering for\n  Multilingual Safety Alignment"
                },
                "summary": "Ensuring consistent safety across multiple languages remains a significant\nchallenge for large language models (LLMs). We introduce Soteria, a lightweight\nyet powerful strategy that locates and minimally adjusts the \"functional heads\"\nmost responsible for harmful content generation in each language. By altering\nonly a fraction of parameters, Soteria drastically reduces policy violations\nwithout sacrificing overall model performance, even in low-resource settings.\nTo rigorously evaluate our approach, we also present XThreatBench, a\nspecialized multilingual dataset capturing fine-grained harmful behaviors drawn\nfrom real policy guidelines. Experiments with leading open-source LLMs (e.g.,\nLlama, Qwen, Mistral) show that Soteria consistently improves safety metrics\nacross high-, mid-, and low-resource languages. These findings highlight a\npromising path toward scalable, linguistically attuned, and ethically aligned\nLLMs worldwide.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring consistent safety across multiple languages remains a significant\nchallenge for large language models (LLMs). We introduce Soteria, a lightweight\nyet powerful strategy that locates and minimally adjusts the \"functional heads\"\nmost responsible for harmful content generation in each language. By altering\nonly a fraction of parameters, Soteria drastically reduces policy violations\nwithout sacrificing overall model performance, even in low-resource settings.\nTo rigorously evaluate our approach, we also present XThreatBench, a\nspecialized multilingual dataset capturing fine-grained harmful behaviors drawn\nfrom real policy guidelines. Experiments with leading open-source LLMs (e.g.,\nLlama, Qwen, Mistral) show that Soteria consistently improves safety metrics\nacross high-, mid-, and low-resource languages. These findings highlight a\npromising path toward scalable, linguistically attuned, and ethically aligned\nLLMs worldwide."
                },
                "authors": [
                    {
                        "name": "Somnath Banerjee"
                    },
                    {
                        "name": "Sayan Layek"
                    },
                    {
                        "name": "Pratyush Chatterjee"
                    },
                    {
                        "name": "Animesh Mukherjee"
                    },
                    {
                        "name": "Rima Hazra"
                    }
                ],
                "author_detail": {
                    "name": "Rima Hazra"
                },
                "author": "Rima Hazra",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11244v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11244v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16325v1",
                "updated": "2025-08-22T12:13:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    13,
                    38,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T12:13:38Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    13,
                    38,
                    4,
                    234,
                    0
                ],
                "title": "LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging\n  Interpretable Jailbreak Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging\n  Interpretable Jailbreak Concepts"
                },
                "summary": "Large Language Models have found success in a variety of applications;\nhowever, their safety remains a matter of concern due to the existence of\nvarious types of jailbreaking methods. Despite significant efforts, alignment\nand safety fine-tuning only provide a certain degree of robustness against\njailbreak attacks that covertly mislead LLMs towards the generation of harmful\ncontent. This leaves them prone to a number of vulnerabilities, ranging from\ntargeted misuse to accidental profiling of users. This work introduces\n\\textbf{LLMSymGuard}, a novel framework that leverages Sparse Autoencoders\n(SAEs) to identify interpretable concepts within LLM internals associated with\ndifferent jailbreak themes. By extracting semantically meaningful internal\nrepresentations, LLMSymGuard enables building symbolic, logical safety\nguardrails -- offering transparent and robust defenses without sacrificing\nmodel capabilities or requiring further fine-tuning. Leveraging advances in\nmechanistic interpretability of LLMs, our approach demonstrates that LLMs learn\nhuman-interpretable concepts from jailbreaks, and provides a foundation for\ndesigning more interpretable and logical safeguard measures against attackers.\nCode will be released upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have found success in a variety of applications;\nhowever, their safety remains a matter of concern due to the existence of\nvarious types of jailbreaking methods. Despite significant efforts, alignment\nand safety fine-tuning only provide a certain degree of robustness against\njailbreak attacks that covertly mislead LLMs towards the generation of harmful\ncontent. This leaves them prone to a number of vulnerabilities, ranging from\ntargeted misuse to accidental profiling of users. This work introduces\n\\textbf{LLMSymGuard}, a novel framework that leverages Sparse Autoencoders\n(SAEs) to identify interpretable concepts within LLM internals associated with\ndifferent jailbreak themes. By extracting semantically meaningful internal\nrepresentations, LLMSymGuard enables building symbolic, logical safety\nguardrails -- offering transparent and robust defenses without sacrificing\nmodel capabilities or requiring further fine-tuning. Leveraging advances in\nmechanistic interpretability of LLMs, our approach demonstrates that LLMs learn\nhuman-interpretable concepts from jailbreaks, and provides a foundation for\ndesigning more interpretable and logical safeguard measures against attackers.\nCode will be released upon publication."
                },
                "authors": [
                    {
                        "name": "Darpan Aswal"
                    },
                    {
                        "name": "Cline Hudelot"
                    }
                ],
                "author_detail": {
                    "name": "Cline Hudelot"
                },
                "author": "Cline Hudelot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14540v2",
                "updated": "2025-08-22T12:13:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    13,
                    32,
                    4,
                    234,
                    0
                ],
                "published": "2025-01-24T14:45:21Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    45,
                    21,
                    4,
                    24,
                    0
                ],
                "title": "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning"
                },
                "summary": "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems"
                },
                "authors": [
                    {
                        "name": "Benjamin Callewaert"
                    },
                    {
                        "name": "Simon Vandevelde"
                    },
                    {
                        "name": "Joost Vennekens"
                    }
                ],
                "author_detail": {
                    "name": "Joost Vennekens"
                },
                "author": "Joost Vennekens",
                "arxiv_comment": "Accepted at ICLP 2025, part of ECPTS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13824v2",
                "updated": "2025-08-22T12:12:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    12,
                    12,
                    9,
                    4,
                    234,
                    0
                ],
                "published": "2025-01-23T16:45:51Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    45,
                    51,
                    3,
                    23,
                    0
                ],
                "title": "Can Hallucinations Help? Boosting LLMs for Drug Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Hallucinations Help? Boosting LLMs for Drug Discovery"
                },
                "summary": "Hallucinations in large language models (LLMs), plausible but factually\ninaccurate text, are often viewed as undesirable. However, recent work suggests\nthat such outputs may hold creative potential. In this paper, we investigate\nwhether hallucinations can improve LLMs on molecule property prediction, a key\ntask in early-stage drug discovery. We prompt LLMs to generate natural language\ndescriptions from molecular SMILES strings and incorporate these often\nhallucinated descriptions into downstream classification tasks. Evaluating\nseven instruction-tuned LLMs across five datasets, we find that hallucinations\nsignificantly improve predictive accuracy for some models. Notably,\nFalcon3-Mamba-7B outperforms all baselines when hallucinated text is included,\nwhile hallucinations generated by GPT-4o consistently yield the greatest gains\nbetween models. We further identify and categorize over 18,000 beneficial\nhallucinations, with structural misdescriptions emerging as the most impactful\ntype, suggesting that hallucinated statements about molecular structure may\nincrease model confidence. Ablation studies show that larger models benefit\nmore from hallucinations, while temperature has a limited effect. Our findings\nchallenge conventional views of hallucination as purely problematic and suggest\nnew directions for leveraging hallucinations as a useful signal in scientific\nmodeling tasks like drug discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in large language models (LLMs), plausible but factually\ninaccurate text, are often viewed as undesirable. However, recent work suggests\nthat such outputs may hold creative potential. In this paper, we investigate\nwhether hallucinations can improve LLMs on molecule property prediction, a key\ntask in early-stage drug discovery. We prompt LLMs to generate natural language\ndescriptions from molecular SMILES strings and incorporate these often\nhallucinated descriptions into downstream classification tasks. Evaluating\nseven instruction-tuned LLMs across five datasets, we find that hallucinations\nsignificantly improve predictive accuracy for some models. Notably,\nFalcon3-Mamba-7B outperforms all baselines when hallucinated text is included,\nwhile hallucinations generated by GPT-4o consistently yield the greatest gains\nbetween models. We further identify and categorize over 18,000 beneficial\nhallucinations, with structural misdescriptions emerging as the most impactful\ntype, suggesting that hallucinated statements about molecular structure may\nincrease model confidence. Ablation studies show that larger models benefit\nmore from hallucinations, while temperature has a limited effect. Our findings\nchallenge conventional views of hallucination as purely problematic and suggest\nnew directions for leveraging hallucinations as a useful signal in scientific\nmodeling tasks like drug discovery."
                },
                "authors": [
                    {
                        "name": "Shuzhou Yuan"
                    },
                    {
                        "name": "Zhan Qu"
                    },
                    {
                        "name": "Ashish Yashwanth Kangen"
                    },
                    {
                        "name": "Michael Frber"
                    }
                ],
                "author_detail": {
                    "name": "Michael Frber"
                },
                "author": "Michael Frber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03427v2",
                "updated": "2025-08-22T11:58:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    58,
                    27,
                    4,
                    234,
                    0
                ],
                "published": "2025-05-06T11:07:26Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    7,
                    26,
                    1,
                    126,
                    0
                ],
                "title": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare."
                },
                "authors": [
                    {
                        "name": "Mouath Abu Daoud"
                    },
                    {
                        "name": "Chaimae Abouzahir"
                    },
                    {
                        "name": "Leen Kharouf"
                    },
                    {
                        "name": "Walid Al-Eisawi"
                    },
                    {
                        "name": "Nizar Habash"
                    },
                    {
                        "name": "Farah E. Shamout"
                    }
                ],
                "author_detail": {
                    "name": "Farah E. Shamout"
                },
                "author": "Farah E. Shamout",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16315v1",
                "updated": "2025-08-22T11:52:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    52,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T11:52:04Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    52,
                    4,
                    4,
                    234,
                    0
                ],
                "title": "OwkinZero: Accelerating Biological Discovery with AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OwkinZero: Accelerating Biological Discovery with AI"
                },
                "summary": "While large language models (LLMs) are rapidly advancing scientific research,\nthey continue to struggle with core biological reasoning tasks essential for\ntranslational and biomedical discovery. To address this limitation, we created\nand curated eight comprehensive benchmark datasets comprising over 300,000\nverifiable question-and-answer pairs, each targeting critical challenges in\ndrug discovery including target druggability, modality suitability, and drug\nperturbation effects. Using this resource, we developed the OwkinZero models by\npost-training open-source LLMs through a Reinforcement Learning from Verifiable\nRewards strategy. Our results demonstrate that specialized 8-32B OwkinZero\nmodels substantially outperform larger, state-of-the-art commercial LLMs on\nthese biological benchmarks. Remarkably, we uncover evidence of a key aspect of\ngeneralization: specialist models trained on a single task consistently\noutperform their base models on previously unseen tasks. This generalization\neffect is further amplified in our comprehensive OwkinZero models, which were\ntrained on a mixture of datasets and achieve even broader cross-task\nimprovements. This study represents a significant step toward addressing the\nbiological reasoning blind spot in current LLMs, demonstrating that targeted\nreinforcement learning on carefully curated data can unlock generalizable\nperformance in specialized models, thereby accelerating AI-driven biological\ndiscovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) are rapidly advancing scientific research,\nthey continue to struggle with core biological reasoning tasks essential for\ntranslational and biomedical discovery. To address this limitation, we created\nand curated eight comprehensive benchmark datasets comprising over 300,000\nverifiable question-and-answer pairs, each targeting critical challenges in\ndrug discovery including target druggability, modality suitability, and drug\nperturbation effects. Using this resource, we developed the OwkinZero models by\npost-training open-source LLMs through a Reinforcement Learning from Verifiable\nRewards strategy. Our results demonstrate that specialized 8-32B OwkinZero\nmodels substantially outperform larger, state-of-the-art commercial LLMs on\nthese biological benchmarks. Remarkably, we uncover evidence of a key aspect of\ngeneralization: specialist models trained on a single task consistently\noutperform their base models on previously unseen tasks. This generalization\neffect is further amplified in our comprehensive OwkinZero models, which were\ntrained on a mixture of datasets and achieve even broader cross-task\nimprovements. This study represents a significant step toward addressing the\nbiological reasoning blind spot in current LLMs, demonstrating that targeted\nreinforcement learning on carefully curated data can unlock generalizable\nperformance in specialized models, thereby accelerating AI-driven biological\ndiscovery."
                },
                "authors": [
                    {
                        "name": "Nathan Bigaud"
                    },
                    {
                        "name": "Vincent Cabeli"
                    },
                    {
                        "name": "Meltem Gurel"
                    },
                    {
                        "name": "Arthur Pignet"
                    },
                    {
                        "name": "John Klein"
                    },
                    {
                        "name": "Gilles Wainrib"
                    },
                    {
                        "name": "Eric Durand"
                    }
                ],
                "author_detail": {
                    "name": "Eric Durand"
                },
                "author": "Eric Durand",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16313v1",
                "updated": "2025-08-22T11:50:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    50,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T11:50:04Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    50,
                    4,
                    4,
                    234,
                    0
                ],
                "title": "Retrieval Enhanced Feedback via In-context Neural Error-book",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Enhanced Feedback via In-context Neural Error-book"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved reasoning capabilities, with in-context learning (ICL) emerging as a\nkey technique for adaptation without retraining. While previous works have\nfocused on leveraging correct examples, recent research highlights the\nimportance of learning from errors to enhance performance. However, existing\nmethods lack a structured framework for analyzing and mitigating errors,\nparticularly in Multimodal Large Language Models (MLLMs), where integrating\nvisual and textual inputs adds complexity. To address this issue, we propose\nREFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a\nteacher-student framework that systematically structures errors and provides\ntargeted feedback. REFINE introduces three systematic queries to construct\nstructured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance\nmultimodal reasoning by prioritizing relevant visual information, diagnosing\ncritical failure points, and formulating corrective actions. Unlike prior\napproaches that rely on redundant retrievals, REFINE optimizes structured\nfeedback retrieval, improving inference efficiency, token usage, and\nscalability. Our results demonstrate substantial speedup, reduced computational\ncosts, and successful generalization, highlighting REFINE's potential for\nenhancing multimodal reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved reasoning capabilities, with in-context learning (ICL) emerging as a\nkey technique for adaptation without retraining. While previous works have\nfocused on leveraging correct examples, recent research highlights the\nimportance of learning from errors to enhance performance. However, existing\nmethods lack a structured framework for analyzing and mitigating errors,\nparticularly in Multimodal Large Language Models (MLLMs), where integrating\nvisual and textual inputs adds complexity. To address this issue, we propose\nREFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a\nteacher-student framework that systematically structures errors and provides\ntargeted feedback. REFINE introduces three systematic queries to construct\nstructured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance\nmultimodal reasoning by prioritizing relevant visual information, diagnosing\ncritical failure points, and formulating corrective actions. Unlike prior\napproaches that rely on redundant retrievals, REFINE optimizes structured\nfeedback retrieval, improving inference efficiency, token usage, and\nscalability. Our results demonstrate substantial speedup, reduced computational\ncosts, and successful generalization, highlighting REFINE's potential for\nenhancing multimodal reasoning."
                },
                "authors": [
                    {
                        "name": "Jongyeop Hyun"
                    },
                    {
                        "name": "Bumsoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Bumsoo Kim"
                },
                "author": "Bumsoo Kim",
                "arxiv_comment": "Accepted at EMNLP 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16311v1",
                "updated": "2025-08-22T11:43:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    43,
                    39,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T11:43:39Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    43,
                    39,
                    4,
                    234,
                    0
                ],
                "title": "Exploiting Information Redundancy in Attention Maps for Extreme\n  Quantization of Vision Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Information Redundancy in Attention Maps for Extreme\n  Quantization of Vision Transformers"
                },
                "summary": "Transformer models rely on Multi-Head Self-Attention (MHSA) mechanisms, where\neach attention head contributes to the final representation. However, their\ncomputational complexity and high memory demands due to MHSA hinders their\ndeployment at the edge. In this work, we analyze and exploit information\nredundancy in attention maps to accelerate model inference. By quantifying the\ninformation captured by each attention head using Shannon entropy, our analysis\nreveals that attention heads with lower entropy, i.e., exhibiting more\ndeterministic behavior, tend to contribute less information, motivating\ntargeted compression strategies. Relying on these insights, we propose Entropy\nAttention Maps (EAM), a model that freezes the weights of low-entropy attention\nmaps and quantizes these values to low precision to avoid redundant\nre-computation. Empirical validation on ImageNet-1k shows that EAM achieves\nsimilar or higher accuracy at $\\leq$20\\% sparsity in attention maps and\ncompetitive performance beyond this level for the DeiT and Swin Transformer\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models rely on Multi-Head Self-Attention (MHSA) mechanisms, where\neach attention head contributes to the final representation. However, their\ncomputational complexity and high memory demands due to MHSA hinders their\ndeployment at the edge. In this work, we analyze and exploit information\nredundancy in attention maps to accelerate model inference. By quantifying the\ninformation captured by each attention head using Shannon entropy, our analysis\nreveals that attention heads with lower entropy, i.e., exhibiting more\ndeterministic behavior, tend to contribute less information, motivating\ntargeted compression strategies. Relying on these insights, we propose Entropy\nAttention Maps (EAM), a model that freezes the weights of low-entropy attention\nmaps and quantizes these values to low precision to avoid redundant\nre-computation. Empirical validation on ImageNet-1k shows that EAM achieves\nsimilar or higher accuracy at $\\leq$20\\% sparsity in attention maps and\ncompetitive performance beyond this level for the DeiT and Swin Transformer\nmodels."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Karim Haroun"
                    },
                    {
                        "name": "Tom Pegeot"
                    }
                ],
                "author_detail": {
                    "name": "Tom Pegeot"
                },
                "author": "Tom Pegeot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13227v2",
                "updated": "2025-08-22T11:26:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    26,
                    53,
                    4,
                    234,
                    0
                ],
                "published": "2025-04-17T13:09:38Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    9,
                    38,
                    3,
                    107,
                    0
                ],
                "title": "DIDS: Domain Impact-aware Data Sampling for Large Language Model\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIDS: Domain Impact-aware Data Sampling for Large Language Model\n  Training"
                },
                "summary": "Large language models (LLMs) are commonly trained on multi-domain datasets,\nwhere domain sampling strategies significantly impact model performance due to\nvarying domain importance across downstream tasks. Existing approaches for\noptimizing domain-level sampling strategies struggle with maintaining\nintra-domain consistency and accurately measuring domain impact. In this paper,\nwe present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain\nconsistency, a gradient clustering algorithm is proposed to group training data\nbased on their learning effects, where a proxy language model and\ndimensionality reduction are employed to reduce computational overhead. To\naccurately measure domain impact, we develop a Fisher Information Matrix (FIM)\nguided metric that quantifies how domain-specific parameter updates affect the\nmodel's output distributions on downstream tasks, with theoretical guarantees.\nFurthermore, to determine optimal sampling ratios, DIDS combines both the\nFIM-guided domain impact assessment and loss learning trajectories that\nindicate domain-specific potential, while accounting for diminishing marginal\nreturns. Extensive experiments demonstrate that DIDS achieves 3.4% higher\naverage performance while maintaining comparable training efficiency. The code\nis available at https://github.com/shiweijiezero/DIDS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are commonly trained on multi-domain datasets,\nwhere domain sampling strategies significantly impact model performance due to\nvarying domain importance across downstream tasks. Existing approaches for\noptimizing domain-level sampling strategies struggle with maintaining\nintra-domain consistency and accurately measuring domain impact. In this paper,\nwe present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain\nconsistency, a gradient clustering algorithm is proposed to group training data\nbased on their learning effects, where a proxy language model and\ndimensionality reduction are employed to reduce computational overhead. To\naccurately measure domain impact, we develop a Fisher Information Matrix (FIM)\nguided metric that quantifies how domain-specific parameter updates affect the\nmodel's output distributions on downstream tasks, with theoretical guarantees.\nFurthermore, to determine optimal sampling ratios, DIDS combines both the\nFIM-guided domain impact assessment and loss learning trajectories that\nindicate domain-specific potential, while accounting for diminishing marginal\nreturns. Extensive experiments demonstrate that DIDS achieves 3.4% higher\naverage performance while maintaining comparable training efficiency. The code\nis available at https://github.com/shiweijiezero/DIDS."
                },
                "authors": [
                    {
                        "name": "Weijie Shi"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Yaguang Wu"
                    },
                    {
                        "name": "Jingzhi Fang"
                    },
                    {
                        "name": "Ruiyuan Zhang"
                    },
                    {
                        "name": "Jiajie Xu"
                    },
                    {
                        "name": "Jia Zhu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Sirui Han"
                    },
                    {
                        "name": "Xiaofang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofang Zhou"
                },
                "author": "Xiaofang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14913v2",
                "updated": "2025-08-22T11:14:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    14,
                    44,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-13T20:44:37Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    20,
                    44,
                    37,
                    2,
                    225,
                    0
                ],
                "title": "Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural\n  Localization of Math Word Problems in Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural\n  Localization of Math Word Problems in Low-Resource Languages"
                },
                "summary": "Large language models (LLMs) have demonstrated significant capabilities in\nsolving mathematical problems expressed in natural language. However,\nmultilingual and culturally-grounded mathematical reasoning in low-resource\nlanguages lags behind English due to the scarcity of socio-cultural task\ndatasets that reflect accurate native entities such as person names,\norganization names, and currencies. Existing multilingual benchmarks are\npredominantly produced via translation and typically retain English-centric\nentities, owing to the high cost associated with human annotater-based\nlocalization. Moreover, automated localization tools are limited, and hence,\ntruly localized datasets remain scarce. To bridge this gap, we introduce a\nframework for LLM-driven cultural localization of math word problems that\nautomatically constructs datasets with native names, organizations, and\ncurrencies from existing sources. We find that translated benchmarks can\nobscure true multilingual math ability under appropriate socio-cultural\ncontexts. Through extensive experiments, we also show that our framework can\nhelp mitigate English-centric entity bias and improves robustness when native\nentities are introduced across various languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant capabilities in\nsolving mathematical problems expressed in natural language. However,\nmultilingual and culturally-grounded mathematical reasoning in low-resource\nlanguages lags behind English due to the scarcity of socio-cultural task\ndatasets that reflect accurate native entities such as person names,\norganization names, and currencies. Existing multilingual benchmarks are\npredominantly produced via translation and typically retain English-centric\nentities, owing to the high cost associated with human annotater-based\nlocalization. Moreover, automated localization tools are limited, and hence,\ntruly localized datasets remain scarce. To bridge this gap, we introduce a\nframework for LLM-driven cultural localization of math word problems that\nautomatically constructs datasets with native names, organizations, and\ncurrencies from existing sources. We find that translated benchmarks can\nobscure true multilingual math ability under appropriate socio-cultural\ncontexts. Through extensive experiments, we also show that our framework can\nhelp mitigate English-centric entity bias and improves robustness when native\nentities are introduced across various languages."
                },
                "authors": [
                    {
                        "name": "Israel Abebe Azime"
                    },
                    {
                        "name": "Tadesse Destaw Belay"
                    },
                    {
                        "name": "Dietrich Klakow"
                    },
                    {
                        "name": "Philipp Slusallek"
                    },
                    {
                        "name": "Anshuman Chhabra"
                    }
                ],
                "author_detail": {
                    "name": "Anshuman Chhabra"
                },
                "author": "Anshuman Chhabra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16293v1",
                "updated": "2025-08-22T10:55:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    55,
                    16,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T10:55:16Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    55,
                    16,
                    4,
                    234,
                    0
                ],
                "title": "Two-Timescale Dynamic Service Deployment and Task Scheduling with\n  Spatiotemporal Collaboration in Mobile Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Dynamic Service Deployment and Task Scheduling with\n  Spatiotemporal Collaboration in Mobile Edge Networks"
                },
                "summary": "Collaborative edge computing addresses the resource constraints of individual\nedge nodes by enabling resource sharing and task co-processing across multiple\nnodes. To fully leverage the advantages of collaborative edge computing, joint\noptimization of service deployment and task scheduling is necessary. Existing\noptimization methods insufficiently address the collaboration across spatial\nand temporal dimensions, which hinders their adaptability to the\nspatiotemporally varying nature of user demands and system states. This paper\nfocuses on optimizing the expected task processing delay in edge networks. We\npropose a two-timescale online optimization framework to jointly determine: i)\nservice deployment decisions at each large timescale; and ii) task scheduling\ndecisions at each small timescale. Specifically, the convex optimization\ntechnique is used to solve the task scheduling problem, while a multi-agent\ndeep reinforcement learning technique is employed for the service deployment\nproblem. These two methods are combined for spatiotemporal co-optimization\nthrough a two-timescale alternating optimization approach. Compared to the\nbaseline algorithms, the proposed scheme achieves better delay performance,\nwhile also exhibiting low running time and favorable convergence behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative edge computing addresses the resource constraints of individual\nedge nodes by enabling resource sharing and task co-processing across multiple\nnodes. To fully leverage the advantages of collaborative edge computing, joint\noptimization of service deployment and task scheduling is necessary. Existing\noptimization methods insufficiently address the collaboration across spatial\nand temporal dimensions, which hinders their adaptability to the\nspatiotemporally varying nature of user demands and system states. This paper\nfocuses on optimizing the expected task processing delay in edge networks. We\npropose a two-timescale online optimization framework to jointly determine: i)\nservice deployment decisions at each large timescale; and ii) task scheduling\ndecisions at each small timescale. Specifically, the convex optimization\ntechnique is used to solve the task scheduling problem, while a multi-agent\ndeep reinforcement learning technique is employed for the service deployment\nproblem. These two methods are combined for spatiotemporal co-optimization\nthrough a two-timescale alternating optimization approach. Compared to the\nbaseline algorithms, the proposed scheme achieves better delay performance,\nwhile also exhibiting low running time and favorable convergence behavior."
                },
                "authors": [
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Yunji Zhao"
                    },
                    {
                        "name": "Wenbo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenbo Wang"
                },
                "author": "Wenbo Wang",
                "arxiv_comment": "This paper is accepted by IEEE Globecom 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15601v2",
                "updated": "2025-08-22T10:51:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    51,
                    55,
                    4,
                    234,
                    0
                ],
                "published": "2025-07-21T13:24:38Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    24,
                    38,
                    0,
                    202,
                    0
                ],
                "title": "Optimal Batch-Size Control for Low-Latency Federated Learning with\n  Device Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Batch-Size Control for Low-Latency Federated Learning with\n  Device Heterogeneity"
                },
                "summary": "Federated learning (FL) has emerged as a popular approach for collaborative\nmachine learning in sixth-generation (6G) networks, primarily due to its\nprivacy-preserving capabilities. The deployment of FL algorithms is expected to\nempower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous\ndriving, augmented reality, and healthcare. The mission-critical and\ntime-sensitive nature of these applications necessitates the design of\nlow-latency FL frameworks that guarantee high learning performance. In\npractice, achieving low-latency FL faces two challenges: the overhead of\ncomputing and transmitting high-dimensional model updates, and the\nheterogeneity in communication-and-computation (C$^2$) capabilities across\ndevices. To address these challenges, we propose a novel C$^2$-aware framework\nfor optimal batch-size control that minimizes end-to-end (E2E) learning latency\nwhile ensuring convergence. The framework is designed to balance a fundamental\nC$^2$ tradeoff as revealed through convergence analysis. Specifically,\nincreasing batch sizes improves the accuracy of gradient estimation in FL and\nthus reduces the number of communication rounds required for convergence, but\nresults in higher per-round latency, and vice versa. The associated problem of\nlatency minimization is intractable; however, we solve it by designing an\naccurate and tractable surrogate for convergence speed, with parameters fitted\nto real data. This approach yields two batch-size control strategies tailored\nto scenarios with slow and fast fading, while also accommodating device\nheterogeneity. Extensive experiments using real datasets demonstrate that the\nproposed strategies outperform conventional batch-size adaptation schemes that\ndo not consider the C$^2$ tradeoff or device heterogeneity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) has emerged as a popular approach for collaborative\nmachine learning in sixth-generation (6G) networks, primarily due to its\nprivacy-preserving capabilities. The deployment of FL algorithms is expected to\nempower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous\ndriving, augmented reality, and healthcare. The mission-critical and\ntime-sensitive nature of these applications necessitates the design of\nlow-latency FL frameworks that guarantee high learning performance. In\npractice, achieving low-latency FL faces two challenges: the overhead of\ncomputing and transmitting high-dimensional model updates, and the\nheterogeneity in communication-and-computation (C$^2$) capabilities across\ndevices. To address these challenges, we propose a novel C$^2$-aware framework\nfor optimal batch-size control that minimizes end-to-end (E2E) learning latency\nwhile ensuring convergence. The framework is designed to balance a fundamental\nC$^2$ tradeoff as revealed through convergence analysis. Specifically,\nincreasing batch sizes improves the accuracy of gradient estimation in FL and\nthus reduces the number of communication rounds required for convergence, but\nresults in higher per-round latency, and vice versa. The associated problem of\nlatency minimization is intractable; however, we solve it by designing an\naccurate and tractable surrogate for convergence speed, with parameters fitted\nto real data. This approach yields two batch-size control strategies tailored\nto scenarios with slow and fast fading, while also accommodating device\nheterogeneity. Extensive experiments using real datasets demonstrate that the\nproposed strategies outperform conventional batch-size adaptation schemes that\ndo not consider the C$^2$ tradeoff or device heterogeneity."
                },
                "authors": [
                    {
                        "name": "Huiling Yang"
                    },
                    {
                        "name": "Zhanwei Wang"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16279v1",
                "updated": "2025-08-22T10:35:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    35,
                    56,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T10:35:56Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    35,
                    56,
                    4,
                    234,
                    0
                ],
                "title": "AgentScope 1.0: A Developer-Centric Framework for Building Agentic\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentScope 1.0: A Developer-Centric Framework for Building Agentic\n  Applications"
                },
                "summary": "Driven by rapid advancements of Large Language Models (LLMs), agents are\nempowered to combine intrinsic knowledge with dynamic tool use, greatly\nenhancing their capacity to address real-world tasks. In line with such an\nevolution, AgentScope introduces major improvements in a new version (1.0),\ntowards comprehensively supporting flexible and efficient tool-based\nagent-environment interactions for building agentic applications. Specifically,\nwe abstract foundational components essential for agentic applications and\nprovide unified interfaces and extensible modules, enabling developers to\neasily leverage the latest progress, such as new models and MCPs. Furthermore,\nwe ground agent behaviors in the ReAct paradigm and offer advanced agent-level\ninfrastructure based on a systematic asynchronous design, which enriches both\nhuman-agent and agent-agent interaction patterns while improving execution\nefficiency. Building on this foundation, we integrate several built-in agents\ntailored to specific practical scenarios. AgentScope also includes robust\nengineering support for developer-friendly experiences. We provide a scalable\nevaluation module with a visual studio interface, making the development of\nlong-trajectory agentic applications more manageable and easier to trace. In\naddition, AgentScope offers a runtime sandbox to ensure safe agent execution\nand facilitates rapid deployment in production environments. With these\nenhancements, AgentScope provides a practical foundation for building scalable,\nadaptive, and effective agentic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driven by rapid advancements of Large Language Models (LLMs), agents are\nempowered to combine intrinsic knowledge with dynamic tool use, greatly\nenhancing their capacity to address real-world tasks. In line with such an\nevolution, AgentScope introduces major improvements in a new version (1.0),\ntowards comprehensively supporting flexible and efficient tool-based\nagent-environment interactions for building agentic applications. Specifically,\nwe abstract foundational components essential for agentic applications and\nprovide unified interfaces and extensible modules, enabling developers to\neasily leverage the latest progress, such as new models and MCPs. Furthermore,\nwe ground agent behaviors in the ReAct paradigm and offer advanced agent-level\ninfrastructure based on a systematic asynchronous design, which enriches both\nhuman-agent and agent-agent interaction patterns while improving execution\nefficiency. Building on this foundation, we integrate several built-in agents\ntailored to specific practical scenarios. AgentScope also includes robust\nengineering support for developer-friendly experiences. We provide a scalable\nevaluation module with a visual studio interface, making the development of\nlong-trajectory agentic applications more manageable and easier to trace. In\naddition, AgentScope offers a runtime sandbox to ensure safe agent execution\nand facilitates rapid deployment in production environments. With these\nenhancements, AgentScope provides a practical foundation for building scalable,\nadaptive, and effective agentic applications."
                },
                "authors": [
                    {
                        "name": "Dawei Gao"
                    },
                    {
                        "name": "Zitao Li"
                    },
                    {
                        "name": "Yuexiang Xie"
                    },
                    {
                        "name": "Weirui Kuang"
                    },
                    {
                        "name": "Liuyi Yao"
                    },
                    {
                        "name": "Bingchen Qian"
                    },
                    {
                        "name": "Zhijian Ma"
                    },
                    {
                        "name": "Yue Cui"
                    },
                    {
                        "name": "Haohao Luo"
                    },
                    {
                        "name": "Shen Li"
                    },
                    {
                        "name": "Lu Yi"
                    },
                    {
                        "name": "Yi Yu"
                    },
                    {
                        "name": "Shiqi He"
                    },
                    {
                        "name": "Zhiling Luo"
                    },
                    {
                        "name": "Wenmeng Zhou"
                    },
                    {
                        "name": "Zhicheng Zhang"
                    },
                    {
                        "name": "Xuguang He"
                    },
                    {
                        "name": "Ziqian Chen"
                    },
                    {
                        "name": "Weikai Liao"
                    },
                    {
                        "name": "Farruh Isakulovich Kushnazarov"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00719v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00719v2",
                "updated": "2025-08-22T10:30:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    30,
                    51,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-01T15:38:21Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    38,
                    21,
                    4,
                    213,
                    0
                ],
                "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and\n  Context-Aware KGQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and\n  Context-Aware KGQA"
                },
                "summary": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yingxu Wang"
                    },
                    {
                        "name": "Shiqi Fan"
                    },
                    {
                        "name": "Mengzhu Wang"
                    },
                    {
                        "name": "Siyang Gao"
                    },
                    {
                        "name": "Siwei Liu"
                    },
                    {
                        "name": "Nan Yin"
                    }
                ],
                "author_detail": {
                    "name": "Nan Yin"
                },
                "author": "Nan Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00719v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00719v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16277v1",
                "updated": "2025-08-22T10:19:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    19,
                    42,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T10:19:42Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    19,
                    42,
                    4,
                    234,
                    0
                ],
                "title": "The next question after Turing's question: Introducing the Grow-AI test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The next question after Turing's question: Introducing the Grow-AI test"
                },
                "summary": "This study aims to extend the framework for assessing artificial\nintelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),\ndesigned to answer the question \"Can machines grow up?\" -- a natural successor\nto the Turing Test. The methodology applied is based on a system of six primary\ncriteria (C1-C6), each assessed through a specific \"game\", divided into four\narenas that explore both the human dimension and its transposition into AI. All\ndecisions and actions of the entity are recorded in a standardized AI Journal,\nthe primary source for calculating composite scores. The assessment uses the\nprior expert method to establish initial weights, and the global score -- Grow\nUp Index -- is calculated as the arithmetic mean of the six scores, with\ninterpretation on maturity thresholds. The results show that the methodology\nallows for a coherent and comparable assessment of the level of \"growth\" of AI\nentities, regardless of their type (robots, software agents, LLMs). The\nmulti-game structure highlights strengths and vulnerable areas, and the use of\na unified journal guarantees traceability and replicability in the evaluation.\nThe originality of the work lies in the conceptual transposition of the process\nof \"growing\" from the human world to that of artificial intelligence, in an\nintegrated testing format that combines perspectives from psychology, robotics,\ncomputer science, and ethics. Through this approach, GROW-AI not only measures\nperformance but also captures the evolutionary path of an AI entity towards\nmaturity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study aims to extend the framework for assessing artificial\nintelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),\ndesigned to answer the question \"Can machines grow up?\" -- a natural successor\nto the Turing Test. The methodology applied is based on a system of six primary\ncriteria (C1-C6), each assessed through a specific \"game\", divided into four\narenas that explore both the human dimension and its transposition into AI. All\ndecisions and actions of the entity are recorded in a standardized AI Journal,\nthe primary source for calculating composite scores. The assessment uses the\nprior expert method to establish initial weights, and the global score -- Grow\nUp Index -- is calculated as the arithmetic mean of the six scores, with\ninterpretation on maturity thresholds. The results show that the methodology\nallows for a coherent and comparable assessment of the level of \"growth\" of AI\nentities, regardless of their type (robots, software agents, LLMs). The\nmulti-game structure highlights strengths and vulnerable areas, and the use of\na unified journal guarantees traceability and replicability in the evaluation.\nThe originality of the work lies in the conceptual transposition of the process\nof \"growing\" from the human world to that of artificial intelligence, in an\nintegrated testing format that combines perspectives from psychology, robotics,\ncomputer science, and ethics. Through this approach, GROW-AI not only measures\nperformance but also captures the evolutionary path of an AI entity towards\nmaturity."
                },
                "authors": [
                    {
                        "name": "Alexandru Tugui"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Tugui"
                },
                "author": "Alexandru Tugui",
                "arxiv_comment": "9th International Conference on Inventive Systems and Control ICISC\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01, 68T05, 68T42, 91A80",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; K.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16270v1",
                "updated": "2025-08-22T10:13:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    13,
                    13,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T10:13:13Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    13,
                    13,
                    4,
                    234,
                    0
                ],
                "title": "LLMs that Understand Processes: Instruction-tuning for Semantics-Aware\n  Process Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs that Understand Processes: Instruction-tuning for Semantics-Aware\n  Process Mining"
                },
                "summary": "Process mining is increasingly using textual information associated with\nevents to tackle tasks such as anomaly detection and process discovery. Such\nsemantics-aware process mining focuses on what behavior should be possible in a\nprocess (i.e., expectations), thus providing an important complement to\ntraditional, frequency-based techniques that focus on recorded behavior (i.e.,\nreality). Large Language Models (LLMs) provide a powerful means for tackling\nsemantics-aware tasks. However, the best performance is so far achieved through\ntask-specific fine-tuning, which is computationally intensive and results in\nmodels that can only handle one specific task. To overcome this lack of\ngeneralization, we use this paper to investigate the potential of\ninstruction-tuning for semantics-aware process mining. The idea of\ninstruction-tuning here is to expose an LLM to prompt-answer pairs for\ndifferent tasks, e.g., anomaly detection and next-activity prediction, making\nit more familiar with process mining, thus allowing it to also perform better\nat unseen tasks, such as process discovery. Our findings demonstrate a varied\nimpact of instruction-tuning: while performance considerably improved on\nprocess discovery and prediction tasks, it varies across models on anomaly\ndetection tasks, highlighting that the selection of tasks for\ninstruction-tuning is critical to achieving desired outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process mining is increasingly using textual information associated with\nevents to tackle tasks such as anomaly detection and process discovery. Such\nsemantics-aware process mining focuses on what behavior should be possible in a\nprocess (i.e., expectations), thus providing an important complement to\ntraditional, frequency-based techniques that focus on recorded behavior (i.e.,\nreality). Large Language Models (LLMs) provide a powerful means for tackling\nsemantics-aware tasks. However, the best performance is so far achieved through\ntask-specific fine-tuning, which is computationally intensive and results in\nmodels that can only handle one specific task. To overcome this lack of\ngeneralization, we use this paper to investigate the potential of\ninstruction-tuning for semantics-aware process mining. The idea of\ninstruction-tuning here is to expose an LLM to prompt-answer pairs for\ndifferent tasks, e.g., anomaly detection and next-activity prediction, making\nit more familiar with process mining, thus allowing it to also perform better\nat unseen tasks, such as process discovery. Our findings demonstrate a varied\nimpact of instruction-tuning: while performance considerably improved on\nprocess discovery and prediction tasks, it varies across models on anomaly\ndetection tasks, highlighting that the selection of tasks for\ninstruction-tuning is critical to achieving desired outcomes."
                },
                "authors": [
                    {
                        "name": "Vira Pyrih"
                    },
                    {
                        "name": "Adrian Rebmann"
                    },
                    {
                        "name": "Han van der Aa"
                    }
                ],
                "author_detail": {
                    "name": "Han van der Aa"
                },
                "author": "Han van der Aa",
                "arxiv_comment": "Accepted at IEEE ICPM 2025, 8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16268v1",
                "updated": "2025-08-22T10:09:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    9,
                    8,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T10:09:08Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    10,
                    9,
                    8,
                    4,
                    234,
                    0
                ],
                "title": "Self-Healing Network of Interconnected Edge Devices Empowered by\n  Infrastructure-as-Code and LoRa Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Healing Network of Interconnected Edge Devices Empowered by\n  Infrastructure-as-Code and LoRa Communication"
                },
                "summary": "This Paper proposes a self-healing, automated network of Raspberry Pi devices\ndesigned for deployment in scenarios where traditional networking is\nunavailable. Leveraging the low-power, long-range capabilities of the LoRa\n(Long Range) protocol alongside Infrastructure as Code (IaC) methodologies, the\nresearch addresses challenges such as limited bandwidth, data collisions, and\nnode failures. Given that LoRa's packet-based system is incompatible with\nconventional IaC tools like Ansible and Terraform, which rely on TCP/IP\nnetworking, the research adapts IaC principles within a containerised\narchitecture deployed across a Raspberry Pi cluster. Evaluation experiments\nindicate that fragmenting data packets and retransmitting any missed fragments\ncan mitigate LoRa's inherent throughput and packet size limitations, although\nissues such as collisions and line-of-sight interference persist. An automated\nfailover mechanism was integrated into the architecture, enabling unresponsive\nservices to be redeployed to alternative nodes within one second, demonstrating\nthe system's resilience in maintaining operational continuity despite node or\nservice failures. The paper also identifies practical challenges, including the\nnecessity for time-slotting transmissions to prevent data packet overlap and\ncollisions. Future research should explore the integration of mesh networking\nto enhance range, develop more advanced scheduling algorithms, and adopt\ncutting-edge low-power wide-area network (LPWAN) techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This Paper proposes a self-healing, automated network of Raspberry Pi devices\ndesigned for deployment in scenarios where traditional networking is\nunavailable. Leveraging the low-power, long-range capabilities of the LoRa\n(Long Range) protocol alongside Infrastructure as Code (IaC) methodologies, the\nresearch addresses challenges such as limited bandwidth, data collisions, and\nnode failures. Given that LoRa's packet-based system is incompatible with\nconventional IaC tools like Ansible and Terraform, which rely on TCP/IP\nnetworking, the research adapts IaC principles within a containerised\narchitecture deployed across a Raspberry Pi cluster. Evaluation experiments\nindicate that fragmenting data packets and retransmitting any missed fragments\ncan mitigate LoRa's inherent throughput and packet size limitations, although\nissues such as collisions and line-of-sight interference persist. An automated\nfailover mechanism was integrated into the architecture, enabling unresponsive\nservices to be redeployed to alternative nodes within one second, demonstrating\nthe system's resilience in maintaining operational continuity despite node or\nservice failures. The paper also identifies practical challenges, including the\nnecessity for time-slotting transmissions to prevent data packet overlap and\ncollisions. Future research should explore the integration of mesh networking\nto enhance range, develop more advanced scheduling algorithms, and adopt\ncutting-edge low-power wide-area network (LPWAN) techniques."
                },
                "authors": [
                    {
                        "name": "Rob Carson"
                    },
                    {
                        "name": "Mohamed Chahine Ghanem"
                    },
                    {
                        "name": "Feriel Bouakkaz"
                    }
                ],
                "author_detail": {
                    "name": "Feriel Bouakkaz"
                },
                "author": "Feriel Bouakkaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16267v1",
                "updated": "2025-08-22T09:59:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    59,
                    23,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T09:59:23Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    59,
                    23,
                    4,
                    234,
                    0
                ],
                "title": "From Confidence to Collapse in LLM Factual Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Confidence to Collapse in LLM Factual Robustness"
                },
                "summary": "Ensuring the robustness of factual knowledge in LLMs is critical for reliable\napplications in tasks such as question answering and reasoning. However,\nexisting evaluation methods predominantly focus on performance-based metrics,\noften investigating from the perspective of prompt perturbations, which\ncaptures only the externally triggered side of knowledge robustness. To bridge\nthis gap, we introduce a principled approach to measure factual robustness from\nthe perspective of the generation process by analyzing token distribution\nentropy in combination with temperature scaling sensitivity. These two factors\nbuild the Factual Robustness Score (FRS), a novel metric which quantifies the\nstability of a fact against perturbations in decoding conditions, given its\ninitial uncertainty. To validate our approach, we conduct extensive experiments\non 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We\nshow that factual robustness varies significantly -- smaller models report an\nFRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\\%$ under\nincreased uncertainty. These insights demonstrate how entropy and temperature\nscaling impact factual accuracy, and lay a foundation for developing more\nrobust knowledge retention and retrieval in future models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the robustness of factual knowledge in LLMs is critical for reliable\napplications in tasks such as question answering and reasoning. However,\nexisting evaluation methods predominantly focus on performance-based metrics,\noften investigating from the perspective of prompt perturbations, which\ncaptures only the externally triggered side of knowledge robustness. To bridge\nthis gap, we introduce a principled approach to measure factual robustness from\nthe perspective of the generation process by analyzing token distribution\nentropy in combination with temperature scaling sensitivity. These two factors\nbuild the Factual Robustness Score (FRS), a novel metric which quantifies the\nstability of a fact against perturbations in decoding conditions, given its\ninitial uncertainty. To validate our approach, we conduct extensive experiments\non 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We\nshow that factual robustness varies significantly -- smaller models report an\nFRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\\%$ under\nincreased uncertainty. These insights demonstrate how entropy and temperature\nscaling impact factual accuracy, and lay a foundation for developing more\nrobust knowledge retention and retrieval in future models."
                },
                "authors": [
                    {
                        "name": "Alina Fastowski"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16265v1",
                "updated": "2025-08-22T09:57:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    57,
                    40,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T09:57:40Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    57,
                    40,
                    4,
                    234,
                    0
                ],
                "title": "M3TQA: Massively Multilingual Multitask Table Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M3TQA: Massively Multilingual Multitask Table Question Answering"
                },
                "summary": "Tabular data is a fundamental component of real-world information systems,\nyet most research in table understanding remains confined to English, leaving\nmultilingual comprehension significantly underexplored. Existing multilingual\ntable benchmarks suffer from geolinguistic imbalance - overrepresenting certain\nlanguages and lacking sufficient scale for rigorous cross-lingual analysis. To\naddress these limitations, we introduce a comprehensive framework for massively\nmultilingual multitask table question answering, featuring m3TQA-Instruct, a\nlarge-scale benchmark spanning 97 languages across diverse language families,\nincluding underrepresented and low-resource languages. We construct m3TQA by\ncurating 50 real-world tables in Chinese and English, then applying a robust\nsix-step LLM-based translation pipeline powered by DeepSeek and GPT-4o,\nachieving high translation fidelity with a median BLEU score of 60.19 as\nvalidated through back-translation. The benchmark includes 2,916 professionally\nannotated question-answering pairs across four tasks designed to evaluate\nnuanced table reasoning capabilities. Experiments on state-of-the-art LLMs\nreveal critical insights into cross-lingual generalization, demonstrating that\nsynthetically generated, unannotated QA data can significantly boost\nperformance, particularly for low-resource languages. M3T-Bench establishes a\nnew standard for multilingual table understanding, providing both a challenging\nevaluation platform and a scalable methodology for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data is a fundamental component of real-world information systems,\nyet most research in table understanding remains confined to English, leaving\nmultilingual comprehension significantly underexplored. Existing multilingual\ntable benchmarks suffer from geolinguistic imbalance - overrepresenting certain\nlanguages and lacking sufficient scale for rigorous cross-lingual analysis. To\naddress these limitations, we introduce a comprehensive framework for massively\nmultilingual multitask table question answering, featuring m3TQA-Instruct, a\nlarge-scale benchmark spanning 97 languages across diverse language families,\nincluding underrepresented and low-resource languages. We construct m3TQA by\ncurating 50 real-world tables in Chinese and English, then applying a robust\nsix-step LLM-based translation pipeline powered by DeepSeek and GPT-4o,\nachieving high translation fidelity with a median BLEU score of 60.19 as\nvalidated through back-translation. The benchmark includes 2,916 professionally\nannotated question-answering pairs across four tasks designed to evaluate\nnuanced table reasoning capabilities. Experiments on state-of-the-art LLMs\nreveal critical insights into cross-lingual generalization, demonstrating that\nsynthetically generated, unannotated QA data can significantly boost\nperformance, particularly for low-resource languages. M3T-Bench establishes a\nnew standard for multilingual table understanding, providing both a challenging\nevaluation platform and a scalable methodology for future research."
                },
                "authors": [
                    {
                        "name": "Daixin Shu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Zhenhe Wu"
                    },
                    {
                        "name": "Xianjie Wu"
                    },
                    {
                        "name": "Xianfu Cheng"
                    },
                    {
                        "name": "Xiangyuan Guan"
                    },
                    {
                        "name": "Yanghai Wang"
                    },
                    {
                        "name": "Pengfei Wu"
                    },
                    {
                        "name": "Tingyang Yang"
                    },
                    {
                        "name": "Hualei Zhu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16261v1",
                "updated": "2025-08-22T09:52:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    52,
                    31,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T09:52:31Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    52,
                    31,
                    4,
                    234,
                    0
                ],
                "title": "On the Evolution of Federated Post-Training Large Language Models: A\n  Model Accessibility View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Evolution of Federated Post-Training Large Language Models: A\n  Model Accessibility View"
                },
                "summary": "Federated Learning (FL) enables training models across decentralized data\nsilos while preserving client data privacy. Recent research has explored\nefficient methods for post-training large language models (LLMs) within FL to\naddress computational and communication challenges. While existing approaches\noften rely on access to LLMs' internal information, which is frequently\nrestricted in real-world scenarios, an inference-only paradigm (black-box\nFedLLM) has emerged to address these limitations. This paper presents a\ncomprehensive survey on federated tuning for LLMs. We propose a taxonomy\ncategorizing existing studies along two axes: model access-based and parameter\nefficiency-based optimization. We classify FedLLM approaches into white-box,\ngray-box, and black-box techniques, highlighting representative methods within\neach category. We review emerging research treating LLMs as black-box inference\nAPIs and discuss promising directions and open challenges for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables training models across decentralized data\nsilos while preserving client data privacy. Recent research has explored\nefficient methods for post-training large language models (LLMs) within FL to\naddress computational and communication challenges. While existing approaches\noften rely on access to LLMs' internal information, which is frequently\nrestricted in real-world scenarios, an inference-only paradigm (black-box\nFedLLM) has emerged to address these limitations. This paper presents a\ncomprehensive survey on federated tuning for LLMs. We propose a taxonomy\ncategorizing existing studies along two axes: model access-based and parameter\nefficiency-based optimization. We classify FedLLM approaches into white-box,\ngray-box, and black-box techniques, highlighting representative methods within\neach category. We review emerging research treating LLMs as black-box inference\nAPIs and discuss promising directions and open challenges for future research."
                },
                "authors": [
                    {
                        "name": "Tao Guo"
                    },
                    {
                        "name": "Junxiao Wang"
                    },
                    {
                        "name": "Fushuo Huo"
                    },
                    {
                        "name": "Laizhong Cui"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Jie Gui"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16260v1",
                "updated": "2025-08-22T09:47:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    47,
                    53,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T09:47:53Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    47,
                    53,
                    4,
                    234,
                    0
                ],
                "title": "MCPVerse: An Expansive, Real-World Benchmark for Agentic Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCPVerse: An Expansive, Real-World Benchmark for Agentic Tool Use"
                },
                "summary": "Large Language Models (LLMs) are evolving from text generators into reasoning\nagents. This transition makes their ability to use external tools a critical\ncapability. However, evaluating this skill presents a significant challenge.\nExisting benchmarks are often limited by their reliance on synthetic tools and\nseverely constrained action spaces. To address these limitations, we introduce\nMCPVerse, an expansive, real-world benchmark for evaluating agentic tool use.\nMCPVerse integrates more than 550 real-world, executable tools to create an\nunprecedented action space exceeding 140k tokens, and employs outcome-based\nevaluation with real-time ground truth for time-sensitive tasks. We benchmarked\nthe state-of-the-art LLMs across three modes (Oracle, Standard, and Max-Scale),\nrevealing that while most models suffer performance degradation when confronted\nwith larger tool sets, the agentic models, such as Claude-4-Sonnet, can\neffectively leverage expanded exploration spaces to improve accuracy. This\nfinding not only exposes the limitations of state-of-the-art models in complex,\nreal-world scenarios but also establishes MCPVerse as a critical benchmark for\nmeasuring and advancing agentic tool use capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are evolving from text generators into reasoning\nagents. This transition makes their ability to use external tools a critical\ncapability. However, evaluating this skill presents a significant challenge.\nExisting benchmarks are often limited by their reliance on synthetic tools and\nseverely constrained action spaces. To address these limitations, we introduce\nMCPVerse, an expansive, real-world benchmark for evaluating agentic tool use.\nMCPVerse integrates more than 550 real-world, executable tools to create an\nunprecedented action space exceeding 140k tokens, and employs outcome-based\nevaluation with real-time ground truth for time-sensitive tasks. We benchmarked\nthe state-of-the-art LLMs across three modes (Oracle, Standard, and Max-Scale),\nrevealing that while most models suffer performance degradation when confronted\nwith larger tool sets, the agentic models, such as Claude-4-Sonnet, can\neffectively leverage expanded exploration spaces to improve accuracy. This\nfinding not only exposes the limitations of state-of-the-art models in complex,\nreal-world scenarios but also establishes MCPVerse as a critical benchmark for\nmeasuring and advancing agentic tool use capabilities."
                },
                "authors": [
                    {
                        "name": "Fei Lei"
                    },
                    {
                        "name": "Yibo Yang"
                    },
                    {
                        "name": "Wenxiu Sun"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16251v1",
                "updated": "2025-08-22T09:32:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    32,
                    40,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T09:32:40Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    32,
                    40,
                    4,
                    234,
                    0
                ],
                "title": "A QoE-Driven Personalized Incentive Mechanism Design for AIGC Services\n  in Resource-Constrained Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A QoE-Driven Personalized Incentive Mechanism Design for AIGC Services\n  in Resource-Constrained Edge Networks"
                },
                "summary": "With rapid advancements in large language models (LLMs), AI-generated content\n(AIGC) has emerged as a key driver of technological innovation and economic\ntransformation. Personalizing AIGC services to meet individual user demands is\nessential but challenging for AIGC service providers (ASPs) due to the\nsubjective and complex demands of mobile users (MUs), as well as the\ncomputational and communication resource constraints faced by ASPs. To tackle\nthese challenges, we first develop a novel multi-dimensional\nquality-of-experience (QoE) metric. This metric comprehensively evaluates AIGC\nservices by integrating accuracy, token count, and timeliness. We focus on a\nmobile edge computing (MEC)-enabled AIGC network, consisting of multiple ASPs\ndeploying differentiated AIGC models on edge servers and multiple MUs with\nheterogeneous QoE requirements requesting AIGC services from ASPs. To\nincentivize ASPs to provide personalized AIGC services under MEC resource\nconstraints, we propose a QoE-driven incentive mechanism. We formulate the\nproblem as an equilibrium problem with equilibrium constraints (EPEC), where\nMUs as leaders determine rewards, while ASPs as followers optimize resource\nallocation. To solve this, we develop a dual-perturbation reward optimization\nalgorithm, reducing the implementation complexity of adaptive pricing.\nExperimental results demonstrate that our proposed mechanism achieves a\nreduction of approximately $64.9\\%$ in average computational and communication\noverhead, while the average service cost for MUs and the resource consumption\nof ASPs decrease by $66.5\\%$ and $76.8\\%$, respectively, compared to\nstate-of-the-art benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With rapid advancements in large language models (LLMs), AI-generated content\n(AIGC) has emerged as a key driver of technological innovation and economic\ntransformation. Personalizing AIGC services to meet individual user demands is\nessential but challenging for AIGC service providers (ASPs) due to the\nsubjective and complex demands of mobile users (MUs), as well as the\ncomputational and communication resource constraints faced by ASPs. To tackle\nthese challenges, we first develop a novel multi-dimensional\nquality-of-experience (QoE) metric. This metric comprehensively evaluates AIGC\nservices by integrating accuracy, token count, and timeliness. We focus on a\nmobile edge computing (MEC)-enabled AIGC network, consisting of multiple ASPs\ndeploying differentiated AIGC models on edge servers and multiple MUs with\nheterogeneous QoE requirements requesting AIGC services from ASPs. To\nincentivize ASPs to provide personalized AIGC services under MEC resource\nconstraints, we propose a QoE-driven incentive mechanism. We formulate the\nproblem as an equilibrium problem with equilibrium constraints (EPEC), where\nMUs as leaders determine rewards, while ASPs as followers optimize resource\nallocation. To solve this, we develop a dual-perturbation reward optimization\nalgorithm, reducing the implementation complexity of adaptive pricing.\nExperimental results demonstrate that our proposed mechanism achieves a\nreduction of approximately $64.9\\%$ in average computational and communication\noverhead, while the average service cost for MUs and the resource consumption\nof ASPs decrease by $66.5\\%$ and $76.8\\%$, respectively, compared to\nstate-of-the-art benchmarks."
                },
                "authors": [
                    {
                        "name": "Hongjia Wu"
                    },
                    {
                        "name": "Minrui Xu"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Lin Gao"
                    },
                    {
                        "name": "Haoyuan Pan"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07050v2",
                "updated": "2025-08-22T09:15:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    15,
                    24,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-09T17:26:18Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    17,
                    26,
                    18,
                    5,
                    221,
                    0
                ],
                "title": "ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability"
                },
                "summary": "Large Language Model (LLM) based listwise ranking has shown superior\nperformance in many passage ranking tasks. With the development of Large\nReasoning Models, many studies have demonstrated that step-by-step reasoning\nduring test-time helps improve listwise ranking performance. However, due to\nthe scarcity of reasoning-intensive training data, existing rerankers perform\npoorly in many complex ranking scenarios and the ranking ability of\nreasoning-intensive rerankers remains largely underdeveloped. In this paper, we\nfirst propose an automated reasoning-intensive training data synthesis\nframework, which sources training queries and passages from diverse domains and\napplies DeepSeek-R1 to generate high-quality training labels. A\nself-consistency data filtering mechanism is designed to ensure the data\nquality. To empower the listwise reranker with strong reasoning ability, we\nfurther propose a two-stage post-training approach, which includes a cold-start\nsupervised fine-tuning (SFT) stage for reasoning pattern learning and a\nreinforcement learning (RL) stage for further ranking ability enhancement.\nDuring the RL stage, based on the nature of listwise ranking, we design a\nmulti-view ranking reward, which is more effective than a ranking metric-based\nreward. Extensive experiments demonstrate that our trained reasoning-intensive\nreranker \\textbf{ReasonRank} outperforms existing baselines significantly and\nalso achieves much lower latency than pointwise reranker Rank1. \\textbf{Through\nfurther experiments, our ReasonRank has achieved state-of-the-art (SOTA)\nperformance 40.6 on the BRIGHT\nleaderboard\\footnote{https://brightbenchmark.github.io/}.} Our codes are\navailable at https://github.com/8421BCD/ReasonRank.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based listwise ranking has shown superior\nperformance in many passage ranking tasks. With the development of Large\nReasoning Models, many studies have demonstrated that step-by-step reasoning\nduring test-time helps improve listwise ranking performance. However, due to\nthe scarcity of reasoning-intensive training data, existing rerankers perform\npoorly in many complex ranking scenarios and the ranking ability of\nreasoning-intensive rerankers remains largely underdeveloped. In this paper, we\nfirst propose an automated reasoning-intensive training data synthesis\nframework, which sources training queries and passages from diverse domains and\napplies DeepSeek-R1 to generate high-quality training labels. A\nself-consistency data filtering mechanism is designed to ensure the data\nquality. To empower the listwise reranker with strong reasoning ability, we\nfurther propose a two-stage post-training approach, which includes a cold-start\nsupervised fine-tuning (SFT) stage for reasoning pattern learning and a\nreinforcement learning (RL) stage for further ranking ability enhancement.\nDuring the RL stage, based on the nature of listwise ranking, we design a\nmulti-view ranking reward, which is more effective than a ranking metric-based\nreward. Extensive experiments demonstrate that our trained reasoning-intensive\nreranker \\textbf{ReasonRank} outperforms existing baselines significantly and\nalso achieves much lower latency than pointwise reranker Rank1. \\textbf{Through\nfurther experiments, our ReasonRank has achieved state-of-the-art (SOTA)\nperformance 40.6 on the BRIGHT\nleaderboard\\footnote{https://brightbenchmark.github.io/}.} Our codes are\navailable at https://github.com/8421BCD/ReasonRank."
                },
                "authors": [
                    {
                        "name": "Wenhan Liu"
                    },
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Weiwei Sun"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16232v1",
                "updated": "2025-08-22T09:10:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    10,
                    37,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T09:10:37Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    10,
                    37,
                    4,
                    234,
                    0
                ],
                "title": "Hybrid Pruning: In-Situ Compression of Self-Supervised Speech Models for\n  Speaker Verification and Anti-Spoofing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Pruning: In-Situ Compression of Self-Supervised Speech Models for\n  Speaker Verification and Anti-Spoofing"
                },
                "summary": "Although large-scale self-supervised learning (SSL) models like WavLM have\nachieved state-of-the-art performance in speech processing, their significant\nsize impedes deployment on resource-constrained devices. While structured\npruning is a key technique for model compression, existing methods typically\nseparate it from task-specific fine-tuning. This multi-stage approach struggles\nto create optimal architectures tailored for diverse downstream tasks. In this\nwork, we introduce a unified framework that integrates structured pruning into\nthe downstream fine-tuning process. Our framework unifies these steps, jointly\noptimizing for task performance and model sparsity in a single stage. This\nallows the model to learn a compressed architecture specifically for the end\ntask, eliminating the need for complex multi-stage pipelines and knowledge\ndistillation. Our pruned models achieve up to a 70\\% parameter reduction with\nnegligible performance degradation on large-scale datasets, achieving equal\nerror rates of 0.7\\%, 0.8\\%, and 1.6\\% on Vox1-O, -E, and -H, respectively.\nFurthermore, our approach demonstrates improved generalization in low-resource\nscenarios, reducing overfitting and achieving a state-of-the-art 3.7\\% EER on\nASVspoof5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large-scale self-supervised learning (SSL) models like WavLM have\nachieved state-of-the-art performance in speech processing, their significant\nsize impedes deployment on resource-constrained devices. While structured\npruning is a key technique for model compression, existing methods typically\nseparate it from task-specific fine-tuning. This multi-stage approach struggles\nto create optimal architectures tailored for diverse downstream tasks. In this\nwork, we introduce a unified framework that integrates structured pruning into\nthe downstream fine-tuning process. Our framework unifies these steps, jointly\noptimizing for task performance and model sparsity in a single stage. This\nallows the model to learn a compressed architecture specifically for the end\ntask, eliminating the need for complex multi-stage pipelines and knowledge\ndistillation. Our pruned models achieve up to a 70\\% parameter reduction with\nnegligible performance degradation on large-scale datasets, achieving equal\nerror rates of 0.7\\%, 0.8\\%, and 1.6\\% on Vox1-O, -E, and -H, respectively.\nFurthermore, our approach demonstrates improved generalization in low-resource\nscenarios, reducing overfitting and achieving a state-of-the-art 3.7\\% EER on\nASVspoof5."
                },
                "authors": [
                    {
                        "name": "Junyi Peng"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Jiangyu Han"
                    },
                    {
                        "name": "Oldich Plchot"
                    },
                    {
                        "name": "Johan Rohdin"
                    },
                    {
                        "name": "Themos Stafylakis"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Jan ernock"
                    }
                ],
                "author_detail": {
                    "name": "Jan ernock"
                },
                "author": "Jan ernock",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20807v2",
                "updated": "2025-08-22T08:48:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    48,
                    16,
                    4,
                    234,
                    0
                ],
                "published": "2025-06-25T19:59:34Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    19,
                    59,
                    34,
                    2,
                    176,
                    0
                ],
                "title": "GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel\n  Optimization"
                },
                "summary": "Optimizing GPU kernels for high performance is a complex task, often\ndemanding deep architectural knowledge, extensive profiling, and iterative\nexperimentation. This challenge is amplified when targeting newer or\nless-documented GPU architectures where traditional development aids are\nscarce. This paper introduces an LLM-powered \"GPU Kernel Scientist,\" an\nautomated methodology for iteratively refining accelerator kernels.\n  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)\nstrategically selecting promising prior code versions as a basis for new\niterations; (b) generating hypotheses for optimization experiments, based on\nexisting code and assimilated knowledge from general GPU literature; and (c)\nautonomously implementing these experiments through code modification and\nsubsequent submission to an external evaluation system, using only observed\ntiming data as performance feedback. We detail how this approach navigates the\nchallenges of the AMD MI300 target architecture and leverages LLMs to\ncompensate for limited domain-specific human expertise.\n  In addition to our results, we present the architectural design, operational\nworkflow, and qualitative insights, highlighting the potential of LLM-driven\nagents to democratise and accelerate GPU kernel optimization, especially in\nresource-constrained or rapidly updating hardware environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing GPU kernels for high performance is a complex task, often\ndemanding deep architectural knowledge, extensive profiling, and iterative\nexperimentation. This challenge is amplified when targeting newer or\nless-documented GPU architectures where traditional development aids are\nscarce. This paper introduces an LLM-powered \"GPU Kernel Scientist,\" an\nautomated methodology for iteratively refining accelerator kernels.\n  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)\nstrategically selecting promising prior code versions as a basis for new\niterations; (b) generating hypotheses for optimization experiments, based on\nexisting code and assimilated knowledge from general GPU literature; and (c)\nautonomously implementing these experiments through code modification and\nsubsequent submission to an external evaluation system, using only observed\ntiming data as performance feedback. We detail how this approach navigates the\nchallenges of the AMD MI300 target architecture and leverages LLMs to\ncompensate for limited domain-specific human expertise.\n  In addition to our results, we present the architectural design, operational\nworkflow, and qualitative insights, highlighting the potential of LLM-driven\nagents to democratise and accelerate GPU kernel optimization, especially in\nresource-constrained or rapidly updating hardware environment."
                },
                "authors": [
                    {
                        "name": "Martin Andrews"
                    },
                    {
                        "name": "Sam Witteveen"
                    }
                ],
                "author_detail": {
                    "name": "Sam Witteveen"
                },
                "author": "Sam Witteveen",
                "arxiv_comment": "4+1 page paper plus Appendices and Supplementary zip file. Presented\n  at the ES-FoMo \"Efficient Systems for Foundation Models\" workshop at ICML\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v2",
                "updated": "2025-08-22T08:45:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    45,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19667v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19667v2",
                "updated": "2025-08-22T08:39:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    39,
                    20,
                    4,
                    234,
                    0
                ],
                "published": "2025-04-28T10:43:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    43,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "Tripartite-GraphRAG via Plugin Ontologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tripartite-GraphRAG via Plugin Ontologies"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, yet they struggle with knowledge-intensive tasks in areas that\ndemand factual accuracy, e.g. industrial automation and healthcare. Key\nlimitations include their tendency to hallucinate, lack of source traceability\n(provenance), and challenges in timely knowledge updates. Combining language\nmodels with knowledge graphs (GraphRAG) offers promising avenues for overcoming\nthese deficits. However, a major challenge lies in creating such a knowledge\ngraph in the first place. Here, we propose a novel approach that combines LLMs\nwith a tripartite knowledge graph representation, which is constructed by\nconnecting complex, domain-specific objects via a curated ontology of\ncorresponding, domain-specific concepts to relevant sections within chunks of\ntext through a concept-anchored pre-analysis of source documents starting from\nan initial lexical graph. Subsequently, we formulate LLM prompt creation as an\nunsupervised node classification problem allowing for the optimization of\ninformation density, coverage, and arrangement of LLM prompts at significantly\nreduced lengths. An initial experimental evaluation of our approach on a\nhealthcare use case, involving multi-faceted analyses of patient anamneses\ngiven a set of medical concepts as well as a series of clinical guideline\nliterature, indicates its potential to optimize information density, coverage,\nand arrangement of LLM prompts while significantly reducing their lengths,\nwhich, in turn, may lead to reduced costs as well as more consistent and\nreliable LLM outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, yet they struggle with knowledge-intensive tasks in areas that\ndemand factual accuracy, e.g. industrial automation and healthcare. Key\nlimitations include their tendency to hallucinate, lack of source traceability\n(provenance), and challenges in timely knowledge updates. Combining language\nmodels with knowledge graphs (GraphRAG) offers promising avenues for overcoming\nthese deficits. However, a major challenge lies in creating such a knowledge\ngraph in the first place. Here, we propose a novel approach that combines LLMs\nwith a tripartite knowledge graph representation, which is constructed by\nconnecting complex, domain-specific objects via a curated ontology of\ncorresponding, domain-specific concepts to relevant sections within chunks of\ntext through a concept-anchored pre-analysis of source documents starting from\nan initial lexical graph. Subsequently, we formulate LLM prompt creation as an\nunsupervised node classification problem allowing for the optimization of\ninformation density, coverage, and arrangement of LLM prompts at significantly\nreduced lengths. An initial experimental evaluation of our approach on a\nhealthcare use case, involving multi-faceted analyses of patient anamneses\ngiven a set of medical concepts as well as a series of clinical guideline\nliterature, indicates its potential to optimize information density, coverage,\nand arrangement of LLM prompts while significantly reducing their lengths,\nwhich, in turn, may lead to reduced costs as well as more consistent and\nreliable LLM outputs."
                },
                "authors": [
                    {
                        "name": "Michael Banf"
                    },
                    {
                        "name": "Johannes Kuhn"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Kuhn"
                },
                "author": "Johannes Kuhn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19667v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19667v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16213v1",
                "updated": "2025-08-22T08:38:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    38,
                    16,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T08:38:16Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    38,
                    16,
                    4,
                    234,
                    0
                ],
                "title": "MedOmni-45: A Safety-Performance Benchmark for Reasoning-Oriented\n  LLMs in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedOmni-45: A Safety-Performance Benchmark for Reasoning-Oriented\n  LLMs in Medicine"
                },
                "summary": "With the increasing use of large language models (LLMs) in medical\ndecision-support, it is essential to evaluate not only their final answers but\nalso the reliability of their reasoning. Two key risks are Chain-of-Thought\n(CoT) faithfulness -- whether reasoning aligns with responses and medical facts\n-- and sycophancy, where models follow misleading cues over correctness.\nExisting benchmarks often collapse such vulnerabilities into single accuracy\nscores. To address this, we introduce MedOmni-45 Degrees, a benchmark and\nworkflow designed to quantify safety-performance trade-offs under manipulative\nhint conditions. It contains 1,804 reasoning-focused medical questions across\nsix specialties and three task types, including 500 from MedMCQA. Each question\nis paired with seven manipulative hint types and a no-hint baseline, producing\nabout 27K inputs. We evaluate seven LLMs spanning open- vs. closed-source,\ngeneral-purpose vs. medical, and base vs. reasoning-enhanced models, totaling\nover 189K inferences. Three metrics -- Accuracy, CoT-Faithfulness, and\nAnti-Sycophancy -- are combined into a composite score visualized with a 45\nDegrees plot. Results show a consistent safety-performance trade-off, with no\nmodel surpassing the diagonal. The open-source QwQ-32B performs closest (43.81\nDegrees), balancing safety and accuracy but not leading in both. MedOmni-45\nDegrees thus provides a focused benchmark for exposing reasoning\nvulnerabilities in medical LLMs and guiding safer model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing use of large language models (LLMs) in medical\ndecision-support, it is essential to evaluate not only their final answers but\nalso the reliability of their reasoning. Two key risks are Chain-of-Thought\n(CoT) faithfulness -- whether reasoning aligns with responses and medical facts\n-- and sycophancy, where models follow misleading cues over correctness.\nExisting benchmarks often collapse such vulnerabilities into single accuracy\nscores. To address this, we introduce MedOmni-45 Degrees, a benchmark and\nworkflow designed to quantify safety-performance trade-offs under manipulative\nhint conditions. It contains 1,804 reasoning-focused medical questions across\nsix specialties and three task types, including 500 from MedMCQA. Each question\nis paired with seven manipulative hint types and a no-hint baseline, producing\nabout 27K inputs. We evaluate seven LLMs spanning open- vs. closed-source,\ngeneral-purpose vs. medical, and base vs. reasoning-enhanced models, totaling\nover 189K inferences. Three metrics -- Accuracy, CoT-Faithfulness, and\nAnti-Sycophancy -- are combined into a composite score visualized with a 45\nDegrees plot. Results show a consistent safety-performance trade-off, with no\nmodel surpassing the diagonal. The open-source QwQ-32B performs closest (43.81\nDegrees), balancing safety and accuracy but not leading in both. MedOmni-45\nDegrees thus provides a focused benchmark for exposing reasoning\nvulnerabilities in medical LLMs and guiding safer model development."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Ji"
                    },
                    {
                        "name": "Yijin Guo"
                    },
                    {
                        "name": "Zicheng Zhang"
                    },
                    {
                        "name": "Xiangyang Zhu"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Guangtao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Zhai"
                },
                "author": "Guangtao Zhai",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16212v1",
                "updated": "2025-08-22T08:36:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    36,
                    58,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T08:36:58Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    36,
                    58,
                    4,
                    234,
                    0
                ],
                "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models"
                },
                "summary": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure.In addition, during cache reuse, we dynamically estimate the\ncorresponding noise and filter it out to reduce its impact on the sampling\ndirection.Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure.In addition, during cache reuse, we dynamically estimate the\ncorresponding noise and filter it out to reduce its impact on the sampling\ndirection.Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models."
                },
                "authors": [
                    {
                        "name": "Huanpeng Chu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Guanyu Fen"
                    },
                    {
                        "name": "Yutao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Zhang"
                },
                "author": "Yutao Zhang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16201v1",
                "updated": "2025-08-22T08:23:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    23,
                    9,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T08:23:09Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    23,
                    9,
                    4,
                    234,
                    0
                ],
                "title": "SpecVLM: Enhancing Speculative Decoding of Video LLMs via\n  Verifier-Guided Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecVLM: Enhancing Speculative Decoding of Video LLMs via\n  Verifier-Guided Token Pruning"
                },
                "summary": "Video large language models (Vid-LLMs) have shown strong capabilities in\nunderstanding video content. However, their reliance on dense video token\nrepresentations introduces substantial memory and computational overhead in\nboth prefilling and decoding. To mitigate the information loss of recent video\ntoken reduction methods and accelerate the decoding stage of Vid-LLMs\nlosslessly, we introduce SpecVLM, a training-free speculative decoding (SD)\nframework tailored for Vid-LLMs that incorporates staged video token pruning.\nBuilding on our novel finding that the draft model's speculation exhibits low\nsensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens,\nenabling efficient speculation without sacrificing accuracy. To achieve this,\nit performs a two-stage pruning process: Stage I selects highly informative\ntokens guided by attention signals from the verifier (target model), while\nStage II prunes remaining redundant ones in a spatially uniform manner.\nExtensive experiments on four video understanding benchmarks demonstrate the\neffectiveness and robustness of SpecVLM, which achieves up to 2.68$\\times$\ndecoding speedup for LLaVA-OneVision-72B and 2.11$\\times$ speedup for\nQwen2.5-VL-32B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (Vid-LLMs) have shown strong capabilities in\nunderstanding video content. However, their reliance on dense video token\nrepresentations introduces substantial memory and computational overhead in\nboth prefilling and decoding. To mitigate the information loss of recent video\ntoken reduction methods and accelerate the decoding stage of Vid-LLMs\nlosslessly, we introduce SpecVLM, a training-free speculative decoding (SD)\nframework tailored for Vid-LLMs that incorporates staged video token pruning.\nBuilding on our novel finding that the draft model's speculation exhibits low\nsensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens,\nenabling efficient speculation without sacrificing accuracy. To achieve this,\nit performs a two-stage pruning process: Stage I selects highly informative\ntokens guided by attention signals from the verifier (target model), while\nStage II prunes remaining redundant ones in a spatially uniform manner.\nExtensive experiments on four video understanding benchmarks demonstrate the\neffectiveness and robustness of SpecVLM, which achieves up to 2.68$\\times$\ndecoding speedup for LLaVA-OneVision-72B and 2.11$\\times$ speedup for\nQwen2.5-VL-32B."
                },
                "authors": [
                    {
                        "name": "Yicheng Ji"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Heming Xia"
                    },
                    {
                        "name": "Jinpeng Chen"
                    },
                    {
                        "name": "Lidan Shou"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Huan Li"
                    }
                ],
                "author_detail": {
                    "name": "Huan Li"
                },
                "author": "Huan Li",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04310v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04310v3",
                "updated": "2025-08-22T08:00:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    0,
                    53,
                    4,
                    234,
                    0
                ],
                "published": "2025-04-06T00:47:43Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    0,
                    47,
                    43,
                    6,
                    96,
                    0
                ],
                "title": "CO-Bench: Benchmarking Language Model Agents in Algorithm Search for\n  Combinatorial Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CO-Bench: Benchmarking Language Model Agents in Algorithm Search for\n  Combinatorial Optimization"
                },
                "summary": "Although LLM-based agents have attracted significant attention in domains\nsuch as software engineering and machine learning research, their role in\nadvancing combinatorial optimization (CO) remains relatively underexplored.\nThis gap underscores the need for a deeper understanding of their potential in\ntackling structured, constraint-intensive problems -- a pursuit currently\nlimited by the absence of comprehensive benchmarks for systematic\ninvestigation. To address this, we introduce CO-Bench, a benchmark suite\nfeaturing 36 real-world CO problems drawn from a broad range of domains and\ncomplexity levels. CO-Bench includes structured problem formulations and\ncurated data to support rigorous investigation of LLM agents. We evaluate\nmultiple agentic frameworks against established human-designed algorithms,\nrevealing the strengths and limitations of existing LLM agents and identifying\npromising directions for future research. CO-Bench is publicly available at\nhttps://github.com/sunnweiwei/CO-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although LLM-based agents have attracted significant attention in domains\nsuch as software engineering and machine learning research, their role in\nadvancing combinatorial optimization (CO) remains relatively underexplored.\nThis gap underscores the need for a deeper understanding of their potential in\ntackling structured, constraint-intensive problems -- a pursuit currently\nlimited by the absence of comprehensive benchmarks for systematic\ninvestigation. To address this, we introduce CO-Bench, a benchmark suite\nfeaturing 36 real-world CO problems drawn from a broad range of domains and\ncomplexity levels. CO-Bench includes structured problem formulations and\ncurated data to support rigorous investigation of LLM agents. We evaluate\nmultiple agentic frameworks against established human-designed algorithms,\nrevealing the strengths and limitations of existing LLM agents and identifying\npromising directions for future research. CO-Bench is publicly available at\nhttps://github.com/sunnweiwei/CO-Bench."
                },
                "authors": [
                    {
                        "name": "Weiwei Sun"
                    },
                    {
                        "name": "Shengyu Feng"
                    },
                    {
                        "name": "Shanda Li"
                    },
                    {
                        "name": "Yiming Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Yang"
                },
                "author": "Yiming Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04310v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04310v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16185v1",
                "updated": "2025-08-22T07:59:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    59,
                    37,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T07:59:37Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    59,
                    37,
                    4,
                    234,
                    0
                ],
                "title": "ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding\n  on Indic Subjects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding\n  on Indic Subjects"
                },
                "summary": "Large language models (LLMs) have been widely evaluated on tasks such as\ncomprehension, question answering, summarization, code generation, etc.\nHowever, their performance on graduate-level, culturally grounded questions in\nthe Indian context remains largely unexplored. Existing Indian benchmarks\nemphasise basic fact-orientated queries that offer limited assessment of a\ndeeper disciplinary understanding tailored to the Indian setting. In this\npaper, we present ParamBench, consisting of around 11.5K questions in Hindi\nlanguage comprising questionnaires from 16 diverse subjects. These questions\nare primarily derived from nation-wide graduate level entrance examination\ncovering topics such as history, music, instruments, yoga, literature,\nphilosophy, law, etc., specifically for the Indian context. Additionally, we\nassess the ability of LLMs to handle diverse question formats-such as\nlist-based matching, assertion-reason pairs, and sequence ordering-alongside\nconventional multiple-choice questions. We evaluated the performance of more\nthan 17 open source LLMs on this benchmark, observing that Llama 3.3 70B\nattains the highest overall accuracy of 48%. Furthermore, subject-wise analysis\nindicates that even for the best performing LLMs, performance remains weak on\ntopics such as music, classical instruments, politics and archaeology,\nunderscoring persistent challenges in culturally grounded reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely evaluated on tasks such as\ncomprehension, question answering, summarization, code generation, etc.\nHowever, their performance on graduate-level, culturally grounded questions in\nthe Indian context remains largely unexplored. Existing Indian benchmarks\nemphasise basic fact-orientated queries that offer limited assessment of a\ndeeper disciplinary understanding tailored to the Indian setting. In this\npaper, we present ParamBench, consisting of around 11.5K questions in Hindi\nlanguage comprising questionnaires from 16 diverse subjects. These questions\nare primarily derived from nation-wide graduate level entrance examination\ncovering topics such as history, music, instruments, yoga, literature,\nphilosophy, law, etc., specifically for the Indian context. Additionally, we\nassess the ability of LLMs to handle diverse question formats-such as\nlist-based matching, assertion-reason pairs, and sequence ordering-alongside\nconventional multiple-choice questions. We evaluated the performance of more\nthan 17 open source LLMs on this benchmark, observing that Llama 3.3 70B\nattains the highest overall accuracy of 48%. Furthermore, subject-wise analysis\nindicates that even for the best performing LLMs, performance remains weak on\ntopics such as music, classical instruments, politics and archaeology,\nunderscoring persistent challenges in culturally grounded reasoning."
                },
                "authors": [
                    {
                        "name": "Kaushal Sharma"
                    },
                    {
                        "name": "Vivek Patel"
                    },
                    {
                        "name": "Ayush Maheshwari"
                    },
                    {
                        "name": "Aditya Maheshwari"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Maheshwari"
                },
                "author": "Aditya Maheshwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10454v2",
                "updated": "2025-08-22T07:58:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    58,
                    49,
                    4,
                    234,
                    0
                ],
                "published": "2025-02-12T02:01:10Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    2,
                    1,
                    10,
                    2,
                    43,
                    0
                ],
                "title": "One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual\n  Reasoning in Mathematical LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual\n  Reasoning in Mathematical LLMs"
                },
                "summary": "Leveraging mathematical Large Language Models (LLMs) for proof generation is\na fundamental topic in LLMs research. We argue that the ability of current LLMs\nto prove statements largely depends on whether they have encountered the\nrelevant proof process during training. This reliance limits their deeper\nunderstanding of mathematical theorems and related concepts. Inspired by the\npedagogical method of \"proof by counterexamples\" commonly used in human\nmathematics education, our work aims to enhance LLMs' ability to conduct\nmathematical reasoning and proof through counterexamples. Specifically, we\nmanually create a high-quality, university-level mathematical benchmark,\nCounterMATH, which requires LLMs to prove mathematical statements by providing\ncounterexamples, thereby assessing their grasp of mathematical concepts.\nAdditionally, we develop a data engineering framework to automatically obtain\ntraining data for further model improvement. Extensive experiments and detailed\nanalyses demonstrate that CounterMATH is challenging, indicating that LLMs,\nsuch as OpenAI o1, have insufficient counterexample-driven proof capabilities.\nMoreover, our exploration into model training reveals that strengthening LLMs'\ncounterexample-driven conceptual reasoning abilities is crucial for improving\ntheir overall mathematical capabilities. We believe that our work offers new\nperspectives on the community of mathematical LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging mathematical Large Language Models (LLMs) for proof generation is\na fundamental topic in LLMs research. We argue that the ability of current LLMs\nto prove statements largely depends on whether they have encountered the\nrelevant proof process during training. This reliance limits their deeper\nunderstanding of mathematical theorems and related concepts. Inspired by the\npedagogical method of \"proof by counterexamples\" commonly used in human\nmathematics education, our work aims to enhance LLMs' ability to conduct\nmathematical reasoning and proof through counterexamples. Specifically, we\nmanually create a high-quality, university-level mathematical benchmark,\nCounterMATH, which requires LLMs to prove mathematical statements by providing\ncounterexamples, thereby assessing their grasp of mathematical concepts.\nAdditionally, we develop a data engineering framework to automatically obtain\ntraining data for further model improvement. Extensive experiments and detailed\nanalyses demonstrate that CounterMATH is challenging, indicating that LLMs,\nsuch as OpenAI o1, have insufficient counterexample-driven proof capabilities.\nMoreover, our exploration into model training reveals that strengthening LLMs'\ncounterexample-driven conceptual reasoning abilities is crucial for improving\ntheir overall mathematical capabilities. We believe that our work offers new\nperspectives on the community of mathematical LLMs."
                },
                "authors": [
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Jiayi Kuang"
                    },
                    {
                        "name": "Haojing Huang"
                    },
                    {
                        "name": "Zhikun Xu"
                    },
                    {
                        "name": "Xinnian Liang"
                    },
                    {
                        "name": "Yi Yu"
                    },
                    {
                        "name": "Wenlian Lu"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Xiaoyu Tan"
                    },
                    {
                        "name": "Chao Qu"
                    },
                    {
                        "name": "Ying Shen"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16181v1",
                "updated": "2025-08-22T07:56:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    56,
                    33,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T07:56:33Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    56,
                    33,
                    4,
                    234,
                    0
                ],
                "title": "LLM-Assisted Semantic Alignment and Integration in Collaborative\n  Model-Based Systems Engineering Using SysML v2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Assisted Semantic Alignment and Integration in Collaborative\n  Model-Based Systems Engineering Using SysML v2"
                },
                "summary": "Cross-organizational collaboration in Model-Based Systems Engineering (MBSE)\nfaces many challenges in achieving semantic alignment across independently\ndeveloped system models. SysML v2 introduces enhanced structural modularity and\nformal semantics, offering a stronger foundation for interoperable modeling.\nMeanwhile, GPT-based Large Language Models (LLMs) provide new capabilities for\nassisting model understanding and integration. This paper proposes a\nstructured, prompt-driven approach for LLM-assisted semantic alignment of SysML\nv2 models. The core contribution lies in the iterative development of an\nalignment approach and interaction prompts, incorporating model extraction,\nsemantic matching, and verification. The approach leverages SysML v2 constructs\nsuch as alias, import, and metadata extensions to support traceable, soft\nalignment integration. It is demonstrated with a GPT-based LLM through an\nexample of a measurement system. Benefits and limitations are discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-organizational collaboration in Model-Based Systems Engineering (MBSE)\nfaces many challenges in achieving semantic alignment across independently\ndeveloped system models. SysML v2 introduces enhanced structural modularity and\nformal semantics, offering a stronger foundation for interoperable modeling.\nMeanwhile, GPT-based Large Language Models (LLMs) provide new capabilities for\nassisting model understanding and integration. This paper proposes a\nstructured, prompt-driven approach for LLM-assisted semantic alignment of SysML\nv2 models. The core contribution lies in the iterative development of an\nalignment approach and interaction prompts, incorporating model extraction,\nsemantic matching, and verification. The approach leverages SysML v2 constructs\nsuch as alias, import, and metadata extensions to support traceable, soft\nalignment integration. It is demonstrated with a GPT-based LLM through an\nexample of a measurement system. Benefits and limitations are discussed."
                },
                "authors": [
                    {
                        "name": "Zirui Li"
                    },
                    {
                        "name": "Stephan Husung"
                    },
                    {
                        "name": "Haoze Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoze Wang"
                },
                "author": "Haoze Wang",
                "arxiv_comment": "Accepted by IEEE ISSE 2025, DOI pending",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16175v1",
                "updated": "2025-08-22T07:51:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    51,
                    46,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T07:51:46Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    51,
                    46,
                    4,
                    234,
                    0
                ],
                "title": "Planning for future EV charging infrastructure: A city-scale assessment\n  of demand and capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning for future EV charging infrastructure: A city-scale assessment\n  of demand and capacity"
                },
                "summary": "As the global shift toward transportation electrification has accelerated,\ncapacity planning for electric vehicle (EV) charging infrastructure has become\na critical challenge in the development of low-carbon urban energy systems.\nThis study proposes the first demand-driven, multi-objective planning model for\noptimizing city-scale capacity allocation of EV charging infrastructure. The\nmodel employs a bottom-up approach to estimate charging demand differentiated\nby vehicle type-battery electric vehicles (BEVs), extended-range electric\nvehicles (EREVs), and plug-in hybrid electric vehicles (PHEVs). Chongqing, a\nrapidly expanding EV hub in China with a strong industrial base, supportive\npolicies, and diverse urban morphologies, is selected as the case study. The\nresults show that (1) monthly EV electricity consumption in Chongqing rose from\n18.9 gigawatt-hours (GWh) in June 2022 to 57.5 GWh in December 2024, with\nassociated carbon emissions increasing from 9.9 kilotons of carbon dioxide\n(ktCO2) to 30 ktCO2, driven primarily by BEVs; (2) 181,622 additional charging\npiles were installed between 2022 and 2024, concentrated in densely populated\nareas, reflecting a demand-responsive strategy that prioritizes population\ndensity over geographic coverage; and (3) between 2025 and 2030, EV electricity\ndemand is projected to reach 1940 GWh, with the number of charging piles\nexceeding 1.4 million, and charging demand from EREVs and PHEVs expected to\novertake BEVs later in the period. While Chongqing serves as the pilot area,\nthe proposed planning platform is adaptable for application in cities\nworldwide, enabling cross-regional comparisons under diverse socio-economic,\ngeographic, and policy conditions. Overall, this work offers policymakers a\nversatile tool to support sustainable, cost-effective EV infrastructure\ndeployment aligned with low-carbon electrification targets in the\ntransportation sector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the global shift toward transportation electrification has accelerated,\ncapacity planning for electric vehicle (EV) charging infrastructure has become\na critical challenge in the development of low-carbon urban energy systems.\nThis study proposes the first demand-driven, multi-objective planning model for\noptimizing city-scale capacity allocation of EV charging infrastructure. The\nmodel employs a bottom-up approach to estimate charging demand differentiated\nby vehicle type-battery electric vehicles (BEVs), extended-range electric\nvehicles (EREVs), and plug-in hybrid electric vehicles (PHEVs). Chongqing, a\nrapidly expanding EV hub in China with a strong industrial base, supportive\npolicies, and diverse urban morphologies, is selected as the case study. The\nresults show that (1) monthly EV electricity consumption in Chongqing rose from\n18.9 gigawatt-hours (GWh) in June 2022 to 57.5 GWh in December 2024, with\nassociated carbon emissions increasing from 9.9 kilotons of carbon dioxide\n(ktCO2) to 30 ktCO2, driven primarily by BEVs; (2) 181,622 additional charging\npiles were installed between 2022 and 2024, concentrated in densely populated\nareas, reflecting a demand-responsive strategy that prioritizes population\ndensity over geographic coverage; and (3) between 2025 and 2030, EV electricity\ndemand is projected to reach 1940 GWh, with the number of charging piles\nexceeding 1.4 million, and charging demand from EREVs and PHEVs expected to\novertake BEVs later in the period. While Chongqing serves as the pilot area,\nthe proposed planning platform is adaptable for application in cities\nworldwide, enabling cross-regional comparisons under diverse socio-economic,\ngeographic, and policy conditions. Overall, this work offers policymakers a\nversatile tool to support sustainable, cost-effective EV infrastructure\ndeployment aligned with low-carbon electrification targets in the\ntransportation sector."
                },
                "authors": [
                    {
                        "name": "Hong Yuan"
                    },
                    {
                        "name": "Minda Ma"
                    },
                    {
                        "name": "Nan Zhou"
                    },
                    {
                        "name": "Yanqiao Deng"
                    },
                    {
                        "name": "Junhong Liu"
                    },
                    {
                        "name": "Shufan Zhang"
                    },
                    {
                        "name": "Zhili Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhili Ma"
                },
                "author": "Zhili Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16172v1",
                "updated": "2025-08-22T07:50:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    50,
                    57,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T07:50:57Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    50,
                    57,
                    4,
                    234,
                    0
                ],
                "title": "Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent\n  with Preference Chain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent\n  with Preference Chain"
                },
                "summary": "Understanding human behavior in urban environments is a crucial field within\ncity sciences. However, collecting accurate behavioral data, particularly in\nnewly developed areas, poses significant challenges. Recent advances in\ngenerative agents, powered by Large Language Models (LLMs), have shown promise\nin simulating human behaviors without relying on extensive datasets.\nNevertheless, these methods often struggle with generating consistent,\ncontext-sensitive, and realistic behavioral outputs. To address these\nlimitations, this paper introduces the Preference Chain, a novel method that\nintegrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance\ncontext-aware simulation of human behavior in transportation systems.\nExperiments conducted on the Replica dataset demonstrate that the Preference\nChain outperforms standard LLM in aligning with real-world transportation mode\nchoices. The development of the Mobility Agent highlights potential\napplications of proposed method in urban mobility modeling for emerging cities,\npersonalized travel behavior analysis, and dynamic traffic forecasting. Despite\nlimitations such as slow inference and the risk of hallucination, the method\noffers a promising framework for simulating complex human behavior in\ndata-scarce environments, where traditional data-driven models struggle due to\nlimited data availability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding human behavior in urban environments is a crucial field within\ncity sciences. However, collecting accurate behavioral data, particularly in\nnewly developed areas, poses significant challenges. Recent advances in\ngenerative agents, powered by Large Language Models (LLMs), have shown promise\nin simulating human behaviors without relying on extensive datasets.\nNevertheless, these methods often struggle with generating consistent,\ncontext-sensitive, and realistic behavioral outputs. To address these\nlimitations, this paper introduces the Preference Chain, a novel method that\nintegrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance\ncontext-aware simulation of human behavior in transportation systems.\nExperiments conducted on the Replica dataset demonstrate that the Preference\nChain outperforms standard LLM in aligning with real-world transportation mode\nchoices. The development of the Mobility Agent highlights potential\napplications of proposed method in urban mobility modeling for emerging cities,\npersonalized travel behavior analysis, and dynamic traffic forecasting. Despite\nlimitations such as slow inference and the risk of hallucination, the method\noffers a promising framework for simulating complex human behavior in\ndata-scarce environments, where traditional data-driven models struggle due to\nlimited data availability."
                },
                "authors": [
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Parfait Atchade-Adelomou"
                    },
                    {
                        "name": "Carlo Adornetto"
                    },
                    {
                        "name": "Adrian Mora-Carrero"
                    },
                    {
                        "name": "Luis Alonso-Pastor"
                    },
                    {
                        "name": "Ariel Noyman"
                    },
                    {
                        "name": "Yubo Liu"
                    },
                    {
                        "name": "Kent Larson"
                    }
                ],
                "author_detail": {
                    "name": "Kent Larson"
                },
                "author": "Kent Larson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16169v1",
                "updated": "2025-08-22T07:45:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    45,
                    34,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T07:45:34Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    45,
                    34,
                    4,
                    234,
                    0
                ],
                "title": "A Scalable Hybrid Track-Before-Detect Tracking System: Application to\n  Coastal Maritime Radar Surveillance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Hybrid Track-Before-Detect Tracking System: Application to\n  Coastal Maritime Radar Surveillance"
                },
                "summary": "Despite their theoretical advantages, track-before-detect (TBD) methods\nremain largely absent from real-world multi-target tracking applications due to\ntheir computational complexity and limited scalability. This paper presents a\nscalable hybrid tracking framework that combines a TBD multi-target tracking\nalgorithm with a detection-based multi-target tracking algorithm for coastal\nradar surveillance. In particular, the approach uses an integrated existence\nPoisson histogram-probabilistic multi-hypothesis tracking (IE-PHPMHT)-based TBD\nmodule with a conventional Poisson multi-Bernoulli Mixture (PMBM) point\ntracker. The system processes raw radar data through land clutter suppression,\ncell-wise detection, and clustering-based feature extraction. High-threshold\ndetections are used to track strong targets via the point tracker, while\nlow-threshold detections are employed for adaptive birth in the TBD module,\nenabling early initiation and sustained tracking of weak or ambiguous targets.\nValidated using real X-band radar data from the Trondheim Fjord, Norway, the\napproach demonstrates robust multi-target tracking performance in a full-scale\napplication with a large observation area under resource constraints,\nhighlighting its suitability for operational deployment in complex maritime\nenvironments needed for coastal surveillance and to support autonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their theoretical advantages, track-before-detect (TBD) methods\nremain largely absent from real-world multi-target tracking applications due to\ntheir computational complexity and limited scalability. This paper presents a\nscalable hybrid tracking framework that combines a TBD multi-target tracking\nalgorithm with a detection-based multi-target tracking algorithm for coastal\nradar surveillance. In particular, the approach uses an integrated existence\nPoisson histogram-probabilistic multi-hypothesis tracking (IE-PHPMHT)-based TBD\nmodule with a conventional Poisson multi-Bernoulli Mixture (PMBM) point\ntracker. The system processes raw radar data through land clutter suppression,\ncell-wise detection, and clustering-based feature extraction. High-threshold\ndetections are used to track strong targets via the point tracker, while\nlow-threshold detections are employed for adaptive birth in the TBD module,\nenabling early initiation and sustained tracking of weak or ambiguous targets.\nValidated using real X-band radar data from the Trondheim Fjord, Norway, the\napproach demonstrates robust multi-target tracking performance in a full-scale\napplication with a large observation area under resource constraints,\nhighlighting its suitability for operational deployment in complex maritime\nenvironments needed for coastal surveillance and to support autonomy."
                },
                "authors": [
                    {
                        "name": "Lukas Herrmann"
                    },
                    {
                        "name": "ngel F. Garca-Fernndez"
                    },
                    {
                        "name": "Edmund F. Brekke"
                    },
                    {
                        "name": "Egil Eide"
                    }
                ],
                "author_detail": {
                    "name": "Egil Eide"
                },
                "author": "Egil Eide",
                "arxiv_comment": "Submitted for possible publication in IEEE Journal of Oceanic\n  Engineering (JOE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16165v1",
                "updated": "2025-08-22T07:38:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    38,
                    37,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T07:38:37Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    38,
                    37,
                    4,
                    234,
                    0
                ],
                "title": "Towards Recommending Usability Improvements with Multimodal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Recommending Usability Improvements with Multimodal Large\n  Language Models"
                },
                "summary": "Usability describes a set of essential quality attributes of user interfaces\n(UI) that influence human-computer interaction. Common evaluation methods, such\nas usability testing and inspection, are effective but resource-intensive and\nrequire expert involvement. This makes them less accessible for smaller\norganizations. Recent advances in multimodal LLMs offer promising opportunities\nto automate usability evaluation processes partly by analyzing textual, visual,\nand structural aspects of software interfaces. To investigate this possibility,\nwe formulate usability evaluation as a recommendation task, where multimodal\nLLMs rank usability issues by severity. We conducted an initial\nproof-of-concept study to compare LLM-generated usability improvement\nrecommendations with usability expert assessments. Our findings indicate the\npotential of LLMs to enable faster and more cost-effective usability\nevaluation, which makes it a practical alternative in contexts with limited\nexpert resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Usability describes a set of essential quality attributes of user interfaces\n(UI) that influence human-computer interaction. Common evaluation methods, such\nas usability testing and inspection, are effective but resource-intensive and\nrequire expert involvement. This makes them less accessible for smaller\norganizations. Recent advances in multimodal LLMs offer promising opportunities\nto automate usability evaluation processes partly by analyzing textual, visual,\nand structural aspects of software interfaces. To investigate this possibility,\nwe formulate usability evaluation as a recommendation task, where multimodal\nLLMs rank usability issues by severity. We conducted an initial\nproof-of-concept study to compare LLM-generated usability improvement\nrecommendations with usability expert assessments. Our findings indicate the\npotential of LLMs to enable faster and more cost-effective usability\nevaluation, which makes it a practical alternative in contexts with limited\nexpert resources."
                },
                "authors": [
                    {
                        "name": "Sebastian Lubos"
                    },
                    {
                        "name": "Alexander Felfernig"
                    },
                    {
                        "name": "Gerhard Leitner"
                    },
                    {
                        "name": "Julian Schwazer"
                    }
                ],
                "author_detail": {
                    "name": "Julian Schwazer"
                },
                "author": "Julian Schwazer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]