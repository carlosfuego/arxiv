[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.08795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v1",
                "updated": "2024-08-16T15:11:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v1",
                "updated": "2024-08-13T13:14:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Büchner"
                    },
                    {
                        "name": "Leonardo Agudo Jácome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jácome"
                },
                "author": "Leonardo Agudo Jácome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adrià Armejach"
                    },
                    {
                        "name": "Miquel Moretó"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moretó"
                },
                "author": "Miquel Moretó",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19604v1",
                "updated": "2024-07-28T22:34:20Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T22:34:20Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "title": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning"
                },
                "summary": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kyle Kuan"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/IGSC48788.2019.8957182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IGSC48788.2019.8957182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: 2019 Tenth International Green and Sustainable\n  Computing Conference (IGSC)",
                "arxiv_journal_ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19318v1",
                "updated": "2024-07-27T18:26:32Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T18:26:32Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "title": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review"
                },
                "summary": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture."
                },
                "authors": [
                    {
                        "name": "Anujkumarsinh Donvir"
                    },
                    {
                        "name": "Apeksha Jain"
                    },
                    {
                        "name": "Pradeep Kumar Saraswathi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Kumar Saraswathi"
                },
                "author": "Pradeep Kumar Saraswathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v2",
                "updated": "2024-07-27T08:52:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    52,
                    39,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference"
                },
                "summary": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19205v1",
                "updated": "2024-07-27T08:21:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T08:21:14Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions"
                },
                "summary": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Ashkan Taghipour"
                    },
                    {
                        "name": "Morteza Ghahremani"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Aref Miri Rekavandi"
                    },
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Hamid Laga"
                    },
                    {
                        "name": "Farid Boussaid"
                    }
                ],
                "author_detail": {
                    "name": "Farid Boussaid"
                },
                "author": "Farid Boussaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19090v1",
                "updated": "2024-07-26T21:11:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "published": "2024-07-26T21:11:58Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores"
                },
                "summary": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance."
                },
                "authors": [
                    {
                        "name": "Alireza Heidari"
                    },
                    {
                        "name": "Amirhossein Ahmadi"
                    },
                    {
                        "name": "Zefeng Zhi"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Cloud Databases",
                "arxiv_journal_ref": "VLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18121v1",
                "updated": "2024-07-25T15:29:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T15:29:05Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache"
                },
                "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Guangyi Chen"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02750v2",
                "updated": "2024-07-25T09:16:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    16,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-02-05T06:06:47Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    6,
                    6,
                    47,
                    0,
                    36,
                    0
                ],
                "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
                },
                "summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_doi": "10.13140/RG.2.2.28167.37282",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.28167.37282",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.02750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICML2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20272v1",
                "updated": "2024-07-25T07:50:17Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T07:50:17Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Inference Framework for Early-exit Large Language Models"
                },
                "summary": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up."
                },
                "authors": [
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yihan Yan"
                    },
                    {
                        "name": "Xinshuo Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v1",
                "updated": "2024-07-25T00:27:07Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08711v3",
                "updated": "2024-07-24T13:36:03Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    13,
                    36,
                    3,
                    2,
                    206,
                    0
                ],
                "published": "2023-01-20T18:13:38Z",
                "published_parsed": [
                    2023,
                    1,
                    20,
                    18,
                    13,
                    38,
                    4,
                    20,
                    0
                ],
                "title": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers"
                },
                "summary": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers."
                },
                "authors": [
                    {
                        "name": "Qifa Yan"
                    },
                    {
                        "name": "Xiaohu Tang"
                    },
                    {
                        "name": "Zhengchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengchun Zhou"
                },
                "author": "Zhengchun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15771v2",
                "updated": "2024-07-24T12:56:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    56,
                    41,
                    2,
                    206,
                    0
                ],
                "published": "2024-03-13T17:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    47,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations"
                },
                "summary": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations."
                },
                "authors": [
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15569v2",
                "updated": "2024-07-24T08:56:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    56,
                    11,
                    2,
                    206,
                    0
                ],
                "published": "2024-01-28T05:12:09Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    5,
                    12,
                    9,
                    6,
                    28,
                    0
                ],
                "title": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs"
                },
                "summary": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted by IJCAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09636v2",
                "updated": "2024-07-23T17:55:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    55,
                    30,
                    1,
                    205,
                    0
                ],
                "published": "2024-03-14T17:59:26Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    59,
                    26,
                    3,
                    74,
                    0
                ],
                "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"
                },
                "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "David Tarjan"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024) 37396-37412",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16672v1",
                "updated": "2024-07-23T17:42:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T17:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "title": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications"
                },
                "summary": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications."
                },
                "authors": [
                    {
                        "name": "Sahan Liyanaarachchi"
                    },
                    {
                        "name": "Stavros Mitrolaris"
                    },
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16303v1",
                "updated": "2024-07-23T08:58:06Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:58:06Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "title": "Hidden Web Caches Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Web Caches Discovery"
                },
                "summary": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers."
                },
                "authors": [
                    {
                        "name": "Matteo Golinelli"
                    },
                    {
                        "name": "Bruno Crispo"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Crispo"
                },
                "author": "Bruno Crispo",
                "arxiv_doi": "10.1145/3678890.3678931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678890.3678931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.16303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The definitive Version of Record was published in The 27th\n  International Symposium on Research in Attacks, Intrusions and Defenses (RAID\n  2024), September 30-October 02, 2024, Padua, Italy,\n  https://doi.org/10.1145/3678890.3678931",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v1",
                "updated": "2024-07-23T08:55:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16286v1",
                "updated": "2024-07-23T08:40:27Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:40:27Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "title": "A deeper look at depth pruning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A deeper look at depth pruning of LLMs"
                },
                "summary": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Thomas Breuel"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15309v1",
                "updated": "2024-07-22T14:37:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T14:37:58Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "title": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15581v1",
                "updated": "2024-07-22T12:17:01Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T12:17:01Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "title": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores"
                },
                "summary": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget."
                },
                "authors": [
                    {
                        "name": "Giorgos Xanthakis"
                    },
                    {
                        "name": "Antonios Katsarakis"
                    },
                    {
                        "name": "Giorgos Saloustros"
                    },
                    {
                        "name": "Angelos Bilas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Bilas"
                },
                "author": "Angelos Bilas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.11055v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.11055v5",
                "updated": "2024-07-22T10:02:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    10,
                    2,
                    57,
                    0,
                    204,
                    0
                ],
                "published": "2022-12-21T14:59:23Z",
                "published_parsed": [
                    2022,
                    12,
                    21,
                    14,
                    59,
                    23,
                    2,
                    355,
                    0
                ],
                "title": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi"
                },
                "summary": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities."
                },
                "authors": [
                    {
                        "name": "Daniel Hausmann"
                    },
                    {
                        "name": "Lutz Schröder"
                    }
                ],
                "author_detail": {
                    "name": "Lutz Schröder"
                },
                "author": "Lutz Schröder",
                "arxiv_doi": "10.46298/lmcs-20(3:9)2024",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.46298/lmcs-20(3:9)2024",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2212.11055v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.11055v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Logical Methods in Computer Science, Volume 20, Issue 3 (July 23,\n  2024) lmcs:10532",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B70, 03B44",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15360v1",
                "updated": "2024-07-22T04:07:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T04:07:26Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "title": "Dissecting Multiplication in Transformers: Insights into LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Multiplication in Transformers: Insights into LLMs"
                },
                "summary": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications."
                },
                "authors": [
                    {
                        "name": "Luyu Qiu"
                    },
                    {
                        "name": "Jianing Li"
                    },
                    {
                        "name": "Chi Su"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15891v1",
                "updated": "2024-07-22T01:12:23Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T01:12:23Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "title": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads"
                },
                "summary": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model."
                },
                "authors": [
                    {
                        "name": "Hanlin Tang"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Qingsen Han"
                    },
                    {
                        "name": "Shikuan Hong"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Gongyi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gongyi Wang"
                },
                "author": "Gongyi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15264v1",
                "updated": "2024-07-21T20:41:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T20:41:39Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "title": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme"
                },
                "summary": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training"
                },
                "authors": [
                    {
                        "name": "Jeongmin Brian Park"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Zaid Quresh"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Wen-mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-mei Hwu"
                },
                "author": "Wen-mei Hwu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15176v1",
                "updated": "2024-07-21T14:23:37Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T14:23:37Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "title": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope"
                },
                "summary": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Kai Lv"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.00250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.00250v3",
                "updated": "2024-07-21T11:47:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    11,
                    47,
                    4,
                    6,
                    203,
                    0
                ],
                "published": "2022-12-01T03:35:14Z",
                "published_parsed": [
                    2022,
                    12,
                    1,
                    3,
                    35,
                    14,
                    3,
                    335,
                    0
                ],
                "title": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy"
                },
                "summary": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients."
                },
                "authors": [
                    {
                        "name": "Ngoc Duy Pham"
                    },
                    {
                        "name": "Tran Khoa Phan"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Doan Nguyen"
                    },
                    {
                        "name": "Naveen Chilamkurti"
                    }
                ],
                "author_detail": {
                    "name": "Naveen Chilamkurti"
                },
                "author": "Naveen Chilamkurti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.00250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.00250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08454v2",
                "updated": "2024-07-21T02:37:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    2,
                    37,
                    11,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-11T12:50:42Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    12,
                    50,
                    42,
                    3,
                    193,
                    0
                ],
                "title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks"
                },
                "summary": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Boxiao Jin"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10516v2",
                "updated": "2024-07-20T22:14:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    22,
                    14,
                    42,
                    5,
                    202,
                    0
                ],
                "published": "2023-03-28T03:55:47Z",
                "published_parsed": [
                    2023,
                    3,
                    28,
                    3,
                    55,
                    47,
                    1,
                    87,
                    0
                ],
                "title": "Distributed Neural Representation for Reactive in situ Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Neural Representation for Reactive in situ Visualization"
                },
                "summary": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations."
                },
                "authors": [
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Joseph A. Insley"
                    },
                    {
                        "name": "Victor A. Mateevitsi"
                    },
                    {
                        "name": "Silvio Rizzi"
                    },
                    {
                        "name": "Michael E. Papka"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14801v1",
                "updated": "2024-07-20T08:21:46Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "published": "2024-07-20T08:21:46Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "title": "SquareSort: a cache-oblivious sorting algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SquareSort: a cache-oblivious sorting algorithm"
                },
                "summary": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort."
                },
                "authors": [
                    {
                        "name": "Michal Koucký"
                    },
                    {
                        "name": "Josef Matějka"
                    }
                ],
                "author_detail": {
                    "name": "Josef Matějka"
                },
                "author": "Josef Matějka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07240v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07240v6",
                "updated": "2024-07-19T21:04:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    21,
                    4,
                    14,
                    4,
                    201,
                    0
                ],
                "published": "2023-10-11T07:08:20Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    7,
                    8,
                    20,
                    2,
                    284,
                    0
                ],
                "title": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving"
                },
                "summary": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Michael Maire"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "arxiv_comment": "SIGCOMM'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07240v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07240v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v1",
                "updated": "2024-07-19T14:28:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "8 pages, 8 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v5",
                "updated": "2024-07-19T09:37:19Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    9,
                    37,
                    19,
                    4,
                    201,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14057v1",
                "updated": "2024-07-19T06:34:45Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T06:34:45Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "title": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference"
                },
                "summary": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Qichen Fu"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Thomas Merth"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Mahyar Najibi"
                    }
                ],
                "author_detail": {
                    "name": "Mahyar Najibi"
                },
                "author": "Mahyar Najibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v1",
                "updated": "2024-07-18T18:47:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Data-driven Forecasting of Deep Learning Performance on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven Forecasting of Deep Learning Performance on GPUs"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03482v2",
                "updated": "2024-07-18T16:31:29Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    16,
                    31,
                    29,
                    3,
                    200,
                    0
                ],
                "published": "2024-06-05T17:42:05Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    17,
                    42,
                    5,
                    2,
                    157,
                    0
                ],
                "title": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead"
                },
                "summary": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Insu Han"
                    }
                ],
                "author_detail": {
                    "name": "Insu Han"
                },
                "author": "Insu Han",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.12925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.12925v2",
                "updated": "2024-07-18T09:06:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    9,
                    6,
                    0,
                    3,
                    200,
                    0
                ],
                "published": "2023-09-22T15:23:57Z",
                "published_parsed": [
                    2023,
                    9,
                    22,
                    15,
                    23,
                    57,
                    4,
                    265,
                    0
                ],
                "title": "MCU-Wide Timing Side Channels and Their Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCU-Wide Timing Side Channels and Their Detection"
                },
                "summary": "Microarchitectural timing side channels have been thoroughly investigated as\na security threat in hardware designs featuring shared buffers (e.g., caches)\nor parallelism between attacker and victim task execution. However,\ncontradicting common intuitions, recent activities demonstrate that this threat\nis real even in microcontroller SoCs without such features. In this paper, we\ndescribe SoC-wide timing side channels previously neglected by security\nanalysis and present a new formal method to close this gap. In a case study on\nthe RISC-V Pulpissimo SoC, our method detected a vulnerability to a previously\nunknown attack variant that allows an attacker to obtain information about a\nvictim's memory access behavior. After implementing a conservative fix, we were\nable to verify that the SoC is now secure w.r.t. the considered class of timing\nside channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural timing side channels have been thoroughly investigated as\na security threat in hardware designs featuring shared buffers (e.g., caches)\nor parallelism between attacker and victim task execution. However,\ncontradicting common intuitions, recent activities demonstrate that this threat\nis real even in microcontroller SoCs without such features. In this paper, we\ndescribe SoC-wide timing side channels previously neglected by security\nanalysis and present a new formal method to close this gap. In a case study on\nthe RISC-V Pulpissimo SoC, our method detected a vulnerability to a previously\nunknown attack variant that allows an attacker to obtain information about a\nvictim's memory access behavior. After implementing a conservative fix, we were\nable to verify that the SoC is now secure w.r.t. the considered class of timing\nside channels."
                },
                "authors": [
                    {
                        "name": "Johannes Müller"
                    },
                    {
                        "name": "Anna Lena Duque Antón"
                    },
                    {
                        "name": "Lucas Deutschmann"
                    },
                    {
                        "name": "Dino Mehmedagić"
                    },
                    {
                        "name": "Cristiano Rodrigues"
                    },
                    {
                        "name": "Daniel Oliveira"
                    },
                    {
                        "name": "Keerthikumara Devarajegowda"
                    },
                    {
                        "name": "Mohammad Rahmani Fadiheh"
                    },
                    {
                        "name": "Sandro Pinto"
                    },
                    {
                        "name": "Dominik Stoffel"
                    },
                    {
                        "name": "Wolfgang Kunz"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Kunz"
                },
                "author": "Wolfgang Kunz",
                "arxiv_doi": "10.1145/3649329.3656541",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3656541",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.12925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.12925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This version extends the work of the previous version and was\n  accepted and presented at DAC'24",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v3",
                "updated": "2024-07-18T06:18:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    6,
                    18,
                    4,
                    3,
                    200,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02747v2",
                "updated": "2024-07-17T23:09:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    23,
                    9,
                    10,
                    2,
                    199,
                    0
                ],
                "published": "2024-04-03T13:44:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    13,
                    44,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "Faster Diffusion via Temporal Attention Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Diffusion via Temporal Attention Decomposition"
                },
                "summary": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."
                },
                "authors": [
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Wentian Zhang"
                    },
                    {
                        "name": "Jinheng Xie"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Mengmeng Xu"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12850v2",
                "updated": "2024-07-17T16:56:18Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    16,
                    56,
                    18,
                    2,
                    199,
                    0
                ],
                "published": "2024-04-19T12:39:11Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    12,
                    39,
                    11,
                    4,
                    110,
                    0
                ],
                "title": "CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and\n  Feature Balance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and\n  Feature Balance"
                },
                "summary": "Federated Learning (FL) as a promising distributed machine learning paradigm\nhas been widely adopted in Artificial Intelligence of Things (AIoT)\napplications. However, the efficiency and inference capability of FL is\nseriously limited due to the presence of stragglers and data imbalance across\nmassive AIoT devices, respectively. To address the above challenges, we present\na novel asynchronous FL approach named CaBaFL, which includes a hierarchical\nCache-based aggregation mechanism and a feature Balance-guided device selection\nstrategy. CaBaFL maintains multiple intermediate models simultaneously for\nlocal training. The hierarchical cache-based aggregation mechanism enables each\nintermediate model to be trained on multiple devices to align the training time\nand mitigate the straggler issue. In specific, each intermediate model is\nstored in a low-level cache for local training and when it is trained by\nsufficient local devices, it will be stored in a high-level cache for\naggregation. To address the problem of imbalanced data, the feature\nbalance-guided device selection strategy in CaBaFL adopts the activation\ndistribution as a metric, which enables each intermediate model to be trained\nacross devices with totally balanced data distributions before aggregation.\nExperimental results show that compared with the state-of-the-art FL methods,\nCaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) as a promising distributed machine learning paradigm\nhas been widely adopted in Artificial Intelligence of Things (AIoT)\napplications. However, the efficiency and inference capability of FL is\nseriously limited due to the presence of stragglers and data imbalance across\nmassive AIoT devices, respectively. To address the above challenges, we present\na novel asynchronous FL approach named CaBaFL, which includes a hierarchical\nCache-based aggregation mechanism and a feature Balance-guided device selection\nstrategy. CaBaFL maintains multiple intermediate models simultaneously for\nlocal training. The hierarchical cache-based aggregation mechanism enables each\nintermediate model to be trained on multiple devices to align the training time\nand mitigate the straggler issue. In specific, each intermediate model is\nstored in a low-level cache for local training and when it is trained by\nsufficient local devices, it will be stored in a high-level cache for\naggregation. To address the problem of imbalanced data, the feature\nbalance-guided device selection strategy in CaBaFL adopts the activation\ndistribution as a metric, which enables each intermediate model to be trained\nacross devices with totally balanced data distributions before aggregation.\nExperimental results show that compared with the state-of-the-art FL methods,\nCaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy\nimprovements."
                },
                "authors": [
                    {
                        "name": "Zeke Xia"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Dengke Yan"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Anran Li"
                    },
                    {
                        "name": "Junlong Zhou"
                    },
                    {
                        "name": "Mingsong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingsong Chen"
                },
                "author": "Mingsong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19626v2",
                "updated": "2024-07-17T03:02:49Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    3,
                    2,
                    49,
                    2,
                    199,
                    0
                ],
                "published": "2024-05-30T02:23:50Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    2,
                    23,
                    50,
                    3,
                    151,
                    0
                ],
                "title": "CXL Shared Memory Programming: Barely Distributed and Almost Persistent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL Shared Memory Programming: Barely Distributed and Almost Persistent"
                },
                "summary": "While Compute Express Link (CXL) enables support for cache-coherent shared\nmemory among multiple nodes, it also introduces new types of\nfailures--processes can fail before data does, or data might fail before a\nprocess does. The lack of a failure model for CXL-based shared memory makes it\nchallenging to understand and mitigate these failures.\n  To solve these challenges, in this paper, we describe a model categorizing\nand handling the CXL-based shared memory's failures: data and process failures.\nData failures in CXL-based shared memory render data inaccessible or\ninconsistent for a currently running application. We argue that such failures\nare unlike data failures in distributed storage systems and require\nCXL-specific handling. To address this, we look into traditional data failure\nmitigation techniques like erasure coding and replication and propose new\nsolutions to better handle data failures in CXL-based shared memory systems.\nNext, we look into process failures and compare the failures and potential\nsolutions with PMEM's failure model and programming solutions. We argue that\nalthough PMEM shares some of CXL's characteristics, it does not fully address\nCXL's volatile nature and low access latencies. Finally, taking inspiration\nfrom PMEM programming solutions, we propose techniques to handle these new\nfailures.\n  Thus, this paper is the first work to define the CXL-based shared memory\nfailure model and propose tailored solutions that address challenges specific\nto CXL-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Compute Express Link (CXL) enables support for cache-coherent shared\nmemory among multiple nodes, it also introduces new types of\nfailures--processes can fail before data does, or data might fail before a\nprocess does. The lack of a failure model for CXL-based shared memory makes it\nchallenging to understand and mitigate these failures.\n  To solve these challenges, in this paper, we describe a model categorizing\nand handling the CXL-based shared memory's failures: data and process failures.\nData failures in CXL-based shared memory render data inaccessible or\ninconsistent for a currently running application. We argue that such failures\nare unlike data failures in distributed storage systems and require\nCXL-specific handling. To address this, we look into traditional data failure\nmitigation techniques like erasure coding and replication and propose new\nsolutions to better handle data failures in CXL-based shared memory systems.\nNext, we look into process failures and compare the failures and potential\nsolutions with PMEM's failure model and programming solutions. We argue that\nalthough PMEM shares some of CXL's characteristics, it does not fully address\nCXL's volatile nature and low access latencies. Finally, taking inspiration\nfrom PMEM programming solutions, we propose techniques to handle these new\nfailures.\n  Thus, this paper is the first work to define the CXL-based shared memory\nfailure model and propose tailored solutions that address challenges specific\nto CXL-based systems."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ziheng Liu"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12077v1",
                "updated": "2024-07-16T18:00:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    0,
                    0,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T18:00:00Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    0,
                    0,
                    1,
                    198,
                    0
                ],
                "title": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill\n  and Extreme KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill\n  and Extreme KV-Cache Compression"
                },
                "summary": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use."
                },
                "authors": [
                    {
                        "name": "Daniel Goldstein"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Eric Alcaide"
                    },
                    {
                        "name": "Guangyu Song"
                    },
                    {
                        "name": "Eugene Cheah"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Cheah"
                },
                "author": "Eugene Cheah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.04877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.04877v2",
                "updated": "2024-07-16T09:05:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    5,
                    39,
                    1,
                    198,
                    0
                ],
                "published": "2023-05-08T17:20:30Z",
                "published_parsed": [
                    2023,
                    5,
                    8,
                    17,
                    20,
                    30,
                    0,
                    128,
                    0
                ],
                "title": "Coherently amplified ultrafast imaging using a free-electron\n  interferometer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherently amplified ultrafast imaging using a free-electron\n  interferometer"
                },
                "summary": "Accessing the low-energy non-equilibrium dynamics of materials and their\npolaritons with simultaneous high spatial and temporal resolution has been a\nbold frontier of electron microscopy in recent years. One of the main\nchallenges lies in the ability to retrieve extremely weak signals while\nsimultaneously disentangling amplitude and phase information. Here, we present\nFree-Electron Ramsey Imaging (FERI), a microscopy approach based on\nlight-induced electron modulation that enables coherent amplification of\noptical near-fields in electron imaging. We provide simultaneous time-, space-,\nand phase-resolved measurements of a micro-drum made from a hexagonal boron\nnitride membrane visualizing the sub-cycle dynamics of 2D polariton wavepackets\ntherein. The phase-resolved measurements reveals vortex-anti-vortex\nsingularities on the polariton wavefronts, together with an intriguing\nphenomenon of a traveling wave mimicking the amplitude profile of a standing\nwave. Our experiments show a 20-fold coherent amplification of the near-field\nsignal compared to conventional electron near-field imaging, resolving peak\nfield intensities in the order of ~W/cm2, corresponding to field amplitudes of\na few kV/m. As a result, our work paves the way for spatio-temporal electron\nmicroscopy of biological specimens and quantum materials, exciting yet delicate\nsamples that are currently difficult to investigate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessing the low-energy non-equilibrium dynamics of materials and their\npolaritons with simultaneous high spatial and temporal resolution has been a\nbold frontier of electron microscopy in recent years. One of the main\nchallenges lies in the ability to retrieve extremely weak signals while\nsimultaneously disentangling amplitude and phase information. Here, we present\nFree-Electron Ramsey Imaging (FERI), a microscopy approach based on\nlight-induced electron modulation that enables coherent amplification of\noptical near-fields in electron imaging. We provide simultaneous time-, space-,\nand phase-resolved measurements of a micro-drum made from a hexagonal boron\nnitride membrane visualizing the sub-cycle dynamics of 2D polariton wavepackets\ntherein. The phase-resolved measurements reveals vortex-anti-vortex\nsingularities on the polariton wavefronts, together with an intriguing\nphenomenon of a traveling wave mimicking the amplitude profile of a standing\nwave. Our experiments show a 20-fold coherent amplification of the near-field\nsignal compared to conventional electron near-field imaging, resolving peak\nfield intensities in the order of ~W/cm2, corresponding to field amplitudes of\na few kV/m. As a result, our work paves the way for spatio-temporal electron\nmicroscopy of biological specimens and quantum materials, exciting yet delicate\nsamples that are currently difficult to investigate."
                },
                "authors": [
                    {
                        "name": "Tomer Bucher"
                    },
                    {
                        "name": "Harel Nahari"
                    },
                    {
                        "name": "Hanan Herzig Sheinfux"
                    },
                    {
                        "name": "Ron Ruimy"
                    },
                    {
                        "name": "Arthur Niedermayr"
                    },
                    {
                        "name": "Raphael Dahan"
                    },
                    {
                        "name": "Qinghui Yan"
                    },
                    {
                        "name": "Yuval Adiv"
                    },
                    {
                        "name": "Michael Yannai"
                    },
                    {
                        "name": "Jialin Chen"
                    },
                    {
                        "name": "Yaniv Kurman"
                    },
                    {
                        "name": "Sang Tae Park"
                    },
                    {
                        "name": "Daniel J. Masiel"
                    },
                    {
                        "name": "Eli Janzen"
                    },
                    {
                        "name": "James H. Edgar"
                    },
                    {
                        "name": "Fabrizio Carbone"
                    },
                    {
                        "name": "Guy Bartal"
                    },
                    {
                        "name": "Shai Tsesses"
                    },
                    {
                        "name": "Frank H. L. Koppens"
                    },
                    {
                        "name": "Giovanni Maria Vanacore"
                    },
                    {
                        "name": "Ido Kaminer"
                    }
                ],
                "author_detail": {
                    "name": "Ido Kaminer"
                },
                "author": "Ido Kaminer",
                "arxiv_doi": "10.1038/s41566-024-01451-w",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41566-024-01451-w",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.04877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.04877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11483v1",
                "updated": "2024-07-16T08:18:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    18,
                    41,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T08:18:41Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    18,
                    41,
                    1,
                    198,
                    0
                ],
                "title": "Performance Analysis of Internet of Vehicles Mesh Networks Based on\n  Actual Switch Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of Internet of Vehicles Mesh Networks Based on\n  Actual Switch Models"
                },
                "summary": "The rapid growth of the automotive industry has exacerbated the conflict\nbetween the complex traffic environment, increasing communication demands, and\nlimited resources. Given the imperative to mitigate traffic and network\ncongestion, analyzing the performance of Internet of Vehicles (IoV) mesh\nnetworks is of great practical significance. Most studies focus solely on\nindividual performance metrics and influencing factors, and the adopted\nsimulation tools, such as OPNET, cannot achieve the dynamic link generation of\nIoV mesh networks. To address these problems, a network performance analysis\nmodel based on actual switches is proposed. First, a typical IoV mesh network\narchitecture is constructed and abstracted into a mathematical model that\ndescribes how the link and topology changes over time. Then, the task\ngeneration model and the task forwarding model based on actual switches are\nproposed to obtain the real traffic distribution of the network. Finally, a\nscientific network performance indicator system is constructed. Simulation\nresults demonstrate that, with rising task traffic and decreasing node caching\ncapacity, the packet loss rate increases, and the task arrival rate decreases\nin the network. The proposed model can effectively evaluate the network\nperformance across various traffic states and provide valuable insights for\nnetwork construction and enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of the automotive industry has exacerbated the conflict\nbetween the complex traffic environment, increasing communication demands, and\nlimited resources. Given the imperative to mitigate traffic and network\ncongestion, analyzing the performance of Internet of Vehicles (IoV) mesh\nnetworks is of great practical significance. Most studies focus solely on\nindividual performance metrics and influencing factors, and the adopted\nsimulation tools, such as OPNET, cannot achieve the dynamic link generation of\nIoV mesh networks. To address these problems, a network performance analysis\nmodel based on actual switches is proposed. First, a typical IoV mesh network\narchitecture is constructed and abstracted into a mathematical model that\ndescribes how the link and topology changes over time. Then, the task\ngeneration model and the task forwarding model based on actual switches are\nproposed to obtain the real traffic distribution of the network. Finally, a\nscientific network performance indicator system is constructed. Simulation\nresults demonstrate that, with rising task traffic and decreasing node caching\ncapacity, the packet loss rate increases, and the task arrival rate decreases\nin the network. The proposed model can effectively evaluate the network\nperformance across various traffic states and provide valuable insights for\nnetwork construction and enhancement."
                },
                "authors": [
                    {
                        "name": "Jialin Hu"
                    },
                    {
                        "name": "Zhiyuan Ren"
                    },
                    {
                        "name": "Wenchi Cheng"
                    },
                    {
                        "name": "Zhiliang Shuai"
                    },
                    {
                        "name": "Zhao Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Li"
                },
                "author": "Zhao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11338v1",
                "updated": "2024-07-16T03:08:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    3,
                    8,
                    41,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T03:08:41Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    3,
                    8,
                    41,
                    1,
                    198,
                    0
                ],
                "title": "Heterogeneous integration of high endurance ferroelectric and\n  piezoelectric epitaxial BaTiO$_3$ devices on Si",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous integration of high endurance ferroelectric and\n  piezoelectric epitaxial BaTiO$_3$ devices on Si"
                },
                "summary": "Integrating epitaxial BaTiO$_3$ (BTO) with Si is essential for leveraging its\nferroelectric, piezoelectric, and nonlinear optical properties in\nmicroelectronics. Recently, heterogeneous integration approaches that involve\ngrowth of BTO on ideal substrates followed by transfer to a desired substrate\nshow promise of achieving excellent device-quality films. However, beyond\nsimple demonstrations of the existence of ferroelectricity, robust devices with\nhigh endurance were not yet demonstrated on Si using the latter approach. Here,\nusing a novel two-step approach to synthesize epitaxial BTO using pulsed laser\ndeposition (PLD) on water soluble Sr3Al2O7 (SAO) (on SrTiO$_3$ (STO)\nsubstrates), we demonstrate successful integration of high-quality BTO\ncapacitors on Si, with Pr of 7 uC/cm2, Ec 150 kV/cm, ferroelectric and\nelectromechanical endurance of greater than $10^6$ cycles. We further address\nthe challenge of cracking and disintegration of thicker films by first\ntransferring a large area (5 mm x 5 mm) of the templated layer of BTO (~30 nm\nthick) on the desired substrate, followed by the growth of high-quality BTO on\nthis substrate, as revealed by HRXRD and HRSTEM measurements. These templated\nSi substrates offer a versatile platform for integrating any epitaxial complex\noxides with diverse functionalities onto any inorganic substrate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating epitaxial BaTiO$_3$ (BTO) with Si is essential for leveraging its\nferroelectric, piezoelectric, and nonlinear optical properties in\nmicroelectronics. Recently, heterogeneous integration approaches that involve\ngrowth of BTO on ideal substrates followed by transfer to a desired substrate\nshow promise of achieving excellent device-quality films. However, beyond\nsimple demonstrations of the existence of ferroelectricity, robust devices with\nhigh endurance were not yet demonstrated on Si using the latter approach. Here,\nusing a novel two-step approach to synthesize epitaxial BTO using pulsed laser\ndeposition (PLD) on water soluble Sr3Al2O7 (SAO) (on SrTiO$_3$ (STO)\nsubstrates), we demonstrate successful integration of high-quality BTO\ncapacitors on Si, with Pr of 7 uC/cm2, Ec 150 kV/cm, ferroelectric and\nelectromechanical endurance of greater than $10^6$ cycles. We further address\nthe challenge of cracking and disintegration of thicker films by first\ntransferring a large area (5 mm x 5 mm) of the templated layer of BTO (~30 nm\nthick) on the desired substrate, followed by the growth of high-quality BTO on\nthis substrate, as revealed by HRXRD and HRSTEM measurements. These templated\nSi substrates offer a versatile platform for integrating any epitaxial complex\noxides with diverse functionalities onto any inorganic substrate."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Harshal Jason D'Souza"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Rama Satya Sandilya"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "author": "Pavan Nukala",
                "arxiv_comment": "29 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v3",
                "updated": "2024-07-15T22:33:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    22,
                    33,
                    58,
                    0,
                    197,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Cache for Large Language Model Based\n  Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Cache for Large Language Model Based\n  Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "This study presents the first privacy aware semantic cache for LLMs\n  based on Federated Learning. MeanCache is the first cache that can handle\n  contextual queries efficiently. Total pages 14",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.05740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.05740v2",
                "updated": "2024-07-15T18:38:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    18,
                    38,
                    54,
                    0,
                    197,
                    0
                ],
                "published": "2023-07-11T19:08:06Z",
                "published_parsed": [
                    2023,
                    7,
                    11,
                    19,
                    8,
                    6,
                    1,
                    192,
                    0
                ],
                "title": "Minimum Cost Loop Nests for Contraction of a Sparse Tensor with a Tensor\n  Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimum Cost Loop Nests for Contraction of a Sparse Tensor with a Tensor\n  Network"
                },
                "summary": "Sparse tensor decomposition and completion are common in numerous\napplications, ranging from machine learning to computational quantum chemistry.\nTypically, the main bottleneck in optimization of these models are contractions\nof a single large sparse tensor with a network of several dense matrices or\ntensors (SpTTN). Prior works on high-performance tensor decomposition and\ncompletion have focused on performance and scalability optimizations for\nspecific SpTTN kernels. We present algorithms and a runtime system for\nidentifying and executing the most efficient loop nest for any SpTTN kernel. We\nconsider both enumeration of such loop nests for autotuning and efficient\nalgorithms for finding the lowest cost loop-nest for simpler metrics, such as\nbuffer size or cache miss models. Our runtime system identifies the best choice\nof loop nest without user guidance, and also provides a distributed-memory\nparallelization of SpTTN kernels. We evaluate our framework using both\nreal-world and synthetic tensors. Our results demonstrate that our approach\noutperforms available generalized state-of-the-art libraries and matches the\nperformance of specialized codes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse tensor decomposition and completion are common in numerous\napplications, ranging from machine learning to computational quantum chemistry.\nTypically, the main bottleneck in optimization of these models are contractions\nof a single large sparse tensor with a network of several dense matrices or\ntensors (SpTTN). Prior works on high-performance tensor decomposition and\ncompletion have focused on performance and scalability optimizations for\nspecific SpTTN kernels. We present algorithms and a runtime system for\nidentifying and executing the most efficient loop nest for any SpTTN kernel. We\nconsider both enumeration of such loop nests for autotuning and efficient\nalgorithms for finding the lowest cost loop-nest for simpler metrics, such as\nbuffer size or cache miss models. Our runtime system identifies the best choice\nof loop nest without user guidance, and also provides a distributed-memory\nparallelization of SpTTN kernels. We evaluate our framework using both\nreal-world and synthetic tensors. Our results demonstrate that our approach\noutperforms available generalized state-of-the-art libraries and matches the\nperformance of specialized codes."
                },
                "authors": [
                    {
                        "name": "Raghavendra Kanakagiri"
                    },
                    {
                        "name": "Edgar Solomonik"
                    }
                ],
                "author_detail": {
                    "name": "Edgar Solomonik"
                },
                "author": "Edgar Solomonik",
                "arxiv_doi": "10.1145/3626183.3659985",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626183.3659985",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.05740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.05740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.3; D.1.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.08869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08869v2",
                "updated": "2024-08-19T04:29:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    29,
                    34,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-16T17:54:09Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    54,
                    9,
                    4,
                    229,
                    0
                ],
                "title": "PEDAL: Enhancing Greedy Decoding with Large Language Models using\n  Diverse Exemplars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEDAL: Enhancing Greedy Decoding with Large Language Models using\n  Diverse Exemplars"
                },
                "summary": "Self-ensembling techniques with diverse reasoning paths such as\nSelf-Consistency have demonstrated remarkable performance gains in text\ngeneration with Large Language Models (LLMs). However, such techniques depend\non the availability of an accurate answer extraction process to aggregate\nacross multiple outputs. Moreover, they acquire higher inference cost, in\ncomparison to Greedy Decoding, due to generation of relatively higher number of\noutput tokens. Research has shown that the free form text outputs from\nSelf-Consistency can be aggregated reliably using LLMs to produce the final\noutput. Additionally, recent advancements in LLM inference have demonstrated\nthat usage of diverse exemplars in prompts have the ability to induce diversity\nin the LLM outputs. Such proven techniques can be easily extended to\nself-ensembling based approaches to achieve enhanced results in text\ngeneration. In this paper, we introduce PEDAL (Prompts based on Exemplar\nDiversity Aggregated using LLMs), a hybrid self-ensembling approach, that\ncombines the strengths of diverse exemplar based prompts and LLM based\naggregation to achieve improvement in overall performance. On the publicly\navailable SVAMP and ARC datasets, our experiments reveal that PEDAL can achieve\nbetter accuracy than Greedy Decoding based strategies with lower inference cost\ncompared to Self Consistency based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-ensembling techniques with diverse reasoning paths such as\nSelf-Consistency have demonstrated remarkable performance gains in text\ngeneration with Large Language Models (LLMs). However, such techniques depend\non the availability of an accurate answer extraction process to aggregate\nacross multiple outputs. Moreover, they acquire higher inference cost, in\ncomparison to Greedy Decoding, due to generation of relatively higher number of\noutput tokens. Research has shown that the free form text outputs from\nSelf-Consistency can be aggregated reliably using LLMs to produce the final\noutput. Additionally, recent advancements in LLM inference have demonstrated\nthat usage of diverse exemplars in prompts have the ability to induce diversity\nin the LLM outputs. Such proven techniques can be easily extended to\nself-ensembling based approaches to achieve enhanced results in text\ngeneration. In this paper, we introduce PEDAL (Prompts based on Exemplar\nDiversity Aggregated using LLMs), a hybrid self-ensembling approach, that\ncombines the strengths of diverse exemplar based prompts and LLM based\naggregation to achieve improvement in overall performance. On the publicly\navailable SVAMP and ARC datasets, our experiments reveal that PEDAL can achieve\nbetter accuracy than Greedy Decoding based strategies with lower inference cost\ncompared to Self Consistency based approaches."
                },
                "authors": [
                    {
                        "name": "Sumanth Prabhu"
                    }
                ],
                "author_detail": {
                    "name": "Sumanth Prabhu"
                },
                "author": "Sumanth Prabhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13717v2",
                "updated": "2024-08-16T17:43:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    43,
                    57,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-18T17:16:35Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    17,
                    16,
                    35,
                    3,
                    200,
                    0
                ],
                "title": "CoDefeater: Using LLMs To Find Defeaters in Assurance Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoDefeater: Using LLMs To Find Defeaters in Assurance Cases"
                },
                "summary": "Constructing assurance cases is a widely used, and sometimes required,\nprocess toward demonstrating that safety-critical systems will operate safely\nin their planned environment. To mitigate the risk of errors and missing edge\ncases, the concept of defeaters - arguments or evidence that challenge claims\nin an assurance case - has been introduced. Defeaters can provide timely\ndetection of weaknesses in the arguments, prompting further investigation and\ntimely mitigations. However, capturing defeaters relies on expert judgment,\nexperience, and creativity and must be done iteratively due to evolving\nrequirements and regulations. This paper proposes CoDefeater, an automated\nprocess to leverage large language models (LLMs) for finding defeaters. Initial\nresults on two systems show that LLMs can efficiently find known and unforeseen\nfeasible defeaters to support safety analysts in enhancing the completeness and\nconfidence of assurance cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing assurance cases is a widely used, and sometimes required,\nprocess toward demonstrating that safety-critical systems will operate safely\nin their planned environment. To mitigate the risk of errors and missing edge\ncases, the concept of defeaters - arguments or evidence that challenge claims\nin an assurance case - has been introduced. Defeaters can provide timely\ndetection of weaknesses in the arguments, prompting further investigation and\ntimely mitigations. However, capturing defeaters relies on expert judgment,\nexperience, and creativity and must be done iteratively due to evolving\nrequirements and regulations. This paper proposes CoDefeater, an automated\nprocess to leverage large language models (LLMs) for finding defeaters. Initial\nresults on two systems show that LLMs can efficiently find known and unforeseen\nfeasible defeaters to support safety analysts in enhancing the completeness and\nconfidence of assurance cases."
                },
                "authors": [
                    {
                        "name": "Usman Gohar"
                    },
                    {
                        "name": "Michael C. Hunter"
                    },
                    {
                        "name": "Robyn R. Lutz"
                    },
                    {
                        "name": "Myra B. Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Myra B. Cohen"
                },
                "author": "Myra B. Cohen",
                "arxiv_comment": "ASE 2024 NIER",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08848v1",
                "updated": "2024-08-16T17:19:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    19,
                    23,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T17:19:23Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    19,
                    23,
                    4,
                    229,
                    0
                ],
                "title": "PsychoLex: Unveiling the Psychological Mind of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsychoLex: Unveiling the Psychological Mind of Large Language Models"
                },
                "summary": "This paper explores the intersection of psychology and artificial\nintelligence through the development and evaluation of specialized Large\nLanguage Models (LLMs). We introduce PsychoLex, a suite of resources designed\nto enhance LLMs' proficiency in psychological tasks in both Persian and\nEnglish. Key contributions include the PsychoLexQA dataset for instructional\ncontent and the PsychoLexEval dataset for rigorous evaluation of LLMs in\ncomplex psychological scenarios. Additionally, we present the PsychoLexLLaMA\nmodel, optimized specifically for psychological applications, demonstrating\nsuperior performance compared to general-purpose models. The findings\nunderscore the potential of tailored LLMs for advancing psychological research\nand applications, while also highlighting areas for further refinement. This\nresearch offers a foundational step towards integrating LLMs into specialized\npsychological domains, with implications for future advancements in AI-driven\npsychological practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the intersection of psychology and artificial\nintelligence through the development and evaluation of specialized Large\nLanguage Models (LLMs). We introduce PsychoLex, a suite of resources designed\nto enhance LLMs' proficiency in psychological tasks in both Persian and\nEnglish. Key contributions include the PsychoLexQA dataset for instructional\ncontent and the PsychoLexEval dataset for rigorous evaluation of LLMs in\ncomplex psychological scenarios. Additionally, we present the PsychoLexLLaMA\nmodel, optimized specifically for psychological applications, demonstrating\nsuperior performance compared to general-purpose models. The findings\nunderscore the potential of tailored LLMs for advancing psychological research\nand applications, while also highlighting areas for further refinement. This\nresearch offers a foundational step towards integrating LLMs into specialized\npsychological domains, with implications for future advancements in AI-driven\npsychological practice."
                },
                "authors": [
                    {
                        "name": "Mohammad Amin Abbasi"
                    },
                    {
                        "name": "Farnaz Sadat Mirnezami"
                    },
                    {
                        "name": "Hassan Naderi"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Naderi"
                },
                "author": "Hassan Naderi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03640v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03640v4",
                "updated": "2024-08-16T17:06:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    6,
                    39,
                    4,
                    229,
                    0
                ],
                "published": "2024-03-06T11:56:02Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    11,
                    56,
                    2,
                    2,
                    66,
                    0
                ],
                "title": "Apollo: A Lightweight Multilingual Medical LLM towards Democratizing\n  Medical AI to 6B People",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apollo: A Lightweight Multilingual Medical LLM towards Democratizing\n  Medical AI to 6B People"
                },
                "summary": "Despite the vast repository of global medical knowledge predominantly being\nin English, local languages are crucial for delivering tailored healthcare\nservices, particularly in areas with limited medical resources. To extend the\nreach of medical AI advancements to a broader population, we aim to develop\nmedical LLMs across the six most widely spoken languages, encompassing a global\npopulation of 6.1 billion. This effort culminates in the creation of the\nApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the\nmultilingual medical benchmark, the released Apollo models, at various\nrelatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best\nperformance among models of equivalent size. Especially, Apollo-7B is the\nstate-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite\nmodels could be used to improve the multi-lingual medical capabilities of\nlarger models without fine-tuning in a proxy-tuning fashion. We will\nopen-source training corpora, code, model weights and evaluation benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the vast repository of global medical knowledge predominantly being\nin English, local languages are crucial for delivering tailored healthcare\nservices, particularly in areas with limited medical resources. To extend the\nreach of medical AI advancements to a broader population, we aim to develop\nmedical LLMs across the six most widely spoken languages, encompassing a global\npopulation of 6.1 billion. This effort culminates in the creation of the\nApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the\nmultilingual medical benchmark, the released Apollo models, at various\nrelatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best\nperformance among models of equivalent size. Especially, Apollo-7B is the\nstate-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite\nmodels could be used to improve the multi-lingual medical capabilities of\nlarger models without fine-tuning in a proxy-tuning fashion. We will\nopen-source training corpora, code, model weights and evaluation benchmark."
                },
                "authors": [
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Junyin Chen"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Guorui Zhen"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Xiangbo Wu"
                    },
                    {
                        "name": "Anningzhe Gao"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03640v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03640v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08845v1",
                "updated": "2024-08-16T17:06:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    6,
                    7,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T17:06:07Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    6,
                    7,
                    4,
                    229,
                    0
                ],
                "title": "Shapley Marginal Surplus for Strong Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shapley Marginal Surplus for Strong Models"
                },
                "summary": "Shapley values have seen widespread use in machine learning as a way to\nexplain model predictions and estimate the importance of covariates. Accurately\nexplaining models is critical in real-world models to both aid in decision\nmaking and to infer the properties of the true data-generating process (DGP).\nIn this paper, we demonstrate that while model-based Shapley values might be\naccurate explainers of model predictions, machine learning models themselves\nare often poor explainers of the DGP even if the model is highly accurate.\nParticularly in the presence of interrelated or noisy variables, the output of\na highly predictive model may fail to account for these relationships. This\nimplies explanations of a trained model's behavior may fail to provide\nmeaningful insight into the DGP. In this paper we introduce a novel variable\nimportance algorithm, Shapley Marginal Surplus for Strong Models, that samples\nthe space of possible models to come up with an inferential measure of feature\nimportance. We compare this method to other popular feature importance methods,\nboth Shapley-based and non-Shapley based, and demonstrate significant\noutperformance in inferential capabilities relative to other methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shapley values have seen widespread use in machine learning as a way to\nexplain model predictions and estimate the importance of covariates. Accurately\nexplaining models is critical in real-world models to both aid in decision\nmaking and to infer the properties of the true data-generating process (DGP).\nIn this paper, we demonstrate that while model-based Shapley values might be\naccurate explainers of model predictions, machine learning models themselves\nare often poor explainers of the DGP even if the model is highly accurate.\nParticularly in the presence of interrelated or noisy variables, the output of\na highly predictive model may fail to account for these relationships. This\nimplies explanations of a trained model's behavior may fail to provide\nmeaningful insight into the DGP. In this paper we introduce a novel variable\nimportance algorithm, Shapley Marginal Surplus for Strong Models, that samples\nthe space of possible models to come up with an inferential measure of feature\nimportance. We compare this method to other popular feature importance methods,\nboth Shapley-based and non-Shapley based, and demonstrate significant\noutperformance in inferential capabilities relative to other methods."
                },
                "authors": [
                    {
                        "name": "Daniel de Marchi"
                    },
                    {
                        "name": "Michael Kosorok"
                    },
                    {
                        "name": "Scott de Marchi"
                    }
                ],
                "author_detail": {
                    "name": "Scott de Marchi"
                },
                "author": "Scott de Marchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08841v1",
                "updated": "2024-08-16T17:00:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    0,
                    11,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T17:00:11Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    0,
                    11,
                    4,
                    229,
                    0
                ],
                "title": "FLEXTAF: Enhancing Table Reasoning with Flexible Tabular Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLEXTAF: Enhancing Table Reasoning with Flexible Tabular Formats"
                },
                "summary": "The table reasoning task aims to answer the question according to the given\ntable. Currently, using Large Language Models (LLMs) is the predominant method\nfor table reasoning. Most existing methods employ a fixed tabular format to\nrepresent the table, which could limit the performance. Given that each\ninstance requires different capabilities and models possess varying abilities,\nwe assert that different instances and models suit different tabular formats.\nWe prove the aforementioned claim through quantitative analysis of experimental\nresults, where different instances and models achieve different performances\nusing various tabular formats. Building on this discussion, we propose\nFLEXTAF-Single and FLEXTAF-Vote to enhance table reasoning performance by\nemploying flexible tabular formats. Specifically, (i) FLEXTAF-Single trains a\nclassifier to predict the most suitable tabular format based on the instance\nand the LLM. (ii) FLEXTAF-Vote integrates the results across different formats.\nOur experiments on WikiTableQuestions and TabFact reveal significant\nimprovements, with average gains of 2.3% and 4.8% compared to the best\nperformance achieved using a fixed tabular format with greedy decoding and\nself-consistency decoding, thereby validating the effectiveness of our methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The table reasoning task aims to answer the question according to the given\ntable. Currently, using Large Language Models (LLMs) is the predominant method\nfor table reasoning. Most existing methods employ a fixed tabular format to\nrepresent the table, which could limit the performance. Given that each\ninstance requires different capabilities and models possess varying abilities,\nwe assert that different instances and models suit different tabular formats.\nWe prove the aforementioned claim through quantitative analysis of experimental\nresults, where different instances and models achieve different performances\nusing various tabular formats. Building on this discussion, we propose\nFLEXTAF-Single and FLEXTAF-Vote to enhance table reasoning performance by\nemploying flexible tabular formats. Specifically, (i) FLEXTAF-Single trains a\nclassifier to predict the most suitable tabular format based on the instance\nand the LLM. (ii) FLEXTAF-Vote integrates the results across different formats.\nOur experiments on WikiTableQuestions and TabFact reveal significant\nimprovements, with average gains of 2.3% and 4.8% compared to the best\nperformance achieved using a fixed tabular format with greedy decoding and\nself-consistency decoding, thereby validating the effectiveness of our methods."
                },
                "authors": [
                    {
                        "name": "Xuanliang Zhang"
                    },
                    {
                        "name": "Dingzirui Wang"
                    },
                    {
                        "name": "Longxu Dou"
                    },
                    {
                        "name": "Baoxin Wang"
                    },
                    {
                        "name": "Dayong Wu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07246v2",
                "updated": "2024-08-16T16:46:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    16,
                    46,
                    32,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-14T01:16:40Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    16,
                    40,
                    2,
                    227,
                    0
                ],
                "title": "ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B."
                },
                "authors": [
                    {
                        "name": "Junxian Li"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Xunzhi Wang"
                    },
                    {
                        "name": "Zeying Hao"
                    },
                    {
                        "name": "Jingdi Lei"
                    },
                    {
                        "name": "Qian Tan"
                    },
                    {
                        "name": "Cai Zhou"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Yaotian Yang"
                    },
                    {
                        "name": "Xinrui Xiong"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Mao Su"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "arxiv_comment": "11 pages, updated version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15019v2",
                "updated": "2024-08-16T15:56:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    56,
                    46,
                    4,
                    229,
                    0
                ],
                "published": "2024-05-23T19:44:03Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    19,
                    44,
                    3,
                    3,
                    144,
                    0
                ],
                "title": "Agentic Skill Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Skill Discovery"
                },
                "summary": "Language-conditioned robotic skills make it possible to apply the high-level\nreasoning of Large Language Models (LLMs) to low-level robotic control. A\nremaining challenge is to acquire a diverse set of fundamental skills. Existing\napproaches either manually decompose a complex task into atomic robotic actions\nin a top-down fashion, or bootstrap as many combinations as possible in a\nbottom-up fashion to cover a wider range of task possibilities. These\ndecompositions or combinations, however, require an initial skill library. For\nexample, a ``grasping'' capability can never emerge from a skill library\ncontaining only diverse ``pushing'' skills. Existing skill discovery techniques\nwith reinforcement learning acquire skills by an exhaustive exploration but\noften yield non-meaningful behaviors. In this study, we introduce a novel\nframework for skill discovery that is entirely driven by LLMs. The framework\nbegins with an LLM generating task proposals based on the provided scene\ndescription and the robot's configurations, aiming to incrementally acquire new\nskills upon task completion. For each proposed task, a series of reinforcement\nlearning processes are initiated, utilizing reward and success determination\nfunctions sampled by the LLM to develop the corresponding policy. The\nreliability and trustworthiness of learned behaviors are further ensured by an\nindependent vision-language model. We show that starting with zero skill, the\nskill library emerges and expands to more and more meaningful and reliable\nskills, enabling the robot to efficiently further propose and complete advanced\ntasks. Project page: \\url{https://agentic-skill-discovery.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-conditioned robotic skills make it possible to apply the high-level\nreasoning of Large Language Models (LLMs) to low-level robotic control. A\nremaining challenge is to acquire a diverse set of fundamental skills. Existing\napproaches either manually decompose a complex task into atomic robotic actions\nin a top-down fashion, or bootstrap as many combinations as possible in a\nbottom-up fashion to cover a wider range of task possibilities. These\ndecompositions or combinations, however, require an initial skill library. For\nexample, a ``grasping'' capability can never emerge from a skill library\ncontaining only diverse ``pushing'' skills. Existing skill discovery techniques\nwith reinforcement learning acquire skills by an exhaustive exploration but\noften yield non-meaningful behaviors. In this study, we introduce a novel\nframework for skill discovery that is entirely driven by LLMs. The framework\nbegins with an LLM generating task proposals based on the provided scene\ndescription and the robot's configurations, aiming to incrementally acquire new\nskills upon task completion. For each proposed task, a series of reinforcement\nlearning processes are initiated, utilizing reward and success determination\nfunctions sampled by the LLM to develop the corresponding policy. The\nreliability and trustworthiness of learned behaviors are further ensured by an\nindependent vision-language model. We show that starting with zero skill, the\nskill library emerges and expands to more and more meaningful and reliable\nskills, enabling the robot to efficiently further propose and complete advanced\ntasks. Project page: \\url{https://agentic-skill-discovery.github.io}."
                },
                "authors": [
                    {
                        "name": "Xufeng Zhao"
                    },
                    {
                        "name": "Cornelius Weber"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "Webpage see https://agentic-skill-discovery.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08811v1",
                "updated": "2024-08-16T15:46:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    46,
                    15,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T15:46:15Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    46,
                    15,
                    4,
                    229,
                    0
                ],
                "title": "Artificial Intelligence and Strategic Decision-Making: Evidence from\n  Entrepreneurs and Investors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence and Strategic Decision-Making: Evidence from\n  Entrepreneurs and Investors"
                },
                "summary": "This paper explores how artificial intelligence (AI) may impact the strategic\ndecision-making (SDM) process in firms. We illustrate how AI could augment\nexisting SDM tools and provide empirical evidence from a leading accelerator\nprogram and a startup competition that current Large Language Models (LLMs) can\ngenerate and evaluate strategies at a level comparable to entrepreneurs and\ninvestors. We then examine implications for key cognitive processes underlying\nSDM -- search, representation, and aggregation. Our analysis suggests AI has\nthe potential to enhance the speed, quality, and scale of strategic analysis,\nwhile also enabling new approaches like virtual strategy simulations. However,\nthe ultimate impact on firm performance will depend on competitive dynamics as\nAI capabilities progress. We propose a framework connecting AI use in SDM to\nfirm outcomes and discuss how AI may reshape sources of competitive advantage.\nWe conclude by considering how AI could both support and challenge core tenets\nof the theory-based view of strategy. Overall, our work maps out an emerging\nresearch frontier at the intersection of AI and strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how artificial intelligence (AI) may impact the strategic\ndecision-making (SDM) process in firms. We illustrate how AI could augment\nexisting SDM tools and provide empirical evidence from a leading accelerator\nprogram and a startup competition that current Large Language Models (LLMs) can\ngenerate and evaluate strategies at a level comparable to entrepreneurs and\ninvestors. We then examine implications for key cognitive processes underlying\nSDM -- search, representation, and aggregation. Our analysis suggests AI has\nthe potential to enhance the speed, quality, and scale of strategic analysis,\nwhile also enabling new approaches like virtual strategy simulations. However,\nthe ultimate impact on firm performance will depend on competitive dynamics as\nAI capabilities progress. We propose a framework connecting AI use in SDM to\nfirm outcomes and discuss how AI may reshape sources of competitive advantage.\nWe conclude by considering how AI could both support and challenge core tenets\nof the theory-based view of strategy. Overall, our work maps out an emerging\nresearch frontier at the intersection of AI and strategy."
                },
                "authors": [
                    {
                        "name": "Felipe A. Csaszar"
                    },
                    {
                        "name": "Harsh Ketkar"
                    },
                    {
                        "name": "Hyunjin Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyunjin Kim"
                },
                "author": "Hyunjin Kim",
                "arxiv_comment": "55 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08808v2",
                "updated": "2024-08-19T16:44:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    44,
                    30,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-16T15:41:43Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    41,
                    43,
                    4,
                    229,
                    0
                ],
                "title": "Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the landscape of machine\nlearning, yet current benchmarks often fall short in capturing the diverse\nbehavior of these models in real-world applications. A benchmark's usefulness\nis determined by its ability to clearly differentiate between models of varying\ncapabilities (separability) and closely align with human preferences. Existing\nframeworks like Alpaca-Eval 2.0 LC\n\\cite{dubois2024lengthcontrolledalpacaevalsimpleway} and Arena-Hard v0.1\n\\cite{li2024crowdsourced} are limited by their focus on general-purpose queries\nand lack of diversity across domains such as law, medicine, and multilingual\ncontexts. In this paper, we address these limitations by introducing a novel\ndata pipeline that curates diverse, domain-specific evaluation sets tailored\nfor LLM-as-a-Judge frameworks. Our approach leverages a combination of manual\ncuration, semi-supervised learning to generate clusters, and stratified\nsampling to ensure balanced representation across a wide range of domains and\nlanguages. The resulting evaluation set, which includes 1573 samples across 14\ncategories, demonstrates high separability (84\\%) across ten top-ranked models,\nand agreement (84\\%) with Chatbot Arena and (0.915) Spearman correlation. The\nagreement values are 9\\% better than Arena Hard and 20\\% better than AlpacaEval\n2.0 LC, while the Spearman coefficient is 0.7 more than the next best\nbenchmark, showcasing a significant improvement in the usefulness of the\nbenchmark. We further provide an open-source evaluation tool that enables\nfine-grained analysis of model performance across user-defined categories,\noffering valuable insights for practitioners. This work contributes to the\nongoing effort to enhance the transparency, diversity, and effectiveness of LLM\nevaluation methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the landscape of machine\nlearning, yet current benchmarks often fall short in capturing the diverse\nbehavior of these models in real-world applications. A benchmark's usefulness\nis determined by its ability to clearly differentiate between models of varying\ncapabilities (separability) and closely align with human preferences. Existing\nframeworks like Alpaca-Eval 2.0 LC\n\\cite{dubois2024lengthcontrolledalpacaevalsimpleway} and Arena-Hard v0.1\n\\cite{li2024crowdsourced} are limited by their focus on general-purpose queries\nand lack of diversity across domains such as law, medicine, and multilingual\ncontexts. In this paper, we address these limitations by introducing a novel\ndata pipeline that curates diverse, domain-specific evaluation sets tailored\nfor LLM-as-a-Judge frameworks. Our approach leverages a combination of manual\ncuration, semi-supervised learning to generate clusters, and stratified\nsampling to ensure balanced representation across a wide range of domains and\nlanguages. The resulting evaluation set, which includes 1573 samples across 14\ncategories, demonstrates high separability (84\\%) across ten top-ranked models,\nand agreement (84\\%) with Chatbot Arena and (0.915) Spearman correlation. The\nagreement values are 9\\% better than Arena Hard and 20\\% better than AlpacaEval\n2.0 LC, while the Spearman coefficient is 0.7 more than the next best\nbenchmark, showcasing a significant improvement in the usefulness of the\nbenchmark. We further provide an open-source evaluation tool that enables\nfine-grained analysis of model performance across user-defined categories,\noffering valuable insights for practitioners. This work contributes to the\nongoing effort to enhance the transparency, diversity, and effectiveness of LLM\nevaluation methodologies."
                },
                "authors": [
                    {
                        "name": "Ravi Raju"
                    },
                    {
                        "name": "Swayambhoo Jain"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Urmish Thakkar"
                    }
                ],
                "author_detail": {
                    "name": "Urmish Thakkar"
                },
                "author": "Urmish Thakkar",
                "arxiv_comment": "14 pages, 8 figures, Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08660v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08660v2",
                "updated": "2024-08-16T15:33:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    33,
                    23,
                    4,
                    229,
                    0
                ],
                "published": "2024-06-12T21:46:13Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    21,
                    46,
                    13,
                    2,
                    164,
                    0
                ],
                "title": "Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot\n  Generative AI Models in Text Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot\n  Generative AI Models in Text Classification"
                },
                "summary": "Generative AI offers a simple, prompt-based alternative to fine-tuning\nsmaller BERT-style LLMs for text classification tasks. This promises to\neliminate the need for manually labeled training data and task-specific model\ntraining. However, it remains an open question whether tools like ChatGPT can\ndeliver on this promise. In this paper, we show that smaller, fine-tuned LLMs\n(still) consistently and significantly outperform larger, zero-shot prompted\nmodels in text classification. We compare three major generative AI models\n(ChatGPT with GPT-3.5/GPT-4 and Claude Opus) with several fine-tuned LLMs\nacross a diverse set of classification tasks (sentiment, approval/disapproval,\nemotions, party positions) and text categories (news, tweets, speeches). We\nfind that fine-tuning with application-specific training data achieves superior\nperformance in all cases. To make this approach more accessible to a broader\naudience, we provide an easy-to-use toolkit alongside this paper. Our toolkit,\naccompanied by non-technical step-by-step guidance, enables users to select and\nfine-tune BERT-like LLMs for any classification task with minimal technical and\ncomputational effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI offers a simple, prompt-based alternative to fine-tuning\nsmaller BERT-style LLMs for text classification tasks. This promises to\neliminate the need for manually labeled training data and task-specific model\ntraining. However, it remains an open question whether tools like ChatGPT can\ndeliver on this promise. In this paper, we show that smaller, fine-tuned LLMs\n(still) consistently and significantly outperform larger, zero-shot prompted\nmodels in text classification. We compare three major generative AI models\n(ChatGPT with GPT-3.5/GPT-4 and Claude Opus) with several fine-tuned LLMs\nacross a diverse set of classification tasks (sentiment, approval/disapproval,\nemotions, party positions) and text categories (news, tweets, speeches). We\nfind that fine-tuning with application-specific training data achieves superior\nperformance in all cases. To make this approach more accessible to a broader\naudience, we provide an easy-to-use toolkit alongside this paper. Our toolkit,\naccompanied by non-technical step-by-step guidance, enables users to select and\nfine-tune BERT-like LLMs for any classification task with minimal technical and\ncomputational effort."
                },
                "authors": [
                    {
                        "name": "Martin Juan José Bucher"
                    },
                    {
                        "name": "Marco Martini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Martini"
                },
                "author": "Marco Martini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08660v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08660v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08794v1",
                "updated": "2024-08-16T15:07:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    7,
                    54,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T15:07:54Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    7,
                    54,
                    4,
                    229,
                    0
                ],
                "title": "Xpikeformer: Hybrid Analog-Digital Hardware Acceleration for Spiking\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Xpikeformer: Hybrid Analog-Digital Hardware Acceleration for Spiking\n  Transformers"
                },
                "summary": "This paper introduces Xpikeformer, a hybrid analog-digital hardware\narchitecture designed to accelerate spiking neural network (SNN)-based\ntransformer models. By combining the energy efficiency and temporal dynamics of\nSNNs with the powerful sequence modeling capabilities of transformers,\nXpikeformer leverages mixed analog-digital computing techniques to enhance\nperformance and energy efficiency. The architecture integrates analog in-memory\ncomputing (AIMC) for feedforward and fully connected layers, and a stochastic\nspiking attention (SSA) engine for efficient attention mechanisms. We detail\nthe design, implementation, and evaluation of Xpikeformer, demonstrating\nsignificant improvements in energy consumption and computational efficiency.\nThrough an image classification task and a wireless communication symbol\ndetection task, we show that Xpikeformer can achieve software-comparable\ninference accuracy. Energy evaluations reveal that Xpikeformer achieves up to a\n$17.8$--$19.2\\times$ reduction in energy consumption compared to\nstate-of-the-art digital ANN transformers and up to a $5.9$--$6.8\\times$\nreduction compared to fully digital SNN transformers. Xpikeformer also achieves\na $12.0\\times$ speedup compared to the GPU implementation of spiking\ntransformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Xpikeformer, a hybrid analog-digital hardware\narchitecture designed to accelerate spiking neural network (SNN)-based\ntransformer models. By combining the energy efficiency and temporal dynamics of\nSNNs with the powerful sequence modeling capabilities of transformers,\nXpikeformer leverages mixed analog-digital computing techniques to enhance\nperformance and energy efficiency. The architecture integrates analog in-memory\ncomputing (AIMC) for feedforward and fully connected layers, and a stochastic\nspiking attention (SSA) engine for efficient attention mechanisms. We detail\nthe design, implementation, and evaluation of Xpikeformer, demonstrating\nsignificant improvements in energy consumption and computational efficiency.\nThrough an image classification task and a wireless communication symbol\ndetection task, we show that Xpikeformer can achieve software-comparable\ninference accuracy. Energy evaluations reveal that Xpikeformer achieves up to a\n$17.8$--$19.2\\times$ reduction in energy consumption compared to\nstate-of-the-art digital ANN transformers and up to a $5.9$--$6.8\\times$\nreduction compared to fully digital SNN transformers. Xpikeformer also achieves\na $12.0\\times$ speedup compared to the GPU implementation of spiking\ntransformers."
                },
                "authors": [
                    {
                        "name": "Zihang Song"
                    },
                    {
                        "name": "Prabodh Katti"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    },
                    {
                        "name": "Bipin Rajendran"
                    }
                ],
                "author_detail": {
                    "name": "Bipin Rajendran"
                },
                "author": "Bipin Rajendran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14322v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14322v3",
                "updated": "2024-08-16T15:02:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    2,
                    45,
                    4,
                    229,
                    0
                ],
                "published": "2024-06-20T13:54:32Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    13,
                    54,
                    32,
                    3,
                    172,
                    0
                ],
                "title": "Mind the Privacy Unit! User-Level Differential Privacy for Language\n  Model Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Privacy Unit! User-Level Differential Privacy for Language\n  Model Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have emerged as powerful tools for tackling\ncomplex tasks across diverse domains, but they also raise privacy concerns when\nfine-tuned on sensitive data due to potential memorization. While differential\nprivacy (DP) offers a promising solution by ensuring models are 'almost\nindistinguishable' with or without any particular privacy unit, current\nevaluations on LLMs mostly treat each example (text record) as the privacy\nunit. This leads to uneven user privacy guarantees when contributions per user\nvary. We therefore study user-level DP motivated by applications where it\nnecessary to ensure uniform privacy protection across users. We present a\nsystematic evaluation of user-level DP for LLM fine-tuning on natural language\ngeneration tasks. Focusing on two mechanisms for achieving user-level DP\nguarantees, Group Privacy and User-wise DP-SGD, we investigate design choices\nlike data selection strategies and parameter tuning for the best\nprivacy-utility tradeoff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have emerged as powerful tools for tackling\ncomplex tasks across diverse domains, but they also raise privacy concerns when\nfine-tuned on sensitive data due to potential memorization. While differential\nprivacy (DP) offers a promising solution by ensuring models are 'almost\nindistinguishable' with or without any particular privacy unit, current\nevaluations on LLMs mostly treat each example (text record) as the privacy\nunit. This leads to uneven user privacy guarantees when contributions per user\nvary. We therefore study user-level DP motivated by applications where it\nnecessary to ensure uniform privacy protection across users. We present a\nsystematic evaluation of user-level DP for LLM fine-tuning on natural language\ngeneration tasks. Focusing on two mechanisms for achieving user-level DP\nguarantees, Group Privacy and User-wise DP-SGD, we investigate design choices\nlike data selection strategies and parameter tuning for the best\nprivacy-utility tradeoff."
                },
                "authors": [
                    {
                        "name": "Lynn Chua"
                    },
                    {
                        "name": "Badih Ghazi"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Pritish Kamath"
                    },
                    {
                        "name": "Ravi Kumar"
                    },
                    {
                        "name": "Daogao Liu"
                    },
                    {
                        "name": "Pasin Manurangsi"
                    },
                    {
                        "name": "Amer Sinha"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chiyuan Zhang"
                },
                "author": "Chiyuan Zhang",
                "arxiv_comment": "Published as a conference paper at COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14322v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14322v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14267v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14267v3",
                "updated": "2024-08-16T14:56:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    56,
                    36,
                    4,
                    229,
                    0
                ],
                "published": "2024-01-25T16:01:49Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    16,
                    1,
                    49,
                    3,
                    25,
                    0
                ],
                "title": "Transformers and Cortical Waves: Encoders for Pulling In Context Across\n  Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers and Cortical Waves: Encoders for Pulling In Context Across\n  Time"
                },
                "summary": "The capabilities of transformer networks such as ChatGPT and other Large\nLanguage Models (LLMs) have captured the world's attention. The crucial\ncomputational mechanism underlying their performance relies on transforming a\ncomplete input sequence - for example, all the words in a sentence - into a\nlong \"encoding vector\" that allows transformers to learn long-range temporal\ndependencies in naturalistic sequences. Specifically, \"self-attention\" applied\nto this encoding vector enhances temporal context in transformers by computing\nassociations between pairs of words in the input sequence. We suggest that\nwaves of neural activity traveling across single cortical areas or multiple\nregions at the whole-brain scale could implement a similar encoding principle.\nBy encapsulating recent input history into a single spatial pattern at each\nmoment in time, cortical waves may enable temporal context to be extracted from\nsequences of sensory inputs, the same computational principle used in\ntransformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities of transformer networks such as ChatGPT and other Large\nLanguage Models (LLMs) have captured the world's attention. The crucial\ncomputational mechanism underlying their performance relies on transforming a\ncomplete input sequence - for example, all the words in a sentence - into a\nlong \"encoding vector\" that allows transformers to learn long-range temporal\ndependencies in naturalistic sequences. Specifically, \"self-attention\" applied\nto this encoding vector enhances temporal context in transformers by computing\nassociations between pairs of words in the input sequence. We suggest that\nwaves of neural activity traveling across single cortical areas or multiple\nregions at the whole-brain scale could implement a similar encoding principle.\nBy encapsulating recent input history into a single spatial pattern at each\nmoment in time, cortical waves may enable temporal context to be extracted from\nsequences of sensory inputs, the same computational principle used in\ntransformers."
                },
                "authors": [
                    {
                        "name": "Lyle Muller"
                    },
                    {
                        "name": "Patricia S. Churchland"
                    },
                    {
                        "name": "Terrence J. Sejnowski"
                    }
                ],
                "author_detail": {
                    "name": "Terrence J. Sejnowski"
                },
                "author": "Terrence J. Sejnowski",
                "arxiv_comment": "27 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14267v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14267v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08782v1",
                "updated": "2024-08-16T14:54:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    54,
                    41,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T14:54:41Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    54,
                    41,
                    4,
                    229,
                    0
                ],
                "title": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics"
                },
                "summary": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nPrevious efforts have focused on developing modular dialogue systems that treat\nsocio-emotional strategy prediction as an auxiliary task and generate\nstrategy-conditioned responses with customized decoders. Recently, with\nadvancements in large language models (LLMs), end-to-end dialogue agents\nwithout explicit socio-emotional strategy prediction steps have become\nprevalent. However, despite their excellence in language generation, recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy predictor,\nEmoDynamiX, which models the discourse dynamics between user emotions and\nsystem strategies using a heterogeneous graph. Additionally, we make use of the\nEmotion Recognition in Conversations (ERC) task and design a flexible\nmixed-emotion module to capture fine-grained emotional states of the user.\nExperimental results on two ESC datasets show EmoDynamiX outperforms previous\nstate-of-the-art methods with a significant margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nPrevious efforts have focused on developing modular dialogue systems that treat\nsocio-emotional strategy prediction as an auxiliary task and generate\nstrategy-conditioned responses with customized decoders. Recently, with\nadvancements in large language models (LLMs), end-to-end dialogue agents\nwithout explicit socio-emotional strategy prediction steps have become\nprevalent. However, despite their excellence in language generation, recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy predictor,\nEmoDynamiX, which models the discourse dynamics between user emotions and\nsystem strategies using a heterogeneous graph. Additionally, we make use of the\nEmotion Recognition in Conversations (ERC) task and design a flexible\nmixed-emotion module to capture fine-grained emotional states of the user.\nExperimental results on two ESC datasets show EmoDynamiX outperforms previous\nstate-of-the-art methods with a significant margin."
                },
                "authors": [
                    {
                        "name": "Chenwei Wan"
                    },
                    {
                        "name": "Matthieu Labeau"
                    },
                    {
                        "name": "Chloé Clavel"
                    }
                ],
                "author_detail": {
                    "name": "Chloé Clavel"
                },
                "author": "Chloé Clavel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08781v1",
                "updated": "2024-08-16T14:49:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    49,
                    35,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T14:49:35Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    49,
                    35,
                    4,
                    229,
                    0
                ],
                "title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation\n  Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation\n  Instructions"
                },
                "summary": "LLMs-as-a-judge is a recently popularized method which replaces human\njudgements in task evaluation (Zheng et al. 2024) with automatic evaluation\nusing LLMs. Due to widespread use of RLHF (Reinforcement Learning from Human\nFeedback), state-of-the-art LLMs like GPT4 and Llama3 are expected to have\nstrong alignment with human preferences when prompted for a quality judgement,\nsuch as the coherence of a text. While this seems beneficial, it is not clear\nwhether the assessments by an LLM-as-a-judge constitute only an evaluation\nbased on the instructions in the prompts, or reflect its preference for\nhigh-quality data similar to its fine-tune data. To investigate how much\ninfluence prompting the LLMs-as-a-judge has on the alignment of AI judgements\nto human judgements, we analyze prompts with increasing levels of instructions\nabout the target quality of an evaluation, for several LLMs-as-a-judge.\nFurther, we compare to a prompt-free method using model perplexity as a quality\nmeasure instead. We aggregate a taxonomy of quality criteria commonly used\nacross state-of-the-art evaluations with LLMs and provide this as a rigorous\nbenchmark of models as judges. Overall, we show that the LLMs-as-a-judge\nbenefit only little from highly detailed instructions in prompts and that\nperplexity can sometimes align better with human judgements than prompting,\nespecially on textual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs-as-a-judge is a recently popularized method which replaces human\njudgements in task evaluation (Zheng et al. 2024) with automatic evaluation\nusing LLMs. Due to widespread use of RLHF (Reinforcement Learning from Human\nFeedback), state-of-the-art LLMs like GPT4 and Llama3 are expected to have\nstrong alignment with human preferences when prompted for a quality judgement,\nsuch as the coherence of a text. While this seems beneficial, it is not clear\nwhether the assessments by an LLM-as-a-judge constitute only an evaluation\nbased on the instructions in the prompts, or reflect its preference for\nhigh-quality data similar to its fine-tune data. To investigate how much\ninfluence prompting the LLMs-as-a-judge has on the alignment of AI judgements\nto human judgements, we analyze prompts with increasing levels of instructions\nabout the target quality of an evaluation, for several LLMs-as-a-judge.\nFurther, we compare to a prompt-free method using model perplexity as a quality\nmeasure instead. We aggregate a taxonomy of quality criteria commonly used\nacross state-of-the-art evaluations with LLMs and provide this as a rigorous\nbenchmark of models as judges. Overall, we show that the LLMs-as-a-judge\nbenefit only little from highly detailed instructions in prompts and that\nperplexity can sometimes align better with human judgements than prompting,\nespecially on textual quality."
                },
                "authors": [
                    {
                        "name": "Bhuvanashree Murugadoss"
                    },
                    {
                        "name": "Christian Poelitz"
                    },
                    {
                        "name": "Ian Drosos"
                    },
                    {
                        "name": "Vu Le"
                    },
                    {
                        "name": "Nick McKenna"
                    },
                    {
                        "name": "Carina Suzana Negreanu"
                    },
                    {
                        "name": "Chris Parnin"
                    },
                    {
                        "name": "Advait Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Advait Sarkar"
                },
                "author": "Advait Sarkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08780v1",
                "updated": "2024-08-16T14:49:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    49,
                    4,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T14:49:04Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    49,
                    4,
                    4,
                    229,
                    0
                ],
                "title": "Large Language Models Might Not Care What You Are Saying: Prompt Format\n  Beats Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Might Not Care What You Are Saying: Prompt Format\n  Beats Descriptions"
                },
                "summary": "With the help of in-context learning (ICL), large language models (LLMs) have\nachieved impressive performance across various tasks. However, the function of\ndescriptive instructions during ICL remains under-explored. In this work, we\npropose an ensemble prompt framework to describe the selection criteria of\nmultiple in-context examples, and preliminary experiments on machine\ntranslation (MT) across six translation directions confirm that this framework\nboosts ICL perfromance. But to our surprise, LLMs might not necessarily care\nwhat the descriptions actually say, and the performance gain is primarily\ncaused by the ensemble format, since the framework could lead to improvement\neven with random descriptive nouns. We further apply this new ensemble prompt\non a range of commonsense, math, logical reasoning and hallucination tasks with\nthree LLMs and achieve promising results, suggesting again that designing a\nproper prompt format would be much more effective and efficient than paying\neffort into specific descriptions. Our code will be publicly available once\nthis paper is published.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the help of in-context learning (ICL), large language models (LLMs) have\nachieved impressive performance across various tasks. However, the function of\ndescriptive instructions during ICL remains under-explored. In this work, we\npropose an ensemble prompt framework to describe the selection criteria of\nmultiple in-context examples, and preliminary experiments on machine\ntranslation (MT) across six translation directions confirm that this framework\nboosts ICL perfromance. But to our surprise, LLMs might not necessarily care\nwhat the descriptions actually say, and the performance gain is primarily\ncaused by the ensemble format, since the framework could lead to improvement\neven with random descriptive nouns. We further apply this new ensemble prompt\non a range of commonsense, math, logical reasoning and hallucination tasks with\nthree LLMs and achieve promising results, suggesting again that designing a\nproper prompt format would be much more effective and efficient than paying\neffort into specific descriptions. Our code will be publicly available once\nthis paper is published."
                },
                "authors": [
                    {
                        "name": "Chenming Tang"
                    },
                    {
                        "name": "Zhixiang Wang"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "arxiv_comment": "10 pages, 6 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08779v1",
                "updated": "2024-08-16T14:43:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    43,
                    15,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T14:43:15Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    43,
                    15,
                    4,
                    229,
                    0
                ],
                "title": "DAC: Decomposed Automation Correction for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAC: Decomposed Automation Correction for Text-to-SQL"
                },
                "summary": "Text-to-SQL is an important task that helps people obtain information from\ndatabases by automatically generating SQL queries. Considering the brilliant\nperformance, approaches based on Large Language Models (LLMs) become the\nmainstream for text-to-SQL. Among these approaches, automated correction is an\neffective approach that further enhances performance by correcting the mistakes\nin the generated results. The existing correction methods require LLMs to\ndirectly correct with generated SQL, while previous research shows that LLMs do\nnot know how to detect mistakes, leading to poor performance. Therefore, in\nthis paper, we propose to employ the decomposed correction to enhance\ntext-to-SQL performance. We first demonstrate that decomposed correction\noutperforms direct correction since detecting and fixing mistakes with the\nresults of the decomposed sub-tasks is easier than with SQL. Based on this\nanalysis, we introduce Decomposed Automation Correction (DAC), which corrects\nSQL by decomposing text-to-SQL into entity linking and skeleton parsing. DAC\nfirst generates the entity and skeleton corresponding to the question and then\ncompares the differences between the initial SQL and the generated entities and\nskeleton as feedback for correction. Experimental results show that our method\nimproves performance by $3.7\\%$ on average of Spider, Bird, and KaggleDBQA\ncompared with the baseline method, demonstrating the effectiveness of DAC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is an important task that helps people obtain information from\ndatabases by automatically generating SQL queries. Considering the brilliant\nperformance, approaches based on Large Language Models (LLMs) become the\nmainstream for text-to-SQL. Among these approaches, automated correction is an\neffective approach that further enhances performance by correcting the mistakes\nin the generated results. The existing correction methods require LLMs to\ndirectly correct with generated SQL, while previous research shows that LLMs do\nnot know how to detect mistakes, leading to poor performance. Therefore, in\nthis paper, we propose to employ the decomposed correction to enhance\ntext-to-SQL performance. We first demonstrate that decomposed correction\noutperforms direct correction since detecting and fixing mistakes with the\nresults of the decomposed sub-tasks is easier than with SQL. Based on this\nanalysis, we introduce Decomposed Automation Correction (DAC), which corrects\nSQL by decomposing text-to-SQL into entity linking and skeleton parsing. DAC\nfirst generates the entity and skeleton corresponding to the question and then\ncompares the differences between the initial SQL and the generated entities and\nskeleton as feedback for correction. Experimental results show that our method\nimproves performance by $3.7\\%$ on average of Spider, Bird, and KaggleDBQA\ncompared with the baseline method, demonstrating the effectiveness of DAC."
                },
                "authors": [
                    {
                        "name": "Dingzirui Wang"
                    },
                    {
                        "name": "Longxu Dou"
                    },
                    {
                        "name": "Xuanliang Zhang"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08769v1",
                "updated": "2024-08-16T14:23:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    23,
                    59,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T14:23:59Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    23,
                    59,
                    4,
                    229,
                    0
                ],
                "title": "Lower Layer Matters: Alleviating Hallucination via Multi-Layer Fusion\n  Contrastive Decoding with Truthfulness Refocused",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lower Layer Matters: Alleviating Hallucination via Multi-Layer Fusion\n  Contrastive Decoding with Truthfulness Refocused"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks, yet they occasionally tend to yield\ncontent that factually inaccurate or discordant with the expected output, a\nphenomenon empirically referred to as \"hallucination\". To tackle this issue,\nrecent works have investigated contrastive decoding between the original model\nand an amateur model with induced hallucination, which has shown promising\nresults. Nonetheless, this method may undermine the output distribution of the\noriginal LLM caused by its coarse contrast and simplistic subtraction\noperation, potentially leading to errors in certain cases. In this paper, we\nintroduce a novel contrastive decoding framework termed LOL (LOwer Layer\nMatters). Our approach involves concatenating the contrastive decoding of both\nthe final and lower layers between the original model and the amateur model,\nthereby achieving multi-layer fusion to aid in the mitigation of hallucination.\nAdditionally, we incorporate a truthfulness refocused module that leverages\ncontextual guidance to enhance factual encoding, further capturing truthfulness\nduring contrastive decoding. Extensive experiments conducted on two publicly\navailable datasets illustrate that our proposed LOL framework can substantially\nalleviate hallucination while surpassing existing baselines in most cases.\nCompared with the best baseline, we improve by average 4.5 points on all\nmetrics of TruthfulQA. The source code is coming soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks, yet they occasionally tend to yield\ncontent that factually inaccurate or discordant with the expected output, a\nphenomenon empirically referred to as \"hallucination\". To tackle this issue,\nrecent works have investigated contrastive decoding between the original model\nand an amateur model with induced hallucination, which has shown promising\nresults. Nonetheless, this method may undermine the output distribution of the\noriginal LLM caused by its coarse contrast and simplistic subtraction\noperation, potentially leading to errors in certain cases. In this paper, we\nintroduce a novel contrastive decoding framework termed LOL (LOwer Layer\nMatters). Our approach involves concatenating the contrastive decoding of both\nthe final and lower layers between the original model and the amateur model,\nthereby achieving multi-layer fusion to aid in the mitigation of hallucination.\nAdditionally, we incorporate a truthfulness refocused module that leverages\ncontextual guidance to enhance factual encoding, further capturing truthfulness\nduring contrastive decoding. Extensive experiments conducted on two publicly\navailable datasets illustrate that our proposed LOL framework can substantially\nalleviate hallucination while surpassing existing baselines in most cases.\nCompared with the best baseline, we improve by average 4.5 points on all\nmetrics of TruthfulQA. The source code is coming soon."
                },
                "authors": [
                    {
                        "name": "Dingwei Chen"
                    },
                    {
                        "name": "Feiteng Fang"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Feng Liang"
                    },
                    {
                        "name": "Ruifeng Xu"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Chengming Li"
                    }
                ],
                "author_detail": {
                    "name": "Chengming Li"
                },
                "author": "Chengming Li",
                "arxiv_comment": "9 pages, 4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08765v1",
                "updated": "2024-08-16T14:18:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    18,
                    37,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T14:18:37Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    18,
                    37,
                    4,
                    229,
                    0
                ],
                "title": "Rethinking Generative Semantic Communication for Multi-User Systems with\n  Multi-Modal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Generative Semantic Communication for Multi-User Systems with\n  Multi-Modal LLM"
                },
                "summary": "The surge in connected devices in 6G with typical massive access scenarios,\nsuch as smart agriculture, and smart cities, poses significant challenges to\nunsustainable traditional communication with limited radio resources and\nalready high system complexity. Fortunately, the booming artificial\nintelligence technology and the growing computational power of devices offer a\npromising 6G enabler: semantic communication (SemCom). However, existing deep\nlearning-based SemCom paradigms struggle to extend to multi-user scenarios due\nto their rigid end-to-end training approach. Consequently, to truly empower 6G\nnetworks with this critical technology, this article rethinks generative SemCom\nfor multi-user system with multi-modal large language model (MLLM), and propose\na novel framework called \"M2GSC\". In this framework, the MLLM, which serves as\nshared knowledge base (SKB), plays three critical roles for complex tasks,\nspawning a series of benefits such as semantic encoding standardization and\nsemantic decoding personalization. Meanwhile, to enhance the performance of\nM2GSC framework and to advance its implementation in 6G, we highlight three\nresearch directions on M2GSC framework, namely, upgrading SKB to closed loop\nagent, adaptive semantic encoding offloading, and streamlined semantic decoding\noffloading. Finally, a case study is conducted to demonstrate the preliminary\nvalidation on the effectiveness of the M2GSC framework in terms of streamlined\ndecoding offloading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surge in connected devices in 6G with typical massive access scenarios,\nsuch as smart agriculture, and smart cities, poses significant challenges to\nunsustainable traditional communication with limited radio resources and\nalready high system complexity. Fortunately, the booming artificial\nintelligence technology and the growing computational power of devices offer a\npromising 6G enabler: semantic communication (SemCom). However, existing deep\nlearning-based SemCom paradigms struggle to extend to multi-user scenarios due\nto their rigid end-to-end training approach. Consequently, to truly empower 6G\nnetworks with this critical technology, this article rethinks generative SemCom\nfor multi-user system with multi-modal large language model (MLLM), and propose\na novel framework called \"M2GSC\". In this framework, the MLLM, which serves as\nshared knowledge base (SKB), plays three critical roles for complex tasks,\nspawning a series of benefits such as semantic encoding standardization and\nsemantic decoding personalization. Meanwhile, to enhance the performance of\nM2GSC framework and to advance its implementation in 6G, we highlight three\nresearch directions on M2GSC framework, namely, upgrading SKB to closed loop\nagent, adaptive semantic encoding offloading, and streamlined semantic decoding\noffloading. Finally, a case study is conducted to demonstrate the preliminary\nvalidation on the effectiveness of the M2GSC framework in terms of streamlined\ndecoding offloading."
                },
                "authors": [
                    {
                        "name": "Wanting Yang"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Shiwen Mao"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    },
                    {
                        "name": "Ping Zhang"
                    },
                    {
                        "name": "Merouane Debbah"
                    },
                    {
                        "name": "Rahim Tafazolli"
                    }
                ],
                "author_detail": {
                    "name": "Rahim Tafazolli"
                },
                "author": "Rahim Tafazolli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08764v1",
                "updated": "2024-08-16T14:14:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    14,
                    48,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T14:14:48Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    14,
                    48,
                    4,
                    229,
                    0
                ],
                "title": "Generalized logistic model for $r$ largest order statistics, with\n  hydrological application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized logistic model for $r$ largest order statistics, with\n  hydrological application"
                },
                "summary": "The effective use of available information in extreme value analysis is\ncritical because extreme values are scarce. Thus, using the $r$ largest order\nstatistics (rLOS) instead of the block maxima is encouraged. Based on the\nfour-parameter kappa model for the rLOS (rK4D), we introduce a new distribution\nfor the rLOS as a special case of the rK4D. That is the generalized logistic\nmodel for rLOS (rGLO). This distribution can be useful when the generalized\nextreme value model for rLOS is no longer efficient to capture the variability\nof extreme values. Moreover, the rGLO enriches a pool of candidate\ndistributions to determine the best model to yield accurate and robust quantile\nestimates. We derive a joint probability density function, the marginal and\nconditional distribution functions of new model. The maximum likelihood\nestimation, delta method, profile likelihood, order selection by the entropy\ndifference test, cross-validated likelihood criteria, and model averaging were\nconsidered for inferences. The usefulness and practical effectiveness of the\nrGLO are illustrated by the Monte Carlo simulation and an application to\nextreme streamflow data in Bevern Stream, UK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effective use of available information in extreme value analysis is\ncritical because extreme values are scarce. Thus, using the $r$ largest order\nstatistics (rLOS) instead of the block maxima is encouraged. Based on the\nfour-parameter kappa model for the rLOS (rK4D), we introduce a new distribution\nfor the rLOS as a special case of the rK4D. That is the generalized logistic\nmodel for rLOS (rGLO). This distribution can be useful when the generalized\nextreme value model for rLOS is no longer efficient to capture the variability\nof extreme values. Moreover, the rGLO enriches a pool of candidate\ndistributions to determine the best model to yield accurate and robust quantile\nestimates. We derive a joint probability density function, the marginal and\nconditional distribution functions of new model. The maximum likelihood\nestimation, delta method, profile likelihood, order selection by the entropy\ndifference test, cross-validated likelihood criteria, and model averaging were\nconsidered for inferences. The usefulness and practical effectiveness of the\nrGLO are illustrated by the Monte Carlo simulation and an application to\nextreme streamflow data in Bevern Stream, UK."
                },
                "authors": [
                    {
                        "name": "Yire Shin"
                    },
                    {
                        "name": "Jeong-Soo Park"
                    }
                ],
                "author_detail": {
                    "name": "Jeong-Soo Park"
                },
                "author": "Jeong-Soo Park",
                "arxiv_doi": "10.1007/s00477-023-02642-7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00477-023-02642-7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.08764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Stoch Environ Res Risk Assess 38 (2024) 1567-1581",
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08752v1",
                "updated": "2024-08-16T13:53:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    13,
                    53,
                    52,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T13:53:52Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    13,
                    53,
                    52,
                    4,
                    229,
                    0
                ],
                "title": "Quantifying Signal-to-Noise Ratio in Neural Latent Trajectories via\n  Fisher Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Signal-to-Noise Ratio in Neural Latent Trajectories via\n  Fisher Information"
                },
                "summary": "Spike train signals recorded from a large population of neurons often exhibit\nlow-dimensional spatio-temporal structure and modeled as conditional Poisson\nobservations. The low-dimensional signals that capture internal brain states\nare useful for building brain machine interfaces and understanding the neural\ncomputation underlying meaningful behavior. We derive a practical upper bound\nto the signal-to-noise ratio (SNR) of inferred neural latent trajectories using\nFisher information. We show that the SNR bound is proportional to the\noverdispersion factor and the Fisher information per neuron. Further numerical\nexperiments show that inference methods that exploit the temporal regularities\ncan achieve higher SNRs that are proportional to the bound. Our results provide\ninsights for fitting models to data, simulating neural responses, and design of\nexperiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spike train signals recorded from a large population of neurons often exhibit\nlow-dimensional spatio-temporal structure and modeled as conditional Poisson\nobservations. The low-dimensional signals that capture internal brain states\nare useful for building brain machine interfaces and understanding the neural\ncomputation underlying meaningful behavior. We derive a practical upper bound\nto the signal-to-noise ratio (SNR) of inferred neural latent trajectories using\nFisher information. We show that the SNR bound is proportional to the\noverdispersion factor and the Fisher information per neuron. Further numerical\nexperiments show that inference methods that exploit the temporal regularities\ncan achieve higher SNRs that are proportional to the bound. Our results provide\ninsights for fitting models to data, simulating neural responses, and design of\nexperiments."
                },
                "authors": [
                    {
                        "name": "Hyungju Jeon"
                    },
                    {
                        "name": "Il Memming Park"
                    }
                ],
                "author_detail": {
                    "name": "Il Memming Park"
                },
                "author": "Il Memming Park",
                "arxiv_comment": "This article is accepted for publication in the 2024 European Signal\n  Processing Conference (EUSIPCO)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00916v2",
                "updated": "2024-08-16T13:52:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    13,
                    52,
                    0,
                    4,
                    229,
                    0
                ],
                "published": "2024-03-01T19:00:45Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    19,
                    0,
                    45,
                    4,
                    61,
                    0
                ],
                "title": "Characterizing Signalling: Connections between Causal Inference and\n  Space-time Geometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing Signalling: Connections between Causal Inference and\n  Space-time Geometry"
                },
                "summary": "Causality is pivotal to our understanding of the world, presenting itself in\ndifferent forms: information-theoretic and relativistic, the former linked to\nthe flow of information, the latter to the structure of space-time. Leveraging\na framework introduced in PRA, 106, 032204 (2022), which formally connects\nthese two notions in general physical theories, we study their interplay. Here,\ninformation-theoretic causality is defined through a causal modelling approach.\nFirst, we improve the characterization of information-theoretic signalling as\ndefined through so-called affects relations. Specifically, we provide\nconditions for identifying redundancies in different parts of such a relation,\nintroducing techniques for causal inference in unfaithful causal models (where\nthe observable data does not \"faithfully\" reflect the causal dependences). In\nparticular, this demonstrates the possibility of causal inference using the\nabsence of signalling between certain nodes. Second, we define an\norder-theoretic property called conicality, showing that it is satisfied for\nlight cones in Minkowski space-times with $d>1$ spatial dimensions but violated\nfor $d=1$. Finally, we study the embedding of information-theoretic causal\nmodels in space-time without violating relativistic principles such as no\nsuperluminal signalling (NSS). In general, we observe that constraints imposed\nby NSS in a space-time and those imposed by purely information-theoretic causal\ninference behave differently. We then prove a correspondence between conical\nspace-times and faithful causal models: in both cases, there emerges a parallel\nbetween these two types of constraints. This indicates a connection between\ninformational and geometric notions of causality, and offers new insights for\nstudying the relations between the principles of NSS and no causal loops in\ndifferent space-time geometries and theories of information processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causality is pivotal to our understanding of the world, presenting itself in\ndifferent forms: information-theoretic and relativistic, the former linked to\nthe flow of information, the latter to the structure of space-time. Leveraging\na framework introduced in PRA, 106, 032204 (2022), which formally connects\nthese two notions in general physical theories, we study their interplay. Here,\ninformation-theoretic causality is defined through a causal modelling approach.\nFirst, we improve the characterization of information-theoretic signalling as\ndefined through so-called affects relations. Specifically, we provide\nconditions for identifying redundancies in different parts of such a relation,\nintroducing techniques for causal inference in unfaithful causal models (where\nthe observable data does not \"faithfully\" reflect the causal dependences). In\nparticular, this demonstrates the possibility of causal inference using the\nabsence of signalling between certain nodes. Second, we define an\norder-theoretic property called conicality, showing that it is satisfied for\nlight cones in Minkowski space-times with $d>1$ spatial dimensions but violated\nfor $d=1$. Finally, we study the embedding of information-theoretic causal\nmodels in space-time without violating relativistic principles such as no\nsuperluminal signalling (NSS). In general, we observe that constraints imposed\nby NSS in a space-time and those imposed by purely information-theoretic causal\ninference behave differently. We then prove a correspondence between conical\nspace-times and faithful causal models: in both cases, there emerges a parallel\nbetween these two types of constraints. This indicates a connection between\ninformational and geometric notions of causality, and offers new insights for\nstudying the relations between the principles of NSS and no causal loops in\ndifferent space-time geometries and theories of information processing."
                },
                "authors": [
                    {
                        "name": "Maarten Grothus"
                    },
                    {
                        "name": "V. Vilasini"
                    }
                ],
                "author_detail": {
                    "name": "V. Vilasini"
                },
                "author": "V. Vilasini",
                "arxiv_comment": "27 + 24 pages, 12 figures. This work includes significantly improved\n  versions of initial results presented in MG's master's thesis\n  arXiv:2211.03593. v2 adds new figures, examples, and explanations to enhance\n  the presentation of the main results and concepts. Comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06277v2",
                "updated": "2024-08-16T13:51:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    13,
                    51,
                    59,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-12T16:39:18Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    39,
                    18,
                    0,
                    225,
                    0
                ],
                "title": "Multi-marginal Schrödinger Bridges with Iterative Reference Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-marginal Schrödinger Bridges with Iterative Reference Refinement"
                },
                "summary": "Practitioners frequently aim to infer an unobserved population trajectory\nusing sample snapshots at multiple time points. For instance, in single-cell\nsequencing, scientists would like to learn how gene expression evolves over\ntime. But sequencing any cell destroys that cell. So we cannot access any\ncell's full trajectory, but we can access snapshot samples from many cells.\nStochastic differential equations are commonly used to analyze systems with\nfull individual-trajectory access; since here we have only sample snapshots,\nthese methods are inapplicable. The deep learning community has recently\nexplored using Schr\\\"odinger bridges (SBs) and their extensions to estimate\nthese dynamics. However, these methods either (1) interpolate between just two\ntime points or (2) require a single fixed reference dynamic within the SB,\nwhich is often just set to be Brownian motion. But learning piecewise from\nadjacent time points can fail to capture long-term dependencies. And\npractitioners are typically able to specify a model class for the reference\ndynamic but not the exact values of the parameters within it. So we propose a\nnew method that (1) learns the unobserved trajectories from sample snapshots\nacross multiple time points and (2) requires specification only of a class of\nreference dynamics, not a single fixed one. In particular, we suggest an\niterative projection method inspired by Schr\\\"odinger bridges; we alternate\nbetween learning a piecewise SB on the unobserved trajectories and using the\nlearned SB to refine our best guess for the dynamics within the reference\nclass. We demonstrate the advantages of our method via a well-known simulated\nparametric model from ecology, simulated and real data from systems biology,\nand real motion-capture data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practitioners frequently aim to infer an unobserved population trajectory\nusing sample snapshots at multiple time points. For instance, in single-cell\nsequencing, scientists would like to learn how gene expression evolves over\ntime. But sequencing any cell destroys that cell. So we cannot access any\ncell's full trajectory, but we can access snapshot samples from many cells.\nStochastic differential equations are commonly used to analyze systems with\nfull individual-trajectory access; since here we have only sample snapshots,\nthese methods are inapplicable. The deep learning community has recently\nexplored using Schr\\\"odinger bridges (SBs) and their extensions to estimate\nthese dynamics. However, these methods either (1) interpolate between just two\ntime points or (2) require a single fixed reference dynamic within the SB,\nwhich is often just set to be Brownian motion. But learning piecewise from\nadjacent time points can fail to capture long-term dependencies. And\npractitioners are typically able to specify a model class for the reference\ndynamic but not the exact values of the parameters within it. So we propose a\nnew method that (1) learns the unobserved trajectories from sample snapshots\nacross multiple time points and (2) requires specification only of a class of\nreference dynamics, not a single fixed one. In particular, we suggest an\niterative projection method inspired by Schr\\\"odinger bridges; we alternate\nbetween learning a piecewise SB on the unobserved trajectories and using the\nlearned SB to refine our best guess for the dynamics within the reference\nclass. We demonstrate the advantages of our method via a well-known simulated\nparametric model from ecology, simulated and real data from systems biology,\nand real motion-capture data."
                },
                "authors": [
                    {
                        "name": "Yunyi Shen"
                    },
                    {
                        "name": "Renato Berlinghieri"
                    },
                    {
                        "name": "Tamara Broderick"
                    }
                ],
                "author_detail": {
                    "name": "Tamara Broderick"
                },
                "author": "Tamara Broderick",
                "arxiv_comment": "Updated to fix title error",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08736v1",
                "updated": "2024-08-16T13:35:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    13,
                    35,
                    52,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T13:35:52Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    13,
                    35,
                    52,
                    4,
                    229,
                    0
                ],
                "title": "Task-Aware Dynamic Transformer for Efficient Arbitrary-Scale Image\n  Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Aware Dynamic Transformer for Efficient Arbitrary-Scale Image\n  Super-Resolution"
                },
                "summary": "Arbitrary-scale super-resolution (ASSR) aims to learn a single model for\nimage super-resolution at arbitrary magnifying scales. Existing ASSR networks\ntypically comprise an off-the-shelf scale-agnostic feature extractor and an\narbitrary scale upsampler. These feature extractors often use fixed network\narchitectures to address different ASSR inference tasks, each of which is\ncharacterized by an input image and an upsampling scale. However, this\noverlooks the difficulty variance of super-resolution on different inference\nscenarios, where simple images or small SR scales could be resolved with less\ncomputational effort than difficult images or large SR scales. To tackle this\ndifficulty variability, in this paper, we propose a Task-Aware Dynamic\nTransformer (TADT) as an input-adaptive feature extractor for efficient image\nASSR. Our TADT consists of a multi-scale feature extraction backbone built upon\ngroups of Multi-Scale Transformer Blocks (MSTBs) and a Task-Aware Routing\nController (TARC). The TARC predicts the inference paths within feature\nextraction backbone, specifically selecting MSTBs based on the input images and\nSR scales. The prediction of inference path is guided by a new loss function to\ntrade-off the SR accuracy and efficiency. Experiments demonstrate that, when\nworking with three popular arbitrary-scale upsamplers, our TADT achieves\nstate-of-the-art ASSR performance when compared with mainstream feature\nextractors, but with relatively fewer computational costs. The code will be\npublicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arbitrary-scale super-resolution (ASSR) aims to learn a single model for\nimage super-resolution at arbitrary magnifying scales. Existing ASSR networks\ntypically comprise an off-the-shelf scale-agnostic feature extractor and an\narbitrary scale upsampler. These feature extractors often use fixed network\narchitectures to address different ASSR inference tasks, each of which is\ncharacterized by an input image and an upsampling scale. However, this\noverlooks the difficulty variance of super-resolution on different inference\nscenarios, where simple images or small SR scales could be resolved with less\ncomputational effort than difficult images or large SR scales. To tackle this\ndifficulty variability, in this paper, we propose a Task-Aware Dynamic\nTransformer (TADT) as an input-adaptive feature extractor for efficient image\nASSR. Our TADT consists of a multi-scale feature extraction backbone built upon\ngroups of Multi-Scale Transformer Blocks (MSTBs) and a Task-Aware Routing\nController (TARC). The TARC predicts the inference paths within feature\nextraction backbone, specifically selecting MSTBs based on the input images and\nSR scales. The prediction of inference path is guided by a new loss function to\ntrade-off the SR accuracy and efficiency. Experiments demonstrate that, when\nworking with three popular arbitrary-scale upsamplers, our TADT achieves\nstate-of-the-art ASSR performance when compared with mainstream feature\nextractors, but with relatively fewer computational costs. The code will be\npublicly released."
                },
                "authors": [
                    {
                        "name": "Tianyi Xu"
                    },
                    {
                        "name": "Yiji Zhou"
                    },
                    {
                        "name": "Xiaotao Hu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Anran Zhang"
                    },
                    {
                        "name": "Xingye Qiu"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "arxiv_comment": "ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08724v1",
                "updated": "2024-08-16T13:11:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    13,
                    11,
                    53,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T13:11:53Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    13,
                    11,
                    53,
                    4,
                    229,
                    0
                ],
                "title": "ChatZero:Zero-shot Cross-Lingual Dialogue Generation via Pseudo-Target\n  Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatZero:Zero-shot Cross-Lingual Dialogue Generation via Pseudo-Target\n  Language"
                },
                "summary": "Although large language models(LLMs) show amazing capabilities, among various\nexciting applications discovered for LLMs fall short in other low-resource\nlanguages. Besides, most existing methods depend on large-scale dialogue\ncorpora and thus building systems for dialogue generation in a zero-shot\nscenario remains a considerable challenge. To address this challenge, we\npropose a novel end-to-end zero-shot dialogue generation model ChatZero based\non cross-lingual code-switching method. First, we construct code-switching\nlanguage and pseudo-target language with placeholders. Then for cross-lingual\nsemantic transfer, we employ unsupervised contrastive learning to minimize the\nsemantics gap of the source language, code-switching language, and\npseudo-target language that are mutually positive examples in the high\ndimensional semantic space. Experiments on the multilingual DailyDialog and\nDSTC7-AVSD datasets demonstrate that ChatZero can achieve more than 90\\% of the\noriginal performance under the zero-shot case compared to supervised learning,\nand achieve state-of-the-art performance compared with other baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models(LLMs) show amazing capabilities, among various\nexciting applications discovered for LLMs fall short in other low-resource\nlanguages. Besides, most existing methods depend on large-scale dialogue\ncorpora and thus building systems for dialogue generation in a zero-shot\nscenario remains a considerable challenge. To address this challenge, we\npropose a novel end-to-end zero-shot dialogue generation model ChatZero based\non cross-lingual code-switching method. First, we construct code-switching\nlanguage and pseudo-target language with placeholders. Then for cross-lingual\nsemantic transfer, we employ unsupervised contrastive learning to minimize the\nsemantics gap of the source language, code-switching language, and\npseudo-target language that are mutually positive examples in the high\ndimensional semantic space. Experiments on the multilingual DailyDialog and\nDSTC7-AVSD datasets demonstrate that ChatZero can achieve more than 90\\% of the\noriginal performance under the zero-shot case compared to supervised learning,\nand achieve state-of-the-art performance compared with other baselines."
                },
                "authors": [
                    {
                        "name": "Yongkang Liu"
                    },
                    {
                        "name": "Feng Shi"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "arxiv_comment": "ECAI2024",
                "arxiv_journal_ref": "ECAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08722v1",
                "updated": "2024-08-16T13:01:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    13,
                    1,
                    59,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T13:01:59Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    13,
                    1,
                    59,
                    4,
                    229,
                    0
                ],
                "title": "A Novel Buffered Federated Learning Framework for Privacy-Driven Anomaly\n  Detection in IIoT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Buffered Federated Learning Framework for Privacy-Driven Anomaly\n  Detection in IIoT"
                },
                "summary": "Industrial Internet of Things (IIoT) is highly sensitive to data privacy and\ncybersecurity threats. Federated Learning (FL) has emerged as a solution for\npreserving privacy, enabling private data to remain on local IIoT clients while\ncooperatively training models to detect network anomalies. However, both\nsynchronous and asynchronous FL architectures exhibit limitations, particularly\nwhen dealing with clients with varying speeds due to data heterogeneity and\nresource constraints. Synchronous architecture suffers from straggler effects,\nwhile asynchronous methods encounter communication bottlenecks. Additionally,\nFL models are prone to adversarial inference attacks aimed at disclosing\nprivate training data. To address these challenges, we propose a Buffered FL\n(BFL) framework empowered by homomorphic encryption for anomaly detection in\nheterogeneous IIoT environments. BFL utilizes a novel weighted average time\napproach to mitigate both straggler effects and communication bottlenecks,\nensuring fairness between clients with varying processing speeds through\ncollaboration with a buffer-based server. The performance results, derived from\ntwo datasets, show the superiority of BFL compared to state-of-the-art FL\nmethods, demonstrating improved accuracy and convergence speed while enhancing\nprivacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial Internet of Things (IIoT) is highly sensitive to data privacy and\ncybersecurity threats. Federated Learning (FL) has emerged as a solution for\npreserving privacy, enabling private data to remain on local IIoT clients while\ncooperatively training models to detect network anomalies. However, both\nsynchronous and asynchronous FL architectures exhibit limitations, particularly\nwhen dealing with clients with varying speeds due to data heterogeneity and\nresource constraints. Synchronous architecture suffers from straggler effects,\nwhile asynchronous methods encounter communication bottlenecks. Additionally,\nFL models are prone to adversarial inference attacks aimed at disclosing\nprivate training data. To address these challenges, we propose a Buffered FL\n(BFL) framework empowered by homomorphic encryption for anomaly detection in\nheterogeneous IIoT environments. BFL utilizes a novel weighted average time\napproach to mitigate both straggler effects and communication bottlenecks,\nensuring fairness between clients with varying processing speeds through\ncollaboration with a buffer-based server. The performance results, derived from\ntwo datasets, show the superiority of BFL compared to state-of-the-art FL\nmethods, demonstrating improved accuracy and convergence speed while enhancing\nprivacy preservation."
                },
                "authors": [
                    {
                        "name": "Samira Kamali Poorazad"
                    },
                    {
                        "name": "Chafika Benzaid"
                    },
                    {
                        "name": "Tarik Taleb"
                    }
                ],
                "author_detail": {
                    "name": "Tarik Taleb"
                },
                "author": "Tarik Taleb",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08717v1",
                "updated": "2024-08-16T12:59:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    59,
                    14,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T12:59:14Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    59,
                    14,
                    4,
                    229,
                    0
                ],
                "title": "A soft-clamped topological waveguide for phonons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A soft-clamped topological waveguide for phonons"
                },
                "summary": "Topological insulators were originally discovered for electron waves in\ncondensed matter systems. Recently this concept has been transferred to bosonic\nsystems such as photons and phonons, which propagate in materials patterned\nwith artificial lattices that emulate spin-Hall physics. This work has been\nmotivated, in part, by the prospect of topologically protected transport along\nedge channels in on-chip circuits. Importantly, even in principle, topology\nprotects propagation against backscattering, but not against loss, which has\nremained limited to the dB/cm-level for phonon waveguides, be they topological\nor not. Here, we combine advanced dissipation engineering, in particular the\nrecently introduced method of soft-clamping, with the concept of a valley-Hall\ntopological insulator for phonons. This enables on-chip phononic waveguides\nwith propagation losses of 3 dB/km at room temperature, orders of magnitude\nbelow any previous chip-scale devices. For the first time, the low losses also\nallow us to accurately quantify backscattering protection in a topological\nphonon waveguide, using high-resolution ultrasound spectroscopy. We infer that\nphonons follow a sharp, 120 degree-bend with a 99.99%-probability instead of\nbeing scattered back, and less than one phonon in a million is lost. The\nextraordinary combination of features of this novel platform suggest\napplications in classical and quantum signal routing, processing, and storage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topological insulators were originally discovered for electron waves in\ncondensed matter systems. Recently this concept has been transferred to bosonic\nsystems such as photons and phonons, which propagate in materials patterned\nwith artificial lattices that emulate spin-Hall physics. This work has been\nmotivated, in part, by the prospect of topologically protected transport along\nedge channels in on-chip circuits. Importantly, even in principle, topology\nprotects propagation against backscattering, but not against loss, which has\nremained limited to the dB/cm-level for phonon waveguides, be they topological\nor not. Here, we combine advanced dissipation engineering, in particular the\nrecently introduced method of soft-clamping, with the concept of a valley-Hall\ntopological insulator for phonons. This enables on-chip phononic waveguides\nwith propagation losses of 3 dB/km at room temperature, orders of magnitude\nbelow any previous chip-scale devices. For the first time, the low losses also\nallow us to accurately quantify backscattering protection in a topological\nphonon waveguide, using high-resolution ultrasound spectroscopy. We infer that\nphonons follow a sharp, 120 degree-bend with a 99.99%-probability instead of\nbeing scattered back, and less than one phonon in a million is lost. The\nextraordinary combination of features of this novel platform suggest\napplications in classical and quantum signal routing, processing, and storage."
                },
                "authors": [
                    {
                        "name": "Xiang Xi"
                    },
                    {
                        "name": "Ilia Chernobrovkin"
                    },
                    {
                        "name": "Jan Košata"
                    },
                    {
                        "name": "Mads B. Kristensen"
                    },
                    {
                        "name": "Eric C. Langman"
                    },
                    {
                        "name": "Anders S. Sørensen"
                    },
                    {
                        "name": "Oded Zilberberg"
                    },
                    {
                        "name": "Albert Schliesser"
                    }
                ],
                "author_detail": {
                    "name": "Albert Schliesser"
                },
                "author": "Albert Schliesser",
                "arxiv_comment": "Main manuscript (16 pages, 4 figures) and supplementary information\n  (19 pages, 10 figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08713v1",
                "updated": "2024-08-16T12:51:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    51,
                    52,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T12:51:52Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    51,
                    52,
                    4,
                    229,
                    0
                ],
                "title": "Beyond KAN: Introducing KarSein for Adaptive High-Order Feature\n  Interaction Modeling in CTR Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond KAN: Introducing KarSein for Adaptive High-Order Feature\n  Interaction Modeling in CTR Prediction"
                },
                "summary": "Modeling feature interactions is crucial for click-through rate (CTR)\nprediction, particularly when it comes to high-order explicit interactions.\nTraditional methods struggle with this task because they often predefine a\nmaximum interaction order, which relies heavily on prior knowledge and can\nlimit the model's effectiveness. Additionally, modeling high-order interactions\ntypically leads to increased computational costs. Therefore, the challenge lies\nin adaptively modeling high-order feature interactions while maintaining\nefficiency. To address this issue, we introduce Kolmogorov-Arnold Represented\nSparse Efficient Interaction Network (KarSein), designed to optimize both\npredictive accuracy and computational efficiency. We firstly identify\nlimitations of directly applying Kolmogorov-Arnold Networks (KAN) to CTR and\nthen introduce KarSein to overcome these issues. It features a novel\narchitecture that reduces the computational costs of KAN and supports embedding\nvectors as feature inputs. Additionally, KarSein employs guided symbolic\nregression to address the challenge of KAN in spontaneously learning\nmultiplicative relationships. Extensive experiments demonstrate KarSein's\nsuperior performance, achieving significant predictive accuracy with minimal\ncomputational overhead. Furthermore, KarSein maintains strong global\nexplainability while enabling the removal of redundant features, resulting in a\nsparse network structure. These advantages also position KarSein as a promising\nmethod for efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling feature interactions is crucial for click-through rate (CTR)\nprediction, particularly when it comes to high-order explicit interactions.\nTraditional methods struggle with this task because they often predefine a\nmaximum interaction order, which relies heavily on prior knowledge and can\nlimit the model's effectiveness. Additionally, modeling high-order interactions\ntypically leads to increased computational costs. Therefore, the challenge lies\nin adaptively modeling high-order feature interactions while maintaining\nefficiency. To address this issue, we introduce Kolmogorov-Arnold Represented\nSparse Efficient Interaction Network (KarSein), designed to optimize both\npredictive accuracy and computational efficiency. We firstly identify\nlimitations of directly applying Kolmogorov-Arnold Networks (KAN) to CTR and\nthen introduce KarSein to overcome these issues. It features a novel\narchitecture that reduces the computational costs of KAN and supports embedding\nvectors as feature inputs. Additionally, KarSein employs guided symbolic\nregression to address the challenge of KAN in spontaneously learning\nmultiplicative relationships. Extensive experiments demonstrate KarSein's\nsuperior performance, achieving significant predictive accuracy with minimal\ncomputational overhead. Furthermore, KarSein maintains strong global\nexplainability while enabling the removal of redundant features, resulting in a\nsparse network structure. These advantages also position KarSein as a promising\nmethod for efficient inference."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Wu"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Haimin Zhang"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "KarSein for CTR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08707v1",
                "updated": "2024-08-16T12:40:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    40,
                    1,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T12:40:01Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    40,
                    1,
                    4,
                    229,
                    0
                ],
                "title": "Beam Prediction based on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beam Prediction based on Large Language Models"
                },
                "summary": "Millimeter-wave (mmWave) communication is promising for next-generation\nwireless networks but suffers from significant path loss, requiring extensive\nantenna arrays and frequent beam training. Traditional deep learning models,\nsuch as long short-term memory (LSTM), enhance beam tracking accuracy however\nare limited by poor robustness and generalization. In this letter, we use large\nlanguage models (LLMs) to improve the robustness of beam prediction. By\nconverting time series data into text-based representations and employing the\nPrompt-as-Prefix (PaP) technique for contextual enrichment, our approach\nunleashes the strength of LLMs for time series forecasting. Simulation results\ndemonstrate that our LLM-based method offers superior robustness and\ngeneralization compared to LSTM-based models, showcasing the potential of LLMs\nin wireless communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Millimeter-wave (mmWave) communication is promising for next-generation\nwireless networks but suffers from significant path loss, requiring extensive\nantenna arrays and frequent beam training. Traditional deep learning models,\nsuch as long short-term memory (LSTM), enhance beam tracking accuracy however\nare limited by poor robustness and generalization. In this letter, we use large\nlanguage models (LLMs) to improve the robustness of beam prediction. By\nconverting time series data into text-based representations and employing the\nPrompt-as-Prefix (PaP) technique for contextual enrichment, our approach\nunleashes the strength of LLMs for time series forecasting. Simulation results\ndemonstrate that our LLM-based method offers superior robustness and\ngeneralization compared to LSTM-based models, showcasing the potential of LLMs\nin wireless communications."
                },
                "authors": [
                    {
                        "name": "Yucheng Sheng"
                    },
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Le Liang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Shi Jin"
                    },
                    {
                        "name": "Geoffrey Ye Li"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Ye Li"
                },
                "author": "Geoffrey Ye Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02558v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02558v3",
                "updated": "2024-08-16T12:33:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    33,
                    59,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-05T15:35:34Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    15,
                    35,
                    34,
                    0,
                    218,
                    0
                ],
                "title": "Peer-induced Fairness: A Causal Approach for Algorithmic Fairness\n  Auditing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer-induced Fairness: A Causal Approach for Algorithmic Fairness\n  Auditing"
                },
                "summary": "With the EU AI Act effective from 1 August 2024, high-risk applications like\ncredit scoring must adhere to stringent transparency and quality standards,\nincluding algorithmic fairness evaluations. Consequently, developing tools for\nauditing algorithmic fairness has become crucial. This paper addresses a key\nquestion: how can we scientifically audit algorithmic fairness? It is vital to\ndetermine whether adverse decisions result from algorithmic discrimination or\nthe subjects' inherent limitations. We introduce a novel auditing framework,\n``peer-induced fairness'', leveraging counterfactual fairness and advanced\ncausal inference techniques within credit approval systems. Our approach\nassesses fairness at the individual level through peer comparisons, independent\nof specific AI methodologies. It effectively tackles challenges like data\nscarcity and imbalance, common in traditional models, particularly in credit\napproval. Model-agnostic and flexible, the framework functions as both a\nself-audit tool for stakeholders and an external audit tool for regulators,\noffering ease of integration. It also meets the EU AI Act's transparency\nrequirements by providing clear feedback on whether adverse decisions stem from\npersonal capabilities or discrimination. We demonstrate the framework's\nusefulness by applying it to SME credit approval, revealing significant bias:\n41.51% of micro-firms face discrimination compared to non-micro firms. These\nfindings highlight the framework's potential for diverse AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the EU AI Act effective from 1 August 2024, high-risk applications like\ncredit scoring must adhere to stringent transparency and quality standards,\nincluding algorithmic fairness evaluations. Consequently, developing tools for\nauditing algorithmic fairness has become crucial. This paper addresses a key\nquestion: how can we scientifically audit algorithmic fairness? It is vital to\ndetermine whether adverse decisions result from algorithmic discrimination or\nthe subjects' inherent limitations. We introduce a novel auditing framework,\n``peer-induced fairness'', leveraging counterfactual fairness and advanced\ncausal inference techniques within credit approval systems. Our approach\nassesses fairness at the individual level through peer comparisons, independent\nof specific AI methodologies. It effectively tackles challenges like data\nscarcity and imbalance, common in traditional models, particularly in credit\napproval. Model-agnostic and flexible, the framework functions as both a\nself-audit tool for stakeholders and an external audit tool for regulators,\noffering ease of integration. It also meets the EU AI Act's transparency\nrequirements by providing clear feedback on whether adverse decisions stem from\npersonal capabilities or discrimination. We demonstrate the framework's\nusefulness by applying it to SME credit approval, revealing significant bias:\n41.51% of micro-firms face discrimination compared to non-micro firms. These\nfindings highlight the framework's potential for diverse AI applications."
                },
                "authors": [
                    {
                        "name": "Shiqi Fang"
                    },
                    {
                        "name": "Zexun Chen"
                    },
                    {
                        "name": "Jake Ansell"
                    }
                ],
                "author_detail": {
                    "name": "Jake Ansell"
                },
                "author": "Jake Ansell",
                "arxiv_comment": "28 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02558v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02558v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08703v1",
                "updated": "2024-08-16T12:30:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    30,
                    29,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T12:30:29Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    30,
                    29,
                    4,
                    229,
                    0
                ],
                "title": "TsCA: On the Semantic Consistency Alignment via Conditional Transport\n  for Compositional Zero-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TsCA: On the Semantic Consistency Alignment via Conditional Transport\n  for Compositional Zero-Shot Learning"
                },
                "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel\n\\textit{state-object} compositions by leveraging the shared knowledge of their\nprimitive components. Despite considerable progress, effectively calibrating\nthe bias between semantically similar multimodal representations, as well as\ngeneralizing pre-trained knowledge to novel compositional contexts, remains an\nenduring challenge. In this paper, our interest is to revisit the conditional\ntransport (CT) theory and its homology to the visual-semantics interaction in\nCZSL and further, propose a novel Trisets Consistency Alignment framework\n(dubbed TsCA) that well-addresses these issues. Concretely, we utilize three\ndistinct yet semantically homologous sets, i.e., patches, primitives, and\ncompositions, to construct pairwise CT costs to minimize their semantic\ndiscrepancies. To further ensure the consistency transfer within these sets, we\nimplement a cycle-consistency constraint that refines the learning by\nguaranteeing the feature consistency of the self-mapping during transport flow,\nregardless of modality. Moreover, we extend the CT plans to an open-world\nsetting, which enables the model to effectively filter out unfeasible pairs,\nthereby speeding up the inference as well as increasing the accuracy. Extensive\nexperiments are conducted to verify the effectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel\n\\textit{state-object} compositions by leveraging the shared knowledge of their\nprimitive components. Despite considerable progress, effectively calibrating\nthe bias between semantically similar multimodal representations, as well as\ngeneralizing pre-trained knowledge to novel compositional contexts, remains an\nenduring challenge. In this paper, our interest is to revisit the conditional\ntransport (CT) theory and its homology to the visual-semantics interaction in\nCZSL and further, propose a novel Trisets Consistency Alignment framework\n(dubbed TsCA) that well-addresses these issues. Concretely, we utilize three\ndistinct yet semantically homologous sets, i.e., patches, primitives, and\ncompositions, to construct pairwise CT costs to minimize their semantic\ndiscrepancies. To further ensure the consistency transfer within these sets, we\nimplement a cycle-consistency constraint that refines the learning by\nguaranteeing the feature consistency of the self-mapping during transport flow,\nregardless of modality. Moreover, we extend the CT plans to an open-world\nsetting, which enables the model to effectively filter out unfeasible pairs,\nthereby speeding up the inference as well as increasing the accuracy. Extensive\nexperiments are conducted to verify the effectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Miaoge Li"
                    },
                    {
                        "name": "Jingcai Guo"
                    },
                    {
                        "name": "Richard Yi Da Xu"
                    },
                    {
                        "name": "Dongsheng Wang"
                    },
                    {
                        "name": "Xiaofeng Cao"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06249v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06249v3",
                "updated": "2024-08-16T12:30:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    30,
                    7,
                    4,
                    229,
                    0
                ],
                "published": "2024-03-10T16:22:20Z",
                "published_parsed": [
                    2024,
                    3,
                    10,
                    16,
                    22,
                    20,
                    6,
                    70,
                    0
                ],
                "title": "No Language is an Island: Unifying Chinese and English in Financial\n  Large Language Models, Instruction Data, and Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Language is an Island: Unifying Chinese and English in Financial\n  Large Language Models, Instruction Data, and Benchmarks"
                },
                "summary": "While the progression of Large Language Models (LLMs) has notably propelled\nfinancial analysis, their application has largely been confined to singular\nlanguage realms, leaving untapped the potential of bilingual Chinese-English\ncapacity. To bridge this chasm, we introduce ICE-PIXIU, seamlessly amalgamating\nthe ICE-INTENT model and ICE-FLARE benchmark for bilingual financial analysis.\nICE-PIXIU uniquely integrates a spectrum of Chinese tasks, alongside translated\nand original English datasets, enriching the breadth and depth of bilingual\nfinancial modeling. It provides unrestricted access to diverse model variants,\na substantial compilation of diverse cross-lingual and multi-modal instruction\ndata, and an evaluation benchmark with expert annotations, comprising 10 NLP\ntasks, 20 bilingual specific tasks, totaling 95k datasets. Our thorough\nevaluation emphasizes the advantages of incorporating these bilingual datasets,\nespecially in translation tasks and utilizing original English data, enhancing\nboth linguistic flexibility and analytical acuity in financial contexts.\nNotably, ICE-INTENT distinguishes itself by showcasing significant enhancements\nover conventional LLMs and existing financial LLMs in bilingual milieus,\nunderscoring the profound impact of robust bilingual data on the accuracy and\nefficacy of financial NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the progression of Large Language Models (LLMs) has notably propelled\nfinancial analysis, their application has largely been confined to singular\nlanguage realms, leaving untapped the potential of bilingual Chinese-English\ncapacity. To bridge this chasm, we introduce ICE-PIXIU, seamlessly amalgamating\nthe ICE-INTENT model and ICE-FLARE benchmark for bilingual financial analysis.\nICE-PIXIU uniquely integrates a spectrum of Chinese tasks, alongside translated\nand original English datasets, enriching the breadth and depth of bilingual\nfinancial modeling. It provides unrestricted access to diverse model variants,\na substantial compilation of diverse cross-lingual and multi-modal instruction\ndata, and an evaluation benchmark with expert annotations, comprising 10 NLP\ntasks, 20 bilingual specific tasks, totaling 95k datasets. Our thorough\nevaluation emphasizes the advantages of incorporating these bilingual datasets,\nespecially in translation tasks and utilizing original English data, enhancing\nboth linguistic flexibility and analytical acuity in financial contexts.\nNotably, ICE-INTENT distinguishes itself by showcasing significant enhancements\nover conventional LLMs and existing financial LLMs in bilingual milieus,\nunderscoring the profound impact of robust bilingual data on the accuracy and\nefficacy of financial NLP."
                },
                "authors": [
                    {
                        "name": "Gang Hu"
                    },
                    {
                        "name": "Ke Qin"
                    },
                    {
                        "name": "Chenhan Yuan"
                    },
                    {
                        "name": "Min Peng"
                    },
                    {
                        "name": "Alejandro Lopez-Lira"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Qianqian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qianqian Xie"
                },
                "author": "Qianqian Xie",
                "arxiv_comment": "19 pages, 3 figures, 12 tables, including Appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06249v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06249v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08696v1",
                "updated": "2024-08-16T12:20:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    20,
                    56,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T12:20:56Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    20,
                    56,
                    4,
                    229,
                    0
                ],
                "title": "Turning Trash into Treasure: Accelerating Inference of Large Language\n  Models with Token Recycling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turning Trash into Treasure: Accelerating Inference of Large Language\n  Models with Token Recycling"
                },
                "summary": "The rapid growth in the parameters of large language models (LLMs) has made\ninference latency a fundamental bottleneck, limiting broader application of\nLLMs. Speculative decoding represents a lossless approach to accelerate\ninference through a guess-and-verify paradigm, leveraging the parallel\ncapabilities of modern hardware. Some speculative decoding methods rely on\nadditional structures to guess draft tokens, such as small models or\nparameter-efficient architectures, which need extra training before use.\nAlternatively, retrieval-based train-free techniques build libraries from\npre-existing corpora or by n-gram generation. However, they face challenges\nlike large storage requirements, time-consuming retrieval, and limited\nadaptability. Observing that candidate tokens generated during the decoding\nprocess are likely to reoccur in future sequences, we propose Token Recycling.\nThis approach stores candidate tokens in an adjacency matrix and employs a\nbreadth-first search (BFS)-like algorithm on the matrix to construct a draft\ntree. The tree is then validated through tree attention. New candidate tokens\nfrom the decoding process are then used to update the matrix. Token Recycling\nrequires \\textless2MB of additional storage and achieves approximately 2x\nspeedup across all sizes of LLMs. It significantly outperforms existing\ntrain-free methods by 30\\% and even a training method by 25\\%. It can be\ndirectly applied to any existing LLMs and tasks without the need for\nadaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth in the parameters of large language models (LLMs) has made\ninference latency a fundamental bottleneck, limiting broader application of\nLLMs. Speculative decoding represents a lossless approach to accelerate\ninference through a guess-and-verify paradigm, leveraging the parallel\ncapabilities of modern hardware. Some speculative decoding methods rely on\nadditional structures to guess draft tokens, such as small models or\nparameter-efficient architectures, which need extra training before use.\nAlternatively, retrieval-based train-free techniques build libraries from\npre-existing corpora or by n-gram generation. However, they face challenges\nlike large storage requirements, time-consuming retrieval, and limited\nadaptability. Observing that candidate tokens generated during the decoding\nprocess are likely to reoccur in future sequences, we propose Token Recycling.\nThis approach stores candidate tokens in an adjacency matrix and employs a\nbreadth-first search (BFS)-like algorithm on the matrix to construct a draft\ntree. The tree is then validated through tree attention. New candidate tokens\nfrom the decoding process are then used to update the matrix. Token Recycling\nrequires \\textless2MB of additional storage and achieves approximately 2x\nspeedup across all sizes of LLMs. It significantly outperforms existing\ntrain-free methods by 30\\% and even a training method by 25\\%. It can be\ndirectly applied to any existing LLMs and tasks without the need for\nadaptation."
                },
                "authors": [
                    {
                        "name": "Xianzhen Luo"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Zhiming Zhang"
                    },
                    {
                        "name": "Xuanyu Zhang"
                    },
                    {
                        "name": "Qing Yang"
                    },
                    {
                        "name": "Dongliang Xu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10160v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10160v6",
                "updated": "2024-08-16T12:20:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    20,
                    22,
                    4,
                    229,
                    0
                ],
                "published": "2024-04-15T22:18:50Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    22,
                    18,
                    50,
                    0,
                    106,
                    0
                ],
                "title": "Reinforcement Learning from Multi-role Debates as Feedback for Bias\n  Mitigation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Multi-role Debates as Feedback for Bias\n  Mitigation in LLMs"
                },
                "summary": "Bias in LLMs can harm user experience and societal outcomes. However, current\nbias mitigation methods often require intensive human feedback, lack\ntransferability to other topics or yield overconfident and random outputs. We\nfind that involving LLMs in role-playing scenario boosts their ability to\nrecognize and mitigate biases. Based on this, we propose Reinforcement Learning\nfrom Multi-role Debates as Feedback (RLDF), a novel approach for bias\nmitigation replacing human feedback in traditional RLHF. We utilize LLMs in\nmulti-role debates to create a dataset that includes both high-bias and\nlow-bias instances for training the reward model in reinforcement learning. Our\napproach comprises two modes: (1) self-reflection, where the same LLM\nparticipates in multi-role debates, and (2) teacher-student, where a more\nadvanced LLM like GPT-3.5-turbo guides the LLM to perform this task.\nExperimental results across different LLMs on BBQ and our datasets demonstrate\nthe effectiveness of our approach in bias mitigation. Our source code and\ndatasets are available at \\texttt{https://anonymous.4open.science/r/RLDF-E344}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in LLMs can harm user experience and societal outcomes. However, current\nbias mitigation methods often require intensive human feedback, lack\ntransferability to other topics or yield overconfident and random outputs. We\nfind that involving LLMs in role-playing scenario boosts their ability to\nrecognize and mitigate biases. Based on this, we propose Reinforcement Learning\nfrom Multi-role Debates as Feedback (RLDF), a novel approach for bias\nmitigation replacing human feedback in traditional RLHF. We utilize LLMs in\nmulti-role debates to create a dataset that includes both high-bias and\nlow-bias instances for training the reward model in reinforcement learning. Our\napproach comprises two modes: (1) self-reflection, where the same LLM\nparticipates in multi-role debates, and (2) teacher-student, where a more\nadvanced LLM like GPT-3.5-turbo guides the LLM to perform this task.\nExperimental results across different LLMs on BBQ and our datasets demonstrate\nthe effectiveness of our approach in bias mitigation. Our source code and\ndatasets are available at \\texttt{https://anonymous.4open.science/r/RLDF-E344}."
                },
                "authors": [
                    {
                        "name": "Ruoxi Cheng"
                    },
                    {
                        "name": "Haoxuan Ma"
                    },
                    {
                        "name": "Shuirong Cao"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Aihua Pei"
                    },
                    {
                        "name": "Zhiqiang Wang"
                    },
                    {
                        "name": "Pengliang Ji"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Jiaqi Huo"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Huo"
                },
                "author": "Jiaqi Huo",
                "arxiv_comment": "The first three authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10160v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10160v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08694v1",
                "updated": "2024-08-16T12:16:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    16,
                    59,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T12:16:59Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    16,
                    59,
                    4,
                    229,
                    0
                ],
                "title": "Quantifying the Effectiveness of Student Organization Activities using\n  Natural Language Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the Effectiveness of Student Organization Activities using\n  Natural Language Processing"
                },
                "summary": "Student extracurricular activities play an important role in enriching the\nstudents' educational experiences. With the increasing popularity of Machine\nLearning and Natural Language Processing, it becomes a logical step that\nincorporating ML-NLP in improving extracurricular activities is a potential\nfocus of study in Artificial Intelligence (AI). This research study aims to\ndevelop a machine learning workflow that will quantify the effectiveness of\nstudent-organized activities based on student emotional responses using\nsentiment analysis. The study uses the Bidirectional Encoder Representations\nfrom Transformers (BERT) Large Language Model (LLM) called via the\npysentimiento toolkit, as a Transformer pipeline in Hugging Face. A sample data\nset from Organization C, a Recognized Student Organization (RSO) of a higher\neducational institute in the Philippines, College X, was used to develop the\nworkflow. The workflow consisted of data preprocessing, key feature selection,\nLLM feature processing, and score aggregation, resulting in an Event Score for\neach data set. The results show that the BERT LLM can also be used effectively\nin analyzing sentiment beyond product reviews and post comments. For the\nstudent affairs offices of educational institutions, this study can provide a\npractical example of how NLP can be applied to real-world scenarios, showcasing\nthe potential impact of data-driven decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Student extracurricular activities play an important role in enriching the\nstudents' educational experiences. With the increasing popularity of Machine\nLearning and Natural Language Processing, it becomes a logical step that\nincorporating ML-NLP in improving extracurricular activities is a potential\nfocus of study in Artificial Intelligence (AI). This research study aims to\ndevelop a machine learning workflow that will quantify the effectiveness of\nstudent-organized activities based on student emotional responses using\nsentiment analysis. The study uses the Bidirectional Encoder Representations\nfrom Transformers (BERT) Large Language Model (LLM) called via the\npysentimiento toolkit, as a Transformer pipeline in Hugging Face. A sample data\nset from Organization C, a Recognized Student Organization (RSO) of a higher\neducational institute in the Philippines, College X, was used to develop the\nworkflow. The workflow consisted of data preprocessing, key feature selection,\nLLM feature processing, and score aggregation, resulting in an Event Score for\neach data set. The results show that the BERT LLM can also be used effectively\nin analyzing sentiment beyond product reviews and post comments. For the\nstudent affairs offices of educational institutions, this study can provide a\npractical example of how NLP can be applied to real-world scenarios, showcasing\nthe potential impact of data-driven decision making."
                },
                "authors": [
                    {
                        "name": "Lyberius Ennio F. Taruc"
                    },
                    {
                        "name": "Arvin R. De La Cruz"
                    }
                ],
                "author_detail": {
                    "name": "Arvin R. De La Cruz"
                },
                "author": "Arvin R. De La Cruz",
                "arxiv_comment": "11 pages, 4 figures, presented in International Conference on\n  Generative Al and its Applications (ICGAIA-24) last 22nd - 23rd, July, 2024\n  at Jakarta, Indonesia",
                "arxiv_journal_ref": "IJISAE, 2024, 12(22s), 1553-1563",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08688v1",
                "updated": "2024-08-16T12:01:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    1,
                    55,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T12:01:55Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    1,
                    55,
                    4,
                    229,
                    0
                ],
                "title": "The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic\n  Preference Optimization Dataset Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic\n  Preference Optimization Dataset Generation"
                },
                "summary": "This paper presents and evaluates multi-agent workflows for synthetic\nPreference Optimization (PO) dataset generation. PO dataset generation requires\ntwo modules: (1) response evaluation, and (2) response generation. In the\nresponse evaluation module, the responses from Large Language Models (LLMs) are\nevaluated and ranked - a task typically carried out by human annotators that we\nautomate using LLMs. We assess the response evaluation module in a 2 step\nprocess. In step 1, we assess LLMs as evaluators using three distinct prompting\nstrategies. In step 2, we apply the winning prompting strategy to compare the\nperformance of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. In each step, we\nuse inter-rater agreement using Cohen's Kappa between human annotators and\nLLMs. For the response generation module, we compare different configurations\nfor the LLM Feedback Loop using the identified LLM evaluator configuration. We\nuse the win rate (the fraction of times a generation framework is selected as\nthe best by an LLM evaluator) to determine the best multi-agent configuration\nfor generation. After identifying the best configurations for both modules, we\nuse models from the GPT, Gemma, and Llama families to generate our PO datasets\nusing the above pipeline. We generate two types of PO datasets, one to improve\nthe generation capabilities of individual LLM and the other to improve the\nmulti-agent workflow. Our evaluation shows that GPT-4o-as-a-Judge is more\nconsistent across datasets when the candidate responses do not include\nresponses from the GPT family. Additionally, we find that the LLM Feedback\nLoop, with Llama as the generator and Gemma as the reviewer, achieves a notable\n71.8% and 73.8% win rate over single-agent Llama and Gemma, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents and evaluates multi-agent workflows for synthetic\nPreference Optimization (PO) dataset generation. PO dataset generation requires\ntwo modules: (1) response evaluation, and (2) response generation. In the\nresponse evaluation module, the responses from Large Language Models (LLMs) are\nevaluated and ranked - a task typically carried out by human annotators that we\nautomate using LLMs. We assess the response evaluation module in a 2 step\nprocess. In step 1, we assess LLMs as evaluators using three distinct prompting\nstrategies. In step 2, we apply the winning prompting strategy to compare the\nperformance of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. In each step, we\nuse inter-rater agreement using Cohen's Kappa between human annotators and\nLLMs. For the response generation module, we compare different configurations\nfor the LLM Feedback Loop using the identified LLM evaluator configuration. We\nuse the win rate (the fraction of times a generation framework is selected as\nthe best by an LLM evaluator) to determine the best multi-agent configuration\nfor generation. After identifying the best configurations for both modules, we\nuse models from the GPT, Gemma, and Llama families to generate our PO datasets\nusing the above pipeline. We generate two types of PO datasets, one to improve\nthe generation capabilities of individual LLM and the other to improve the\nmulti-agent workflow. Our evaluation shows that GPT-4o-as-a-Judge is more\nconsistent across datasets when the candidate responses do not include\nresponses from the GPT family. Additionally, we find that the LLM Feedback\nLoop, with Llama as the generator and Gemma as the reviewer, achieves a notable\n71.8% and 73.8% win rate over single-agent Llama and Gemma, respectively."
                },
                "authors": [
                    {
                        "name": "Samee Arif"
                    },
                    {
                        "name": "Sualeha Farid"
                    },
                    {
                        "name": "Abdul Hameed Azeemi"
                    },
                    {
                        "name": "Awais Athar"
                    },
                    {
                        "name": "Agha Ali Raza"
                    }
                ],
                "author_detail": {
                    "name": "Agha Ali Raza"
                },
                "author": "Agha Ali Raza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08686v2",
                "updated": "2024-08-19T04:31:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    31,
                    51,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-16T11:59:01Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    59,
                    1,
                    4,
                    229,
                    0
                ],
                "title": "SC-Rec: Enhancing Generative Retrieval with Self-Consistent Reranking\n  for Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SC-Rec: Enhancing Generative Retrieval with Self-Consistent Reranking\n  for Sequential Recommendation"
                },
                "summary": "Language Models (LMs) are increasingly employed in recommendation systems due\nto their advanced language understanding and generation capabilities. Recent\nrecommender systems based on generative retrieval have leveraged the\ninferential abilities of LMs to directly generate the index tokens of the next\nitem, based on item sequences within the user's interaction history. Previous\nstudies have mostly focused on item indices based solely on textual semantic or\ncollaborative information. However, although the standalone effectiveness of\nthese aspects has been demonstrated, the integration of this information has\nremained unexplored. Our in-depth analysis finds that there is a significant\ndifference in the knowledge captured by the model from heterogeneous item\nindices and diverse input prompts, which can have a high potential for\ncomplementarity. In this paper, we propose SC-Rec, a unified recommender system\nthat learns diverse preference knowledge from two distinct item indices and\nmultiple prompt templates. Furthermore, SC-Rec adopts a novel reranking\nstrategy that aggregates a set of ranking results, inferred based on different\nindices and prompts, to achieve the self-consistency of the model. Our\nempirical evaluation on three real-world datasets demonstrates that SC-Rec\nconsiderably outperforms the state-of-the-art methods for sequential\nrecommendation, effectively incorporating complementary knowledge from varied\noutputs of the model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) are increasingly employed in recommendation systems due\nto their advanced language understanding and generation capabilities. Recent\nrecommender systems based on generative retrieval have leveraged the\ninferential abilities of LMs to directly generate the index tokens of the next\nitem, based on item sequences within the user's interaction history. Previous\nstudies have mostly focused on item indices based solely on textual semantic or\ncollaborative information. However, although the standalone effectiveness of\nthese aspects has been demonstrated, the integration of this information has\nremained unexplored. Our in-depth analysis finds that there is a significant\ndifference in the knowledge captured by the model from heterogeneous item\nindices and diverse input prompts, which can have a high potential for\ncomplementarity. In this paper, we propose SC-Rec, a unified recommender system\nthat learns diverse preference knowledge from two distinct item indices and\nmultiple prompt templates. Furthermore, SC-Rec adopts a novel reranking\nstrategy that aggregates a set of ranking results, inferred based on different\nindices and prompts, to achieve the self-consistency of the model. Our\nempirical evaluation on three real-world datasets demonstrates that SC-Rec\nconsiderably outperforms the state-of-the-art methods for sequential\nrecommendation, effectively incorporating complementary knowledge from varied\noutputs of the model."
                },
                "authors": [
                    {
                        "name": "Tongyoung Kim"
                    },
                    {
                        "name": "Soojin Yoon"
                    },
                    {
                        "name": "Seongku Kang"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08685v1",
                "updated": "2024-08-16T11:58:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    58,
                    34,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T11:58:34Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    58,
                    34,
                    4,
                    229,
                    0
                ],
                "title": "Can Large Language Models Improve the Adversarial Robustness of Graph\n  Neural Networks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Improve the Adversarial Robustness of Graph\n  Neural Networks?"
                },
                "summary": "Graph neural networks (GNNs) are vulnerable to adversarial perturbations,\nespecially for topology attacks, and many methods that improve the robustness\nof GNNs have received considerable attention. Recently, we have witnessed the\nsignificant success of large language models (LLMs), leading many to explore\nthe great potential of LLMs on GNNs. However, they mainly focus on improving\nthe performance of GNNs by utilizing LLMs to enhance the node features.\nTherefore, we ask: Will the robustness of GNNs also be enhanced with the\npowerful understanding and inference capabilities of LLMs? By presenting the\nempirical results, we find that despite that LLMs can improve the robustness of\nGNNs, there is still an average decrease of 23.1% in accuracy, implying that\nthe GNNs remain extremely vulnerable against topology attack. Therefore,\nanother question is how to extend the capabilities of LLMs on graph adversarial\nrobustness. In this paper, we propose an LLM-based robust graph structure\ninference framework, LLM4RGNN, which distills the inference capabilities of\nGPT-4 into a local LLM for identifying malicious edges and an LM-based edge\npredictor for finding missing important edges, so as to recover a robust graph\nstructure. Extensive experiments demonstrate that LLM4RGNN consistently\nimproves the robustness across various GNNs. Even in some cases where the\nperturbation ratio increases to 40%, the accuracy of GNNs is still better than\nthat on the clean graph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) are vulnerable to adversarial perturbations,\nespecially for topology attacks, and many methods that improve the robustness\nof GNNs have received considerable attention. Recently, we have witnessed the\nsignificant success of large language models (LLMs), leading many to explore\nthe great potential of LLMs on GNNs. However, they mainly focus on improving\nthe performance of GNNs by utilizing LLMs to enhance the node features.\nTherefore, we ask: Will the robustness of GNNs also be enhanced with the\npowerful understanding and inference capabilities of LLMs? By presenting the\nempirical results, we find that despite that LLMs can improve the robustness of\nGNNs, there is still an average decrease of 23.1% in accuracy, implying that\nthe GNNs remain extremely vulnerable against topology attack. Therefore,\nanother question is how to extend the capabilities of LLMs on graph adversarial\nrobustness. In this paper, we propose an LLM-based robust graph structure\ninference framework, LLM4RGNN, which distills the inference capabilities of\nGPT-4 into a local LLM for identifying malicious edges and an LM-based edge\npredictor for finding missing important edges, so as to recover a robust graph\nstructure. Extensive experiments demonstrate that LLM4RGNN consistently\nimproves the robustness across various GNNs. Even in some cases where the\nperturbation ratio increases to 40%, the accuracy of GNNs is still better than\nthat on the clean graph."
                },
                "authors": [
                    {
                        "name": "Zhongjian Zhang"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Mengmei Zhang"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Chuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Shi"
                },
                "author": "Chuan Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08212v2",
                "updated": "2024-08-16T11:57:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    57,
                    53,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-15T15:23:00Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    23,
                    0,
                    3,
                    228,
                    0
                ],
                "title": "Covert Bias: The Severity of Social Views' Unalignment in Language\n  Models Towards Implicit and Explicit Opinion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covert Bias: The Severity of Social Views' Unalignment in Language\n  Models Towards Implicit and Explicit Opinion"
                },
                "summary": "While various approaches have recently been studied for bias identification,\nlittle is known about how implicit language that does not explicitly convey a\nviewpoint affects bias amplification in large language models. To examine the\nseverity of bias toward a view, we evaluated the performance of two downstream\ntasks where the implicit and explicit knowledge of social groups were used.\nFirst, we present a stress test evaluation by using a biased model in edge\ncases of excessive bias scenarios. Then, we evaluate how LLMs calibrate\nlinguistically in response to both implicit and explicit opinions when they are\naligned with conflicting viewpoints. Our findings reveal a discrepancy in LLM\nperformance in identifying implicit and explicit opinions, with a general\ntendency of bias toward explicit opinions of opposing stances. Moreover, the\nbias-aligned models generate more cautious responses using uncertainty phrases\ncompared to the unaligned (zero-shot) base models. The direct, incautious\nresponses of the unaligned models suggest a need for further refinement of\ndecisiveness by incorporating uncertainty markers to enhance their reliability,\nespecially on socially nuanced topics with high subjectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While various approaches have recently been studied for bias identification,\nlittle is known about how implicit language that does not explicitly convey a\nviewpoint affects bias amplification in large language models. To examine the\nseverity of bias toward a view, we evaluated the performance of two downstream\ntasks where the implicit and explicit knowledge of social groups were used.\nFirst, we present a stress test evaluation by using a biased model in edge\ncases of excessive bias scenarios. Then, we evaluate how LLMs calibrate\nlinguistically in response to both implicit and explicit opinions when they are\naligned with conflicting viewpoints. Our findings reveal a discrepancy in LLM\nperformance in identifying implicit and explicit opinions, with a general\ntendency of bias toward explicit opinions of opposing stances. Moreover, the\nbias-aligned models generate more cautious responses using uncertainty phrases\ncompared to the unaligned (zero-shot) base models. The direct, incautious\nresponses of the unaligned models suggest a need for further refinement of\ndecisiveness by incorporating uncertainty markers to enhance their reliability,\nespecially on socially nuanced topics with high subjectivity."
                },
                "authors": [
                    {
                        "name": "Abeer Aldayel"
                    },
                    {
                        "name": "Areej Alokaili"
                    },
                    {
                        "name": "Rehab Alahmadi"
                    }
                ],
                "author_detail": {
                    "name": "Rehab Alahmadi"
                },
                "author": "Rehab Alahmadi",
                "arxiv_comment": "This work is under-review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08684v1",
                "updated": "2024-08-16T11:56:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    56,
                    49,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T11:56:49Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    56,
                    49,
                    4,
                    229,
                    0
                ],
                "title": "Research on Personalized Compression Algorithm for Pre-trained Models\n  Based on Homomorphic Entropy Increase",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on Personalized Compression Algorithm for Pre-trained Models\n  Based on Homomorphic Entropy Increase"
                },
                "summary": "In this article, we explore the challenges and evolution of two key\ntechnologies in the current field of AI: Vision Transformer model and Large\nLanguage Model (LLM). Vision Transformer captures global information by\nsplitting images into small pieces and leveraging Transformer's multi-head\nattention mechanism, but its high reference count and compute overhead limit\ndeployment on mobile devices. At the same time, the rapid development of LLM\nhas revolutionized natural language processing, but it also faces huge\ndeployment challenges. To address these issues, we investigate model pruning\ntechniques, with a particular focus on how to reduce redundant parameters\nwithout losing accuracy to accommodate personalized data and\nresource-constrained environments. In this paper, a new layered pruning\nstrategy is proposed to distinguish the personalized layer from the common\nlayer by compressed sensing and random sampling, thus significantly reducing\nthe model parameters. Our experimental results show that the introduced step\nbuffering mechanism further improves the accuracy of the model after pruning,\nproviding new directions and possibilities for the deployment of efficient and\npersonalized AI models on mobile devices in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we explore the challenges and evolution of two key\ntechnologies in the current field of AI: Vision Transformer model and Large\nLanguage Model (LLM). Vision Transformer captures global information by\nsplitting images into small pieces and leveraging Transformer's multi-head\nattention mechanism, but its high reference count and compute overhead limit\ndeployment on mobile devices. At the same time, the rapid development of LLM\nhas revolutionized natural language processing, but it also faces huge\ndeployment challenges. To address these issues, we investigate model pruning\ntechniques, with a particular focus on how to reduce redundant parameters\nwithout losing accuracy to accommodate personalized data and\nresource-constrained environments. In this paper, a new layered pruning\nstrategy is proposed to distinguish the personalized layer from the common\nlayer by compressed sensing and random sampling, thus significantly reducing\nthe model parameters. Our experimental results show that the introduced step\nbuffering mechanism further improves the accuracy of the model after pruning,\nproviding new directions and possibilities for the deployment of efficient and\npersonalized AI models on mobile devices in the future."
                },
                "authors": [
                    {
                        "name": "Yicong Li"
                    },
                    {
                        "name": "Xing Guo"
                    },
                    {
                        "name": "Haohua Du"
                    }
                ],
                "author_detail": {
                    "name": "Haohua Du"
                },
                "author": "Haohua Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08682v1",
                "updated": "2024-08-16T11:55:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    55,
                    44,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T11:55:44Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    55,
                    44,
                    4,
                    229,
                    0
                ],
                "title": "LLM-PCGC: Large Language Model-based Point Cloud Geometry Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-PCGC: Large Language Model-based Point Cloud Geometry Compression"
                },
                "summary": "The key to effective point cloud compression is to obtain a robust context\nmodel consistent with complex 3D data structures. Recently, the advancement of\nlarge language models (LLMs) has highlighted their capabilities not only as\npowerful generators for in-context learning and generation but also as\neffective compressors. These dual attributes of LLMs make them particularly\nwell-suited to meet the demands of data compression. Therefore, this paper\nexplores the potential of using LLM for compression tasks, focusing on lossless\npoint cloud geometry compression (PCGC) experiments. However, applying LLM\ndirectly to PCGC tasks presents some significant challenges, i.e., LLM does not\nunderstand the structure of the point cloud well, and it is a difficult task to\nfill the gap between text and point cloud through text description, especially\nfor large complicated and small shapeless point clouds. To address these\nproblems, we introduce a novel architecture, namely the Large Language\nModel-based Point Cloud Geometry Compression (LLM-PCGC) method, using LLM to\ncompress point cloud geometry information without any text description or\naligning operation. By utilizing different adaptation techniques for\ncross-modality representation alignment and semantic consistency, including\nclustering, K-tree, token mapping invariance, and Low Rank Adaptation (LoRA),\nthe proposed method can translate LLM to a compressor/generator for point\ncloud. To the best of our knowledge, this is the first structure to employ LLM\nas a compressor for point cloud data. Experiments demonstrate that the LLM-PCGC\noutperforms the other existing methods significantly, by achieving -40.213% bit\nrate reduction compared to the reference software of MPEG Geometry-based Point\nCloud Compression (G-PCC) standard, and by achieving -2.267% bit rate reduction\ncompared to the state-of-the-art learning-based method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key to effective point cloud compression is to obtain a robust context\nmodel consistent with complex 3D data structures. Recently, the advancement of\nlarge language models (LLMs) has highlighted their capabilities not only as\npowerful generators for in-context learning and generation but also as\neffective compressors. These dual attributes of LLMs make them particularly\nwell-suited to meet the demands of data compression. Therefore, this paper\nexplores the potential of using LLM for compression tasks, focusing on lossless\npoint cloud geometry compression (PCGC) experiments. However, applying LLM\ndirectly to PCGC tasks presents some significant challenges, i.e., LLM does not\nunderstand the structure of the point cloud well, and it is a difficult task to\nfill the gap between text and point cloud through text description, especially\nfor large complicated and small shapeless point clouds. To address these\nproblems, we introduce a novel architecture, namely the Large Language\nModel-based Point Cloud Geometry Compression (LLM-PCGC) method, using LLM to\ncompress point cloud geometry information without any text description or\naligning operation. By utilizing different adaptation techniques for\ncross-modality representation alignment and semantic consistency, including\nclustering, K-tree, token mapping invariance, and Low Rank Adaptation (LoRA),\nthe proposed method can translate LLM to a compressor/generator for point\ncloud. To the best of our knowledge, this is the first structure to employ LLM\nas a compressor for point cloud data. Experiments demonstrate that the LLM-PCGC\noutperforms the other existing methods significantly, by achieving -40.213% bit\nrate reduction compared to the reference software of MPEG Geometry-based Point\nCloud Compression (G-PCC) standard, and by achieving -2.267% bit rate reduction\ncompared to the state-of-the-art learning-based method."
                },
                "authors": [
                    {
                        "name": "Yuqi Ye"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08681v1",
                "updated": "2024-08-16T11:53:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    53,
                    52,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T11:53:52Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    53,
                    52,
                    4,
                    229,
                    0
                ],
                "title": "A Mean Field Ansatz for Zero-Shot Weight Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Mean Field Ansatz for Zero-Shot Weight Transfer"
                },
                "summary": "The pre-training cost of large language models (LLMs) is prohibitive. One\ncutting-edge approach to reduce the cost is zero-shot weight transfer, also\nknown as model growth for some cases, which magically transfers the weights\ntrained in a small model to a large model. However, there are still some\ntheoretical mysteries behind the weight transfer. In this paper, inspired by\nprior applications of mean field theory to neural network dynamics, we\nintroduce a mean field ansatz to provide a theoretical explanation for weight\ntransfer. Specifically, we propose the row-column (RC) ansatz under the mean\nfield point of view, which describes the measure structure of the weights in\nthe neural network (NN) and admits a close measure dynamic. Thus, the weights\nof different sizes NN admit a common distribution under proper assumptions, and\nweight transfer methods can be viewed as sampling methods. We empirically\nvalidate the RC ansatz by exploring simple MLP examples and LLMs such as GPT-3\nand Llama-3.1. We show the mean-field point of view is adequate under suitable\nassumptions which can provide theoretical support for zero-shot weight\ntransfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pre-training cost of large language models (LLMs) is prohibitive. One\ncutting-edge approach to reduce the cost is zero-shot weight transfer, also\nknown as model growth for some cases, which magically transfers the weights\ntrained in a small model to a large model. However, there are still some\ntheoretical mysteries behind the weight transfer. In this paper, inspired by\nprior applications of mean field theory to neural network dynamics, we\nintroduce a mean field ansatz to provide a theoretical explanation for weight\ntransfer. Specifically, we propose the row-column (RC) ansatz under the mean\nfield point of view, which describes the measure structure of the weights in\nthe neural network (NN) and admits a close measure dynamic. Thus, the weights\nof different sizes NN admit a common distribution under proper assumptions, and\nweight transfer methods can be viewed as sampling methods. We empirically\nvalidate the RC ansatz by exploring simple MLP examples and LLMs such as GPT-3\nand Llama-3.1. We show the mean-field point of view is adequate under suitable\nassumptions which can provide theoretical support for zero-shot weight\ntransfer."
                },
                "authors": [
                    {
                        "name": "Xingyuan Chen"
                    },
                    {
                        "name": "Wenwei Kuang"
                    },
                    {
                        "name": "Lei Deng"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Goncalo dos Reis"
                    }
                ],
                "author_detail": {
                    "name": "Goncalo dos Reis"
                },
                "author": "Goncalo dos Reis",
                "arxiv_comment": "40 pages, 6 Figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08676v1",
                "updated": "2024-08-16T11:43:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    43,
                    31,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T11:43:31Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    43,
                    31,
                    4,
                    229,
                    0
                ],
                "title": "Fine-tuning LLMs for Autonomous Spacecraft Control: A Case Study Using\n  Kerbal Space Program",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning LLMs for Autonomous Spacecraft Control: A Case Study Using\n  Kerbal Space Program"
                },
                "summary": "Recent trends are emerging in the use of Large Language Models (LLMs) as\nautonomous agents that take actions based on the content of the user text\nprompt. This study explores the use of fine-tuned Large Language Models (LLMs)\nfor autonomous spacecraft control, using the Kerbal Space Program Differential\nGames suite (KSPDG) as a testing environment. Traditional Reinforcement\nLearning (RL) approaches face limitations in this domain due to insufficient\nsimulation capabilities and data. By leveraging LLMs, specifically fine-tuning\nmodels like GPT-3.5 and LLaMA, we demonstrate how these models can effectively\ncontrol spacecraft using language-based inputs and outputs. Our approach\nintegrates real-time mission telemetry into textual prompts processed by the\nLLM, which then generate control actions via an agent. The results open a\ndiscussion about the potential of LLMs for space operations beyond their\nnominal use for text-related tasks. Future work aims to expand this methodology\nto other space control tasks and evaluate the performance of different LLM\nfamilies. The code is available at this URL:\n\\texttt{https://github.com/ARCLab-MIT/kspdg}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent trends are emerging in the use of Large Language Models (LLMs) as\nautonomous agents that take actions based on the content of the user text\nprompt. This study explores the use of fine-tuned Large Language Models (LLMs)\nfor autonomous spacecraft control, using the Kerbal Space Program Differential\nGames suite (KSPDG) as a testing environment. Traditional Reinforcement\nLearning (RL) approaches face limitations in this domain due to insufficient\nsimulation capabilities and data. By leveraging LLMs, specifically fine-tuning\nmodels like GPT-3.5 and LLaMA, we demonstrate how these models can effectively\ncontrol spacecraft using language-based inputs and outputs. Our approach\nintegrates real-time mission telemetry into textual prompts processed by the\nLLM, which then generate control actions via an agent. The results open a\ndiscussion about the potential of LLMs for space operations beyond their\nnominal use for text-related tasks. Future work aims to expand this methodology\nto other space control tasks and evaluate the performance of different LLM\nfamilies. The code is available at this URL:\n\\texttt{https://github.com/ARCLab-MIT/kspdg}."
                },
                "authors": [
                    {
                        "name": "Alejandro Carrasco"
                    },
                    {
                        "name": "Victor Rodriguez-Fernandez"
                    },
                    {
                        "name": "Richard Linares"
                    }
                ],
                "author_detail": {
                    "name": "Richard Linares"
                },
                "author": "Richard Linares",
                "arxiv_comment": "ESA SPAICE Conference 2024. arXiv admin note: text overlap with\n  arXiv:2404.00413",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2103.13860v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2103.13860v4",
                "updated": "2024-08-16T11:28:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    28,
                    31,
                    4,
                    229,
                    0
                ],
                "published": "2021-03-25T14:17:09Z",
                "published_parsed": [
                    2021,
                    3,
                    25,
                    14,
                    17,
                    9,
                    3,
                    84,
                    0
                ],
                "title": "Active Inference Tree Search in Large POMDPs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Inference Tree Search in Large POMDPs"
                },
                "summary": "The ability to plan ahead efficiently is key for both living organisms and\nartificial systems. Model-based planning and prospection are widely studied in\ncognitive neuroscience and artificial intelligence (AI), but from different\nperspectives--and with different desiderata in mind (biological realism versus\nscalability) that are difficult to reconcile. Here, we introduce a novel method\nto plan in POMDPs--Active Inference Tree Search (AcT)--that combines the\nnormative character and biological realism of a leading planning theory in\nneuroscience (Active Inference) and the scalability of tree search methods in\nAI. This unification enhances both approaches. On the one hand, tree searches\nenable the biologically grounded, first principle method of active inference to\nbe applied to large-scale problems. On the other hand, active inference\nprovides a principled solution to the exploration-exploitation dilemma, which\nis often addressed heuristically in tree search methods. Our simulations show\nthat AcT successfully navigates binary trees that are challenging for\nsampling-based methods, problems that require adaptive exploration, and the\nlarge POMDP problem 'RockSample'--in which AcT reproduces state-of-the-art\nPOMDP solutions. Furthermore, we illustrate how AcT can be used to simulate\nneurophysiological responses (e.g., in the hippocampus and prefrontal cortex)\nof humans and other animals that solve large planning problems. These numerical\nanalyses show that Active Tree Search is a principled realisation of\nneuroscientific and AI planning theories, which offer both biological realism\nand scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to plan ahead efficiently is key for both living organisms and\nartificial systems. Model-based planning and prospection are widely studied in\ncognitive neuroscience and artificial intelligence (AI), but from different\nperspectives--and with different desiderata in mind (biological realism versus\nscalability) that are difficult to reconcile. Here, we introduce a novel method\nto plan in POMDPs--Active Inference Tree Search (AcT)--that combines the\nnormative character and biological realism of a leading planning theory in\nneuroscience (Active Inference) and the scalability of tree search methods in\nAI. This unification enhances both approaches. On the one hand, tree searches\nenable the biologically grounded, first principle method of active inference to\nbe applied to large-scale problems. On the other hand, active inference\nprovides a principled solution to the exploration-exploitation dilemma, which\nis often addressed heuristically in tree search methods. Our simulations show\nthat AcT successfully navigates binary trees that are challenging for\nsampling-based methods, problems that require adaptive exploration, and the\nlarge POMDP problem 'RockSample'--in which AcT reproduces state-of-the-art\nPOMDP solutions. Furthermore, we illustrate how AcT can be used to simulate\nneurophysiological responses (e.g., in the hippocampus and prefrontal cortex)\nof humans and other animals that solve large planning problems. These numerical\nanalyses show that Active Tree Search is a principled realisation of\nneuroscientific and AI planning theories, which offer both biological realism\nand scalability."
                },
                "authors": [
                    {
                        "name": "Domenico Maisto"
                    },
                    {
                        "name": "Francesco Gregoretti"
                    },
                    {
                        "name": "Karl Friston"
                    },
                    {
                        "name": "Giovanni Pezzulo"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Pezzulo"
                },
                "author": "Giovanni Pezzulo",
                "arxiv_comment": "47 pages, 9 figures, 1 Appendix of two sections with pseudocodes and\n  one encoding example, submitted preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2103.13860v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2103.13860v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T20, 68Q07, 68W27, 90C40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.6; G.3; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19105v2",
                "updated": "2024-08-16T11:28:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    28,
                    13,
                    4,
                    229,
                    0
                ],
                "published": "2024-02-29T12:36:10Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    12,
                    36,
                    10,
                    3,
                    60,
                    0
                ],
                "title": "CollaFuse: Navigating Limited Resources and Privacy in Collaborative\n  Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CollaFuse: Navigating Limited Resources and Privacy in Collaborative\n  Generative AI"
                },
                "summary": "In the landscape of generative artificial intelligence, diffusion-based\nmodels present challenges for socio-technical systems in data requirements and\nprivacy. Traditional approaches like federated learning distribute the learning\nprocess but strain individual clients, especially with constrained resources\n(e.g., edge devices). In response to these challenges, we introduce CollaFuse,\na novel framework inspired by split learning. Tailored for efficient and\ncollaborative use of denoising diffusion probabilistic models, CollaFuse\nenables shared server training and inference, alleviating client computational\nburdens. This is achieved by retaining data and computationally inexpensive GPU\nprocesses locally at each client while outsourcing the computationally\nexpensive processes to the shared server. Demonstrated in a healthcare context,\nCollaFuse enhances privacy by highly reducing the need for sensitive\ninformation sharing. These capabilities hold the potential to impact various\napplication areas, such as the design of edge computing solutions, healthcare\nresearch, or autonomous driving. In essence, our work advances distributed\nmachine learning, shaping the future of collaborative GenAI networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the landscape of generative artificial intelligence, diffusion-based\nmodels present challenges for socio-technical systems in data requirements and\nprivacy. Traditional approaches like federated learning distribute the learning\nprocess but strain individual clients, especially with constrained resources\n(e.g., edge devices). In response to these challenges, we introduce CollaFuse,\na novel framework inspired by split learning. Tailored for efficient and\ncollaborative use of denoising diffusion probabilistic models, CollaFuse\nenables shared server training and inference, alleviating client computational\nburdens. This is achieved by retaining data and computationally inexpensive GPU\nprocesses locally at each client while outsourcing the computationally\nexpensive processes to the shared server. Demonstrated in a healthcare context,\nCollaFuse enhances privacy by highly reducing the need for sensitive\ninformation sharing. These capabilities hold the potential to impact various\napplication areas, such as the design of edge computing solutions, healthcare\nresearch, or autonomous driving. In essence, our work advances distributed\nmachine learning, shaping the future of collaborative GenAI networks."
                },
                "authors": [
                    {
                        "name": "Domenique Zipperling"
                    },
                    {
                        "name": "Simeon Allmendinger"
                    },
                    {
                        "name": "Lukas Struppek"
                    },
                    {
                        "name": "Niklas Kühl"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Kühl"
                },
                "author": "Niklas Kühl",
                "arxiv_comment": "Thirty-Second European Conference on Information Systems (ECIS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08669v1",
                "updated": "2024-08-16T11:26:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    26,
                    39,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T11:26:39Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    26,
                    39,
                    4,
                    229,
                    0
                ],
                "title": "HSDreport: Heart Sound Diagnosis with Echocardiography Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HSDreport: Heart Sound Diagnosis with Echocardiography Reports"
                },
                "summary": "Heart sound auscultation holds significant importance in the diagnosis of\ncongenital heart disease. However, existing methods for Heart Sound Diagnosis\n(HSD) tasks are predominantly limited to a few fixed categories, framing the\nHSD task as a rigid classification problem that does not fully align with\nmedical practice and offers only limited information to physicians. Besides,\nsuch methods do not utilize echocardiography reports, the gold standard in the\ndiagnosis of related diseases. To tackle this challenge, we introduce\nHSDreport, a new benchmark for HSD, which mandates the direct utilization of\nheart sounds obtained from auscultation to predict echocardiography reports.\nThis benchmark aims to merge the convenience of auscultation with the\ncomprehensive nature of echocardiography reports. First, we collect a new\ndataset for this benchmark, comprising 2,275 heart sound samples along with\ntheir corresponding reports. Subsequently, we develop a knowledge-aware\nquery-based transformer to handle this task. The intent is to leverage the\ncapabilities of medically pre-trained models and the internal knowledge of\nlarge language models (LLMs) to address the task's inherent complexity and\nvariability, thereby enhancing the robustness and scientific validity of the\nmethod. Furthermore, our experimental results indicate that our method\nsignificantly outperforms traditional HSD approaches and existing multimodal\nLLMs in detecting key abnormalities in heart sounds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heart sound auscultation holds significant importance in the diagnosis of\ncongenital heart disease. However, existing methods for Heart Sound Diagnosis\n(HSD) tasks are predominantly limited to a few fixed categories, framing the\nHSD task as a rigid classification problem that does not fully align with\nmedical practice and offers only limited information to physicians. Besides,\nsuch methods do not utilize echocardiography reports, the gold standard in the\ndiagnosis of related diseases. To tackle this challenge, we introduce\nHSDreport, a new benchmark for HSD, which mandates the direct utilization of\nheart sounds obtained from auscultation to predict echocardiography reports.\nThis benchmark aims to merge the convenience of auscultation with the\ncomprehensive nature of echocardiography reports. First, we collect a new\ndataset for this benchmark, comprising 2,275 heart sound samples along with\ntheir corresponding reports. Subsequently, we develop a knowledge-aware\nquery-based transformer to handle this task. The intent is to leverage the\ncapabilities of medically pre-trained models and the internal knowledge of\nlarge language models (LLMs) to address the task's inherent complexity and\nvariability, thereby enhancing the robustness and scientific validity of the\nmethod. Furthermore, our experimental results indicate that our method\nsignificantly outperforms traditional HSD approaches and existing multimodal\nLLMs in detecting key abnormalities in heart sounds."
                },
                "authors": [
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Pingjie Wang"
                    },
                    {
                        "name": "Liudan Zhao"
                    },
                    {
                        "name": "Yuchen Yang"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Kun Sun"
                    },
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanfeng Wang"
                },
                "author": "Yanfeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08664v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08664v2",
                "updated": "2024-08-19T12:20:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    20,
                    26,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-16T11:11:56Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    11,
                    56,
                    4,
                    229,
                    0
                ],
                "title": "A new perspective on Bayesian Operational Modal Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new perspective on Bayesian Operational Modal Analysis"
                },
                "summary": "In the field of operational modal analysis (OMA), obtained modal information\nis frequently used to assess the current state of aerospace, mechanical,\noffshore and civil structures. However, the stochasticity of operational\nsystems and the lack of forcing information can lead to inconsistent results.\nQuantifying the uncertainty of the recovered modal parameters through OMA is\ntherefore of significant value. In this article, a new perspective on Bayesian\nOMA is proposed: a Bayesian stochastic subspace identification (SSI) algorithm.\nDistinct from existing approaches to Bayesian OMA, a hierarchical probabilistic\nmodel is embedded at the core of covariance-driven SSI. Through substitution of\ncanonical correlation analysis with a Bayesian equivalent, posterior\ndistributions over the modal properties are obtained. Two inference schemes are\npresented for the proposed Bayesian formulation: Markov Chain Monte Carlo and\nvariational Bayes. Two case studies are then explored. The first is benchmark\nstudy using data from a simulated, multi degree-of-freedom, linear system.\nFollowing application of Bayesian SSI, it is shown that the same posterior is\ntargeted and recovered by both inference schemes, with good agreement between\nthe posterior mean and the conventional SSI result. The second study applies\nthe variational form to data obtained from an in-service structure: The Z24\nbridge. The results of this study are presented at single model orders, and\nthen using a stabilisation diagram. The recovered posterior uncertainty is\npresented and compared to the classic SSI result. It is observed that the\nposterior distributions with mean values coinciding with the natural\nfrequencies exhibit lower variance than values situated away from the natural\nfrequencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of operational modal analysis (OMA), obtained modal information\nis frequently used to assess the current state of aerospace, mechanical,\noffshore and civil structures. However, the stochasticity of operational\nsystems and the lack of forcing information can lead to inconsistent results.\nQuantifying the uncertainty of the recovered modal parameters through OMA is\ntherefore of significant value. In this article, a new perspective on Bayesian\nOMA is proposed: a Bayesian stochastic subspace identification (SSI) algorithm.\nDistinct from existing approaches to Bayesian OMA, a hierarchical probabilistic\nmodel is embedded at the core of covariance-driven SSI. Through substitution of\ncanonical correlation analysis with a Bayesian equivalent, posterior\ndistributions over the modal properties are obtained. Two inference schemes are\npresented for the proposed Bayesian formulation: Markov Chain Monte Carlo and\nvariational Bayes. Two case studies are then explored. The first is benchmark\nstudy using data from a simulated, multi degree-of-freedom, linear system.\nFollowing application of Bayesian SSI, it is shown that the same posterior is\ntargeted and recovered by both inference schemes, with good agreement between\nthe posterior mean and the conventional SSI result. The second study applies\nthe variational form to data obtained from an in-service structure: The Z24\nbridge. The results of this study are presented at single model orders, and\nthen using a stabilisation diagram. The recovered posterior uncertainty is\npresented and compared to the classic SSI result. It is observed that the\nposterior distributions with mean values coinciding with the natural\nfrequencies exhibit lower variance than values situated away from the natural\nfrequencies."
                },
                "authors": [
                    {
                        "name": "Brandon J. O'Connell"
                    },
                    {
                        "name": "Max D. Champneys"
                    },
                    {
                        "name": "Timothy J. Rogers"
                    }
                ],
                "author_detail": {
                    "name": "Timothy J. Rogers"
                },
                "author": "Timothy J. Rogers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08664v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08664v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08661v1",
                "updated": "2024-08-16T11:09:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    9,
                    56,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T11:09:56Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    9,
                    56,
                    4,
                    229,
                    0
                ],
                "title": "MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector"
                },
                "summary": "The increasing parameters and expansive dataset of large language models\n(LLMs) highlight the urgent demand for a technical solution to audit the\nunderlying privacy risks and copyright issues associated with LLMs. Existing\nstudies have partially addressed this need through an exploration of the\npre-training data detection problem, which is an instance of a membership\ninference attack (MIA). This problem involves determining whether a given piece\nof text has been used during the pre-training phase of the target LLM. Although\nexisting methods have designed various sophisticated MIA score functions to\nachieve considerable detection performance in pre-trained LLMs, how to achieve\nhigh-confidence detection and how to perform MIA on aligned LLMs remain\nchallenging. In this paper, we propose MIA-Tuner, a novel instruction-based MIA\nmethod, which instructs LLMs themselves to serve as a more precise pre-training\ndata detector internally, rather than design an external MIA score function.\nFurthermore, we design two instruction-based safeguards to respectively\nmitigate the privacy risks brought by the existing methods and MIA-Tuner. To\ncomprehensively evaluate the most recent state-of-the-art LLMs, we collect a\nmore up-to-date MIA benchmark dataset, named WIKIMIA-24, to replace the widely\nadopted benchmark WIKIMIA. We conduct extensive experiments across various\naligned and unaligned LLMs over the two benchmark datasets. The results\ndemonstrate that MIA-Tuner increases the AUC of MIAs from 0.7 to a\nsignificantly high level of 0.9.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing parameters and expansive dataset of large language models\n(LLMs) highlight the urgent demand for a technical solution to audit the\nunderlying privacy risks and copyright issues associated with LLMs. Existing\nstudies have partially addressed this need through an exploration of the\npre-training data detection problem, which is an instance of a membership\ninference attack (MIA). This problem involves determining whether a given piece\nof text has been used during the pre-training phase of the target LLM. Although\nexisting methods have designed various sophisticated MIA score functions to\nachieve considerable detection performance in pre-trained LLMs, how to achieve\nhigh-confidence detection and how to perform MIA on aligned LLMs remain\nchallenging. In this paper, we propose MIA-Tuner, a novel instruction-based MIA\nmethod, which instructs LLMs themselves to serve as a more precise pre-training\ndata detector internally, rather than design an external MIA score function.\nFurthermore, we design two instruction-based safeguards to respectively\nmitigate the privacy risks brought by the existing methods and MIA-Tuner. To\ncomprehensively evaluate the most recent state-of-the-art LLMs, we collect a\nmore up-to-date MIA benchmark dataset, named WIKIMIA-24, to replace the widely\nadopted benchmark WIKIMIA. We conduct extensive experiments across various\naligned and unaligned LLMs over the two benchmark datasets. The results\ndemonstrate that MIA-Tuner increases the AUC of MIAs from 0.7 to a\nsignificantly high level of 0.9."
                },
                "authors": [
                    {
                        "name": "Wenjie Fu"
                    },
                    {
                        "name": "Huandong Wang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Guanghua Liu"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Tao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Tao Jiang"
                },
                "author": "Tao Jiang",
                "arxiv_comment": "code and dataset: https://github.com/wjfu99/MIA-Tuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02349v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02349v2",
                "updated": "2024-08-16T11:09:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    9,
                    18,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-05T09:54:08Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    54,
                    8,
                    0,
                    218,
                    0
                ],
                "title": "Active Sensing of Knee Osteoarthritis Progression with Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Sensing of Knee Osteoarthritis Progression with Reinforcement\n  Learning"
                },
                "summary": "Osteoarthritis (OA) is the most common musculoskeletal disease, which has no\ncure. Knee OA (KOA) is one of the highest causes of disability worldwide, and\nit costs billions of United States dollars to the global community. Prediction\nof KOA progression has been of high interest to the community for years, as it\ncan advance treatment development through more efficient clinical trials and\nimprove patient outcomes through more efficient healthcare utilization.\nExisting approaches for predicting KOA, however, are predominantly static, i.e.\nconsider data from a single time point to predict progression many years into\nthe future, and knee level, i.e. consider progression in a single joint only.\nDue to these and related reasons, these methods fail to deliver the level of\npredictive performance, which is sufficient to result in cost savings and\nbetter patient outcomes. Collecting extensive data from all patients on a\nregular basis could address the issue, but it is limited by the high cost at a\npopulation level. In this work, we propose to go beyond static prediction\nmodels in OA, and bring a novel Active Sensing (AS) approach, designed to\ndynamically follow up patients with the objective of maximizing the number of\ninformative data acquisitions, while minimizing their total cost over a period\nof time. Our approach is based on Reinforcement Learning (RL), and it leverages\na novel reward function designed specifically for AS of disease progression in\nmore than one part of a human body. Our method is end-to-end, relies on\nmulti-modal Deep Learning, and requires no human input at inference time.\nThroughout an exhaustive experimental evaluation, we show that using RL can\nprovide a higher monetary benefit when compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Osteoarthritis (OA) is the most common musculoskeletal disease, which has no\ncure. Knee OA (KOA) is one of the highest causes of disability worldwide, and\nit costs billions of United States dollars to the global community. Prediction\nof KOA progression has been of high interest to the community for years, as it\ncan advance treatment development through more efficient clinical trials and\nimprove patient outcomes through more efficient healthcare utilization.\nExisting approaches for predicting KOA, however, are predominantly static, i.e.\nconsider data from a single time point to predict progression many years into\nthe future, and knee level, i.e. consider progression in a single joint only.\nDue to these and related reasons, these methods fail to deliver the level of\npredictive performance, which is sufficient to result in cost savings and\nbetter patient outcomes. Collecting extensive data from all patients on a\nregular basis could address the issue, but it is limited by the high cost at a\npopulation level. In this work, we propose to go beyond static prediction\nmodels in OA, and bring a novel Active Sensing (AS) approach, designed to\ndynamically follow up patients with the objective of maximizing the number of\ninformative data acquisitions, while minimizing their total cost over a period\nof time. Our approach is based on Reinforcement Learning (RL), and it leverages\na novel reward function designed specifically for AS of disease progression in\nmore than one part of a human body. Our method is end-to-end, relies on\nmulti-modal Deep Learning, and requires no human input at inference time.\nThroughout an exhaustive experimental evaluation, we show that using RL can\nprovide a higher monetary benefit when compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Khanh Nguyen"
                    },
                    {
                        "name": "Huy Hoang Nguyen"
                    },
                    {
                        "name": "Egor Panfilov"
                    },
                    {
                        "name": "Aleksei Tiulpin"
                    }
                ],
                "author_detail": {
                    "name": "Aleksei Tiulpin"
                },
                "author": "Aleksei Tiulpin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02349v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02349v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08068v2",
                "updated": "2024-08-16T10:50:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    10,
                    50,
                    45,
                    4,
                    229,
                    0
                ],
                "published": "2024-06-12T10:36:27Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    10,
                    36,
                    27,
                    2,
                    164,
                    0
                ],
                "title": "Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A\n  Survey"
                },
                "summary": "Compared to traditional sentiment analysis, which only considers text,\nmultimodal sentiment analysis needs to consider emotional signals from\nmultimodal sources simultaneously and is therefore more consistent with the way\nhow humans process sentiment in real-world scenarios. It involves processing\nemotional information from various sources such as natural language, images,\nvideos, audio, physiological signals, etc. However, although other modalities\nalso contain diverse emotional cues, natural language usually contains richer\ncontextual information and therefore always occupies a crucial position in\nmultimodal sentiment analysis. The emergence of ChatGPT has opened up immense\npotential for applying large language models (LLMs) to text-centric multimodal\ntasks. However, it is still unclear how existing LLMs can adapt better to\ntext-centric multimodal sentiment analysis tasks. This survey aims to (1)\npresent a comprehensive review of recent research in text-centric multimodal\nsentiment analysis tasks, (2) examine the potential of LLMs for text-centric\nmultimodal sentiment analysis, outlining their approaches, advantages, and\nlimitations, (3) summarize the application scenarios of LLM-based multimodal\nsentiment analysis technology, and (4) explore the challenges and potential\nresearch directions for multimodal sentiment analysis in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared to traditional sentiment analysis, which only considers text,\nmultimodal sentiment analysis needs to consider emotional signals from\nmultimodal sources simultaneously and is therefore more consistent with the way\nhow humans process sentiment in real-world scenarios. It involves processing\nemotional information from various sources such as natural language, images,\nvideos, audio, physiological signals, etc. However, although other modalities\nalso contain diverse emotional cues, natural language usually contains richer\ncontextual information and therefore always occupies a crucial position in\nmultimodal sentiment analysis. The emergence of ChatGPT has opened up immense\npotential for applying large language models (LLMs) to text-centric multimodal\ntasks. However, it is still unclear how existing LLMs can adapt better to\ntext-centric multimodal sentiment analysis tasks. This survey aims to (1)\npresent a comprehensive review of recent research in text-centric multimodal\nsentiment analysis tasks, (2) examine the potential of LLMs for text-centric\nmultimodal sentiment analysis, outlining their approaches, advantages, and\nlimitations, (3) summarize the application scenarios of LLM-based multimodal\nsentiment analysis technology, and (4) explore the challenges and potential\nresearch directions for multimodal sentiment analysis in the future."
                },
                "authors": [
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Yanyan Zhao"
                    },
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Tian Zheng"
                    },
                    {
                        "name": "Hongbo Zhang"
                    },
                    {
                        "name": "Zongyang Ma"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2210.14556 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08656v1",
                "updated": "2024-08-16T10:45:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    10,
                    45,
                    45,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T10:45:45Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    10,
                    45,
                    45,
                    4,
                    229,
                    0
                ],
                "title": "LLMs Are Biased Towards Output Formats! Systematically Evaluating and\n  Mitigating Output Format Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Are Biased Towards Output Formats! Systematically Evaluating and\n  Mitigating Output Format Bias of LLMs"
                },
                "summary": "We present the first systematic evaluation examining format bias in\nperformance of large language models (LLMs). Our approach distinguishes between\ntwo categories of an evaluation metric under format constraints to reliably and\naccurately assess performance: one measures performance when format constraints\nare adhered to, while the other evaluates performance regardless of constraint\nadherence. We then define a metric for measuring the format bias of LLMs and\nestablish effective strategies to reduce it. Subsequently, we present our\nempirical format bias evaluation spanning four commonly used categories --\nmultiple-choice question-answer, wrapping, list, and mapping -- covering 15\nwidely-used formats. Our evaluation on eight generation tasks uncovers\nsignificant format bias across state-of-the-art LLMs. We further discover that\nimproving the format-instruction following capabilities of LLMs across formats\npotentially reduces format bias. Based on our evaluation findings, we study\nprompting and fine-tuning with synthesized format data techniques to mitigate\nformat bias. Our methods successfully reduce the variance in ChatGPT's\nperformance among wrapping formats from 235.33 to 0.71 (%$^2$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first systematic evaluation examining format bias in\nperformance of large language models (LLMs). Our approach distinguishes between\ntwo categories of an evaluation metric under format constraints to reliably and\naccurately assess performance: one measures performance when format constraints\nare adhered to, while the other evaluates performance regardless of constraint\nadherence. We then define a metric for measuring the format bias of LLMs and\nestablish effective strategies to reduce it. Subsequently, we present our\nempirical format bias evaluation spanning four commonly used categories --\nmultiple-choice question-answer, wrapping, list, and mapping -- covering 15\nwidely-used formats. Our evaluation on eight generation tasks uncovers\nsignificant format bias across state-of-the-art LLMs. We further discover that\nimproving the format-instruction following capabilities of LLMs across formats\npotentially reduces format bias. Based on our evaluation findings, we study\nprompting and fine-tuning with synthesized format data techniques to mitigate\nformat bias. Our methods successfully reduce the variance in ChatGPT's\nperformance among wrapping formats from 235.33 to 0.71 (%$^2$)."
                },
                "authors": [
                    {
                        "name": "Do Xuan Long"
                    },
                    {
                        "name": "Hai Nguyen Ngoc"
                    },
                    {
                        "name": "Tiviatis Sim"
                    },
                    {
                        "name": "Hieu Dao"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15377v2",
                "updated": "2024-08-16T10:44:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    10,
                    44,
                    16,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-22T04:57:51Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    57,
                    51,
                    0,
                    204,
                    0
                ],
                "title": "Replicable Bandits for Digital Health Interventions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Replicable Bandits for Digital Health Interventions"
                },
                "summary": "Adaptive treatment assignment algorithms, such as bandit and reinforcement\nlearning algorithms, are increasingly used in digital health intervention\nclinical trials. Causal inference and related data analyses are critical for\nevaluating digital health interventions, deciding how to refine the\nintervention, and deciding whether to roll-out the intervention more broadly.\nHowever the replicability of these analyses has received relatively little\nattention. This work investigates the replicability of statistical analyses\nfrom trials deploying adaptive treatment assignment algorithms. We demonstrate\nthat many standard statistical estimators can be inconsistent and fail to be\nreplicable across repetitions of the clinical trial, even as the sample size\ngrows large. We show that this non-replicability is intimately related to\nproperties of the adaptive algorithm itself. We introduce a formal definition\nof a \"replicable bandit algorithm\" and prove that under such algorithms, a wide\nvariety of common statistical analyses are guaranteed to be consistent. We\npresent both theoretical results and simulation studies based on a mobile\nhealth oral health self-care intervention. Our findings underscore the\nimportance of designing adaptive algorithms with replicability in mind,\nespecially for settings like digital health where deployment decisions rely\nheavily on replicated evidence. We conclude by discussing open questions on the\nconnections between algorithm design, statistical inference, and experimental\nreplicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive treatment assignment algorithms, such as bandit and reinforcement\nlearning algorithms, are increasingly used in digital health intervention\nclinical trials. Causal inference and related data analyses are critical for\nevaluating digital health interventions, deciding how to refine the\nintervention, and deciding whether to roll-out the intervention more broadly.\nHowever the replicability of these analyses has received relatively little\nattention. This work investigates the replicability of statistical analyses\nfrom trials deploying adaptive treatment assignment algorithms. We demonstrate\nthat many standard statistical estimators can be inconsistent and fail to be\nreplicable across repetitions of the clinical trial, even as the sample size\ngrows large. We show that this non-replicability is intimately related to\nproperties of the adaptive algorithm itself. We introduce a formal definition\nof a \"replicable bandit algorithm\" and prove that under such algorithms, a wide\nvariety of common statistical analyses are guaranteed to be consistent. We\npresent both theoretical results and simulation studies based on a mobile\nhealth oral health self-care intervention. Our findings underscore the\nimportance of designing adaptive algorithms with replicability in mind,\nespecially for settings like digital health where deployment decisions rely\nheavily on replicated evidence. We conclude by discussing open questions on the\nconnections between algorithm design, statistical inference, and experimental\nreplicability."
                },
                "authors": [
                    {
                        "name": "Kelly W. Zhang"
                    },
                    {
                        "name": "Nowell Closser"
                    },
                    {
                        "name": "Anna L. Trella"
                    },
                    {
                        "name": "Susan A. Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Susan A. Murphy"
                },
                "author": "Susan A. Murphy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16058v2",
                "updated": "2024-08-16T10:29:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    10,
                    29,
                    46,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-22T21:26:39Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    21,
                    26,
                    39,
                    0,
                    204,
                    0
                ],
                "title": "Revisiting Score Function Estimators for $k$-Subset Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Score Function Estimators for $k$-Subset Sampling"
                },
                "summary": "Are score function estimators an underestimated approach to learning with\n$k$-subset sampling? Sampling $k$-subsets is a fundamental operation in many\nmachine learning tasks that is not amenable to differentiable parametrization,\nimpeding gradient-based optimization. Prior work has focused on relaxed\nsampling or pathwise gradient estimators. Inspired by the success of score\nfunction estimators in variational inference and reinforcement learning, we\nrevisit them within the context of $k$-subset sampling. Specifically, we\ndemonstrate how to efficiently compute the $k$-subset distribution's score\nfunction using a discrete Fourier transform, and reduce the estimator's\nvariance with control variates. The resulting estimator provides both exact\nsamples and unbiased gradient estimates while also applying to\nnon-differentiable downstream models, unlike existing methods. Experiments in\nfeature selection show results competitive with current methods, despite weaker\nassumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are score function estimators an underestimated approach to learning with\n$k$-subset sampling? Sampling $k$-subsets is a fundamental operation in many\nmachine learning tasks that is not amenable to differentiable parametrization,\nimpeding gradient-based optimization. Prior work has focused on relaxed\nsampling or pathwise gradient estimators. Inspired by the success of score\nfunction estimators in variational inference and reinforcement learning, we\nrevisit them within the context of $k$-subset sampling. Specifically, we\ndemonstrate how to efficiently compute the $k$-subset distribution's score\nfunction using a discrete Fourier transform, and reduce the estimator's\nvariance with control variates. The resulting estimator provides both exact\nsamples and unbiased gradient estimates while also applying to\nnon-differentiable downstream models, unlike existing methods. Experiments in\nfeature selection show results competitive with current methods, despite weaker\nassumptions."
                },
                "authors": [
                    {
                        "name": "Klas Wijk"
                    },
                    {
                        "name": "Ricardo Vinuesa"
                    },
                    {
                        "name": "Hossein Azizpour"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Azizpour"
                },
                "author": "Hossein Azizpour",
                "arxiv_comment": "ICML 2024 Workshop on Differentiable Almost Everything:\n  Differentiable Relaxations, Algorithms, Operators, and Simulators",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08645v1",
                "updated": "2024-08-16T10:21:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    10,
                    21,
                    13,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T10:21:13Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    10,
                    21,
                    13,
                    4,
                    229,
                    0
                ],
                "title": "Extracting polygonal footprints in off-nadir images with Segment\n  Anything Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting polygonal footprints in off-nadir images with Segment\n  Anything Model"
                },
                "summary": "Building Footprint Extraction (BFE) in off-nadir aerial images often relies\non roof segmentation and roof-to-footprint offset prediction, then drugging\nroof-to-footprint via the offset. However, the results from this multi-stage\ninference are not applicable in data production, because of the low quality of\nmasks given by prediction. To solve this problem, we proposed OBMv2 in this\npaper, which supports both end-to-end and promptable polygonal footprint\nprediction. Different from OBM, OBMv2 using a newly proposed Self Offset\nAttention (SOFA) to bridge the performance gap on bungalow and skyscraper,\nwhich realized a real end-to-end footprint polygon prediction without\npostprocessing. %, such as Non-Maximum Suppression (NMS) and Distance NMS\n(DNMS). % To fully use information contained in roof masks, building masks and\noffsets, we proposed a Multi-level Information SyStem (MISS) for footprint\nprediction, with which OBMv2 can predict footprints even with insufficient\npredictions. Additionally, to squeeze information from the same model, we were\ninspired by Retrieval-Augmented Generation (RAG) in Nature Language Processing\nand proposed \"RAG in BFE\" problem. To verify the effectiveness of the proposed\nmethod, experiments were conducted on open datasets BONAI and OmniCity-view3. A\ngeneralization test was also conducted on Huizhou test set. The code will be\navailable at \\url{https://github.com/likaiucas/OBM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Footprint Extraction (BFE) in off-nadir aerial images often relies\non roof segmentation and roof-to-footprint offset prediction, then drugging\nroof-to-footprint via the offset. However, the results from this multi-stage\ninference are not applicable in data production, because of the low quality of\nmasks given by prediction. To solve this problem, we proposed OBMv2 in this\npaper, which supports both end-to-end and promptable polygonal footprint\nprediction. Different from OBM, OBMv2 using a newly proposed Self Offset\nAttention (SOFA) to bridge the performance gap on bungalow and skyscraper,\nwhich realized a real end-to-end footprint polygon prediction without\npostprocessing. %, such as Non-Maximum Suppression (NMS) and Distance NMS\n(DNMS). % To fully use information contained in roof masks, building masks and\noffsets, we proposed a Multi-level Information SyStem (MISS) for footprint\nprediction, with which OBMv2 can predict footprints even with insufficient\npredictions. Additionally, to squeeze information from the same model, we were\ninspired by Retrieval-Augmented Generation (RAG) in Nature Language Processing\nand proposed \"RAG in BFE\" problem. To verify the effectiveness of the proposed\nmethod, experiments were conducted on open datasets BONAI and OmniCity-view3. A\ngeneralization test was also conducted on Huizhou test set. The code will be\navailable at \\url{https://github.com/likaiucas/OBM}."
                },
                "authors": [
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Jingbo Chen"
                    },
                    {
                        "name": "Yupeng Deng"
                    },
                    {
                        "name": "Yu Meng"
                    },
                    {
                        "name": "Diyou Liu"
                    },
                    {
                        "name": "Junxian Ma"
                    },
                    {
                        "name": "Chenhao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenhao Wang"
                },
                "author": "Chenhao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08639v1",
                "updated": "2024-08-16T10:09:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    10,
                    9,
                    45,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T10:09:45Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    10,
                    9,
                    45,
                    4,
                    229,
                    0
                ],
                "title": "Solving The Quantum Many-Body Hamiltonian Learning Problem with Neural\n  Differential Equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving The Quantum Many-Body Hamiltonian Learning Problem with Neural\n  Differential Equations"
                },
                "summary": "Understanding and characterising quantum many-body dynamics remains a\nsignificant challenge due to both the exponential complexity required to\nrepresent quantum many-body Hamiltonians, and the need to accurately track\nstates in time under the action of such Hamiltonians. This inherent complexity\nlimits our ability to characterise quantum many-body systems, highlighting the\nneed for innovative approaches to unlock their full potential. To address this\nchallenge, we propose a novel method to solve the Hamiltonian Learning (HL)\nproblem-inferring quantum dynamics from many-body state trajectories-using\nNeural Differential Equations combined with an Ansatz Hamiltonian. Our method\nis reliably convergent, experimentally friendly, and interpretable, making it a\nstable solution for HL on a set of Hamiltonians previously unlearnable in the\nliterature. In addition to this, we propose a new quantitative benchmark based\non power laws, which can objectively compare the reliability and generalisation\ncapabilities of any two HL algorithms. Finally, we benchmark our method against\nstate-of-the-art HL algorithms with a 1D spin-1/2 chain proof of concept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and characterising quantum many-body dynamics remains a\nsignificant challenge due to both the exponential complexity required to\nrepresent quantum many-body Hamiltonians, and the need to accurately track\nstates in time under the action of such Hamiltonians. This inherent complexity\nlimits our ability to characterise quantum many-body systems, highlighting the\nneed for innovative approaches to unlock their full potential. To address this\nchallenge, we propose a novel method to solve the Hamiltonian Learning (HL)\nproblem-inferring quantum dynamics from many-body state trajectories-using\nNeural Differential Equations combined with an Ansatz Hamiltonian. Our method\nis reliably convergent, experimentally friendly, and interpretable, making it a\nstable solution for HL on a set of Hamiltonians previously unlearnable in the\nliterature. In addition to this, we propose a new quantitative benchmark based\non power laws, which can objectively compare the reliability and generalisation\ncapabilities of any two HL algorithms. Finally, we benchmark our method against\nstate-of-the-art HL algorithms with a 1D spin-1/2 chain proof of concept."
                },
                "authors": [
                    {
                        "name": "Timothy Heightman"
                    },
                    {
                        "name": "Edward Jiang"
                    },
                    {
                        "name": "Antonio Acín"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Acín"
                },
                "author": "Antonio Acín",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01129v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01129v3",
                "updated": "2024-08-16T10:03:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    10,
                    3,
                    53,
                    4,
                    229,
                    0
                ],
                "published": "2024-04-01T14:11:45Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    14,
                    11,
                    45,
                    0,
                    92,
                    0
                ],
                "title": "Emphasising Structured Information: Integrating Abstract Meaning\n  Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emphasising Structured Information: Integrating Abstract Meaning\n  Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation"
                },
                "summary": "Automatic open-domain dialogue evaluation has attracted increasing attention.\nTrainable evaluation metrics, typically trained with true positive and randomly\nselected negative responses, tend to assign higher scores to responses that\nshare greater content similarity with a given context. However, adversarial\nnegative responses, despite possessing high content similarity with the\ncontexts, are semantically different. Consequently, existing evaluation metrics\nare not robust enough to evaluate such responses, resulting in low correlations\nwith human judgments. While recent studies have demonstrated the effectiveness\nof Large Language Models (LLMs) for open-domain dialogue evaluation, they still\nface challenges in effectively handling adversarial negative examples. In this\npaper, we propose an effective framework for open-domain dialogue evaluation,\nwhich combines domain-specific language models (SLMs) enhanced with Abstract\nMeaning Representation (AMR) knowledge with LLMs. The SLMs can explicitly\nincorporate AMR graph information of the dialogue through a gating mechanism\nfor enhanced dialogue semantic representation learning. Both the evaluation\nresult from the SLMs and the AMR graph information are incorporated into the\nLLM's prompt for enhanced evaluation performance. Experimental results on\nopen-domain dialogue evaluation tasks demonstrate the superiority of our method\ncompared to a wide range of state-of-the-art baselines, especially in\ndiscriminating adversarial negative responses. Our code and data are publicly\navailable at https://github.com/Bernard-Yang/SIMAMR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic open-domain dialogue evaluation has attracted increasing attention.\nTrainable evaluation metrics, typically trained with true positive and randomly\nselected negative responses, tend to assign higher scores to responses that\nshare greater content similarity with a given context. However, adversarial\nnegative responses, despite possessing high content similarity with the\ncontexts, are semantically different. Consequently, existing evaluation metrics\nare not robust enough to evaluate such responses, resulting in low correlations\nwith human judgments. While recent studies have demonstrated the effectiveness\nof Large Language Models (LLMs) for open-domain dialogue evaluation, they still\nface challenges in effectively handling adversarial negative examples. In this\npaper, we propose an effective framework for open-domain dialogue evaluation,\nwhich combines domain-specific language models (SLMs) enhanced with Abstract\nMeaning Representation (AMR) knowledge with LLMs. The SLMs can explicitly\nincorporate AMR graph information of the dialogue through a gating mechanism\nfor enhanced dialogue semantic representation learning. Both the evaluation\nresult from the SLMs and the AMR graph information are incorporated into the\nLLM's prompt for enhanced evaluation performance. Experimental results on\nopen-domain dialogue evaluation tasks demonstrate the superiority of our method\ncompared to a wide range of state-of-the-art baselines, especially in\ndiscriminating adversarial negative responses. Our code and data are publicly\navailable at https://github.com/Bernard-Yang/SIMAMR."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Kun Zhao"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Liang Zhan"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01129v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01129v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08632v1",
                "updated": "2024-08-16T09:52:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    52,
                    2,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T09:52:02Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    52,
                    2,
                    4,
                    229,
                    0
                ],
                "title": "A Survey on Benchmarks of Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Benchmarks of Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are gaining increasing popularity in\nboth academia and industry due to their remarkable performance in various\napplications such as visual question answering, visual perception,\nunderstanding, and reasoning. Over the past few years, significant efforts have\nbeen made to examine MLLMs from multiple perspectives. This paper presents a\ncomprehensive review of \\textbf{180 benchmarks} and evaluation for MLLMs,\nfocusing on (1)perception and understanding, (2)cognition and reasoning,\n(3)specific domains, (4)key capabilities, and (5)other modalities. Finally, we\ndiscuss the limitations of the current evaluation methods for MLLMs and explore\npromising future directions. Our key argument is that evaluation should be\nregarded as a crucial discipline to better support the development of MLLMs.\nFor more details, please visit our GitHub repository:\nhttps://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are gaining increasing popularity in\nboth academia and industry due to their remarkable performance in various\napplications such as visual question answering, visual perception,\nunderstanding, and reasoning. Over the past few years, significant efforts have\nbeen made to examine MLLMs from multiple perspectives. This paper presents a\ncomprehensive review of \\textbf{180 benchmarks} and evaluation for MLLMs,\nfocusing on (1)perception and understanding, (2)cognition and reasoning,\n(3)specific domains, (4)key capabilities, and (5)other modalities. Finally, we\ndiscuss the limitations of the current evaluation methods for MLLMs and explore\npromising future directions. Our key argument is that evaluation should be\nregarded as a crucial discipline to better support the development of MLLMs.\nFor more details, please visit our GitHub repository:\nhttps://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey."
                },
                "authors": [
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Weiheng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Weiheng Lu"
                },
                "author": "Weiheng Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08631v1",
                "updated": "2024-08-16T09:49:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    49,
                    51,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T09:49:51Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    49,
                    51,
                    4,
                    229,
                    0
                ],
                "title": "Persona is a Double-edged Sword: Enhancing the Zero-shot Reasoning by\n  Ensembling the Role-playing and Neutral Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona is a Double-edged Sword: Enhancing the Zero-shot Reasoning by\n  Ensembling the Role-playing and Neutral Prompts"
                },
                "summary": "Recent studies demonstrate that prompting an appropriate role-playing persona\nto an LLM improves its reasoning capability. However, assigning a proper\npersona is difficult since an LLM's performance is extremely sensitive to\nassigned prompts; therefore, personas sometimes hinder LLMs and degrade their\nreasoning capabilities. In this paper, we propose a novel framework, Jekyll \\&\nHyde, which ensembles the results of role-playing and neutral prompts to\neradicate performance degradation via unilateral use of role-playing prompted\nLLM and enhance the robustness of an LLM's reasoning ability. Specifically,\nJekyll \\& Hyde collects two potential solutions from both role-playing and\nneutral prompts and selects a better solution after cross-checking via an LLM\nevaluator. However, LLM-based evaluators tend to be affected by the order of\nthose potential solutions within the prompt when selecting the proper solution;\nthus, we also propose a robust LLM evaluator to mitigate the position bias. The\nexperimental analysis demonstrates that role-playing prompts distract LLMs and\ndegrade their reasoning abilities in 4 out of 12 datasets, even when using\nGPT-4. In addition, we reveal that Jekyll \\& Hyde improves reasoning\ncapabilities by selecting better choices among the potential solutions on\ntwelve widely-used reasoning datasets. We further show that our proposed LLM\nevaluator outperforms other baselines, proving the LLMs' position bias is\nsuccessfully mitigated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies demonstrate that prompting an appropriate role-playing persona\nto an LLM improves its reasoning capability. However, assigning a proper\npersona is difficult since an LLM's performance is extremely sensitive to\nassigned prompts; therefore, personas sometimes hinder LLMs and degrade their\nreasoning capabilities. In this paper, we propose a novel framework, Jekyll \\&\nHyde, which ensembles the results of role-playing and neutral prompts to\neradicate performance degradation via unilateral use of role-playing prompted\nLLM and enhance the robustness of an LLM's reasoning ability. Specifically,\nJekyll \\& Hyde collects two potential solutions from both role-playing and\nneutral prompts and selects a better solution after cross-checking via an LLM\nevaluator. However, LLM-based evaluators tend to be affected by the order of\nthose potential solutions within the prompt when selecting the proper solution;\nthus, we also propose a robust LLM evaluator to mitigate the position bias. The\nexperimental analysis demonstrates that role-playing prompts distract LLMs and\ndegrade their reasoning abilities in 4 out of 12 datasets, even when using\nGPT-4. In addition, we reveal that Jekyll \\& Hyde improves reasoning\ncapabilities by selecting better choices among the potential solutions on\ntwelve widely-used reasoning datasets. We further show that our proposed LLM\nevaluator outperforms other baselines, proving the LLMs' position bias is\nsuccessfully mitigated."
                },
                "authors": [
                    {
                        "name": "Junseok Kim"
                    },
                    {
                        "name": "Nakyeong Yang"
                    },
                    {
                        "name": "Kyomin Jung"
                    }
                ],
                "author_detail": {
                    "name": "Kyomin Jung"
                },
                "author": "Kyomin Jung",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07343v2",
                "updated": "2024-08-16T09:44:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    44,
                    49,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-14T07:37:07Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    37,
                    7,
                    2,
                    227,
                    0
                ],
                "title": "Gradient Alignment Improves Test-Time Adaptation for Medical Image\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Alignment Improves Test-Time Adaptation for Medical Image\n  Segmentation"
                },
                "summary": "Although recent years have witnessed significant advancements in medical\nimage segmentation, the pervasive issue of domain shift among medical images\nfrom diverse centres hinders the effective deployment of pre-trained models.\nMany Test-time Adaptation (TTA) methods have been proposed to address this\nissue by fine-tuning pre-trained models with test data during inference. These\nmethods, however, often suffer from less-satisfactory optimization due to\nsuboptimal optimization direction (dictated by the gradient) and fixed\nstep-size (predicated on the learning rate). In this paper, we propose the\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\nthe gradient direction and learning rate in the optimization procedure. Unlike\nconventional TTA methods, which primarily optimize the pseudo gradient derived\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\nenables the model to excavate the similarities between different gradients and\ncorrect the gradient direction to approximate the empirical gradient related to\nthe current segmentation task. Additionally, we design a dynamic learning rate\nbased on the cosine similarity between the pseudo and auxiliary gradients,\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\ntest data. Extensive experiments establish the effectiveness of the proposed\ngradient alignment and dynamic learning rate and substantiate the superiority\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\nmedical image segmentation task. The code and weights of pre-trained source\nmodels will be available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although recent years have witnessed significant advancements in medical\nimage segmentation, the pervasive issue of domain shift among medical images\nfrom diverse centres hinders the effective deployment of pre-trained models.\nMany Test-time Adaptation (TTA) methods have been proposed to address this\nissue by fine-tuning pre-trained models with test data during inference. These\nmethods, however, often suffer from less-satisfactory optimization due to\nsuboptimal optimization direction (dictated by the gradient) and fixed\nstep-size (predicated on the learning rate). In this paper, we propose the\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\nthe gradient direction and learning rate in the optimization procedure. Unlike\nconventional TTA methods, which primarily optimize the pseudo gradient derived\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\nenables the model to excavate the similarities between different gradients and\ncorrect the gradient direction to approximate the empirical gradient related to\nthe current segmentation task. Additionally, we design a dynamic learning rate\nbased on the cosine similarity between the pseudo and auxiliary gradients,\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\ntest data. Extensive experiments establish the effectiveness of the proposed\ngradient alignment and dynamic learning rate and substantiate the superiority\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\nmedical image segmentation task. The code and weights of pre-trained source\nmodels will be available."
                },
                "authors": [
                    {
                        "name": "Ziyang Chen"
                    },
                    {
                        "name": "Yiwen Ye"
                    },
                    {
                        "name": "Yongsheng Pan"
                    },
                    {
                        "name": "Yong Xia"
                    }
                ],
                "author_detail": {
                    "name": "Yong Xia"
                },
                "author": "Yong Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08628v1",
                "updated": "2024-08-16T09:42:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    42,
                    19,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T09:42:19Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    42,
                    19,
                    4,
                    229,
                    0
                ],
                "title": "A survey on secure decentralized optimization and learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A survey on secure decentralized optimization and learning"
                },
                "summary": "Decentralized optimization has become a standard paradigm for solving\nlarge-scale decision-making problems and training large machine learning models\nwithout centralizing data. However, this paradigm introduces new privacy and\nsecurity risks, with malicious agents potentially able to infer private data or\nimpair the model accuracy. Over the past decade, significant advancements have\nbeen made in developing secure decentralized optimization and learning\nframeworks and algorithms. This survey provides a comprehensive tutorial on\nthese advancements. We begin with the fundamentals of decentralized\noptimization and learning, highlighting centralized aggregation and distributed\nconsensus as key modules exposed to security risks in federated and distributed\noptimization, respectively. Next, we focus on privacy-preserving algorithms,\ndetailing three cryptographic tools and their integration into decentralized\noptimization and learning systems. Additionally, we examine resilient\nalgorithms, exploring the design and analysis of resilient aggregation and\nconsensus protocols that support these systems. We conclude the survey by\ndiscussing current trends and potential future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized optimization has become a standard paradigm for solving\nlarge-scale decision-making problems and training large machine learning models\nwithout centralizing data. However, this paradigm introduces new privacy and\nsecurity risks, with malicious agents potentially able to infer private data or\nimpair the model accuracy. Over the past decade, significant advancements have\nbeen made in developing secure decentralized optimization and learning\nframeworks and algorithms. This survey provides a comprehensive tutorial on\nthese advancements. We begin with the fundamentals of decentralized\noptimization and learning, highlighting centralized aggregation and distributed\nconsensus as key modules exposed to security risks in federated and distributed\noptimization, respectively. Next, we focus on privacy-preserving algorithms,\ndetailing three cryptographic tools and their integration into decentralized\noptimization and learning systems. Additionally, we examine resilient\nalgorithms, exploring the design and analysis of resilient aggregation and\nconsensus protocols that support these systems. We conclude the survey by\ndiscussing current trends and potential future directions."
                },
                "authors": [
                    {
                        "name": "Changxin Liu"
                    },
                    {
                        "name": "Nicola Bastianello"
                    },
                    {
                        "name": "Wei Huo"
                    },
                    {
                        "name": "Yang Shi"
                    },
                    {
                        "name": "Karl H. Johansson"
                    }
                ],
                "author_detail": {
                    "name": "Karl H. Johansson"
                },
                "author": "Karl H. Johansson",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08624v1",
                "updated": "2024-08-16T09:32:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    32,
                    43,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T09:32:43Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    32,
                    43,
                    4,
                    229,
                    0
                ],
                "title": "RealMedQA: A pilot biomedical question answering dataset containing\n  realistic clinical questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RealMedQA: A pilot biomedical question answering dataset containing\n  realistic clinical questions"
                },
                "summary": "Clinical question answering systems have the potential to provide clinicians\nwith relevant and timely answers to their questions. Nonetheless, despite the\nadvances that have been made, adoption of these systems in clinical settings\nhas been slow. One issue is a lack of question-answering datasets which reflect\nthe real-world needs of health professionals. In this work, we present\nRealMedQA, a dataset of realistic clinical questions generated by humans and an\nLLM. We describe the process for generating and verifying the QA pairs and\nassess several QA models on BioASQ and RealMedQA to assess the relative\ndifficulty of matching answers to questions. We show that the LLM is more\ncost-efficient for generating \"ideal\" QA pairs. Additionally, we achieve a\nlower lexical similarity between questions and answers than BioASQ which\nprovides an additional challenge to the top two QA models, as per the results.\nWe release our code and our dataset publicly to encourage further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical question answering systems have the potential to provide clinicians\nwith relevant and timely answers to their questions. Nonetheless, despite the\nadvances that have been made, adoption of these systems in clinical settings\nhas been slow. One issue is a lack of question-answering datasets which reflect\nthe real-world needs of health professionals. In this work, we present\nRealMedQA, a dataset of realistic clinical questions generated by humans and an\nLLM. We describe the process for generating and verifying the QA pairs and\nassess several QA models on BioASQ and RealMedQA to assess the relative\ndifficulty of matching answers to questions. We show that the LLM is more\ncost-efficient for generating \"ideal\" QA pairs. Additionally, we achieve a\nlower lexical similarity between questions and answers than BioASQ which\nprovides an additional challenge to the top two QA models, as per the results.\nWe release our code and our dataset publicly to encourage further research."
                },
                "authors": [
                    {
                        "name": "Gregory Kell"
                    },
                    {
                        "name": "Angus Roberts"
                    },
                    {
                        "name": "Serge Umansky"
                    },
                    {
                        "name": "Yuti Khare"
                    },
                    {
                        "name": "Najma Ahmed"
                    },
                    {
                        "name": "Nikhil Patel"
                    },
                    {
                        "name": "Chloe Simela"
                    },
                    {
                        "name": "Jack Coumbe"
                    },
                    {
                        "name": "Julian Rozario"
                    },
                    {
                        "name": "Ryan-Rhys Griffiths"
                    },
                    {
                        "name": "Iain J. Marshall"
                    }
                ],
                "author_detail": {
                    "name": "Iain J. Marshall"
                },
                "author": "Iain J. Marshall",
                "arxiv_comment": "Accepted at AMIA Annual Symposium 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08619v1",
                "updated": "2024-08-16T09:19:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    19,
                    27,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T09:19:27Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    19,
                    27,
                    4,
                    229,
                    0
                ],
                "title": "PatUntrack: Automated Generating Patch Examples for Issue Reports\n  without Tracked Insecure Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatUntrack: Automated Generating Patch Examples for Issue Reports\n  without Tracked Insecure Code"
                },
                "summary": "Security patches are essential for enhancing the stability and robustness of\nprojects in the software community. While vulnerabilities are officially\nexpected to be patched before being disclosed, patching vulnerabilities is\ncomplicated and remains a struggle for many organizations. To patch\nvulnerabilities, security practitioners typically track vulnerable issue\nreports (IRs), and analyze their relevant insecure code to generate potential\npatches. However, the relevant insecure code may not be explicitly specified\nand practitioners cannot track the insecure code in the repositories, thus\nlimiting their ability to generate patches. In such cases, providing examples\nof insecure code and the corresponding patches would benefit the security\ndevelopers to better locate and fix the insecure code. In this paper, we\npropose PatUntrack to automatically generating patch examples from IRs without\ntracked insecure code. It auto-prompts Large Language Models (LLMs) to make\nthem applicable to analyze the vulnerabilities. It first generates the\ncompleted description of the Vulnerability-Triggering Path (VTP) from\nvulnerable IRs. Then, it corrects hallucinations in the VTP description with\nexternal golden knowledge. Finally, it generates Top-K pairs of Insecure Code\nand Patch Example based on the corrected VTP description. To evaluate the\nperformance, we conducted experiments on 5,465 vulnerable IRs. The experimental\nresults show that PatUntrack can obtain the highest performance and improve the\ntraditional LLM baselines by +14.6% (Fix@10) on average in patch example\ngeneration. Furthermore, PatUntrack was applied to generate patch examples for\n76 newly disclosed vulnerable IRs. 27 out of 37 replies from the authors of\nthese IRs confirmed the usefulness of the patch examples generated by\nPatUntrack, indicating that they can benefit from these examples for patching\nthe vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security patches are essential for enhancing the stability and robustness of\nprojects in the software community. While vulnerabilities are officially\nexpected to be patched before being disclosed, patching vulnerabilities is\ncomplicated and remains a struggle for many organizations. To patch\nvulnerabilities, security practitioners typically track vulnerable issue\nreports (IRs), and analyze their relevant insecure code to generate potential\npatches. However, the relevant insecure code may not be explicitly specified\nand practitioners cannot track the insecure code in the repositories, thus\nlimiting their ability to generate patches. In such cases, providing examples\nof insecure code and the corresponding patches would benefit the security\ndevelopers to better locate and fix the insecure code. In this paper, we\npropose PatUntrack to automatically generating patch examples from IRs without\ntracked insecure code. It auto-prompts Large Language Models (LLMs) to make\nthem applicable to analyze the vulnerabilities. It first generates the\ncompleted description of the Vulnerability-Triggering Path (VTP) from\nvulnerable IRs. Then, it corrects hallucinations in the VTP description with\nexternal golden knowledge. Finally, it generates Top-K pairs of Insecure Code\nand Patch Example based on the corrected VTP description. To evaluate the\nperformance, we conducted experiments on 5,465 vulnerable IRs. The experimental\nresults show that PatUntrack can obtain the highest performance and improve the\ntraditional LLM baselines by +14.6% (Fix@10) on average in patch example\ngeneration. Furthermore, PatUntrack was applied to generate patch examples for\n76 newly disclosed vulnerable IRs. 27 out of 37 replies from the authors of\nthese IRs confirmed the usefulness of the patch examples generated by\nPatUntrack, indicating that they can benefit from these examples for patching\nthe vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Ziyou Jiang"
                    },
                    {
                        "name": "Lin Shi"
                    },
                    {
                        "name": "Guowei Yang"
                    },
                    {
                        "name": "Qing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qing Wang"
                },
                "author": "Qing Wang",
                "arxiv_comment": "Accepted by ASE'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17962v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17962v3",
                "updated": "2024-08-16T08:48:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    48,
                    26,
                    4,
                    229,
                    0
                ],
                "published": "2024-06-25T22:44:17Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    22,
                    44,
                    17,
                    1,
                    177,
                    0
                ],
                "title": "Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework"
                },
                "summary": "Large Language Models (LLMs) demonstrate a remarkable ability to comprehend\nhuman instructions and generate high-quality text. This capability allows LLMs\nto function as agents that can emulate human beings at a more sophisticated\nlevel, beyond the mere replication of basic human behaviours. However, there is\na lack of exploring into leveraging LLMs to craft characters from diverse\naspects. In this work, we introduce the Customisable Conversation Agent\nFramework, which leverages LLMs to simulate real-world characters that can be\nfreely customised according to various user preferences. This adaptable\nframework is beneficial for the design of customisable characters and\nrole-playing agents aligned with human preferences. We propose the SimsConv\ndataset, which encompasses 68 different customised characters, 1,360 multi-turn\nrole-playing dialogues, and a total of 13,971 interaction dialogues. The\ncharacters are created from several real-world elements, such as career,\naspiration, trait, and skill. Building upon these foundations, we present\nSimsChat, a freely customisable role-playing agent. It incorporates diverse\nreal-world scenes and topic-specific character interaction dialogues, thereby\nsimulating characters' life experiences in various scenarios and topic-specific\ninteractions with specific emotions. Experimental results indicate that our\nproposed framework achieves desirable performance and provides a valuable\nguideline for the construction of more accurate human simulacra in the future.\nOur data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate a remarkable ability to comprehend\nhuman instructions and generate high-quality text. This capability allows LLMs\nto function as agents that can emulate human beings at a more sophisticated\nlevel, beyond the mere replication of basic human behaviours. However, there is\na lack of exploring into leveraging LLMs to craft characters from diverse\naspects. In this work, we introduce the Customisable Conversation Agent\nFramework, which leverages LLMs to simulate real-world characters that can be\nfreely customised according to various user preferences. This adaptable\nframework is beneficial for the design of customisable characters and\nrole-playing agents aligned with human preferences. We propose the SimsConv\ndataset, which encompasses 68 different customised characters, 1,360 multi-turn\nrole-playing dialogues, and a total of 13,971 interaction dialogues. The\ncharacters are created from several real-world elements, such as career,\naspiration, trait, and skill. Building upon these foundations, we present\nSimsChat, a freely customisable role-playing agent. It incorporates diverse\nreal-world scenes and topic-specific character interaction dialogues, thereby\nsimulating characters' life experiences in various scenarios and topic-specific\ninteractions with specific emotions. Experimental results indicate that our\nproposed framework achieves desirable performance and provides a valuable\nguideline for the construction of more accurate human simulacra in the future.\nOur data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Kun Zhao"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Lanxiao Huang"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17962v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17962v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08603v1",
                "updated": "2024-08-16T08:43:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    43,
                    11,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T08:43:11Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    43,
                    11,
                    4,
                    229,
                    0
                ],
                "title": "Magnetic fields in the outskirts of PSZ2 G096.88+24.18 from\n  depolarization analysis of radio relics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic fields in the outskirts of PSZ2 G096.88+24.18 from\n  depolarization analysis of radio relics"
                },
                "summary": "In this paper, we investigate the polarization properties of the double radio\nrelics in PSZ2 G096.88+24.18 using the rotation measure synthesis, and try to\nconstrain the characteristics of the magnetic field that reproduce the observed\nbeam depolarization. Our aim is to understand the nature of the low\npolarization fraction that characterizes the southern relic with respect to the\nnorthern relic. Using new 1-2 GHz VLA observations, we derive the rotation\nmeasure and polarization of the two relics by applying the RM synthesis\ntechnique, thus solving for bandwidth depolarization in the wide observing\nbandwidth. To study the effect of beam depolarization, we degraded the image\nresolution and studied the decreasing trend of polarization fraction with\nincreasing beam size. Finally, we performed 3D magnetic field simulations using\nmultiple models for the magnetic field power spectrum over a wide range of\nscales, in order to constrain the characteristics of the cluster magnetic field\nthat can reproduce the observed beam depolarization trend. Using RM synthesis,\nwe obtained a polarization fraction of ($18.6 \\pm 0.3$)% for the norther relic\nand ($14.6 \\pm 0.1$)% for the southern one. Having corrected for bandwidth\ndepolarization, we infer that the nature of the depolarization for the southern\nrelic is external, and possibly related to the turbulent gas distribution\nwithin the cluster, or to the complex spatial structure of the relic. The\nbest-fit magnetic field power spectrum, that reproduces the observed\ndepolarization trend for the southern relic, is obtained for a turbulent\nmagnetic field model, described by a power spectrum derived from cosmological\nsimulations, and defined within the scales of $\\Lambda_{\\rm{min}}=35~\\rm{kpc}$\nand $\\Lambda_{\\rm{max}}=400~\\rm{kpc}$. This yields an average magnetic field of\nthe cluster within 1$~\\rm{Mpc}^3$ volume of $\\sim 2~\\rm{\\mu G}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the polarization properties of the double radio\nrelics in PSZ2 G096.88+24.18 using the rotation measure synthesis, and try to\nconstrain the characteristics of the magnetic field that reproduce the observed\nbeam depolarization. Our aim is to understand the nature of the low\npolarization fraction that characterizes the southern relic with respect to the\nnorthern relic. Using new 1-2 GHz VLA observations, we derive the rotation\nmeasure and polarization of the two relics by applying the RM synthesis\ntechnique, thus solving for bandwidth depolarization in the wide observing\nbandwidth. To study the effect of beam depolarization, we degraded the image\nresolution and studied the decreasing trend of polarization fraction with\nincreasing beam size. Finally, we performed 3D magnetic field simulations using\nmultiple models for the magnetic field power spectrum over a wide range of\nscales, in order to constrain the characteristics of the cluster magnetic field\nthat can reproduce the observed beam depolarization trend. Using RM synthesis,\nwe obtained a polarization fraction of ($18.6 \\pm 0.3$)% for the norther relic\nand ($14.6 \\pm 0.1$)% for the southern one. Having corrected for bandwidth\ndepolarization, we infer that the nature of the depolarization for the southern\nrelic is external, and possibly related to the turbulent gas distribution\nwithin the cluster, or to the complex spatial structure of the relic. The\nbest-fit magnetic field power spectrum, that reproduces the observed\ndepolarization trend for the southern relic, is obtained for a turbulent\nmagnetic field model, described by a power spectrum derived from cosmological\nsimulations, and defined within the scales of $\\Lambda_{\\rm{min}}=35~\\rm{kpc}$\nand $\\Lambda_{\\rm{max}}=400~\\rm{kpc}$. This yields an average magnetic field of\nthe cluster within 1$~\\rm{Mpc}^3$ volume of $\\sim 2~\\rm{\\mu G}$."
                },
                "authors": [
                    {
                        "name": "E. De Rubeis"
                    },
                    {
                        "name": "C. Stuardi"
                    },
                    {
                        "name": "A. Bonafede"
                    },
                    {
                        "name": "F. Vazza"
                    },
                    {
                        "name": "R. J. van Weeren"
                    },
                    {
                        "name": "F. de Gasperin"
                    },
                    {
                        "name": "M. Brüggen"
                    }
                ],
                "author_detail": {
                    "name": "M. Brüggen"
                },
                "author": "M. Brüggen",
                "arxiv_comment": "17 pages, 10 figures, accepted for publication in Astronomy &\n  Astrophysics on 13th August 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.10759v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.10759v5",
                "updated": "2024-08-16T08:24:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    24,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2023-06-19T08:03:25Z",
                "published_parsed": [
                    2023,
                    6,
                    19,
                    8,
                    3,
                    25,
                    0,
                    170,
                    0
                ],
                "title": "SGFormer: Simplifying and Empowering Transformers for Large-Graph\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGFormer: Simplifying and Empowering Transformers for Large-Graph\n  Representations"
                },
                "summary": "Learning representations on large-sized graphs is a long-standing challenge\ndue to the inter-dependence nature involved in massive data points.\nTransformers, as an emerging class of foundation encoders for graph-structured\ndata, have shown promising performance on small graphs due to its global\nattention capable of capturing all-pair influence beyond neighboring nodes.\nEven so, existing approaches tend to inherit the spirit of Transformers in\nlanguage and vision tasks, and embrace complicated models by stacking deep\nmulti-head attentions. In this paper, we critically demonstrate that even using\na one-layer attention can bring up surprisingly competitive performance across\nnode property prediction benchmarks where node numbers range from\nthousand-level to billion-level. This encourages us to rethink the design\nphilosophy for Transformers on large graphs, where the global attention is a\ncomputation overhead hindering the scalability. We frame the proposed scheme as\nSimplified Graph Transformers (SGFormer), which is empowered by a simple\nattention model that can efficiently propagate information among arbitrary\nnodes in one layer. SGFormer requires none of positional encodings,\nfeature/graph pre-processing or augmented loss. Empirically, SGFormer\nsuccessfully scales to the web-scale graph ogbn-papers100M and yields up to\n141x inference acceleration over SOTA Transformers on medium-sized graphs.\nBeyond current results, we believe the proposed methodology alone enlightens a\nnew technical path of independent interest for building Transformers on large\ngraphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning representations on large-sized graphs is a long-standing challenge\ndue to the inter-dependence nature involved in massive data points.\nTransformers, as an emerging class of foundation encoders for graph-structured\ndata, have shown promising performance on small graphs due to its global\nattention capable of capturing all-pair influence beyond neighboring nodes.\nEven so, existing approaches tend to inherit the spirit of Transformers in\nlanguage and vision tasks, and embrace complicated models by stacking deep\nmulti-head attentions. In this paper, we critically demonstrate that even using\na one-layer attention can bring up surprisingly competitive performance across\nnode property prediction benchmarks where node numbers range from\nthousand-level to billion-level. This encourages us to rethink the design\nphilosophy for Transformers on large graphs, where the global attention is a\ncomputation overhead hindering the scalability. We frame the proposed scheme as\nSimplified Graph Transformers (SGFormer), which is empowered by a simple\nattention model that can efficiently propagate information among arbitrary\nnodes in one layer. SGFormer requires none of positional encodings,\nfeature/graph pre-processing or augmented loss. Empirically, SGFormer\nsuccessfully scales to the web-scale graph ogbn-papers100M and yields up to\n141x inference acceleration over SOTA Transformers on medium-sized graphs.\nBeyond current results, we believe the proposed methodology alone enlightens a\nnew technical path of independent interest for building Transformers on large\ngraphs."
                },
                "authors": [
                    {
                        "name": "Qitian Wu"
                    },
                    {
                        "name": "Wentao Zhao"
                    },
                    {
                        "name": "Chenxiao Yang"
                    },
                    {
                        "name": "Hengrui Zhang"
                    },
                    {
                        "name": "Fan Nie"
                    },
                    {
                        "name": "Haitian Jiang"
                    },
                    {
                        "name": "Yatao Bian"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "arxiv_comment": "Accepted to NeurIPS 2023, the codes are available at\n  https://github.com/qitianwu/SGFormer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.10759v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.10759v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.11561v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.11561v3",
                "updated": "2024-08-16T08:22:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    22,
                    53,
                    4,
                    229,
                    0
                ],
                "published": "2023-05-19T09:58:10Z",
                "published_parsed": [
                    2023,
                    5,
                    19,
                    9,
                    58,
                    10,
                    4,
                    139,
                    0
                ],
                "title": "Causal Inference on Process Graphs, Part I: The Structural Equation\n  Process Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Inference on Process Graphs, Part I: The Structural Equation\n  Process Representation"
                },
                "summary": "When dealing with time series data, causal inference methods often employ\nstructural vector autoregressive (SVAR) processes to model time-evolving random\nsystems. In this work, we rephrase recursive SVAR processes with possible\nlatent component processes as a linear Structural Causal Model (SCM) of\nstochastic processes on a simple causal graph, the process graph, that models\nevery process as a single node. Using this reformulation, we generalise\nWright's well-known path-rule for linear Gaussian SCMs to the newly introduced\nprocess SCMs and we express the auto-covariance sequence of an SVAR process by\nmeans of a generalised trek-rule. Employing the Fourier-Transformation, we\nderive compact expressions for causal effects in the frequency domain that\nallow us to efficiently visualise the causal interactions in a multivariate\nSVAR process. Finally, we observe that the process graph can be used to\nformulate graphical criteria for identifying causal effects and to derive\nalgebraic relations with which these frequency domain causal effects can be\nrecovered from the observed spectral density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When dealing with time series data, causal inference methods often employ\nstructural vector autoregressive (SVAR) processes to model time-evolving random\nsystems. In this work, we rephrase recursive SVAR processes with possible\nlatent component processes as a linear Structural Causal Model (SCM) of\nstochastic processes on a simple causal graph, the process graph, that models\nevery process as a single node. Using this reformulation, we generalise\nWright's well-known path-rule for linear Gaussian SCMs to the newly introduced\nprocess SCMs and we express the auto-covariance sequence of an SVAR process by\nmeans of a generalised trek-rule. Employing the Fourier-Transformation, we\nderive compact expressions for causal effects in the frequency domain that\nallow us to efficiently visualise the causal interactions in a multivariate\nSVAR process. Finally, we observe that the process graph can be used to\nformulate graphical criteria for identifying causal effects and to derive\nalgebraic relations with which these frequency domain causal effects can be\nrecovered from the observed spectral density."
                },
                "authors": [
                    {
                        "name": "Nicolas-Domenic Reiter"
                    },
                    {
                        "name": "Andreas Gerhardus"
                    },
                    {
                        "name": "Jonas Wahl"
                    },
                    {
                        "name": "Jakob Runge"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Runge"
                },
                "author": "Jakob Runge",
                "arxiv_comment": "48 pages. Title changed compared to initial submission. Former title:\n  'Formalising causal inference in time and frequency on process graphs with\n  latent components'",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.11561v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.11561v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11494v2",
                "updated": "2024-08-16T08:22:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    22,
                    17,
                    4,
                    229,
                    0
                ],
                "published": "2024-02-18T07:49:22Z",
                "published_parsed": [
                    2024,
                    2,
                    18,
                    7,
                    49,
                    22,
                    6,
                    49,
                    0
                ],
                "title": "Graph Out-of-Distribution Generalization via Causal Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Out-of-Distribution Generalization via Causal Intervention"
                },
                "summary": "Out-of-distribution (OOD) generalization has gained increasing attentions for\nlearning on graphs, as graph neural networks (GNNs) often exhibit performance\ndegradation with distribution shifts. The challenge is that distribution shifts\non graphs involve intricate interconnections between nodes, and the environment\nlabels are often absent in data. In this paper, we adopt a bottom-up\ndata-generative perspective and reveal a key observation through causal\nanalysis: the crux of GNNs' failure in OOD generalization lies in the latent\nconfounding bias from the environment. The latter misguides the model to\nleverage environment-sensitive correlations between ego-graph features and\ntarget nodes' labels, resulting in undesirable generalization on new unseen\nnodes. Built upon this analysis, we introduce a conceptually simple yet\nprincipled approach for training robust GNNs under node-level distribution\nshifts, without prior knowledge of environment labels. Our method resorts to a\nnew learning objective derived from causal inference that coordinates an\nenvironment estimator and a mixture-of-expert GNN predictor. The new approach\ncan counteract the confounding bias in training data and facilitate learning\ngeneralizable predictive relations. Extensive experiment demonstrates that our\nmodel can effectively enhance generalization with various types of distribution\nshifts and yield up to 27.4\\% accuracy improvement over state-of-the-arts on\ngraph OOD generalization benchmarks. Source codes are available at\nhttps://github.com/fannie1208/CaNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) generalization has gained increasing attentions for\nlearning on graphs, as graph neural networks (GNNs) often exhibit performance\ndegradation with distribution shifts. The challenge is that distribution shifts\non graphs involve intricate interconnections between nodes, and the environment\nlabels are often absent in data. In this paper, we adopt a bottom-up\ndata-generative perspective and reveal a key observation through causal\nanalysis: the crux of GNNs' failure in OOD generalization lies in the latent\nconfounding bias from the environment. The latter misguides the model to\nleverage environment-sensitive correlations between ego-graph features and\ntarget nodes' labels, resulting in undesirable generalization on new unseen\nnodes. Built upon this analysis, we introduce a conceptually simple yet\nprincipled approach for training robust GNNs under node-level distribution\nshifts, without prior knowledge of environment labels. Our method resorts to a\nnew learning objective derived from causal inference that coordinates an\nenvironment estimator and a mixture-of-expert GNN predictor. The new approach\ncan counteract the confounding bias in training data and facilitate learning\ngeneralizable predictive relations. Extensive experiment demonstrates that our\nmodel can effectively enhance generalization with various types of distribution\nshifts and yield up to 27.4\\% accuracy improvement over state-of-the-arts on\ngraph OOD generalization benchmarks. Source codes are available at\nhttps://github.com/fannie1208/CaNet."
                },
                "authors": [
                    {
                        "name": "Qitian Wu"
                    },
                    {
                        "name": "Fan Nie"
                    },
                    {
                        "name": "Chenxiao Yang"
                    },
                    {
                        "name": "Tianyi Bao"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "arxiv_comment": "Accepted by the research paper track of The Web Conference (WWW)\n  2024. The codes are available at https://github.com/fannie1208/CaNet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17422v2",
                "updated": "2024-08-16T08:15:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    15,
                    36,
                    4,
                    229,
                    0
                ],
                "published": "2024-06-25T09:54:16Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    9,
                    54,
                    16,
                    1,
                    177,
                    0
                ],
                "title": "Causal Inference on Process Graphs, Part II: Causal Structure and Effect\n  Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Inference on Process Graphs, Part II: Causal Structure and Effect\n  Identification"
                },
                "summary": "A structural vector autoregressive (SVAR) process is a linear causal model\nfor variables that evolve over a discrete set of time points and between which\nthere may be lagged and instantaneous effects. The qualitative causal structure\nof an SVAR process can be represented by its finite and directed process graph,\nin which a directed link connects two processes whenever there is a lagged or\ninstantaneous effect between them. At the process graph level, the causal\nstructure of SVAR processes is compactly parameterised in the frequency domain.\nIn this paper, we consider the problem of causal discovery and causal effect\nestimation from the spectral density, the frequency domain analogue of the auto\ncovariance, of the SVAR process. Causal discovery concerns the recovery of the\nprocess graph and causal effect estimation concerns the identification and\nestimation of causal effects in the frequency domain.\n  We show that information about the process graph, in terms of $d$- and\n$t$-separation statements, can be identified by verifying algebraic constraints\non the spectral density. Furthermore, we introduce a notion of rational\nidentifiability for frequency causal effects that may be confounded by\nexogenous latent processes, and show that the recent graphical latent factor\nhalf-trek criterion can be used on the process graph to assess whether a given\n(confounded) effect can be identified by rational operations on the entries of\nthe spectral density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A structural vector autoregressive (SVAR) process is a linear causal model\nfor variables that evolve over a discrete set of time points and between which\nthere may be lagged and instantaneous effects. The qualitative causal structure\nof an SVAR process can be represented by its finite and directed process graph,\nin which a directed link connects two processes whenever there is a lagged or\ninstantaneous effect between them. At the process graph level, the causal\nstructure of SVAR processes is compactly parameterised in the frequency domain.\nIn this paper, we consider the problem of causal discovery and causal effect\nestimation from the spectral density, the frequency domain analogue of the auto\ncovariance, of the SVAR process. Causal discovery concerns the recovery of the\nprocess graph and causal effect estimation concerns the identification and\nestimation of causal effects in the frequency domain.\n  We show that information about the process graph, in terms of $d$- and\n$t$-separation statements, can be identified by verifying algebraic constraints\non the spectral density. Furthermore, we introduce a notion of rational\nidentifiability for frequency causal effects that may be confounded by\nexogenous latent processes, and show that the recent graphical latent factor\nhalf-trek criterion can be used on the process graph to assess whether a given\n(confounded) effect can be identified by rational operations on the entries of\nthe spectral density."
                },
                "authors": [
                    {
                        "name": "Nicolas-Domenic Reiter"
                    },
                    {
                        "name": "Jonas Wahl"
                    },
                    {
                        "name": "Andreas Gerhardus"
                    },
                    {
                        "name": "Jakob Runge"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Runge"
                },
                "author": "Jakob Runge",
                "arxiv_comment": "33 pages. Part I of the paper series is available at\n  arXiv:2305.11561. Proposition 4 and the proofs of Proposition 4 and Theorem 5\n  have been changed in comparison to the previous version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08243v2",
                "updated": "2024-08-16T07:57:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    7,
                    57,
                    31,
                    4,
                    229,
                    0
                ],
                "published": "2024-04-12T05:12:22Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    5,
                    12,
                    22,
                    4,
                    103,
                    0
                ],
                "title": "Thermodynamic topology of Phantom AdS Black Holes in Massive Gravity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thermodynamic topology of Phantom AdS Black Holes in Massive Gravity"
                },
                "summary": "In this work, we explore the thermodynamic topology of phantom AdS black\nholes in the context of massive gravity. To this end, we evaluate these black\nholes in two distinct ensembles: the canonical and grand canonical ensembles\n(GCE). We begin by examining the topological charge linked to the critical\npoint and confirming the existence of a conventional critical point $(CP_{1})$\nin the canonical ensemble (CE), this critical point has a topological charge of\n$-1$ and acts as a point of phase annihilation, this situation can only be\nconsidered within the context of the classical Einstein-Maxwell (CEM) theory\n$(\\eta=1)$, while no critical point is identified in the GCE. Furthermore, we\nconsider black holes as a topological defect within the thermodynamic space. To\ngain an understanding of the local and global topological configuration of this\ndefect, we will analyze its winding numbers, and observe that the total\ntopological charge in the CE consistently remains at $1$. When the system\nexperiences a pressure below the critical threshold, it gives rise to the\noccurrence of annihilation and generation points. The value of electric\npotential determines whether the total topological charge in the GCE is zero or\none. As a result, we detect a point of generation point or absence of\ngeneration/annihilation point. Based on our analysis, it can be inferred that\nensembles significantly impact the topological class of phantom AdS black holes\nin massive gravity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we explore the thermodynamic topology of phantom AdS black\nholes in the context of massive gravity. To this end, we evaluate these black\nholes in two distinct ensembles: the canonical and grand canonical ensembles\n(GCE). We begin by examining the topological charge linked to the critical\npoint and confirming the existence of a conventional critical point $(CP_{1})$\nin the canonical ensemble (CE), this critical point has a topological charge of\n$-1$ and acts as a point of phase annihilation, this situation can only be\nconsidered within the context of the classical Einstein-Maxwell (CEM) theory\n$(\\eta=1)$, while no critical point is identified in the GCE. Furthermore, we\nconsider black holes as a topological defect within the thermodynamic space. To\ngain an understanding of the local and global topological configuration of this\ndefect, we will analyze its winding numbers, and observe that the total\ntopological charge in the CE consistently remains at $1$. When the system\nexperiences a pressure below the critical threshold, it gives rise to the\noccurrence of annihilation and generation points. The value of electric\npotential determines whether the total topological charge in the GCE is zero or\none. As a result, we detect a point of generation point or absence of\ngeneration/annihilation point. Based on our analysis, it can be inferred that\nensembles significantly impact the topological class of phantom AdS black holes\nin massive gravity."
                },
                "authors": [
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Meng-Yao Zhang"
                    },
                    {
                        "name": "Hassan Hassanabadi"
                    },
                    {
                        "name": "Zheng-Wen Long"
                    }
                ],
                "author_detail": {
                    "name": "Zheng-Wen Long"
                },
                "author": "Zheng-Wen Long",
                "arxiv_doi": "10.1016/j.dark.2024.101617",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.dark.2024.101617",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.08243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12pages, 19 figures",
                "arxiv_journal_ref": "Phys. Dark Univ. 46 (2024) 101617",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14227v2",
                "updated": "2024-08-16T07:51:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    7,
                    51,
                    6,
                    4,
                    229,
                    0
                ],
                "published": "2024-04-22T14:37:33Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    14,
                    37,
                    33,
                    0,
                    113,
                    0
                ],
                "title": "Finite sample expansions and risk bounds in high-dimensional SLS models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite sample expansions and risk bounds in high-dimensional SLS models"
                },
                "summary": "This note extends the results of classical parametric statistics like Fisher\nand Wilks theorem to modern setups with a high or infinite parameter dimension,\nlimited sample size, and possible model misspecification. We consider a special\nclass of stochastically linear smooth (SLS) models satisfying three major\nconditions: the stochastic component of the log-likelihood is linear in the\nmodel parameter and the expected log-likelihood is a smooth and concave\nfunction. For the penalized maximum likelihood estimators (pMLE), we establish\nthree types of results: (1) concentration in a small vicinity of the ``truth'';\n(2) Fisher and Wilks expansions; (3) risk bounds. In all results, the remainder\nis given explicitly and can be evaluated in terms of the effective sample size\nand effective parameter dimension which allows us to identify the so-called\n\\emph{critical parameter dimension}. The results are also dimension and\ncoordinate-free. The obtained finite sample expansions are of special interest\nbecause they can be used not only for obtaining the risk bounds but also for\ninference, studying the asymptotic distribution, analysis of resampling\nprocedures, etc. The main tool for all these expansions is the so-called\n``basic lemma'' about linearly perturbed optimization. Despite their\ngenerality, all the presented bounds are nearly sharp and the classical\nasymptotic results can be obtained as simple corollaries. Our results indicate\nthat the use of advanced fourth-order expansions allows to relax the critical\ndimension condition $ \\mathbb{p}^{3} \\ll n $ from Spokoiny (2023a) to $\n\\mathbb{p}^{3/2} \\ll n $. Examples for classical models like logistic\nregression, log-density and precision matrix estimation illustrate the\napplicability of general results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This note extends the results of classical parametric statistics like Fisher\nand Wilks theorem to modern setups with a high or infinite parameter dimension,\nlimited sample size, and possible model misspecification. We consider a special\nclass of stochastically linear smooth (SLS) models satisfying three major\nconditions: the stochastic component of the log-likelihood is linear in the\nmodel parameter and the expected log-likelihood is a smooth and concave\nfunction. For the penalized maximum likelihood estimators (pMLE), we establish\nthree types of results: (1) concentration in a small vicinity of the ``truth'';\n(2) Fisher and Wilks expansions; (3) risk bounds. In all results, the remainder\nis given explicitly and can be evaluated in terms of the effective sample size\nand effective parameter dimension which allows us to identify the so-called\n\\emph{critical parameter dimension}. The results are also dimension and\ncoordinate-free. The obtained finite sample expansions are of special interest\nbecause they can be used not only for obtaining the risk bounds but also for\ninference, studying the asymptotic distribution, analysis of resampling\nprocedures, etc. The main tool for all these expansions is the so-called\n``basic lemma'' about linearly perturbed optimization. Despite their\ngenerality, all the presented bounds are nearly sharp and the classical\nasymptotic results can be obtained as simple corollaries. Our results indicate\nthat the use of advanced fourth-order expansions allows to relax the critical\ndimension condition $ \\mathbb{p}^{3} \\ll n $ from Spokoiny (2023a) to $\n\\mathbb{p}^{3/2} \\ll n $. Examples for classical models like logistic\nregression, log-density and precision matrix estimation illustrate the\napplicability of general results."
                },
                "authors": [
                    {
                        "name": "Vladimir Spokoiny"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Spokoiny"
                },
                "author": "Vladimir Spokoiny",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F10, 62E17 (Primary) 62J12 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06566v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06566v4",
                "updated": "2024-08-16T07:43:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    7,
                    43,
                    55,
                    4,
                    229,
                    0
                ],
                "published": "2024-06-03T07:44:32Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    7,
                    44,
                    32,
                    0,
                    155,
                    0
                ],
                "title": "Natural Language Interaction with a Household Electricity\n  Knowledge-based Digital Twin",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Interaction with a Household Electricity\n  Knowledge-based Digital Twin"
                },
                "summary": "Domain specific digital twins, representing a digital replica of various\nsegments of the smart grid, are foreseen as able to model, simulate, and\ncontrol the respective segments. At the same time, knowledge-based digital\ntwins, coupled with AI, may also empower humans to understand aspects of the\nsystem through natural language interaction in view of planning and policy\nmaking. This paper is the first to assess and report on the potential of\nRetrieval Augmented Generation (RAG) question answers related to household\nelectrical energy measurement aspects leveraging a knowledge-based energy\ndigital twin. Relying on the recently published electricity consumption\nknowledge graph that actually represents a knowledge-based digital twin, we\nstudy the capabilities of ChatGPT, Gemini and Llama in answering electricity\nrelated questions. Furthermore, we compare the answers with the ones generated\nthrough a RAG techniques that leverages an existing electricity knowledge-based\ndigital twin. Our findings illustrate that the RAG approach not only reduces\nthe incidence of incorrect information typically generated by LLMs but also\nsignificantly improves the quality of the output by grounding responses in\nverifiable data. This paper details our methodology, presents a comparative\nanalysis of responses with and without RAG, and discusses the implications of\nour findings for future applications of AI in specialized sectors like energy\ndata analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain specific digital twins, representing a digital replica of various\nsegments of the smart grid, are foreseen as able to model, simulate, and\ncontrol the respective segments. At the same time, knowledge-based digital\ntwins, coupled with AI, may also empower humans to understand aspects of the\nsystem through natural language interaction in view of planning and policy\nmaking. This paper is the first to assess and report on the potential of\nRetrieval Augmented Generation (RAG) question answers related to household\nelectrical energy measurement aspects leveraging a knowledge-based energy\ndigital twin. Relying on the recently published electricity consumption\nknowledge graph that actually represents a knowledge-based digital twin, we\nstudy the capabilities of ChatGPT, Gemini and Llama in answering electricity\nrelated questions. Furthermore, we compare the answers with the ones generated\nthrough a RAG techniques that leverages an existing electricity knowledge-based\ndigital twin. Our findings illustrate that the RAG approach not only reduces\nthe incidence of incorrect information typically generated by LLMs but also\nsignificantly improves the quality of the output by grounding responses in\nverifiable data. This paper details our methodology, presents a comparative\nanalysis of responses with and without RAG, and discusses the implications of\nour findings for future applications of AI in specialized sectors like energy\ndata analysis."
                },
                "authors": [
                    {
                        "name": "Carolina Fortuna"
                    },
                    {
                        "name": "Vid Hanžel"
                    },
                    {
                        "name": "Blaž Bertalanič"
                    }
                ],
                "author_detail": {
                    "name": "Blaž Bertalanič"
                },
                "author": "Blaž Bertalanič",
                "arxiv_comment": "Accepted at IEEE SmartGridComm'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06566v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06566v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14573v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14573v3",
                "updated": "2024-08-16T07:30:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    7,
                    30,
                    29,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-21T06:27:45Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    6,
                    27,
                    45,
                    6,
                    203,
                    0
                ],
                "title": "Trading Devil Final: Backdoor attack via Stock market and Bayesian\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trading Devil Final: Backdoor attack via Stock market and Bayesian\n  Optimization"
                },
                "summary": "Since the advent of generative artificial intelligence, every company and\nresearcher has been rushing to develop their own generative models, whether\ncommercial or not. Given the large number of users of these powerful new tools,\nthere is currently no intrinsically verifiable way to explain from the ground\nup what happens when LLMs (large language models) learn. For example, those\nbased on automatic speech recognition systems, which have to rely on huge and\nastronomical amounts of data collected from all over the web to produce fast\nand efficient results, In this article, we develop a backdoor attack called\nMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is\nmainly based on modern stock market models. In order to show the possible\nvulnerabilities of speech-based transformers that may rely on LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the advent of generative artificial intelligence, every company and\nresearcher has been rushing to develop their own generative models, whether\ncommercial or not. Given the large number of users of these powerful new tools,\nthere is currently no intrinsically verifiable way to explain from the ground\nup what happens when LLMs (large language models) learn. For example, those\nbased on automatic speech recognition systems, which have to rely on huge and\nastronomical amounts of data collected from all over the web to produce fast\nand efficient results, In this article, we develop a backdoor attack called\nMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is\nmainly based on modern stock market models. In order to show the possible\nvulnerabilities of speech-based transformers that may rely on LLMs."
                },
                "authors": [
                    {
                        "name": "Orson Mengara"
                    }
                ],
                "author_detail": {
                    "name": "Orson Mengara"
                },
                "author": "Orson Mengara",
                "arxiv_comment": "END (will never be modified again) :Jumps-Diffusion and stock market:\n  Better quantify uncertainty in financial simulations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14573v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14573v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08578v1",
                "updated": "2024-08-16T07:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    7,
                    24,
                    19,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T07:24:19Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    7,
                    24,
                    19,
                    4,
                    229,
                    0
                ],
                "title": "TAMER: Tree-Aware Transformer for Handwritten Mathematical Expression\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAMER: Tree-Aware Transformer for Handwritten Mathematical Expression\n  Recognition"
                },
                "summary": "Handwritten Mathematical Expression Recognition (HMER) has extensive\napplications in automated grading and office automation. However, existing\nsequence-based decoding methods, which directly predict $\\LaTeX$ sequences,\nstruggle to understand and model the inherent tree structure of $\\LaTeX$ and\noften fail to ensure syntactic correctness in the decoded results. To address\nthese challenges, we propose a novel model named TAMER (Tree-Aware Transformer)\nfor handwritten mathematical expression recognition. TAMER introduces an\ninnovative Tree-aware Module while maintaining the flexibility and efficient\ntraining of Transformer. TAMER combines the advantages of both sequence\ndecoding and tree decoding models by jointly optimizing sequence prediction and\ntree structure prediction tasks, which enhances the model's understanding and\ngeneralization of complex mathematical expression structures. During inference,\nTAMER employs a Tree Structure Prediction Scoring Mechanism to improve the\nstructural validity of the generated $\\LaTeX$ sequences. Experimental results\non CROHME datasets demonstrate that TAMER outperforms traditional sequence\ndecoding and tree decoding models, especially in handling complex mathematical\nstructures, achieving state-of-the-art (SOTA) performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handwritten Mathematical Expression Recognition (HMER) has extensive\napplications in automated grading and office automation. However, existing\nsequence-based decoding methods, which directly predict $\\LaTeX$ sequences,\nstruggle to understand and model the inherent tree structure of $\\LaTeX$ and\noften fail to ensure syntactic correctness in the decoded results. To address\nthese challenges, we propose a novel model named TAMER (Tree-Aware Transformer)\nfor handwritten mathematical expression recognition. TAMER introduces an\ninnovative Tree-aware Module while maintaining the flexibility and efficient\ntraining of Transformer. TAMER combines the advantages of both sequence\ndecoding and tree decoding models by jointly optimizing sequence prediction and\ntree structure prediction tasks, which enhances the model's understanding and\ngeneralization of complex mathematical expression structures. During inference,\nTAMER employs a Tree Structure Prediction Scoring Mechanism to improve the\nstructural validity of the generated $\\LaTeX$ sequences. Experimental results\non CROHME datasets demonstrate that TAMER outperforms traditional sequence\ndecoding and tree decoding models, especially in handling complex mathematical\nstructures, achieving state-of-the-art (SOTA) performance."
                },
                "authors": [
                    {
                        "name": "Jianhua Zhu"
                    },
                    {
                        "name": "Wenqi Zhao"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Xingjian Hu"
                    },
                    {
                        "name": "Liangcai Gao"
                    }
                ],
                "author_detail": {
                    "name": "Liangcai Gao"
                },
                "author": "Liangcai Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08566v1",
                "updated": "2024-08-16T07:00:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    7,
                    0,
                    8,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T07:00:08Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    7,
                    0,
                    8,
                    4,
                    229,
                    0
                ],
                "title": "Overview of the BioLaySumm 2024 Shared Task on the Lay Summarization of\n  Biomedical Research Articles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overview of the BioLaySumm 2024 Shared Task on the Lay Summarization of\n  Biomedical Research Articles"
                },
                "summary": "This paper presents the setup and results of the second edition of the\nBioLaySumm shared task on the Lay Summarisation of Biomedical Research\nArticles, hosted at the BioNLP Workshop at ACL 2024. In this task edition, we\naim to build on the first edition's success by further increasing research\ninterest in this important task and encouraging participants to explore novel\napproaches that will help advance the state-of-the-art. Encouragingly, we found\nresearch interest in the task to be high, with this edition of the task\nattracting a total of 53 participating teams, a significant increase in\nengagement from the previous edition. Overall, our results show that a broad\nrange of innovative approaches were adopted by task participants, with a\npredictable shift towards the use of Large Language Models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the setup and results of the second edition of the\nBioLaySumm shared task on the Lay Summarisation of Biomedical Research\nArticles, hosted at the BioNLP Workshop at ACL 2024. In this task edition, we\naim to build on the first edition's success by further increasing research\ninterest in this important task and encouraging participants to explore novel\napproaches that will help advance the state-of-the-art. Encouragingly, we found\nresearch interest in the task to be high, with this edition of the task\nattracting a total of 53 participating teams, a significant increase in\nengagement from the previous edition. Overall, our results show that a broad\nrange of innovative approaches were adopted by task participants, with a\npredictable shift towards the use of Large Language Models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Tomas Goldsack"
                    },
                    {
                        "name": "Carolina Scarton"
                    },
                    {
                        "name": "Matthew Shardlow"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Published in: Proceedings of the 23rd Workshop on Biomedical Natural\n  Language Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08564v1",
                "updated": "2024-08-16T06:54:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    54,
                    10,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:54:10Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    54,
                    10,
                    4,
                    229,
                    0
                ],
                "title": "Collaborative Cross-modal Fusion with Large Language Model for\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Cross-modal Fusion with Large Language Model for\n  Recommendation"
                },
                "summary": "Despite the success of conventional collaborative filtering (CF) approaches\nfor recommendation systems, they exhibit limitations in leveraging semantic\nknowledge within the textual attributes of users and items. Recent focus on the\napplication of large language models for recommendation (LLM4Rec) has\nhighlighted their capability for effective semantic knowledge capture. However,\nthese methods often overlook the collaborative signals in user behaviors. Some\nsimply instruct-tune a language model, while others directly inject the\nembeddings of a CF-based model, lacking a synergistic fusion of different\nmodalities. To address these issues, we propose a framework of Collaborative\nCross-modal Fusion with Large Language Models, termed CCF-LLM, for\nrecommendation. In this framework, we translate the user-item interactions into\na hybrid prompt to encode both semantic knowledge and collaborative signals,\nand then employ an attentive cross-modal fusion strategy to effectively fuse\nlatent embeddings of both modalities. Extensive experiments demonstrate that\nCCF-LLM outperforms existing methods by effectively utilizing semantic and\ncollaborative signals in the LLM4Rec context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of conventional collaborative filtering (CF) approaches\nfor recommendation systems, they exhibit limitations in leveraging semantic\nknowledge within the textual attributes of users and items. Recent focus on the\napplication of large language models for recommendation (LLM4Rec) has\nhighlighted their capability for effective semantic knowledge capture. However,\nthese methods often overlook the collaborative signals in user behaviors. Some\nsimply instruct-tune a language model, while others directly inject the\nembeddings of a CF-based model, lacking a synergistic fusion of different\nmodalities. To address these issues, we propose a framework of Collaborative\nCross-modal Fusion with Large Language Models, termed CCF-LLM, for\nrecommendation. In this framework, we translate the user-item interactions into\na hybrid prompt to encode both semantic knowledge and collaborative signals,\nand then employ an attentive cross-modal fusion strategy to effectively fuse\nlatent embeddings of both modalities. Extensive experiments demonstrate that\nCCF-LLM outperforms existing methods by effectively utilizing semantic and\ncollaborative signals in the LLM4Rec context."
                },
                "authors": [
                    {
                        "name": "Zhongzhou Liu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Kuicai Dong"
                    },
                    {
                        "name": "Yuan Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Fang"
                },
                "author": "Yuan Fang",
                "arxiv_doi": "10.1145/3627673.3679596",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3627673.3679596",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.08564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages, 4 figures, accepted by CIKM 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.09314v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.09314v4",
                "updated": "2024-08-16T06:44:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    44,
                    12,
                    4,
                    229,
                    0
                ],
                "published": "2024-05-15T13:14:05Z",
                "published_parsed": [
                    2024,
                    5,
                    15,
                    13,
                    14,
                    5,
                    2,
                    136,
                    0
                ],
                "title": "Themis: Automatic and Efficient Deep Learning System Testing with Strong\n  Fault Detection Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Themis: Automatic and Efficient Deep Learning System Testing with Strong\n  Fault Detection Capability"
                },
                "summary": "Deep Learning Systems (DLSs) have been widely applied in safety-critical\ntasks such as autopilot. However, when a perturbed input is fed into a DLS for\ninference, the DLS often has incorrect outputs (i.e., faults). DLS testing\ntechniques (e.g., DeepXplore) detect such faults by generating perturbed inputs\nto explore data flows that induce faults. Since a DLS often has infinitely many\ndata flows, existing techniques require developers to manually specify a set of\nactivation values in a DLS's neurons for exploring fault-inducing data flows.\nUnfortunately, recent studies show that such manual effort is tedious and can\ndetect only a tiny proportion of fault-inducing data flows.\n  In this paper, we present Themis, the first automatic DLS testing system,\nwhich attains strong fault detection capability by ensuring a full coverage of\nfault-inducing data flows at a high probability. Themis carries a new workflow\nfor automatically and systematically revealing data flows whose internal\nneurons' outputs vary substantially when the inputs are slightly perturbed, as\nthese data flows are likely fault-inducing. We evaluated Themis on ten\ndifferent DLSs and found that on average the number of faults detected by\nThemis was 3.78X more than four notable DLS testing techniques. By retraining\nall evaluated DLSs with the detected faults, Themis also increased (regained)\nthese DLSs' accuracies on average 14.7X higher than all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Systems (DLSs) have been widely applied in safety-critical\ntasks such as autopilot. However, when a perturbed input is fed into a DLS for\ninference, the DLS often has incorrect outputs (i.e., faults). DLS testing\ntechniques (e.g., DeepXplore) detect such faults by generating perturbed inputs\nto explore data flows that induce faults. Since a DLS often has infinitely many\ndata flows, existing techniques require developers to manually specify a set of\nactivation values in a DLS's neurons for exploring fault-inducing data flows.\nUnfortunately, recent studies show that such manual effort is tedious and can\ndetect only a tiny proportion of fault-inducing data flows.\n  In this paper, we present Themis, the first automatic DLS testing system,\nwhich attains strong fault detection capability by ensuring a full coverage of\nfault-inducing data flows at a high probability. Themis carries a new workflow\nfor automatically and systematically revealing data flows whose internal\nneurons' outputs vary substantially when the inputs are slightly perturbed, as\nthese data flows are likely fault-inducing. We evaluated Themis on ten\ndifferent DLSs and found that on average the number of faults detected by\nThemis was 3.78X more than four notable DLS testing techniques. By retraining\nall evaluated DLSs with the detected faults, Themis also increased (regained)\nthese DLSs' accuracies on average 14.7X higher than all baselines."
                },
                "authors": [
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Tsz On Li"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Heming Cui"
                    }
                ],
                "author_detail": {
                    "name": "Heming Cui"
                },
                "author": "Heming Cui",
                "arxiv_comment": "Camera-ready version for ISSRE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.09314v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.09314v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08554v1",
                "updated": "2024-08-16T06:39:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    39,
                    8,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:39:08Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    39,
                    8,
                    4,
                    229,
                    0
                ],
                "title": "ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\ntasks. However, their practical application is constrained by substantial\nmemory and computational demands. Post-training quantization (PTQ) is\nconsidered an effective method to accelerate LLM inference. Despite its growing\npopularity in LLM model compression, PTQ deployment faces two major challenges.\nFirst, low-bit quantization leads to performance degradation. Second,\nrestricted by the limited integer computing unit type on GPUs, quantized matrix\noperations with different precisions cannot be effectively accelerated. To\naddress these issues, we introduce a novel arbitrary-bit quantization algorithm\nand inference framework, ABQ-LLM. It achieves superior performance across\nvarious quantization settings and enables efficient arbitrary-precision\nquantized inference on the GPU. ABQ-LLM introduces several key innovations: (1)\na distribution correction method for transformer blocks to mitigate\ndistribution differences caused by full quantization of weights and\nactivations, improving performance at low bit-widths. (2) the bit balance\nstrategy to counteract performance degradation from asymmetric distribution\nissues at very low bit-widths (e.g., 2-bit). (3) an innovative quantization\nacceleration framework that reconstructs the quantization matrix multiplication\nof arbitrary precision combinations based on BTC (Binary TensorCore)\nequivalents, gets rid of the limitations of INT4/INT8 computing units. ABQ-LLM\ncan convert each component bit width gain into actual acceleration gain,\nmaximizing performance under mixed precision(e.g., W6A6, W2A8). Based on W2*A8\nquantization configuration on LLaMA-7B model, it achieved a WikiText2\nperplexity of 7.59 (2.17$\\downarrow $ vs 9.76 in AffineQuant). Compared to\nSmoothQuant, we realized 1.6$\\times$ acceleration improvement and 2.7$\\times$\nmemory compression gain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\ntasks. However, their practical application is constrained by substantial\nmemory and computational demands. Post-training quantization (PTQ) is\nconsidered an effective method to accelerate LLM inference. Despite its growing\npopularity in LLM model compression, PTQ deployment faces two major challenges.\nFirst, low-bit quantization leads to performance degradation. Second,\nrestricted by the limited integer computing unit type on GPUs, quantized matrix\noperations with different precisions cannot be effectively accelerated. To\naddress these issues, we introduce a novel arbitrary-bit quantization algorithm\nand inference framework, ABQ-LLM. It achieves superior performance across\nvarious quantization settings and enables efficient arbitrary-precision\nquantized inference on the GPU. ABQ-LLM introduces several key innovations: (1)\na distribution correction method for transformer blocks to mitigate\ndistribution differences caused by full quantization of weights and\nactivations, improving performance at low bit-widths. (2) the bit balance\nstrategy to counteract performance degradation from asymmetric distribution\nissues at very low bit-widths (e.g., 2-bit). (3) an innovative quantization\nacceleration framework that reconstructs the quantization matrix multiplication\nof arbitrary precision combinations based on BTC (Binary TensorCore)\nequivalents, gets rid of the limitations of INT4/INT8 computing units. ABQ-LLM\ncan convert each component bit width gain into actual acceleration gain,\nmaximizing performance under mixed precision(e.g., W6A6, W2A8). Based on W2*A8\nquantization configuration on LLaMA-7B model, it achieved a WikiText2\nperplexity of 7.59 (2.17$\\downarrow $ vs 9.76 in AffineQuant). Compared to\nSmoothQuant, we realized 1.6$\\times$ acceleration improvement and 2.7$\\times$\nmemory compression gain."
                },
                "authors": [
                    {
                        "name": "Chao Zeng"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Yusheng Xie"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Xiaojian Wang"
                    },
                    {
                        "name": "Miao Wei"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Mei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Mei"
                },
                "author": "Xing Mei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08553v1",
                "updated": "2024-08-16T06:37:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    37,
                    59,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:37:59Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    37,
                    59,
                    4,
                    229,
                    0
                ],
                "title": "Enhancing Discriminative Tasks by Guiding the Pre-trained Language Model\n  with Large Language Model's Experience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Discriminative Tasks by Guiding the Pre-trained Language Model\n  with Large Language Model's Experience"
                },
                "summary": "Large Language Models (LLMs) and pre-trained Language Models (LMs) have\nachieved impressive success on many software engineering tasks (e.g., code\ncompletion and code generation). By leveraging huge existing code corpora\n(e.g., GitHub), these models aim to understand the patterns in source code and\nuse these patterns to predict code properties. However, fine-tuning LLMs is\ntime-consuming and costly for end users and small organizations. Furthermore,\nfine-tuning LMs heavily depends on the amount and quality of datasets\navailable. As a result, the current lack of data and the high cost of\ncollecting it in real-world scenarios further limit the applicability of LMs.\nIn this paper, we leverage the powerful generation capabilities of LLMs to\nenhance pre-trained LMs. Specifically, we use LLMs to generate domain-specific\ndata, thereby improving the performance of pre-trained LMs on the target tasks.\nWe conduct experiments by combining different LLMs in our generation phase and\nintroducing various LMs to learn from the LLM-generated data. Then, we compare\nthe performance of these LMs before and after learning the data. We find that\nLLM-generated data significantly enhances the performance of LMs. The\nimprovement can reach up to 58.36% for fault localization and up to 6.09% for\nclone detection. Our study highlights that using LLMs to generate data for LMs\ncan improve performance by a large margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and pre-trained Language Models (LMs) have\nachieved impressive success on many software engineering tasks (e.g., code\ncompletion and code generation). By leveraging huge existing code corpora\n(e.g., GitHub), these models aim to understand the patterns in source code and\nuse these patterns to predict code properties. However, fine-tuning LLMs is\ntime-consuming and costly for end users and small organizations. Furthermore,\nfine-tuning LMs heavily depends on the amount and quality of datasets\navailable. As a result, the current lack of data and the high cost of\ncollecting it in real-world scenarios further limit the applicability of LMs.\nIn this paper, we leverage the powerful generation capabilities of LLMs to\nenhance pre-trained LMs. Specifically, we use LLMs to generate domain-specific\ndata, thereby improving the performance of pre-trained LMs on the target tasks.\nWe conduct experiments by combining different LLMs in our generation phase and\nintroducing various LMs to learn from the LLM-generated data. Then, we compare\nthe performance of these LMs before and after learning the data. We find that\nLLM-generated data significantly enhances the performance of LMs. The\nimprovement can reach up to 58.36% for fault localization and up to 6.09% for\nclone detection. Our study highlights that using LLMs to generate data for LMs\ncan improve performance by a large margin."
                },
                "authors": [
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Chao Ni"
                    },
                    {
                        "name": "Xiaodan Xu"
                    },
                    {
                        "name": "Xinrui Li"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08549v1",
                "updated": "2024-08-16T06:31:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    31,
                    44,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:31:44Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    31,
                    44,
                    4,
                    229,
                    0
                ],
                "title": "Vulnerability Handling of AI-Generated Code -- Existing Solutions and\n  Open Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability Handling of AI-Generated Code -- Existing Solutions and\n  Open Challenges"
                },
                "summary": "The increasing use of generative Artificial Intelligence (AI) in modern\nsoftware engineering, particularly Large Language Models (LLMs) for code\ngeneration, has transformed professional software development by boosting\nproductivity and automating development processes. This adoption, however, has\nhighlighted a significant issue: the introduction of security vulnerabilities\ninto the code. These vulnerabilities result, e.g., from flaws in the training\ndata that propagate into the generated code, creating challenges in disclosing\nthem. Traditional vulnerability handling processes often involve extensive\nmanual review. Applying such traditional processes to AI-generated code is\nchallenging. AI-generated code may include several vulnerabilities, possibly in\nslightly different forms as developers might not build on already implemented\ncode but prompt similar tasks. In this work, we explore the current state of\nLLM-based approaches for vulnerability handling, focusing on approaches for\nvulnerability detection, localization, and repair. We provide an overview of\nrecent progress in this area and highlight open challenges that must be\naddressed in order to establish a reliable and scalable vulnerability handling\nprocess of AI-generated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of generative Artificial Intelligence (AI) in modern\nsoftware engineering, particularly Large Language Models (LLMs) for code\ngeneration, has transformed professional software development by boosting\nproductivity and automating development processes. This adoption, however, has\nhighlighted a significant issue: the introduction of security vulnerabilities\ninto the code. These vulnerabilities result, e.g., from flaws in the training\ndata that propagate into the generated code, creating challenges in disclosing\nthem. Traditional vulnerability handling processes often involve extensive\nmanual review. Applying such traditional processes to AI-generated code is\nchallenging. AI-generated code may include several vulnerabilities, possibly in\nslightly different forms as developers might not build on already implemented\ncode but prompt similar tasks. In this work, we explore the current state of\nLLM-based approaches for vulnerability handling, focusing on approaches for\nvulnerability detection, localization, and repair. We provide an overview of\nrecent progress in this area and highlight open challenges that must be\naddressed in order to establish a reliable and scalable vulnerability handling\nprocess of AI-generated code."
                },
                "authors": [
                    {
                        "name": "Sabrina Kaniewski"
                    },
                    {
                        "name": "Dieter Holstein"
                    },
                    {
                        "name": "Fabian Schmidt"
                    },
                    {
                        "name": "Tobias Heer"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Heer"
                },
                "author": "Tobias Heer",
                "arxiv_comment": "Accepted for publication @ IEEE AIxSET 2024; 4 pages, 2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05074v2",
                "updated": "2024-08-16T06:04:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    4,
                    31,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-09T14:02:24Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    2,
                    24,
                    4,
                    222,
                    0
                ],
                "title": "RT-Surv: Improving Mortality Prediction After Radiotherapy with Large\n  Language Model Structuring of Large-Scale Unstructured Electronic Health\n  Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Surv: Improving Mortality Prediction After Radiotherapy with Large\n  Language Model Structuring of Large-Scale Unstructured Electronic Health\n  Records"
                },
                "summary": "Accurate patient selection is critical in radiotherapy (RT) to prevent\nineffective treatments. Traditional survival prediction models, relying on\nstructured data, often lack precision. This study explores the potential of\nlarge language models (LLMs) to structure unstructured electronic health record\n(EHR) data, thereby improving survival prediction accuracy through\ncomprehensive clinical information integration. Data from 34,276 patients\ntreated with RT at Yonsei Cancer Center between 2013 and 2023 were analyzed,\nencompassing both structured and unstructured data. An open-source LLM was used\nto structure the unstructured EHR data via single-shot learning, with its\nperformance compared against a domain-specific medical LLM and a smaller\nvariant. Survival prediction models were developed using statistical, machine\nlearning, and deep learning approaches, incorporating both structured and\nLLM-structured data. Clinical experts evaluated the accuracy of the\nLLM-structured data. The open-source LLM achieved 87.5% accuracy in structuring\nunstructured EHR data without additional training, significantly outperforming\nthe domain-specific medical LLM, which reached only 35.8% accuracy. Larger LLMs\nwere more effective, particularly in extracting clinically relevant features\nlike general condition and disease extent, which closely correlated with\npatient survival. Incorporating LLM-structured clinical features into survival\nprediction models significantly improved accuracy, with the C-index of deep\nlearning models increasing from 0.737 to 0.820. These models also became more\ninterpretable by emphasizing clinically significant factors. This study shows\nthat general-domain LLMs, even without specific medical training, can\neffectively structure large-scale unstructured EHR data, substantially\nenhancing the accuracy and interpretability of clinical predictive models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate patient selection is critical in radiotherapy (RT) to prevent\nineffective treatments. Traditional survival prediction models, relying on\nstructured data, often lack precision. This study explores the potential of\nlarge language models (LLMs) to structure unstructured electronic health record\n(EHR) data, thereby improving survival prediction accuracy through\ncomprehensive clinical information integration. Data from 34,276 patients\ntreated with RT at Yonsei Cancer Center between 2013 and 2023 were analyzed,\nencompassing both structured and unstructured data. An open-source LLM was used\nto structure the unstructured EHR data via single-shot learning, with its\nperformance compared against a domain-specific medical LLM and a smaller\nvariant. Survival prediction models were developed using statistical, machine\nlearning, and deep learning approaches, incorporating both structured and\nLLM-structured data. Clinical experts evaluated the accuracy of the\nLLM-structured data. The open-source LLM achieved 87.5% accuracy in structuring\nunstructured EHR data without additional training, significantly outperforming\nthe domain-specific medical LLM, which reached only 35.8% accuracy. Larger LLMs\nwere more effective, particularly in extracting clinically relevant features\nlike general condition and disease extent, which closely correlated with\npatient survival. Incorporating LLM-structured clinical features into survival\nprediction models significantly improved accuracy, with the C-index of deep\nlearning models increasing from 0.737 to 0.820. These models also became more\ninterpretable by emphasizing clinically significant factors. This study shows\nthat general-domain LLMs, even without specific medical training, can\neffectively structure large-scale unstructured EHR data, substantially\nenhancing the accuracy and interpretability of clinical predictive models."
                },
                "authors": [
                    {
                        "name": "Sangjoon Park"
                    },
                    {
                        "name": "Chan Woo Wee"
                    },
                    {
                        "name": "Seo Hee Choi"
                    },
                    {
                        "name": "Kyung Hwan Kim"
                    },
                    {
                        "name": "Jee Suk Chang"
                    },
                    {
                        "name": "Hong In Yoon"
                    },
                    {
                        "name": "Ik Jae Lee"
                    },
                    {
                        "name": "Yong Bae Kim"
                    },
                    {
                        "name": "Jaeho Cho"
                    },
                    {
                        "name": "Ki Chang Keum"
                    },
                    {
                        "name": "Chang Geol Lee"
                    },
                    {
                        "name": "Hwa Kyung Byun"
                    },
                    {
                        "name": "Woong Sub Koom"
                    }
                ],
                "author_detail": {
                    "name": "Woong Sub Koom"
                },
                "author": "Woong Sub Koom",
                "arxiv_comment": "23 pages, 2 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04575v2",
                "updated": "2024-08-16T06:01:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    1,
                    15,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-08T16:36:24Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    16,
                    36,
                    24,
                    3,
                    221,
                    0
                ],
                "title": "SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals"
                },
                "summary": "Explainable Artificial Intelligence (XAI) plays a crucial role in enhancing\nthe transparency and accountability of AI models, particularly in natural\nlanguage processing (NLP) tasks. However, popular XAI methods such as LIME and\nSHAP have been found to be unstable and potentially misleading, underscoring\nthe need for a standardized evaluation approach. This paper introduces SCENE\n(Soft Counterfactual Evaluation for Natural language Explainability), a novel\nevaluation method that leverages large language models (LLMs) to generate Soft\nCounterfactual explanations in a zero-shot manner. By focusing on token-based\nsubstitutions, SCENE creates contextually appropriate and semantically\nmeaningful Soft Counterfactuals without extensive fine-tuning. SCENE adopts\nValiditysoft and Csoft metrics to assess the effectiveness of model-agnostic\nXAI methods in text classification tasks. Applied to CNN, RNN, and Transformer\narchitectures, SCENE provides valuable insights into the strengths and\nlimitations of various XAI techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Artificial Intelligence (XAI) plays a crucial role in enhancing\nthe transparency and accountability of AI models, particularly in natural\nlanguage processing (NLP) tasks. However, popular XAI methods such as LIME and\nSHAP have been found to be unstable and potentially misleading, underscoring\nthe need for a standardized evaluation approach. This paper introduces SCENE\n(Soft Counterfactual Evaluation for Natural language Explainability), a novel\nevaluation method that leverages large language models (LLMs) to generate Soft\nCounterfactual explanations in a zero-shot manner. By focusing on token-based\nsubstitutions, SCENE creates contextually appropriate and semantically\nmeaningful Soft Counterfactuals without extensive fine-tuning. SCENE adopts\nValiditysoft and Csoft metrics to assess the effectiveness of model-agnostic\nXAI methods in text classification tasks. Applied to CNN, RNN, and Transformer\narchitectures, SCENE provides valuable insights into the strengths and\nlimitations of various XAI techniques."
                },
                "authors": [
                    {
                        "name": "Haoran Zheng"
                    },
                    {
                        "name": "Utku Pamuksuz"
                    }
                ],
                "author_detail": {
                    "name": "Utku Pamuksuz"
                },
                "author": "Utku Pamuksuz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08541v1",
                "updated": "2024-08-16T05:56:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    5,
                    56,
                    10,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T05:56:10Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    5,
                    56,
                    10,
                    4,
                    229,
                    0
                ],
                "title": "Where is the signal in tokenization space?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where is the signal in tokenization space?"
                },
                "summary": "Large Language Models (LLMs) are typically shipped with tokenizers that\ndeterministically encode text into so-called canonical token sequences, to\nwhich the LLMs assign probability values. One common assumption is that the\nprobability of a piece of text is the probability of its canonical token\nsequence. However, the tokenization of a string is not unique: e.g., the Llama2\ntokenizer encodes Tokens as [Tok,ens], but [Tok,en,s] also represents the same\ntext. In this paper, we study non-canonical tokenizations. We prove that, given\na string, it is computationally hard to find the most likely tokenization for\nan autoregressive LLM, as well as to compute the marginal probability over all\npossible tokenizations. We then show how the marginal is, in most cases,\nindistinguishable from the canonical probability. Surprisingly, we then\nempirically demonstrate the existence of a significant amount of signal hidden\nwithin tokenization space. Notably, by simply aggregating the probabilities of\nnon-canonical tokenizations, we achieve improvements across a range of LLM\nevaluation benchmarks for a variety of architectures, including transformers\nand state space models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are typically shipped with tokenizers that\ndeterministically encode text into so-called canonical token sequences, to\nwhich the LLMs assign probability values. One common assumption is that the\nprobability of a piece of text is the probability of its canonical token\nsequence. However, the tokenization of a string is not unique: e.g., the Llama2\ntokenizer encodes Tokens as [Tok,ens], but [Tok,en,s] also represents the same\ntext. In this paper, we study non-canonical tokenizations. We prove that, given\na string, it is computationally hard to find the most likely tokenization for\nan autoregressive LLM, as well as to compute the marginal probability over all\npossible tokenizations. We then show how the marginal is, in most cases,\nindistinguishable from the canonical probability. Surprisingly, we then\nempirically demonstrate the existence of a significant amount of signal hidden\nwithin tokenization space. Notably, by simply aggregating the probabilities of\nnon-canonical tokenizations, we achieve improvements across a range of LLM\nevaluation benchmarks for a variety of architectures, including transformers\nand state space models."
                },
                "authors": [
                    {
                        "name": "Renato Lui Geh"
                    },
                    {
                        "name": "Honghua Zhang"
                    },
                    {
                        "name": "Kareem Ahmed"
                    },
                    {
                        "name": "Benjie Wang"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    }
                ],
                "author_detail": {
                    "name": "Guy Van den Broeck"
                },
                "author": "Guy Van den Broeck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15240v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15240v3",
                "updated": "2024-08-16T05:53:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    5,
                    53,
                    16,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-21T18:09:40Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    18,
                    9,
                    40,
                    6,
                    203,
                    0
                ],
                "title": "BIGbench: A Unified Benchmark for Social Bias in Text-to-Image\n  Generative Models Based on Multi-modal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIGbench: A Unified Benchmark for Social Bias in Text-to-Image\n  Generative Models Based on Multi-modal LLM"
                },
                "summary": "Text-to-Image (T2I) generative models are becoming increasingly crucial due\nto their ability to generate high-quality images, which also raises concerns\nabout the social biases in their outputs, especially in the human generation.\nSociological research has established systematic classifications of bias.\nHowever, existing bias research about T2I models conflates different types of\nbias, impeding methodological progress. In this paper, we introduce BIGbench, a\nunified benchmark for Biases of Image Generation, featuring a meticulously\ndesigned dataset. Unlike existing benchmarks, BIGbench classifies and evaluates\nbiases across four dimensions: manifestation of bias, visibility of bias,\nacquired attributes, and protected attributes, which ensures exceptional\naccuracy for analysis. Furthermore, BIGbench applies advanced multi-modal large\nlanguage models to achieve fully automated and highly accurate evaluations. We\napply BIGbench to evaluate eight representative general T2I models and three\ndebiased methods. Our human evaluation results underscore BIGbench's\neffectiveness in aligning images and identifying various biases. Besides, our\nstudy also reveal new research directions about biases, such as the effect of\ndistillation and irrelevant protected attributes. Our benchmark is openly\naccessible at https://github.com/BIGbench2024/BIGbench2024/ to ensure\nreproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Image (T2I) generative models are becoming increasingly crucial due\nto their ability to generate high-quality images, which also raises concerns\nabout the social biases in their outputs, especially in the human generation.\nSociological research has established systematic classifications of bias.\nHowever, existing bias research about T2I models conflates different types of\nbias, impeding methodological progress. In this paper, we introduce BIGbench, a\nunified benchmark for Biases of Image Generation, featuring a meticulously\ndesigned dataset. Unlike existing benchmarks, BIGbench classifies and evaluates\nbiases across four dimensions: manifestation of bias, visibility of bias,\nacquired attributes, and protected attributes, which ensures exceptional\naccuracy for analysis. Furthermore, BIGbench applies advanced multi-modal large\nlanguage models to achieve fully automated and highly accurate evaluations. We\napply BIGbench to evaluate eight representative general T2I models and three\ndebiased methods. Our human evaluation results underscore BIGbench's\neffectiveness in aligning images and identifying various biases. Besides, our\nstudy also reveal new research directions about biases, such as the effect of\ndistillation and irrelevant protected attributes. Our benchmark is openly\naccessible at https://github.com/BIGbench2024/BIGbench2024/ to ensure\nreproducibility."
                },
                "authors": [
                    {
                        "name": "Hanjun Luo"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Ziye Deng"
                    },
                    {
                        "name": "Xuecheng Liu"
                    },
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2405.17814",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15240v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15240v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02817v2",
                "updated": "2024-08-16T05:52:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    5,
                    52,
                    17,
                    4,
                    229,
                    0
                ],
                "published": "2024-05-05T05:43:20Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    5,
                    43,
                    20,
                    6,
                    126,
                    0
                ],
                "title": "Labeling supervised fine-tuning data with the scaling law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Labeling supervised fine-tuning data with the scaling law"
                },
                "summary": "This paper introduces a multi-stage manual annotation calibrated by the\nscaling law, offering a high-quality Supervised Fine-Tuning data acquisition\nmethod for environments with constrained resources like GPU poor, limited GPT\naccess, and funding restrictions. We have preprocessed 58k authentic chat data\nand manually annotated 2.3k questions. After this, we conducted fine-tuning on\nQwen models, ranging from 0.5B to 32B parameters. The optimal version improved\n29.07 in F1 score. This confirms the viability of fine-tuning Large Language\nModel (LLM) for downstream Natural Language Processing (NLP) tasks. Our\ncontributions are: 1) Created Supervised Fine-Tuning (SFT) training data in\nalpaca format, along with a set of Low-Rank Adaptation (LoRA) weights, and 2)\nDeveloped a method for acquiring high-quality data leveraging scaling law\nprinciple. The script, raw data with alpaca format and experiments track are\nopen-sourced on Github\n(https://github.com/InternLM/HuixiangDou/tree/main/web/tools), HuggingFace\n(https://huggingface.co/tpoisonooo) and WandB\n(https://wandb.ai/tpoisonooo/huixiangdou-cr/table?nw=nwusertpoisonooo). The\nprivacy of the data involved has been authorized by users. SFT data and license\ncomes from ncnn contributors group.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a multi-stage manual annotation calibrated by the\nscaling law, offering a high-quality Supervised Fine-Tuning data acquisition\nmethod for environments with constrained resources like GPU poor, limited GPT\naccess, and funding restrictions. We have preprocessed 58k authentic chat data\nand manually annotated 2.3k questions. After this, we conducted fine-tuning on\nQwen models, ranging from 0.5B to 32B parameters. The optimal version improved\n29.07 in F1 score. This confirms the viability of fine-tuning Large Language\nModel (LLM) for downstream Natural Language Processing (NLP) tasks. Our\ncontributions are: 1) Created Supervised Fine-Tuning (SFT) training data in\nalpaca format, along with a set of Low-Rank Adaptation (LoRA) weights, and 2)\nDeveloped a method for acquiring high-quality data leveraging scaling law\nprinciple. The script, raw data with alpaca format and experiments track are\nopen-sourced on Github\n(https://github.com/InternLM/HuixiangDou/tree/main/web/tools), HuggingFace\n(https://huggingface.co/tpoisonooo) and WandB\n(https://wandb.ai/tpoisonooo/huixiangdou-cr/table?nw=nwusertpoisonooo). The\nprivacy of the data involved has been authorized by users. SFT data and license\ncomes from ncnn contributors group."
                },
                "authors": [
                    {
                        "name": "Huanjun Kong"
                    }
                ],
                "author_detail": {
                    "name": "Huanjun Kong"
                },
                "author": "Huanjun Kong",
                "arxiv_comment": "5 pages, 3 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16357v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16357v2",
                "updated": "2024-08-16T05:16:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    5,
                    16,
                    31,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-23T10:00:45Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    10,
                    0,
                    45,
                    1,
                    205,
                    0
                ],
                "title": "TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced\n  CTR Prediction at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced\n  CTR Prediction at Kuaishou"
                },
                "summary": "The significance of modeling long-term user interests for CTR prediction\ntasks in large-scale recommendation systems is progressively gaining attention\namong researchers and practitioners. Existing work, such as SIM and TWIN,\ntypically employs a two-stage approach to model long-term user behavior\nsequences for efficiency concerns. The first stage rapidly retrieves a subset\nof sequences related to the target item from a long sequence using a\nsearch-based mechanism namely the General Search Unit (GSU), while the second\nstage calculates the interest scores using the Exact Search Unit (ESU) on the\nretrieved results. Given the extensive length of user behavior sequences\nspanning the entire life cycle, potentially reaching up to 10^6 in scale, there\nis currently no effective solution for fully modeling such expansive user\ninterests. To overcome this issue, we introduced TWIN-V2, an enhancement of\nTWIN, where a divide-and-conquer approach is applied to compress life-cycle\nbehaviors and uncover more accurate and diverse user interests. Specifically, a\nhierarchical clustering method groups items with similar characteristics in\nlife-cycle behaviors into a single cluster during the offline phase. By\nlimiting the size of clusters, we can compress behavior sequences well beyond\nthe magnitude of 10^5 to a length manageable for online inference in GSU\nretrieval. Cluster-aware target attention extracts comprehensive and\nmulti-faceted long-term interests of users, thereby making the final\nrecommendation results more accurate and diverse. Extensive offline experiments\non a multi-billion-scale industrial dataset and online A/B tests have\ndemonstrated the effectiveness of TWIN-V2. Under an efficient deployment\nframework, TWIN-V2 has been successfully deployed to the primary traffic that\nserves hundreds of millions of daily active users at Kuaishou.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significance of modeling long-term user interests for CTR prediction\ntasks in large-scale recommendation systems is progressively gaining attention\namong researchers and practitioners. Existing work, such as SIM and TWIN,\ntypically employs a two-stage approach to model long-term user behavior\nsequences for efficiency concerns. The first stage rapidly retrieves a subset\nof sequences related to the target item from a long sequence using a\nsearch-based mechanism namely the General Search Unit (GSU), while the second\nstage calculates the interest scores using the Exact Search Unit (ESU) on the\nretrieved results. Given the extensive length of user behavior sequences\nspanning the entire life cycle, potentially reaching up to 10^6 in scale, there\nis currently no effective solution for fully modeling such expansive user\ninterests. To overcome this issue, we introduced TWIN-V2, an enhancement of\nTWIN, where a divide-and-conquer approach is applied to compress life-cycle\nbehaviors and uncover more accurate and diverse user interests. Specifically, a\nhierarchical clustering method groups items with similar characteristics in\nlife-cycle behaviors into a single cluster during the offline phase. By\nlimiting the size of clusters, we can compress behavior sequences well beyond\nthe magnitude of 10^5 to a length manageable for online inference in GSU\nretrieval. Cluster-aware target attention extracts comprehensive and\nmulti-faceted long-term interests of users, thereby making the final\nrecommendation results more accurate and diverse. Extensive offline experiments\non a multi-billion-scale industrial dataset and online A/B tests have\ndemonstrated the effectiveness of TWIN-V2. Under an efficient deployment\nframework, TWIN-V2 has been successfully deployed to the primary traffic that\nserves hundreds of millions of daily active users at Kuaishou."
                },
                "authors": [
                    {
                        "name": "Zihua Si"
                    },
                    {
                        "name": "Lin Guan"
                    },
                    {
                        "name": "ZhongXiang Sun"
                    },
                    {
                        "name": "Xiaoxue Zang"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Yiqun Hui"
                    },
                    {
                        "name": "Xingchao Cao"
                    },
                    {
                        "name": "Zeyu Yang"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Dewei Leng"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Chenbin Zhang"
                    },
                    {
                        "name": "Yanan Niu"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "arxiv_doi": "10.1145/3627673.3680030",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3627673.3680030",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.16357v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16357v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by CIKM 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08535v1",
                "updated": "2024-08-16T05:15:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    5,
                    15,
                    12,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T05:15:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    5,
                    15,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for\n  Advanced Retrieval-Augmented Generation in Fact-Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for\n  Advanced Retrieval-Augmented Generation in Fact-Checking"
                },
                "summary": "Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG) systems, their effectiveness is often hindered by a lack of\nintegration with entity relationships and community structures, limiting their\nability to provide contextually rich and accurate information retrieval for\nfact-checking. We introduce CommunityKG-RAG (Community Knowledge\nGraph-Retrieval Augmented Generation), a novel zero-shot framework that\nintegrates community structures within Knowledge Graphs (KGs) with RAG systems\nto enhance the fact-checking process. Capable of adapting to new domains and\nqueries without additional training, CommunityKG-RAG utilizes the multi-hop\nnature of community structures within KGs to significantly improve the accuracy\nand relevance of information retrieval. Our experimental results demonstrate\nthat CommunityKG-RAG outperforms traditional methods, representing a\nsignificant advancement in fact-checking by offering a robust, scalable, and\nefficient solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG) systems, their effectiveness is often hindered by a lack of\nintegration with entity relationships and community structures, limiting their\nability to provide contextually rich and accurate information retrieval for\nfact-checking. We introduce CommunityKG-RAG (Community Knowledge\nGraph-Retrieval Augmented Generation), a novel zero-shot framework that\nintegrates community structures within Knowledge Graphs (KGs) with RAG systems\nto enhance the fact-checking process. Capable of adapting to new domains and\nqueries without additional training, CommunityKG-RAG utilizes the multi-hop\nnature of community structures within KGs to significantly improve the accuracy\nand relevance of information retrieval. Our experimental results demonstrate\nthat CommunityKG-RAG outperforms traditional methods, representing a\nsignificant advancement in fact-checking by offering a robust, scalable, and\nefficient solution."
                },
                "authors": [
                    {
                        "name": "Rong-Ching Chang"
                    },
                    {
                        "name": "Jiawei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Zhang"
                },
                "author": "Jiawei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08530v1",
                "updated": "2024-08-16T04:57:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    57,
                    36,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T04:57:36Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    57,
                    36,
                    4,
                    229,
                    0
                ],
                "title": "Predicting the genetic component of gene expression using gene\n  regulatory networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting the genetic component of gene expression using gene\n  regulatory networks"
                },
                "summary": "Gene expression prediction plays a vital role in transcriptome-wide\nassociation studies (TWAS), which seek to establish associations between tissue\ngene expression and complex traits. Traditional models rely on genetic variants\nin close genomic proximity to the gene of interest to predict the genetic\ncomponent of gene expression. In this study, we propose a novel approach\nincorporating distal genetic variants acting through gene regulatory networks\n(GRNs) into gene expression prediction models, in line with the omnigenic model\nof complex trait inheritance. Using causal and coexpression GRNs reconstructed\nfrom genomic and transcriptomic data and modeling the data as a Bayesian\nnetwork jointly over genetic variants and genes, inference of gene expression\nfrom observed genotypic data is achieved through a two-step process. Initially,\nthe expression level of each gene in the network is predicted using its local\ngenetic variants. The residuals, calculated as the differences between the\nobserved and predicted expression levels, are then modeled using the genotype\ninformation of parent and/or grandparent nodes in the GRN. The final predicted\nexpression level of the gene is obtained by summing the predictions from the\nlocal variants model and the residual model, effectively incorporating both\nlocal and distal genetic influences. Using various regularized regression\ntechniques for parameter estimation, we found that GRN-based gene expression\nprediction outperformed the traditional local-variant approach on simulated\ndata from the DREAM5 Systems Genetics Challenge and real data from the Geuvadis\nstudy and an eQTL mapping study in yeast. This study provides important\ninsights into the challenge of gene expression prediction for TWAS. It\nreaffirms the importance of GRNs for understanding the genetic effects on gene\nexpression and complex traits more generally.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gene expression prediction plays a vital role in transcriptome-wide\nassociation studies (TWAS), which seek to establish associations between tissue\ngene expression and complex traits. Traditional models rely on genetic variants\nin close genomic proximity to the gene of interest to predict the genetic\ncomponent of gene expression. In this study, we propose a novel approach\nincorporating distal genetic variants acting through gene regulatory networks\n(GRNs) into gene expression prediction models, in line with the omnigenic model\nof complex trait inheritance. Using causal and coexpression GRNs reconstructed\nfrom genomic and transcriptomic data and modeling the data as a Bayesian\nnetwork jointly over genetic variants and genes, inference of gene expression\nfrom observed genotypic data is achieved through a two-step process. Initially,\nthe expression level of each gene in the network is predicted using its local\ngenetic variants. The residuals, calculated as the differences between the\nobserved and predicted expression levels, are then modeled using the genotype\ninformation of parent and/or grandparent nodes in the GRN. The final predicted\nexpression level of the gene is obtained by summing the predictions from the\nlocal variants model and the residual model, effectively incorporating both\nlocal and distal genetic influences. Using various regularized regression\ntechniques for parameter estimation, we found that GRN-based gene expression\nprediction outperformed the traditional local-variant approach on simulated\ndata from the DREAM5 Systems Genetics Challenge and real data from the Geuvadis\nstudy and an eQTL mapping study in yeast. This study provides important\ninsights into the challenge of gene expression prediction for TWAS. It\nreaffirms the importance of GRNs for understanding the genetic effects on gene\nexpression and complex traits more generally."
                },
                "authors": [
                    {
                        "name": "Gutama Ibrahim Mohammad"
                    },
                    {
                        "name": "Tom Michoel"
                    }
                ],
                "author_detail": {
                    "name": "Tom Michoel"
                },
                "author": "Tom Michoel",
                "arxiv_comment": "19 pages, 3 figures, 2 tables + 11 pages, 8 figures, 1 table\n  supplementary information; code is available on GitHub at\n  https://github.com/guutama/GRN-TI.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.MN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08527v1",
                "updated": "2024-08-16T04:54:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    54,
                    10,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T04:54:10Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    54,
                    10,
                    4,
                    229,
                    0
                ],
                "title": "Focus on Focus: Focus-oriented Representation Learning and Multi-view\n  Cross-modal Alignment for Glioma Grading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focus on Focus: Focus-oriented Representation Learning and Multi-view\n  Cross-modal Alignment for Glioma Grading"
                },
                "summary": "Recently, multimodal deep learning, which integrates histopathology slides\nand molecular biomarkers, has achieved a promising performance in glioma\ngrading. Despite great progress, due to the intra-modality complexity and\ninter-modality heterogeneity, existing studies suffer from inadequate\nhistopathology representation learning and inefficient molecular-pathology\nknowledge alignment. These two issues hinder existing methods to precisely\ninterpret diagnostic molecular-pathology features, thereby limiting their\ngrading performance. Moreover, the real-world applicability of existing\nmultimodal approaches is significantly restricted as molecular biomarkers are\nnot always available during clinical deployment. To address these problems, we\nintroduce a novel Focus on Focus (FoF) framework with paired pathology-genomic\ntraining and applicable pathology-only inference, enhancing molecular-pathology\nrepresentation effectively. Specifically, we propose a Focus-oriented\nRepresentation Learning (FRL) module to encourage the model to identify regions\npositively or negatively related to glioma grading and guide it to focus on the\ndiagnostic areas with a consistency constraint. To effectively link the\nmolecular biomarkers to morphological features, we propose a Multi-view\nCross-modal Alignment (MCA) module that projects histopathology representations\ninto molecular subspaces, aligning morphological features with corresponding\nmolecular biomarker status by supervised contrastive learning. Experiments on\nthe TCGA GBM-LGG dataset demonstrate that our FoF framework significantly\nimproves the glioma grading. Remarkably, our FoF achieves superior performance\nusing only histopathology slides compared to existing multimodal methods. The\nsource code is available at https://github.com/peterlipan/FoF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, multimodal deep learning, which integrates histopathology slides\nand molecular biomarkers, has achieved a promising performance in glioma\ngrading. Despite great progress, due to the intra-modality complexity and\ninter-modality heterogeneity, existing studies suffer from inadequate\nhistopathology representation learning and inefficient molecular-pathology\nknowledge alignment. These two issues hinder existing methods to precisely\ninterpret diagnostic molecular-pathology features, thereby limiting their\ngrading performance. Moreover, the real-world applicability of existing\nmultimodal approaches is significantly restricted as molecular biomarkers are\nnot always available during clinical deployment. To address these problems, we\nintroduce a novel Focus on Focus (FoF) framework with paired pathology-genomic\ntraining and applicable pathology-only inference, enhancing molecular-pathology\nrepresentation effectively. Specifically, we propose a Focus-oriented\nRepresentation Learning (FRL) module to encourage the model to identify regions\npositively or negatively related to glioma grading and guide it to focus on the\ndiagnostic areas with a consistency constraint. To effectively link the\nmolecular biomarkers to morphological features, we propose a Multi-view\nCross-modal Alignment (MCA) module that projects histopathology representations\ninto molecular subspaces, aligning morphological features with corresponding\nmolecular biomarker status by supervised contrastive learning. Experiments on\nthe TCGA GBM-LGG dataset demonstrate that our FoF framework significantly\nimproves the glioma grading. Remarkably, our FoF achieves superior performance\nusing only histopathology slides compared to existing multimodal methods. The\nsource code is available at https://github.com/peterlipan/FoF."
                },
                "authors": [
                    {
                        "name": "Li Pan"
                    },
                    {
                        "name": "Yupei Zhang"
                    },
                    {
                        "name": "Qiushi Yang"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Xiaohan Xing"
                    },
                    {
                        "name": "Maximus C. F. Yeung"
                    },
                    {
                        "name": "Zhen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Chen"
                },
                "author": "Zhen Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.17074v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.17074v3",
                "updated": "2024-08-16T04:46:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    46,
                    19,
                    4,
                    229,
                    0
                ],
                "published": "2023-09-29T09:10:04Z",
                "published_parsed": [
                    2023,
                    9,
                    29,
                    9,
                    10,
                    4,
                    4,
                    272,
                    0
                ],
                "title": "AdaDiff: Accelerating Diffusion Models through Step-Wise Adaptive\n  Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaDiff: Accelerating Diffusion Models through Step-Wise Adaptive\n  Computation"
                },
                "summary": "Diffusion models achieve great success in generating diverse and\nhigh-fidelity images, yet their widespread application, especially in real-time\nscenarios, is hampered by their inherently slow generation speed. The slow\ngeneration stems from the necessity of multi-step network inference. While some\ncertain predictions benefit from the full computation of the model in each\nsampling iteration, not every iteration requires the same amount of\ncomputation, potentially leading to inefficient computation. Unlike typical\nadaptive computation challenges that deal with single-step generation problems,\ndiffusion processes with a multi-step generation need to dynamically adjust\ntheir computational resource allocation based on the ongoing assessment of each\nstep's importance to the final image output, presenting a unique set of\nchallenges. In this work, we propose AdaDiff, an adaptive framework that\ndynamically allocates computation resources in each sampling step to improve\nthe generation efficiency of diffusion models. To assess the effects of changes\nin computational effort on image quality, we present a timestep-aware\nuncertainty estimation module (UEM). Integrated at each intermediate layer, the\nUEM evaluates the predictive uncertainty. This uncertainty measurement serves\nas an indicator for determining whether to terminate the inference process.\nAdditionally, we introduce an uncertainty-aware layer-wise loss aimed at\nbridging the performance gap between full models and their adaptive\ncounterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models achieve great success in generating diverse and\nhigh-fidelity images, yet their widespread application, especially in real-time\nscenarios, is hampered by their inherently slow generation speed. The slow\ngeneration stems from the necessity of multi-step network inference. While some\ncertain predictions benefit from the full computation of the model in each\nsampling iteration, not every iteration requires the same amount of\ncomputation, potentially leading to inefficient computation. Unlike typical\nadaptive computation challenges that deal with single-step generation problems,\ndiffusion processes with a multi-step generation need to dynamically adjust\ntheir computational resource allocation based on the ongoing assessment of each\nstep's importance to the final image output, presenting a unique set of\nchallenges. In this work, we propose AdaDiff, an adaptive framework that\ndynamically allocates computation resources in each sampling step to improve\nthe generation efficiency of diffusion models. To assess the effects of changes\nin computational effort on image quality, we present a timestep-aware\nuncertainty estimation module (UEM). Integrated at each intermediate layer, the\nUEM evaluates the predictive uncertainty. This uncertainty measurement serves\nas an indicator for determining whether to terminate the inference process.\nAdditionally, we introduce an uncertainty-aware layer-wise loss aimed at\nbridging the performance gap between full models and their adaptive\ncounterparts."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Yaqing Wang"
                    },
                    {
                        "name": "Caiwen Ding"
                    },
                    {
                        "name": "Yi Liang"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Dongkuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Dongkuan Xu"
                },
                "author": "Dongkuan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.17074v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.17074v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10753v2",
                "updated": "2024-08-16T04:12:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    0,
                    4,
                    229,
                    0
                ],
                "published": "2024-02-16T15:19:46Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    15,
                    19,
                    46,
                    4,
                    47,
                    0
                ],
                "title": "ToolSword: Unveiling Safety Issues of Large Language Models in Tool\n  Learning Across Three Stages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolSword: Unveiling Safety Issues of Large Language Models in Tool\n  Learning Across Three Stages"
                },
                "summary": "Tool learning is widely acknowledged as a foundational approach or deploying\nlarge language models (LLMs) in real-world scenarios. While current research\nprimarily emphasizes leveraging tools to augment LLMs, it frequently neglects\nemerging safety considerations tied to their application. To fill this gap, we\npresent *ToolSword*, a comprehensive framework dedicated to meticulously\ninvestigating safety issues linked to LLMs in tool learning. Specifically,\nToolSword delineates six safety scenarios for LLMs in tool learning,\nencompassing **malicious queries** and **jailbreak attacks** in the input\nstage, **noisy misdirection** and **risky cues** in the execution stage, and\n**harmful feedback** and **error conflicts** in the output stage. Experiments\nconducted on 11 open-source and closed-source LLMs reveal enduring safety\nchallenges in tool learning, such as handling harmful queries, employing risky\ntools, and delivering detrimental feedback, which even GPT-4 is susceptible to.\nMoreover, we conduct further studies with the aim of fostering research on tool\nlearning safety. The data is released in\nhttps://github.com/Junjie-Ye/ToolSword.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool learning is widely acknowledged as a foundational approach or deploying\nlarge language models (LLMs) in real-world scenarios. While current research\nprimarily emphasizes leveraging tools to augment LLMs, it frequently neglects\nemerging safety considerations tied to their application. To fill this gap, we\npresent *ToolSword*, a comprehensive framework dedicated to meticulously\ninvestigating safety issues linked to LLMs in tool learning. Specifically,\nToolSword delineates six safety scenarios for LLMs in tool learning,\nencompassing **malicious queries** and **jailbreak attacks** in the input\nstage, **noisy misdirection** and **risky cues** in the execution stage, and\n**harmful feedback** and **error conflicts** in the output stage. Experiments\nconducted on 11 open-source and closed-source LLMs reveal enduring safety\nchallenges in tool learning, such as handling harmful queries, employing risky\ntools, and delivering detrimental feedback, which even GPT-4 is susceptible to.\nMoreover, we conduct further studies with the aim of fostering research on tool\nlearning safety. The data is released in\nhttps://github.com/Junjie-Ye/ToolSword."
                },
                "authors": [
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Sixian Li"
                    },
                    {
                        "name": "Guanyu Li"
                    },
                    {
                        "name": "Caishuang Huang"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Yilong Wu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Accepted by ACL 2024 Main Conference",
                "arxiv_journal_ref": "Proceedings of the 62nd Annual Meeting of the Association for\n  Computational Linguistics 2024 (Volume 1: Long Papers)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13571v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13571v2",
                "updated": "2024-08-16T03:33:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    3,
                    33,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-05-22T12:08:56Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    12,
                    8,
                    56,
                    2,
                    143,
                    0
                ],
                "title": "Incomplete Multimodal Industrial Anomaly Detection via Cross-Modal\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incomplete Multimodal Industrial Anomaly Detection via Cross-Modal\n  Distillation"
                },
                "summary": "Recent studies of multimodal industrial anomaly detection (IAD) based on 3D\npoint clouds and RGB images have highlighted the importance of exploiting the\nredundancy and complementarity among modalities for accurate classification and\nsegmentation. However, achieving multimodal IAD in practical production lines\nremains a work in progress. It is essential to consider the trade-offs between\nthe costs and benefits associated with the introduction of new modalities while\nensuring compatibility with current processes. Existing quality control\nprocesses combine rapid in-line inspections, such as optical and infrared\nimaging with high-resolution but time-consuming near-line characterization\ntechniques, including industrial CT and electron microscopy to manually or\nsemi-automatically locate and analyze defects in the production of Li-ion\nbatteries and composite materials. Given the cost and time limitations, only a\nsubset of the samples can be inspected by all in-line and near-line methods,\nand the remaining samples are only evaluated through one or two forms of\nin-line inspection. To fully exploit data for deep learning-driven automatic\ndefect detection, the models must have the ability to leverage multimodal\ntraining and handle incomplete modalities during inference. In this paper, we\npropose CMDIAD, a Cross-Modal Distillation framework for IAD to demonstrate the\nfeasibility of a Multi-modal Training, Few-modal Inference (MTFI) pipeline. Our\nfindings show that the MTFI pipeline can more effectively utilize incomplete\nmultimodal information compared to applying only a single modality for training\nand inference. Moreover, we investigate the reasons behind the asymmetric\nperformance improvement using point clouds or RGB images as the main modality\nof inference. This provides a foundation for our future multimodal dataset\nconstruction with additional modalities from manufacturing scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies of multimodal industrial anomaly detection (IAD) based on 3D\npoint clouds and RGB images have highlighted the importance of exploiting the\nredundancy and complementarity among modalities for accurate classification and\nsegmentation. However, achieving multimodal IAD in practical production lines\nremains a work in progress. It is essential to consider the trade-offs between\nthe costs and benefits associated with the introduction of new modalities while\nensuring compatibility with current processes. Existing quality control\nprocesses combine rapid in-line inspections, such as optical and infrared\nimaging with high-resolution but time-consuming near-line characterization\ntechniques, including industrial CT and electron microscopy to manually or\nsemi-automatically locate and analyze defects in the production of Li-ion\nbatteries and composite materials. Given the cost and time limitations, only a\nsubset of the samples can be inspected by all in-line and near-line methods,\nand the remaining samples are only evaluated through one or two forms of\nin-line inspection. To fully exploit data for deep learning-driven automatic\ndefect detection, the models must have the ability to leverage multimodal\ntraining and handle incomplete modalities during inference. In this paper, we\npropose CMDIAD, a Cross-Modal Distillation framework for IAD to demonstrate the\nfeasibility of a Multi-modal Training, Few-modal Inference (MTFI) pipeline. Our\nfindings show that the MTFI pipeline can more effectively utilize incomplete\nmultimodal information compared to applying only a single modality for training\nand inference. Moreover, we investigate the reasons behind the asymmetric\nperformance improvement using point clouds or RGB images as the main modality\nof inference. This provides a foundation for our future multimodal dataset\nconstruction with additional modalities from manufacturing scenarios."
                },
                "authors": [
                    {
                        "name": "Wenbo Sui"
                    },
                    {
                        "name": "Daniel Lichau"
                    },
                    {
                        "name": "Josselin Lefèvre"
                    },
                    {
                        "name": "Harold Phelippeau"
                    }
                ],
                "author_detail": {
                    "name": "Harold Phelippeau"
                },
                "author": "Harold Phelippeau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13571v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13571v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01667v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01667v2",
                "updated": "2024-08-16T03:25:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    3,
                    25,
                    51,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-03T05:08:27Z",
                "published_parsed": [
                    2024,
                    8,
                    3,
                    5,
                    8,
                    27,
                    5,
                    216,
                    0
                ],
                "title": "Automated Phishing Detection Using URLs and Webpages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Phishing Detection Using URLs and Webpages"
                },
                "summary": "Phishing detection is a critical cybersecurity task that involves the\nidentification and neutralization of fraudulent attempts to obtain sensitive\ninformation, thereby safeguarding individuals and organizations from data\nbreaches and financial loss. In this project, we address the constraints of\ntraditional reference-based phishing detection by developing an LLM agent\nframework. This agent harnesses Large Language Models to actively fetch and\nutilize online information, thus providing a dynamic reference system for more\naccurate phishing detection. This innovation circumvents the need for a static\nknowledge base, offering a significant enhancement in adaptability and\nefficiency for automated security measures.\n  The project report includes an initial study and problem analysis of existing\nsolutions, which motivated us to develop a new framework. We demonstrate the\nframework with LLMs simulated as agents and detail the techniques required for\nconstruction, followed by a complete implementation with a proof-of-concept as\nwell as experiments to evaluate our solution's performance against other\nsimilar solutions. The results show that our approach has achieved with\naccuracy of 0.945, significantly outperforms the existing solution(DynaPhish)\nby 0.445. Furthermore, we discuss the limitations of our approach and suggest\nimprovements that could make it more effective.\n  Overall, the proposed framework has the potential to enhance the\neffectiveness of current reference-based phishing detection approaches and\ncould be adapted for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing detection is a critical cybersecurity task that involves the\nidentification and neutralization of fraudulent attempts to obtain sensitive\ninformation, thereby safeguarding individuals and organizations from data\nbreaches and financial loss. In this project, we address the constraints of\ntraditional reference-based phishing detection by developing an LLM agent\nframework. This agent harnesses Large Language Models to actively fetch and\nutilize online information, thus providing a dynamic reference system for more\naccurate phishing detection. This innovation circumvents the need for a static\nknowledge base, offering a significant enhancement in adaptability and\nefficiency for automated security measures.\n  The project report includes an initial study and problem analysis of existing\nsolutions, which motivated us to develop a new framework. We demonstrate the\nframework with LLMs simulated as agents and detail the techniques required for\nconstruction, followed by a complete implementation with a proof-of-concept as\nwell as experiments to evaluate our solution's performance against other\nsimilar solutions. The results show that our approach has achieved with\naccuracy of 0.945, significantly outperforms the existing solution(DynaPhish)\nby 0.445. Furthermore, we discuss the limitations of our approach and suggest\nimprovements that could make it more effective.\n  Overall, the proposed framework has the potential to enhance the\neffectiveness of current reference-based phishing detection approaches and\ncould be adapted for real-world applications."
                },
                "authors": [
                    {
                        "name": "Huilin Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01667v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01667v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02223v2",
                "updated": "2024-08-16T03:18:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    3,
                    18,
                    12,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-05T03:54:52Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    3,
                    54,
                    52,
                    0,
                    218,
                    0
                ],
                "title": "Large Language Model Aided QoS Prediction for Service Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Aided QoS Prediction for Service Recommendation"
                },
                "summary": "Large language models (LLMs) have seen rapid improvement in the recent years,\nand have been used in a wider range of applications. After being trained on\nlarge text corpus, LLMs obtain the capability of extracting rich features from\ntextual data. Such capability is potentially useful for the web service\nrecommendation task, where the web users and services have intrinsic attributes\nthat can be described using natural language sentences and are useful for\nrecommendation. In this paper, we explore the possibility and practicality of\nusing LLMs for web service recommendation. We propose the large language model\naided QoS prediction (llmQoS) model, which use LLMs to extract useful\ninformation from attributes of web users and services via descriptive\nsentences. This information is then used in combination with the QoS values of\nhistorical interactions of users and services, to predict QoS values for any\ngiven user-service pair. On the WSDream dataset, llmQoS is shown to overcome\nthe data sparsity issue inherent to the QoS prediction problem, and outperforms\ncomparable baseline models consistently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen rapid improvement in the recent years,\nand have been used in a wider range of applications. After being trained on\nlarge text corpus, LLMs obtain the capability of extracting rich features from\ntextual data. Such capability is potentially useful for the web service\nrecommendation task, where the web users and services have intrinsic attributes\nthat can be described using natural language sentences and are useful for\nrecommendation. In this paper, we explore the possibility and practicality of\nusing LLMs for web service recommendation. We propose the large language model\naided QoS prediction (llmQoS) model, which use LLMs to extract useful\ninformation from attributes of web users and services via descriptive\nsentences. This information is then used in combination with the QoS values of\nhistorical interactions of users and services, to predict QoS values for any\ngiven user-service pair. On the WSDream dataset, llmQoS is shown to overcome\nthe data sparsity issue inherent to the QoS prediction problem, and outperforms\ncomparable baseline models consistently."
                },
                "authors": [
                    {
                        "name": "Huiying Liu"
                    },
                    {
                        "name": "Zekun Zhang"
                    },
                    {
                        "name": "Honghao Li"
                    },
                    {
                        "name": "Qilin Wu"
                    },
                    {
                        "name": "Yiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwen Zhang"
                },
                "author": "Yiwen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08506v1",
                "updated": "2024-08-16T03:06:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    3,
                    6,
                    57,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T03:06:57Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    3,
                    6,
                    57,
                    4,
                    229,
                    0
                ],
                "title": "Ex3: Automatic Novel Writing by Extracting, Excelsior and Expanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ex3: Automatic Novel Writing by Extracting, Excelsior and Expanding"
                },
                "summary": "Generating long-term texts such as novels using artificial intelligence has\nalways been a challenge. A common approach is to use large language models\n(LLMs) to construct a hierarchical framework that first plans and then writes.\nDespite the fact that the generated novels reach a sufficient length, they\nexhibit poor logical coherence and appeal in their plots and deficiencies in\ncharacter and event depiction, ultimately compromising the overall narrative\nquality. In this paper, we propose a method named Extracting Excelsior and\nExpanding. Ex3 initially extracts structure information from raw novel data. By\ncombining this structure information with the novel data, an\ninstruction-following dataset is meticulously crafted. This dataset is then\nutilized to fine-tune the LLM, aiming for excelsior generation performance. In\nthe final stage, a tree-like expansion method is deployed to facilitate the\ngeneration of arbitrarily long novels. Evaluation against previous methods\nshowcases Ex3's ability to produce higher-quality long-form novels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long-term texts such as novels using artificial intelligence has\nalways been a challenge. A common approach is to use large language models\n(LLMs) to construct a hierarchical framework that first plans and then writes.\nDespite the fact that the generated novels reach a sufficient length, they\nexhibit poor logical coherence and appeal in their plots and deficiencies in\ncharacter and event depiction, ultimately compromising the overall narrative\nquality. In this paper, we propose a method named Extracting Excelsior and\nExpanding. Ex3 initially extracts structure information from raw novel data. By\ncombining this structure information with the novel data, an\ninstruction-following dataset is meticulously crafted. This dataset is then\nutilized to fine-tune the LLM, aiming for excelsior generation performance. In\nthe final stage, a tree-like expansion method is deployed to facilitate the\ngeneration of arbitrarily long novels. Evaluation against previous methods\nshowcases Ex3's ability to produce higher-quality long-form novels."
                },
                "authors": [
                    {
                        "name": "Huang Lei"
                    },
                    {
                        "name": "Jiaming Guo"
                    },
                    {
                        "name": "Guanhua He"
                    },
                    {
                        "name": "Xishan Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Shaohui Peng"
                    },
                    {
                        "name": "Shaoli Liu"
                    },
                    {
                        "name": "Tianshi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianshi Chen"
                },
                "author": "Tianshi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08503v1",
                "updated": "2024-08-16T03:01:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    3,
                    1,
                    35,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T03:01:35Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    3,
                    1,
                    35,
                    4,
                    229,
                    0
                ],
                "title": "Computational strategies for cross-species knowledge transfer and\n  translational biomedicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational strategies for cross-species knowledge transfer and\n  translational biomedicine"
                },
                "summary": "Research organisms provide invaluable insights into human biology and\ndiseases, serving as essential tools for functional experiments, disease\nmodeling, and drug testing. However, evolutionary divergence between humans and\nresearch organisms hinders effective knowledge transfer across species. Here,\nwe review state-of-the-art methods for computationally transferring knowledge\nacross species, primarily focusing on methods that utilize transcriptome data\nand/or molecular networks. We introduce the term \"agnology\" to describe the\nfunctional equivalence of molecular components regardless of evolutionary\norigin, as this concept is becoming pervasive in integrative data-driven models\nwhere the role of evolutionary origin can become unclear. Our review addresses\nfour key areas of information and knowledge transfer across species: (1)\ntransferring disease and gene annotation knowledge, (2) identifying agnologous\nmolecular components, (3) inferring equivalent perturbed genes or gene sets,\nand (4) identifying agnologous cell types. We conclude with an outlook on\nfuture directions and several key challenges that remain in cross-species\nknowledge transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research organisms provide invaluable insights into human biology and\ndiseases, serving as essential tools for functional experiments, disease\nmodeling, and drug testing. However, evolutionary divergence between humans and\nresearch organisms hinders effective knowledge transfer across species. Here,\nwe review state-of-the-art methods for computationally transferring knowledge\nacross species, primarily focusing on methods that utilize transcriptome data\nand/or molecular networks. We introduce the term \"agnology\" to describe the\nfunctional equivalence of molecular components regardless of evolutionary\norigin, as this concept is becoming pervasive in integrative data-driven models\nwhere the role of evolutionary origin can become unclear. Our review addresses\nfour key areas of information and knowledge transfer across species: (1)\ntransferring disease and gene annotation knowledge, (2) identifying agnologous\nmolecular components, (3) inferring equivalent perturbed genes or gene sets,\nand (4) identifying agnologous cell types. We conclude with an outlook on\nfuture directions and several key challenges that remain in cross-species\nknowledge transfer."
                },
                "authors": [
                    {
                        "name": "Hao Yuan"
                    },
                    {
                        "name": "Christopher A. Mancuso"
                    },
                    {
                        "name": "Kayla Johnson"
                    },
                    {
                        "name": "Ingo Braasch"
                    },
                    {
                        "name": "Arjun Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "Arjun Krishnan"
                },
                "author": "Arjun Krishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08495v1",
                "updated": "2024-08-16T02:33:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    2,
                    33,
                    55,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T02:33:55Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    2,
                    33,
                    55,
                    4,
                    229,
                    0
                ],
                "title": "Achieving Complex Image Edits via Function Aggregation with Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving Complex Image Edits via Function Aggregation with Diffusion\n  Models"
                },
                "summary": "Diffusion models have demonstrated strong performance in generative tasks,\nmaking them ideal candidates for image editing. Recent studies highlight their\nability to apply desired edits effectively by following textual instructions,\nyet two key challenges persist. First, these models struggle to apply multiple\nedits simultaneously, resulting in computational inefficiencies due to their\nreliance on sequential processing. Second, relying on textual prompts to\ndetermine the editing region can lead to unintended alterations in other parts\nof the image. In this work, we introduce FunEditor, an efficient diffusion\nmodel designed to learn atomic editing functions and perform complex edits by\naggregating simpler functions. This approach enables complex editing tasks,\nsuch as object movement, by aggregating multiple functions and applying them\nsimultaneously to specific areas. FunEditor is 5 to 24 times faster inference\nthan existing methods on complex tasks like object movement. Our experiments\ndemonstrate that FunEditor significantly outperforms recent baselines,\nincluding both inference-time optimization methods and fine-tuned models,\nacross various metrics, such as image quality assessment (IQA) and\nobject-background consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated strong performance in generative tasks,\nmaking them ideal candidates for image editing. Recent studies highlight their\nability to apply desired edits effectively by following textual instructions,\nyet two key challenges persist. First, these models struggle to apply multiple\nedits simultaneously, resulting in computational inefficiencies due to their\nreliance on sequential processing. Second, relying on textual prompts to\ndetermine the editing region can lead to unintended alterations in other parts\nof the image. In this work, we introduce FunEditor, an efficient diffusion\nmodel designed to learn atomic editing functions and perform complex edits by\naggregating simpler functions. This approach enables complex editing tasks,\nsuch as object movement, by aggregating multiple functions and applying them\nsimultaneously to specific areas. FunEditor is 5 to 24 times faster inference\nthan existing methods on complex tasks like object movement. Our experiments\ndemonstrate that FunEditor significantly outperforms recent baselines,\nincluding both inference-time optimization methods and fine-tuned models,\nacross various metrics, such as image quality assessment (IQA) and\nobject-background consistency."
                },
                "authors": [
                    {
                        "name": "Mohammadreza Samadi"
                    },
                    {
                        "name": "Fred X. Han"
                    },
                    {
                        "name": "Mohammad Salameh"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Fengyu Sun"
                    },
                    {
                        "name": "Chunhua Zhou"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09170v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09170v5",
                "updated": "2024-08-16T02:21:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    2,
                    21,
                    13,
                    4,
                    229,
                    0
                ],
                "published": "2024-04-14T07:19:27Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    7,
                    19,
                    27,
                    6,
                    105,
                    0
                ],
                "title": "Distilling Reasoning Ability from Large Language Models with Adaptive\n  Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Reasoning Ability from Large Language Models with Adaptive\n  Thinking"
                },
                "summary": "Chain of thought finetuning (cot-finetuning) aims to endow small language\nmodels (SLM) with reasoning ability to improve their performance towards\nspecific tasks by allowing them to imitate the reasoning procedure of large\nlanguage models (LLM) beyond simply predicting the answers. Most existing\ncot-finetuning methods adopt a pre-thinking mechanism, allowing the SLM to\ngenerate a rationale before providing an answer. This mechanism enables SLM to\nanalyze and think about complex questions, but it also makes answer correctness\nhighly sensitive to minor errors in rationale. Therefore, we propose a robust\npost-thinking mechanism to generate answers before rationale. Thanks to this\nanswer-first setting, 1) the answer can escape from the adverse effects caused\nby minor errors in the rationale; 2) the rationale serves as an error amplifier\nto the answer, which makes the SLM focus on learning hard samples; 3) the\ninferring efficiency can also benefit from the setting since users can stop the\ngeneration right after answers are outputted when inference is conducted.\nHowever, although the post-thinking mechanism brings many advantages and\nimproves the overall performance of SLM on specific tasks, it may lose the\nability to think about the questions and decompose complex questions into\nsimple sub-questions compared to pre-thinking mechanism. Therefore, a\nplug-and-play adaptive-thinking mechanism is proposed with the aid of the soft\nprompt tuning to integrate the merits of the pre-thinking mechanism and\npost-thinking mechanism, in which a perception module is introduced to\nadaptively prompt SLM answer or think first based on perceiving the complexity\nof the questions. Extensive experiments are conducted across 12 reasoning tasks\nand 2 representative language models to demonstrate the effectiveness of the\nproposed mechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of thought finetuning (cot-finetuning) aims to endow small language\nmodels (SLM) with reasoning ability to improve their performance towards\nspecific tasks by allowing them to imitate the reasoning procedure of large\nlanguage models (LLM) beyond simply predicting the answers. Most existing\ncot-finetuning methods adopt a pre-thinking mechanism, allowing the SLM to\ngenerate a rationale before providing an answer. This mechanism enables SLM to\nanalyze and think about complex questions, but it also makes answer correctness\nhighly sensitive to minor errors in rationale. Therefore, we propose a robust\npost-thinking mechanism to generate answers before rationale. Thanks to this\nanswer-first setting, 1) the answer can escape from the adverse effects caused\nby minor errors in the rationale; 2) the rationale serves as an error amplifier\nto the answer, which makes the SLM focus on learning hard samples; 3) the\ninferring efficiency can also benefit from the setting since users can stop the\ngeneration right after answers are outputted when inference is conducted.\nHowever, although the post-thinking mechanism brings many advantages and\nimproves the overall performance of SLM on specific tasks, it may lose the\nability to think about the questions and decompose complex questions into\nsimple sub-questions compared to pre-thinking mechanism. Therefore, a\nplug-and-play adaptive-thinking mechanism is proposed with the aid of the soft\nprompt tuning to integrate the merits of the pre-thinking mechanism and\npost-thinking mechanism, in which a perception module is introduced to\nadaptively prompt SLM answer or think first based on perceiving the complexity\nof the questions. Extensive experiments are conducted across 12 reasoning tasks\nand 2 representative language models to demonstrate the effectiveness of the\nproposed mechanism."
                },
                "authors": [
                    {
                        "name": "Xiaoshu Chen"
                    },
                    {
                        "name": "Sihang Zhou"
                    },
                    {
                        "name": "Ke Liang"
                    },
                    {
                        "name": "Xinwang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xinwang Liu"
                },
                "author": "Xinwang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09170v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09170v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.08869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08869v2",
                "updated": "2024-08-19T04:29:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    29,
                    34,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-16T17:54:09Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    54,
                    9,
                    4,
                    229,
                    0
                ],
                "title": "PEDAL: Enhancing Greedy Decoding with Large Language Models using\n  Diverse Exemplars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEDAL: Enhancing Greedy Decoding with Large Language Models using\n  Diverse Exemplars"
                },
                "summary": "Self-ensembling techniques with diverse reasoning paths such as\nSelf-Consistency have demonstrated remarkable performance gains in text\ngeneration with Large Language Models (LLMs). However, such techniques depend\non the availability of an accurate answer extraction process to aggregate\nacross multiple outputs. Moreover, they acquire higher inference cost, in\ncomparison to Greedy Decoding, due to generation of relatively higher number of\noutput tokens. Research has shown that the free form text outputs from\nSelf-Consistency can be aggregated reliably using LLMs to produce the final\noutput. Additionally, recent advancements in LLM inference have demonstrated\nthat usage of diverse exemplars in prompts have the ability to induce diversity\nin the LLM outputs. Such proven techniques can be easily extended to\nself-ensembling based approaches to achieve enhanced results in text\ngeneration. In this paper, we introduce PEDAL (Prompts based on Exemplar\nDiversity Aggregated using LLMs), a hybrid self-ensembling approach, that\ncombines the strengths of diverse exemplar based prompts and LLM based\naggregation to achieve improvement in overall performance. On the publicly\navailable SVAMP and ARC datasets, our experiments reveal that PEDAL can achieve\nbetter accuracy than Greedy Decoding based strategies with lower inference cost\ncompared to Self Consistency based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-ensembling techniques with diverse reasoning paths such as\nSelf-Consistency have demonstrated remarkable performance gains in text\ngeneration with Large Language Models (LLMs). However, such techniques depend\non the availability of an accurate answer extraction process to aggregate\nacross multiple outputs. Moreover, they acquire higher inference cost, in\ncomparison to Greedy Decoding, due to generation of relatively higher number of\noutput tokens. Research has shown that the free form text outputs from\nSelf-Consistency can be aggregated reliably using LLMs to produce the final\noutput. Additionally, recent advancements in LLM inference have demonstrated\nthat usage of diverse exemplars in prompts have the ability to induce diversity\nin the LLM outputs. Such proven techniques can be easily extended to\nself-ensembling based approaches to achieve enhanced results in text\ngeneration. In this paper, we introduce PEDAL (Prompts based on Exemplar\nDiversity Aggregated using LLMs), a hybrid self-ensembling approach, that\ncombines the strengths of diverse exemplar based prompts and LLM based\naggregation to achieve improvement in overall performance. On the publicly\navailable SVAMP and ARC datasets, our experiments reveal that PEDAL can achieve\nbetter accuracy than Greedy Decoding based strategies with lower inference cost\ncompared to Self Consistency based approaches."
                },
                "authors": [
                    {
                        "name": "Sumanth Prabhu"
                    }
                ],
                "author_detail": {
                    "name": "Sumanth Prabhu"
                },
                "author": "Sumanth Prabhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13717v2",
                "updated": "2024-08-16T17:43:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    43,
                    57,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-18T17:16:35Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    17,
                    16,
                    35,
                    3,
                    200,
                    0
                ],
                "title": "CoDefeater: Using LLMs To Find Defeaters in Assurance Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoDefeater: Using LLMs To Find Defeaters in Assurance Cases"
                },
                "summary": "Constructing assurance cases is a widely used, and sometimes required,\nprocess toward demonstrating that safety-critical systems will operate safely\nin their planned environment. To mitigate the risk of errors and missing edge\ncases, the concept of defeaters - arguments or evidence that challenge claims\nin an assurance case - has been introduced. Defeaters can provide timely\ndetection of weaknesses in the arguments, prompting further investigation and\ntimely mitigations. However, capturing defeaters relies on expert judgment,\nexperience, and creativity and must be done iteratively due to evolving\nrequirements and regulations. This paper proposes CoDefeater, an automated\nprocess to leverage large language models (LLMs) for finding defeaters. Initial\nresults on two systems show that LLMs can efficiently find known and unforeseen\nfeasible defeaters to support safety analysts in enhancing the completeness and\nconfidence of assurance cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing assurance cases is a widely used, and sometimes required,\nprocess toward demonstrating that safety-critical systems will operate safely\nin their planned environment. To mitigate the risk of errors and missing edge\ncases, the concept of defeaters - arguments or evidence that challenge claims\nin an assurance case - has been introduced. Defeaters can provide timely\ndetection of weaknesses in the arguments, prompting further investigation and\ntimely mitigations. However, capturing defeaters relies on expert judgment,\nexperience, and creativity and must be done iteratively due to evolving\nrequirements and regulations. This paper proposes CoDefeater, an automated\nprocess to leverage large language models (LLMs) for finding defeaters. Initial\nresults on two systems show that LLMs can efficiently find known and unforeseen\nfeasible defeaters to support safety analysts in enhancing the completeness and\nconfidence of assurance cases."
                },
                "authors": [
                    {
                        "name": "Usman Gohar"
                    },
                    {
                        "name": "Michael C. Hunter"
                    },
                    {
                        "name": "Robyn R. Lutz"
                    },
                    {
                        "name": "Myra B. Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Myra B. Cohen"
                },
                "author": "Myra B. Cohen",
                "arxiv_comment": "ASE 2024 NIER",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08848v1",
                "updated": "2024-08-16T17:19:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    19,
                    23,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T17:19:23Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    19,
                    23,
                    4,
                    229,
                    0
                ],
                "title": "PsychoLex: Unveiling the Psychological Mind of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsychoLex: Unveiling the Psychological Mind of Large Language Models"
                },
                "summary": "This paper explores the intersection of psychology and artificial\nintelligence through the development and evaluation of specialized Large\nLanguage Models (LLMs). We introduce PsychoLex, a suite of resources designed\nto enhance LLMs' proficiency in psychological tasks in both Persian and\nEnglish. Key contributions include the PsychoLexQA dataset for instructional\ncontent and the PsychoLexEval dataset for rigorous evaluation of LLMs in\ncomplex psychological scenarios. Additionally, we present the PsychoLexLLaMA\nmodel, optimized specifically for psychological applications, demonstrating\nsuperior performance compared to general-purpose models. The findings\nunderscore the potential of tailored LLMs for advancing psychological research\nand applications, while also highlighting areas for further refinement. This\nresearch offers a foundational step towards integrating LLMs into specialized\npsychological domains, with implications for future advancements in AI-driven\npsychological practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the intersection of psychology and artificial\nintelligence through the development and evaluation of specialized Large\nLanguage Models (LLMs). We introduce PsychoLex, a suite of resources designed\nto enhance LLMs' proficiency in psychological tasks in both Persian and\nEnglish. Key contributions include the PsychoLexQA dataset for instructional\ncontent and the PsychoLexEval dataset for rigorous evaluation of LLMs in\ncomplex psychological scenarios. Additionally, we present the PsychoLexLLaMA\nmodel, optimized specifically for psychological applications, demonstrating\nsuperior performance compared to general-purpose models. The findings\nunderscore the potential of tailored LLMs for advancing psychological research\nand applications, while also highlighting areas for further refinement. This\nresearch offers a foundational step towards integrating LLMs into specialized\npsychological domains, with implications for future advancements in AI-driven\npsychological practice."
                },
                "authors": [
                    {
                        "name": "Mohammad Amin Abbasi"
                    },
                    {
                        "name": "Farnaz Sadat Mirnezami"
                    },
                    {
                        "name": "Hassan Naderi"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Naderi"
                },
                "author": "Hassan Naderi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03640v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03640v4",
                "updated": "2024-08-16T17:06:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    6,
                    39,
                    4,
                    229,
                    0
                ],
                "published": "2024-03-06T11:56:02Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    11,
                    56,
                    2,
                    2,
                    66,
                    0
                ],
                "title": "Apollo: A Lightweight Multilingual Medical LLM towards Democratizing\n  Medical AI to 6B People",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apollo: A Lightweight Multilingual Medical LLM towards Democratizing\n  Medical AI to 6B People"
                },
                "summary": "Despite the vast repository of global medical knowledge predominantly being\nin English, local languages are crucial for delivering tailored healthcare\nservices, particularly in areas with limited medical resources. To extend the\nreach of medical AI advancements to a broader population, we aim to develop\nmedical LLMs across the six most widely spoken languages, encompassing a global\npopulation of 6.1 billion. This effort culminates in the creation of the\nApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the\nmultilingual medical benchmark, the released Apollo models, at various\nrelatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best\nperformance among models of equivalent size. Especially, Apollo-7B is the\nstate-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite\nmodels could be used to improve the multi-lingual medical capabilities of\nlarger models without fine-tuning in a proxy-tuning fashion. We will\nopen-source training corpora, code, model weights and evaluation benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the vast repository of global medical knowledge predominantly being\nin English, local languages are crucial for delivering tailored healthcare\nservices, particularly in areas with limited medical resources. To extend the\nreach of medical AI advancements to a broader population, we aim to develop\nmedical LLMs across the six most widely spoken languages, encompassing a global\npopulation of 6.1 billion. This effort culminates in the creation of the\nApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the\nmultilingual medical benchmark, the released Apollo models, at various\nrelatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best\nperformance among models of equivalent size. Especially, Apollo-7B is the\nstate-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite\nmodels could be used to improve the multi-lingual medical capabilities of\nlarger models without fine-tuning in a proxy-tuning fashion. We will\nopen-source training corpora, code, model weights and evaluation benchmark."
                },
                "authors": [
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Junyin Chen"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Guorui Zhen"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Xiangbo Wu"
                    },
                    {
                        "name": "Anningzhe Gao"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03640v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03640v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08841v1",
                "updated": "2024-08-16T17:00:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    0,
                    11,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T17:00:11Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    0,
                    11,
                    4,
                    229,
                    0
                ],
                "title": "FLEXTAF: Enhancing Table Reasoning with Flexible Tabular Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLEXTAF: Enhancing Table Reasoning with Flexible Tabular Formats"
                },
                "summary": "The table reasoning task aims to answer the question according to the given\ntable. Currently, using Large Language Models (LLMs) is the predominant method\nfor table reasoning. Most existing methods employ a fixed tabular format to\nrepresent the table, which could limit the performance. Given that each\ninstance requires different capabilities and models possess varying abilities,\nwe assert that different instances and models suit different tabular formats.\nWe prove the aforementioned claim through quantitative analysis of experimental\nresults, where different instances and models achieve different performances\nusing various tabular formats. Building on this discussion, we propose\nFLEXTAF-Single and FLEXTAF-Vote to enhance table reasoning performance by\nemploying flexible tabular formats. Specifically, (i) FLEXTAF-Single trains a\nclassifier to predict the most suitable tabular format based on the instance\nand the LLM. (ii) FLEXTAF-Vote integrates the results across different formats.\nOur experiments on WikiTableQuestions and TabFact reveal significant\nimprovements, with average gains of 2.3% and 4.8% compared to the best\nperformance achieved using a fixed tabular format with greedy decoding and\nself-consistency decoding, thereby validating the effectiveness of our methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The table reasoning task aims to answer the question according to the given\ntable. Currently, using Large Language Models (LLMs) is the predominant method\nfor table reasoning. Most existing methods employ a fixed tabular format to\nrepresent the table, which could limit the performance. Given that each\ninstance requires different capabilities and models possess varying abilities,\nwe assert that different instances and models suit different tabular formats.\nWe prove the aforementioned claim through quantitative analysis of experimental\nresults, where different instances and models achieve different performances\nusing various tabular formats. Building on this discussion, we propose\nFLEXTAF-Single and FLEXTAF-Vote to enhance table reasoning performance by\nemploying flexible tabular formats. Specifically, (i) FLEXTAF-Single trains a\nclassifier to predict the most suitable tabular format based on the instance\nand the LLM. (ii) FLEXTAF-Vote integrates the results across different formats.\nOur experiments on WikiTableQuestions and TabFact reveal significant\nimprovements, with average gains of 2.3% and 4.8% compared to the best\nperformance achieved using a fixed tabular format with greedy decoding and\nself-consistency decoding, thereby validating the effectiveness of our methods."
                },
                "authors": [
                    {
                        "name": "Xuanliang Zhang"
                    },
                    {
                        "name": "Dingzirui Wang"
                    },
                    {
                        "name": "Longxu Dou"
                    },
                    {
                        "name": "Baoxin Wang"
                    },
                    {
                        "name": "Dayong Wu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15947v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15947v3",
                "updated": "2024-08-16T16:55:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    16,
                    55,
                    22,
                    4,
                    229,
                    0
                ],
                "published": "2024-05-24T21:26:56Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    21,
                    26,
                    56,
                    4,
                    145,
                    0
                ],
                "title": "Mitigating scattering in a quantum system using only an integrating\n  sphere",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating scattering in a quantum system using only an integrating\n  sphere"
                },
                "summary": "Strong quantum-correlated sources are essential but delicate resources for\nquantum information science and engineering protocols. Decoherence and loss are\nthe two main disruptive processes that lead to the loss of nonclassical\nbehavior in quantum correlations. In quantum systems, scattering can contribute\nto both decoherence and loss. In this work, we present an experimental scheme\ncapable of significantly mitigating the adverse impact of scattering in quantum\nsystems. Our quantum system is composed of a two-mode squeezed light generated\nwith the four-wave mixing process in hot rubidium vapor, and a scatterer is\nintroduced to one of the two modes. An integrating sphere is then placed after\nthe scatterer to recollect the scattered photons. We use mutual information\nbetween the two modes as the measure of quantum correlations, and demonstrate a\n47.5% mutual information recovery from scattering, despite an enormous photon\nloss of greater than 85%. Our scheme is a pioneering step towards recovering\nquantum correlations from disruptive random processes, thus has the potential\nto bridge the gap between proof-of-principle demonstrations and practical\nreal-world deployments of quantum protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strong quantum-correlated sources are essential but delicate resources for\nquantum information science and engineering protocols. Decoherence and loss are\nthe two main disruptive processes that lead to the loss of nonclassical\nbehavior in quantum correlations. In quantum systems, scattering can contribute\nto both decoherence and loss. In this work, we present an experimental scheme\ncapable of significantly mitigating the adverse impact of scattering in quantum\nsystems. Our quantum system is composed of a two-mode squeezed light generated\nwith the four-wave mixing process in hot rubidium vapor, and a scatterer is\nintroduced to one of the two modes. An integrating sphere is then placed after\nthe scatterer to recollect the scattered photons. We use mutual information\nbetween the two modes as the measure of quantum correlations, and demonstrate a\n47.5% mutual information recovery from scattering, despite an enormous photon\nloss of greater than 85%. Our scheme is a pioneering step towards recovering\nquantum correlations from disruptive random processes, thus has the potential\nto bridge the gap between proof-of-principle demonstrations and practical\nreal-world deployments of quantum protocols."
                },
                "authors": [
                    {
                        "name": "Zhenfei Jiang"
                    },
                    {
                        "name": "Tian Li"
                    },
                    {
                        "name": "Matthew L. Boone"
                    },
                    {
                        "name": "Zhenhuan Yi"
                    },
                    {
                        "name": "Alexei V. Sokolov"
                    },
                    {
                        "name": "Girish S. Agarwal"
                    },
                    {
                        "name": "Marlan O. Scully"
                    }
                ],
                "author_detail": {
                    "name": "Marlan O. Scully"
                },
                "author": "Marlan O. Scully",
                "arxiv_comment": "7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15947v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15947v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07246v2",
                "updated": "2024-08-16T16:46:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    16,
                    46,
                    32,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-14T01:16:40Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    16,
                    40,
                    2,
                    227,
                    0
                ],
                "title": "ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B."
                },
                "authors": [
                    {
                        "name": "Junxian Li"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Xunzhi Wang"
                    },
                    {
                        "name": "Zeying Hao"
                    },
                    {
                        "name": "Jingdi Lei"
                    },
                    {
                        "name": "Qian Tan"
                    },
                    {
                        "name": "Cai Zhou"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Yaotian Yang"
                    },
                    {
                        "name": "Xinrui Xiong"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Mao Su"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "arxiv_comment": "11 pages, updated version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15019v2",
                "updated": "2024-08-16T15:56:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    56,
                    46,
                    4,
                    229,
                    0
                ],
                "published": "2024-05-23T19:44:03Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    19,
                    44,
                    3,
                    3,
                    144,
                    0
                ],
                "title": "Agentic Skill Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Skill Discovery"
                },
                "summary": "Language-conditioned robotic skills make it possible to apply the high-level\nreasoning of Large Language Models (LLMs) to low-level robotic control. A\nremaining challenge is to acquire a diverse set of fundamental skills. Existing\napproaches either manually decompose a complex task into atomic robotic actions\nin a top-down fashion, or bootstrap as many combinations as possible in a\nbottom-up fashion to cover a wider range of task possibilities. These\ndecompositions or combinations, however, require an initial skill library. For\nexample, a ``grasping'' capability can never emerge from a skill library\ncontaining only diverse ``pushing'' skills. Existing skill discovery techniques\nwith reinforcement learning acquire skills by an exhaustive exploration but\noften yield non-meaningful behaviors. In this study, we introduce a novel\nframework for skill discovery that is entirely driven by LLMs. The framework\nbegins with an LLM generating task proposals based on the provided scene\ndescription and the robot's configurations, aiming to incrementally acquire new\nskills upon task completion. For each proposed task, a series of reinforcement\nlearning processes are initiated, utilizing reward and success determination\nfunctions sampled by the LLM to develop the corresponding policy. The\nreliability and trustworthiness of learned behaviors are further ensured by an\nindependent vision-language model. We show that starting with zero skill, the\nskill library emerges and expands to more and more meaningful and reliable\nskills, enabling the robot to efficiently further propose and complete advanced\ntasks. Project page: \\url{https://agentic-skill-discovery.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-conditioned robotic skills make it possible to apply the high-level\nreasoning of Large Language Models (LLMs) to low-level robotic control. A\nremaining challenge is to acquire a diverse set of fundamental skills. Existing\napproaches either manually decompose a complex task into atomic robotic actions\nin a top-down fashion, or bootstrap as many combinations as possible in a\nbottom-up fashion to cover a wider range of task possibilities. These\ndecompositions or combinations, however, require an initial skill library. For\nexample, a ``grasping'' capability can never emerge from a skill library\ncontaining only diverse ``pushing'' skills. Existing skill discovery techniques\nwith reinforcement learning acquire skills by an exhaustive exploration but\noften yield non-meaningful behaviors. In this study, we introduce a novel\nframework for skill discovery that is entirely driven by LLMs. The framework\nbegins with an LLM generating task proposals based on the provided scene\ndescription and the robot's configurations, aiming to incrementally acquire new\nskills upon task completion. For each proposed task, a series of reinforcement\nlearning processes are initiated, utilizing reward and success determination\nfunctions sampled by the LLM to develop the corresponding policy. The\nreliability and trustworthiness of learned behaviors are further ensured by an\nindependent vision-language model. We show that starting with zero skill, the\nskill library emerges and expands to more and more meaningful and reliable\nskills, enabling the robot to efficiently further propose and complete advanced\ntasks. Project page: \\url{https://agentic-skill-discovery.github.io}."
                },
                "authors": [
                    {
                        "name": "Xufeng Zhao"
                    },
                    {
                        "name": "Cornelius Weber"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "Webpage see https://agentic-skill-discovery.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08812v1",
                "updated": "2024-08-16T15:47:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    47,
                    8,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T15:47:08Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    47,
                    8,
                    4,
                    229,
                    0
                ],
                "title": "CAT: Caution Aware Transfer in Reinforcement Learning via Distributional\n  Risk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAT: Caution Aware Transfer in Reinforcement Learning via Distributional\n  Risk"
                },
                "summary": "Transfer learning in reinforcement learning (RL) has become a pivotal\nstrategy for improving data efficiency in new, unseen tasks by utilizing\nknowledge from previously learned tasks. This approach is especially beneficial\nin real-world deployment scenarios where computational resources are\nconstrained and agents must adapt rapidly to novel environments. However,\ncurrent state-of-the-art methods often fall short in ensuring safety during the\ntransfer process, particularly when unforeseen risks emerge in the deployment\nphase. In this work, we address these limitations by introducing a novel\nCaution-Aware Transfer Learning (CAT) framework. Unlike traditional approaches\nthat limit risk considerations to mean-variance, we define \"caution\" as a more\ngeneralized and comprehensive notion of risk. Our core innovation lies in\noptimizing a weighted sum of reward return and caution-based on state-action\noccupancy measures-during the transfer process, allowing for a rich\nrepresentation of diverse risk factors. To the best of our knowledge, this is\nthe first work to explore the optimization of such a generalized risk notion\nwithin the context of transfer RL. Our contributions are threefold: (1) We\npropose a Caution-Aware Transfer (CAT) framework that evaluates source policies\nwithin the test environment and constructs a new policy that balances reward\nmaximization and caution. (2) We derive theoretical sub-optimality bounds for\nour method, providing rigorous guarantees of its efficacy. (3) We empirically\nvalidate CAT, demonstrating that it consistently outperforms existing methods\nby delivering safer policies under varying risk conditions in the test tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer learning in reinforcement learning (RL) has become a pivotal\nstrategy for improving data efficiency in new, unseen tasks by utilizing\nknowledge from previously learned tasks. This approach is especially beneficial\nin real-world deployment scenarios where computational resources are\nconstrained and agents must adapt rapidly to novel environments. However,\ncurrent state-of-the-art methods often fall short in ensuring safety during the\ntransfer process, particularly when unforeseen risks emerge in the deployment\nphase. In this work, we address these limitations by introducing a novel\nCaution-Aware Transfer Learning (CAT) framework. Unlike traditional approaches\nthat limit risk considerations to mean-variance, we define \"caution\" as a more\ngeneralized and comprehensive notion of risk. Our core innovation lies in\noptimizing a weighted sum of reward return and caution-based on state-action\noccupancy measures-during the transfer process, allowing for a rich\nrepresentation of diverse risk factors. To the best of our knowledge, this is\nthe first work to explore the optimization of such a generalized risk notion\nwithin the context of transfer RL. Our contributions are threefold: (1) We\npropose a Caution-Aware Transfer (CAT) framework that evaluates source policies\nwithin the test environment and constructs a new policy that balances reward\nmaximization and caution. (2) We derive theoretical sub-optimality bounds for\nour method, providing rigorous guarantees of its efficacy. (3) We empirically\nvalidate CAT, demonstrating that it consistently outperforms existing methods\nby delivering safer policies under varying risk conditions in the test tasks."
                },
                "authors": [
                    {
                        "name": "Mohamad Fares El Hajj Chehade"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    },
                    {
                        "name": "Amy Zhang"
                    },
                    {
                        "name": "Hao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhu"
                },
                "author": "Hao Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08811v1",
                "updated": "2024-08-16T15:46:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    46,
                    15,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T15:46:15Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    46,
                    15,
                    4,
                    229,
                    0
                ],
                "title": "Artificial Intelligence and Strategic Decision-Making: Evidence from\n  Entrepreneurs and Investors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence and Strategic Decision-Making: Evidence from\n  Entrepreneurs and Investors"
                },
                "summary": "This paper explores how artificial intelligence (AI) may impact the strategic\ndecision-making (SDM) process in firms. We illustrate how AI could augment\nexisting SDM tools and provide empirical evidence from a leading accelerator\nprogram and a startup competition that current Large Language Models (LLMs) can\ngenerate and evaluate strategies at a level comparable to entrepreneurs and\ninvestors. We then examine implications for key cognitive processes underlying\nSDM -- search, representation, and aggregation. Our analysis suggests AI has\nthe potential to enhance the speed, quality, and scale of strategic analysis,\nwhile also enabling new approaches like virtual strategy simulations. However,\nthe ultimate impact on firm performance will depend on competitive dynamics as\nAI capabilities progress. We propose a framework connecting AI use in SDM to\nfirm outcomes and discuss how AI may reshape sources of competitive advantage.\nWe conclude by considering how AI could both support and challenge core tenets\nof the theory-based view of strategy. Overall, our work maps out an emerging\nresearch frontier at the intersection of AI and strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how artificial intelligence (AI) may impact the strategic\ndecision-making (SDM) process in firms. We illustrate how AI could augment\nexisting SDM tools and provide empirical evidence from a leading accelerator\nprogram and a startup competition that current Large Language Models (LLMs) can\ngenerate and evaluate strategies at a level comparable to entrepreneurs and\ninvestors. We then examine implications for key cognitive processes underlying\nSDM -- search, representation, and aggregation. Our analysis suggests AI has\nthe potential to enhance the speed, quality, and scale of strategic analysis,\nwhile also enabling new approaches like virtual strategy simulations. However,\nthe ultimate impact on firm performance will depend on competitive dynamics as\nAI capabilities progress. We propose a framework connecting AI use in SDM to\nfirm outcomes and discuss how AI may reshape sources of competitive advantage.\nWe conclude by considering how AI could both support and challenge core tenets\nof the theory-based view of strategy. Overall, our work maps out an emerging\nresearch frontier at the intersection of AI and strategy."
                },
                "authors": [
                    {
                        "name": "Felipe A. Csaszar"
                    },
                    {
                        "name": "Harsh Ketkar"
                    },
                    {
                        "name": "Hyunjin Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyunjin Kim"
                },
                "author": "Hyunjin Kim",
                "arxiv_comment": "55 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08808v2",
                "updated": "2024-08-19T16:44:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    44,
                    30,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-16T15:41:43Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    41,
                    43,
                    4,
                    229,
                    0
                ],
                "title": "Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the landscape of machine\nlearning, yet current benchmarks often fall short in capturing the diverse\nbehavior of these models in real-world applications. A benchmark's usefulness\nis determined by its ability to clearly differentiate between models of varying\ncapabilities (separability) and closely align with human preferences. Existing\nframeworks like Alpaca-Eval 2.0 LC\n\\cite{dubois2024lengthcontrolledalpacaevalsimpleway} and Arena-Hard v0.1\n\\cite{li2024crowdsourced} are limited by their focus on general-purpose queries\nand lack of diversity across domains such as law, medicine, and multilingual\ncontexts. In this paper, we address these limitations by introducing a novel\ndata pipeline that curates diverse, domain-specific evaluation sets tailored\nfor LLM-as-a-Judge frameworks. Our approach leverages a combination of manual\ncuration, semi-supervised learning to generate clusters, and stratified\nsampling to ensure balanced representation across a wide range of domains and\nlanguages. The resulting evaluation set, which includes 1573 samples across 14\ncategories, demonstrates high separability (84\\%) across ten top-ranked models,\nand agreement (84\\%) with Chatbot Arena and (0.915) Spearman correlation. The\nagreement values are 9\\% better than Arena Hard and 20\\% better than AlpacaEval\n2.0 LC, while the Spearman coefficient is 0.7 more than the next best\nbenchmark, showcasing a significant improvement in the usefulness of the\nbenchmark. We further provide an open-source evaluation tool that enables\nfine-grained analysis of model performance across user-defined categories,\noffering valuable insights for practitioners. This work contributes to the\nongoing effort to enhance the transparency, diversity, and effectiveness of LLM\nevaluation methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the landscape of machine\nlearning, yet current benchmarks often fall short in capturing the diverse\nbehavior of these models in real-world applications. A benchmark's usefulness\nis determined by its ability to clearly differentiate between models of varying\ncapabilities (separability) and closely align with human preferences. Existing\nframeworks like Alpaca-Eval 2.0 LC\n\\cite{dubois2024lengthcontrolledalpacaevalsimpleway} and Arena-Hard v0.1\n\\cite{li2024crowdsourced} are limited by their focus on general-purpose queries\nand lack of diversity across domains such as law, medicine, and multilingual\ncontexts. In this paper, we address these limitations by introducing a novel\ndata pipeline that curates diverse, domain-specific evaluation sets tailored\nfor LLM-as-a-Judge frameworks. Our approach leverages a combination of manual\ncuration, semi-supervised learning to generate clusters, and stratified\nsampling to ensure balanced representation across a wide range of domains and\nlanguages. The resulting evaluation set, which includes 1573 samples across 14\ncategories, demonstrates high separability (84\\%) across ten top-ranked models,\nand agreement (84\\%) with Chatbot Arena and (0.915) Spearman correlation. The\nagreement values are 9\\% better than Arena Hard and 20\\% better than AlpacaEval\n2.0 LC, while the Spearman coefficient is 0.7 more than the next best\nbenchmark, showcasing a significant improvement in the usefulness of the\nbenchmark. We further provide an open-source evaluation tool that enables\nfine-grained analysis of model performance across user-defined categories,\noffering valuable insights for practitioners. This work contributes to the\nongoing effort to enhance the transparency, diversity, and effectiveness of LLM\nevaluation methodologies."
                },
                "authors": [
                    {
                        "name": "Ravi Raju"
                    },
                    {
                        "name": "Swayambhoo Jain"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Urmish Thakkar"
                    }
                ],
                "author_detail": {
                    "name": "Urmish Thakkar"
                },
                "author": "Urmish Thakkar",
                "arxiv_comment": "14 pages, 8 figures, Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08660v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08660v2",
                "updated": "2024-08-16T15:33:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    33,
                    23,
                    4,
                    229,
                    0
                ],
                "published": "2024-06-12T21:46:13Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    21,
                    46,
                    13,
                    2,
                    164,
                    0
                ],
                "title": "Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot\n  Generative AI Models in Text Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot\n  Generative AI Models in Text Classification"
                },
                "summary": "Generative AI offers a simple, prompt-based alternative to fine-tuning\nsmaller BERT-style LLMs for text classification tasks. This promises to\neliminate the need for manually labeled training data and task-specific model\ntraining. However, it remains an open question whether tools like ChatGPT can\ndeliver on this promise. In this paper, we show that smaller, fine-tuned LLMs\n(still) consistently and significantly outperform larger, zero-shot prompted\nmodels in text classification. We compare three major generative AI models\n(ChatGPT with GPT-3.5/GPT-4 and Claude Opus) with several fine-tuned LLMs\nacross a diverse set of classification tasks (sentiment, approval/disapproval,\nemotions, party positions) and text categories (news, tweets, speeches). We\nfind that fine-tuning with application-specific training data achieves superior\nperformance in all cases. To make this approach more accessible to a broader\naudience, we provide an easy-to-use toolkit alongside this paper. Our toolkit,\naccompanied by non-technical step-by-step guidance, enables users to select and\nfine-tune BERT-like LLMs for any classification task with minimal technical and\ncomputational effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI offers a simple, prompt-based alternative to fine-tuning\nsmaller BERT-style LLMs for text classification tasks. This promises to\neliminate the need for manually labeled training data and task-specific model\ntraining. However, it remains an open question whether tools like ChatGPT can\ndeliver on this promise. In this paper, we show that smaller, fine-tuned LLMs\n(still) consistently and significantly outperform larger, zero-shot prompted\nmodels in text classification. We compare three major generative AI models\n(ChatGPT with GPT-3.5/GPT-4 and Claude Opus) with several fine-tuned LLMs\nacross a diverse set of classification tasks (sentiment, approval/disapproval,\nemotions, party positions) and text categories (news, tweets, speeches). We\nfind that fine-tuning with application-specific training data achieves superior\nperformance in all cases. To make this approach more accessible to a broader\naudience, we provide an easy-to-use toolkit alongside this paper. Our toolkit,\naccompanied by non-technical step-by-step guidance, enables users to select and\nfine-tune BERT-like LLMs for any classification task with minimal technical and\ncomputational effort."
                },
                "authors": [
                    {
                        "name": "Martin Juan José Bucher"
                    },
                    {
                        "name": "Marco Martini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Martini"
                },
                "author": "Marco Martini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08660v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08660v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14322v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14322v3",
                "updated": "2024-08-16T15:02:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    2,
                    45,
                    4,
                    229,
                    0
                ],
                "published": "2024-06-20T13:54:32Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    13,
                    54,
                    32,
                    3,
                    172,
                    0
                ],
                "title": "Mind the Privacy Unit! User-Level Differential Privacy for Language\n  Model Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Privacy Unit! User-Level Differential Privacy for Language\n  Model Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have emerged as powerful tools for tackling\ncomplex tasks across diverse domains, but they also raise privacy concerns when\nfine-tuned on sensitive data due to potential memorization. While differential\nprivacy (DP) offers a promising solution by ensuring models are 'almost\nindistinguishable' with or without any particular privacy unit, current\nevaluations on LLMs mostly treat each example (text record) as the privacy\nunit. This leads to uneven user privacy guarantees when contributions per user\nvary. We therefore study user-level DP motivated by applications where it\nnecessary to ensure uniform privacy protection across users. We present a\nsystematic evaluation of user-level DP for LLM fine-tuning on natural language\ngeneration tasks. Focusing on two mechanisms for achieving user-level DP\nguarantees, Group Privacy and User-wise DP-SGD, we investigate design choices\nlike data selection strategies and parameter tuning for the best\nprivacy-utility tradeoff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have emerged as powerful tools for tackling\ncomplex tasks across diverse domains, but they also raise privacy concerns when\nfine-tuned on sensitive data due to potential memorization. While differential\nprivacy (DP) offers a promising solution by ensuring models are 'almost\nindistinguishable' with or without any particular privacy unit, current\nevaluations on LLMs mostly treat each example (text record) as the privacy\nunit. This leads to uneven user privacy guarantees when contributions per user\nvary. We therefore study user-level DP motivated by applications where it\nnecessary to ensure uniform privacy protection across users. We present a\nsystematic evaluation of user-level DP for LLM fine-tuning on natural language\ngeneration tasks. Focusing on two mechanisms for achieving user-level DP\nguarantees, Group Privacy and User-wise DP-SGD, we investigate design choices\nlike data selection strategies and parameter tuning for the best\nprivacy-utility tradeoff."
                },
                "authors": [
                    {
                        "name": "Lynn Chua"
                    },
                    {
                        "name": "Badih Ghazi"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Pritish Kamath"
                    },
                    {
                        "name": "Ravi Kumar"
                    },
                    {
                        "name": "Daogao Liu"
                    },
                    {
                        "name": "Pasin Manurangsi"
                    },
                    {
                        "name": "Amer Sinha"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chiyuan Zhang"
                },
                "author": "Chiyuan Zhang",
                "arxiv_comment": "Published as a conference paper at COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14322v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14322v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14267v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14267v3",
                "updated": "2024-08-16T14:56:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    56,
                    36,
                    4,
                    229,
                    0
                ],
                "published": "2024-01-25T16:01:49Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    16,
                    1,
                    49,
                    3,
                    25,
                    0
                ],
                "title": "Transformers and Cortical Waves: Encoders for Pulling In Context Across\n  Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers and Cortical Waves: Encoders for Pulling In Context Across\n  Time"
                },
                "summary": "The capabilities of transformer networks such as ChatGPT and other Large\nLanguage Models (LLMs) have captured the world's attention. The crucial\ncomputational mechanism underlying their performance relies on transforming a\ncomplete input sequence - for example, all the words in a sentence - into a\nlong \"encoding vector\" that allows transformers to learn long-range temporal\ndependencies in naturalistic sequences. Specifically, \"self-attention\" applied\nto this encoding vector enhances temporal context in transformers by computing\nassociations between pairs of words in the input sequence. We suggest that\nwaves of neural activity traveling across single cortical areas or multiple\nregions at the whole-brain scale could implement a similar encoding principle.\nBy encapsulating recent input history into a single spatial pattern at each\nmoment in time, cortical waves may enable temporal context to be extracted from\nsequences of sensory inputs, the same computational principle used in\ntransformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities of transformer networks such as ChatGPT and other Large\nLanguage Models (LLMs) have captured the world's attention. The crucial\ncomputational mechanism underlying their performance relies on transforming a\ncomplete input sequence - for example, all the words in a sentence - into a\nlong \"encoding vector\" that allows transformers to learn long-range temporal\ndependencies in naturalistic sequences. Specifically, \"self-attention\" applied\nto this encoding vector enhances temporal context in transformers by computing\nassociations between pairs of words in the input sequence. We suggest that\nwaves of neural activity traveling across single cortical areas or multiple\nregions at the whole-brain scale could implement a similar encoding principle.\nBy encapsulating recent input history into a single spatial pattern at each\nmoment in time, cortical waves may enable temporal context to be extracted from\nsequences of sensory inputs, the same computational principle used in\ntransformers."
                },
                "authors": [
                    {
                        "name": "Lyle Muller"
                    },
                    {
                        "name": "Patricia S. Churchland"
                    },
                    {
                        "name": "Terrence J. Sejnowski"
                    }
                ],
                "author_detail": {
                    "name": "Terrence J. Sejnowski"
                },
                "author": "Terrence J. Sejnowski",
                "arxiv_comment": "27 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14267v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14267v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08782v1",
                "updated": "2024-08-16T14:54:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    54,
                    41,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T14:54:41Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    54,
                    41,
                    4,
                    229,
                    0
                ],
                "title": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics"
                },
                "summary": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nPrevious efforts have focused on developing modular dialogue systems that treat\nsocio-emotional strategy prediction as an auxiliary task and generate\nstrategy-conditioned responses with customized decoders. Recently, with\nadvancements in large language models (LLMs), end-to-end dialogue agents\nwithout explicit socio-emotional strategy prediction steps have become\nprevalent. However, despite their excellence in language generation, recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy predictor,\nEmoDynamiX, which models the discourse dynamics between user emotions and\nsystem strategies using a heterogeneous graph. Additionally, we make use of the\nEmotion Recognition in Conversations (ERC) task and design a flexible\nmixed-emotion module to capture fine-grained emotional states of the user.\nExperimental results on two ESC datasets show EmoDynamiX outperforms previous\nstate-of-the-art methods with a significant margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nPrevious efforts have focused on developing modular dialogue systems that treat\nsocio-emotional strategy prediction as an auxiliary task and generate\nstrategy-conditioned responses with customized decoders. Recently, with\nadvancements in large language models (LLMs), end-to-end dialogue agents\nwithout explicit socio-emotional strategy prediction steps have become\nprevalent. However, despite their excellence in language generation, recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy predictor,\nEmoDynamiX, which models the discourse dynamics between user emotions and\nsystem strategies using a heterogeneous graph. Additionally, we make use of the\nEmotion Recognition in Conversations (ERC) task and design a flexible\nmixed-emotion module to capture fine-grained emotional states of the user.\nExperimental results on two ESC datasets show EmoDynamiX outperforms previous\nstate-of-the-art methods with a significant margin."
                },
                "authors": [
                    {
                        "name": "Chenwei Wan"
                    },
                    {
                        "name": "Matthieu Labeau"
                    },
                    {
                        "name": "Chloé Clavel"
                    }
                ],
                "author_detail": {
                    "name": "Chloé Clavel"
                },
                "author": "Chloé Clavel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08781v1",
                "updated": "2024-08-16T14:49:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    49,
                    35,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T14:49:35Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    49,
                    35,
                    4,
                    229,
                    0
                ],
                "title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation\n  Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation\n  Instructions"
                },
                "summary": "LLMs-as-a-judge is a recently popularized method which replaces human\njudgements in task evaluation (Zheng et al. 2024) with automatic evaluation\nusing LLMs. Due to widespread use of RLHF (Reinforcement Learning from Human\nFeedback), state-of-the-art LLMs like GPT4 and Llama3 are expected to have\nstrong alignment with human preferences when prompted for a quality judgement,\nsuch as the coherence of a text. While this seems beneficial, it is not clear\nwhether the assessments by an LLM-as-a-judge constitute only an evaluation\nbased on the instructions in the prompts, or reflect its preference for\nhigh-quality data similar to its fine-tune data. To investigate how much\ninfluence prompting the LLMs-as-a-judge has on the alignment of AI judgements\nto human judgements, we analyze prompts with increasing levels of instructions\nabout the target quality of an evaluation, for several LLMs-as-a-judge.\nFurther, we compare to a prompt-free method using model perplexity as a quality\nmeasure instead. We aggregate a taxonomy of quality criteria commonly used\nacross state-of-the-art evaluations with LLMs and provide this as a rigorous\nbenchmark of models as judges. Overall, we show that the LLMs-as-a-judge\nbenefit only little from highly detailed instructions in prompts and that\nperplexity can sometimes align better with human judgements than prompting,\nespecially on textual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs-as-a-judge is a recently popularized method which replaces human\njudgements in task evaluation (Zheng et al. 2024) with automatic evaluation\nusing LLMs. Due to widespread use of RLHF (Reinforcement Learning from Human\nFeedback), state-of-the-art LLMs like GPT4 and Llama3 are expected to have\nstrong alignment with human preferences when prompted for a quality judgement,\nsuch as the coherence of a text. While this seems beneficial, it is not clear\nwhether the assessments by an LLM-as-a-judge constitute only an evaluation\nbased on the instructions in the prompts, or reflect its preference for\nhigh-quality data similar to its fine-tune data. To investigate how much\ninfluence prompting the LLMs-as-a-judge has on the alignment of AI judgements\nto human judgements, we analyze prompts with increasing levels of instructions\nabout the target quality of an evaluation, for several LLMs-as-a-judge.\nFurther, we compare to a prompt-free method using model perplexity as a quality\nmeasure instead. We aggregate a taxonomy of quality criteria commonly used\nacross state-of-the-art evaluations with LLMs and provide this as a rigorous\nbenchmark of models as judges. Overall, we show that the LLMs-as-a-judge\nbenefit only little from highly detailed instructions in prompts and that\nperplexity can sometimes align better with human judgements than prompting,\nespecially on textual quality."
                },
                "authors": [
                    {
                        "name": "Bhuvanashree Murugadoss"
                    },
                    {
                        "name": "Christian Poelitz"
                    },
                    {
                        "name": "Ian Drosos"
                    },
                    {
                        "name": "Vu Le"
                    },
                    {
                        "name": "Nick McKenna"
                    },
                    {
                        "name": "Carina Suzana Negreanu"
                    },
                    {
                        "name": "Chris Parnin"
                    },
                    {
                        "name": "Advait Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Advait Sarkar"
                },
                "author": "Advait Sarkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08780v1",
                "updated": "2024-08-16T14:49:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    49,
                    4,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T14:49:04Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    49,
                    4,
                    4,
                    229,
                    0
                ],
                "title": "Large Language Models Might Not Care What You Are Saying: Prompt Format\n  Beats Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Might Not Care What You Are Saying: Prompt Format\n  Beats Descriptions"
                },
                "summary": "With the help of in-context learning (ICL), large language models (LLMs) have\nachieved impressive performance across various tasks. However, the function of\ndescriptive instructions during ICL remains under-explored. In this work, we\npropose an ensemble prompt framework to describe the selection criteria of\nmultiple in-context examples, and preliminary experiments on machine\ntranslation (MT) across six translation directions confirm that this framework\nboosts ICL perfromance. But to our surprise, LLMs might not necessarily care\nwhat the descriptions actually say, and the performance gain is primarily\ncaused by the ensemble format, since the framework could lead to improvement\neven with random descriptive nouns. We further apply this new ensemble prompt\non a range of commonsense, math, logical reasoning and hallucination tasks with\nthree LLMs and achieve promising results, suggesting again that designing a\nproper prompt format would be much more effective and efficient than paying\neffort into specific descriptions. Our code will be publicly available once\nthis paper is published.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the help of in-context learning (ICL), large language models (LLMs) have\nachieved impressive performance across various tasks. However, the function of\ndescriptive instructions during ICL remains under-explored. In this work, we\npropose an ensemble prompt framework to describe the selection criteria of\nmultiple in-context examples, and preliminary experiments on machine\ntranslation (MT) across six translation directions confirm that this framework\nboosts ICL perfromance. But to our surprise, LLMs might not necessarily care\nwhat the descriptions actually say, and the performance gain is primarily\ncaused by the ensemble format, since the framework could lead to improvement\neven with random descriptive nouns. We further apply this new ensemble prompt\non a range of commonsense, math, logical reasoning and hallucination tasks with\nthree LLMs and achieve promising results, suggesting again that designing a\nproper prompt format would be much more effective and efficient than paying\neffort into specific descriptions. Our code will be publicly available once\nthis paper is published."
                },
                "authors": [
                    {
                        "name": "Chenming Tang"
                    },
                    {
                        "name": "Zhixiang Wang"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "arxiv_comment": "10 pages, 6 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08779v1",
                "updated": "2024-08-16T14:43:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    43,
                    15,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T14:43:15Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    43,
                    15,
                    4,
                    229,
                    0
                ],
                "title": "DAC: Decomposed Automation Correction for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAC: Decomposed Automation Correction for Text-to-SQL"
                },
                "summary": "Text-to-SQL is an important task that helps people obtain information from\ndatabases by automatically generating SQL queries. Considering the brilliant\nperformance, approaches based on Large Language Models (LLMs) become the\nmainstream for text-to-SQL. Among these approaches, automated correction is an\neffective approach that further enhances performance by correcting the mistakes\nin the generated results. The existing correction methods require LLMs to\ndirectly correct with generated SQL, while previous research shows that LLMs do\nnot know how to detect mistakes, leading to poor performance. Therefore, in\nthis paper, we propose to employ the decomposed correction to enhance\ntext-to-SQL performance. We first demonstrate that decomposed correction\noutperforms direct correction since detecting and fixing mistakes with the\nresults of the decomposed sub-tasks is easier than with SQL. Based on this\nanalysis, we introduce Decomposed Automation Correction (DAC), which corrects\nSQL by decomposing text-to-SQL into entity linking and skeleton parsing. DAC\nfirst generates the entity and skeleton corresponding to the question and then\ncompares the differences between the initial SQL and the generated entities and\nskeleton as feedback for correction. Experimental results show that our method\nimproves performance by $3.7\\%$ on average of Spider, Bird, and KaggleDBQA\ncompared with the baseline method, demonstrating the effectiveness of DAC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is an important task that helps people obtain information from\ndatabases by automatically generating SQL queries. Considering the brilliant\nperformance, approaches based on Large Language Models (LLMs) become the\nmainstream for text-to-SQL. Among these approaches, automated correction is an\neffective approach that further enhances performance by correcting the mistakes\nin the generated results. The existing correction methods require LLMs to\ndirectly correct with generated SQL, while previous research shows that LLMs do\nnot know how to detect mistakes, leading to poor performance. Therefore, in\nthis paper, we propose to employ the decomposed correction to enhance\ntext-to-SQL performance. We first demonstrate that decomposed correction\noutperforms direct correction since detecting and fixing mistakes with the\nresults of the decomposed sub-tasks is easier than with SQL. Based on this\nanalysis, we introduce Decomposed Automation Correction (DAC), which corrects\nSQL by decomposing text-to-SQL into entity linking and skeleton parsing. DAC\nfirst generates the entity and skeleton corresponding to the question and then\ncompares the differences between the initial SQL and the generated entities and\nskeleton as feedback for correction. Experimental results show that our method\nimproves performance by $3.7\\%$ on average of Spider, Bird, and KaggleDBQA\ncompared with the baseline method, demonstrating the effectiveness of DAC."
                },
                "authors": [
                    {
                        "name": "Dingzirui Wang"
                    },
                    {
                        "name": "Longxu Dou"
                    },
                    {
                        "name": "Xuanliang Zhang"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08769v1",
                "updated": "2024-08-16T14:23:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    23,
                    59,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T14:23:59Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    23,
                    59,
                    4,
                    229,
                    0
                ],
                "title": "Lower Layer Matters: Alleviating Hallucination via Multi-Layer Fusion\n  Contrastive Decoding with Truthfulness Refocused",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lower Layer Matters: Alleviating Hallucination via Multi-Layer Fusion\n  Contrastive Decoding with Truthfulness Refocused"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks, yet they occasionally tend to yield\ncontent that factually inaccurate or discordant with the expected output, a\nphenomenon empirically referred to as \"hallucination\". To tackle this issue,\nrecent works have investigated contrastive decoding between the original model\nand an amateur model with induced hallucination, which has shown promising\nresults. Nonetheless, this method may undermine the output distribution of the\noriginal LLM caused by its coarse contrast and simplistic subtraction\noperation, potentially leading to errors in certain cases. In this paper, we\nintroduce a novel contrastive decoding framework termed LOL (LOwer Layer\nMatters). Our approach involves concatenating the contrastive decoding of both\nthe final and lower layers between the original model and the amateur model,\nthereby achieving multi-layer fusion to aid in the mitigation of hallucination.\nAdditionally, we incorporate a truthfulness refocused module that leverages\ncontextual guidance to enhance factual encoding, further capturing truthfulness\nduring contrastive decoding. Extensive experiments conducted on two publicly\navailable datasets illustrate that our proposed LOL framework can substantially\nalleviate hallucination while surpassing existing baselines in most cases.\nCompared with the best baseline, we improve by average 4.5 points on all\nmetrics of TruthfulQA. The source code is coming soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks, yet they occasionally tend to yield\ncontent that factually inaccurate or discordant with the expected output, a\nphenomenon empirically referred to as \"hallucination\". To tackle this issue,\nrecent works have investigated contrastive decoding between the original model\nand an amateur model with induced hallucination, which has shown promising\nresults. Nonetheless, this method may undermine the output distribution of the\noriginal LLM caused by its coarse contrast and simplistic subtraction\noperation, potentially leading to errors in certain cases. In this paper, we\nintroduce a novel contrastive decoding framework termed LOL (LOwer Layer\nMatters). Our approach involves concatenating the contrastive decoding of both\nthe final and lower layers between the original model and the amateur model,\nthereby achieving multi-layer fusion to aid in the mitigation of hallucination.\nAdditionally, we incorporate a truthfulness refocused module that leverages\ncontextual guidance to enhance factual encoding, further capturing truthfulness\nduring contrastive decoding. Extensive experiments conducted on two publicly\navailable datasets illustrate that our proposed LOL framework can substantially\nalleviate hallucination while surpassing existing baselines in most cases.\nCompared with the best baseline, we improve by average 4.5 points on all\nmetrics of TruthfulQA. The source code is coming soon."
                },
                "authors": [
                    {
                        "name": "Dingwei Chen"
                    },
                    {
                        "name": "Feiteng Fang"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Feng Liang"
                    },
                    {
                        "name": "Ruifeng Xu"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Chengming Li"
                    }
                ],
                "author_detail": {
                    "name": "Chengming Li"
                },
                "author": "Chengming Li",
                "arxiv_comment": "9 pages, 4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08765v1",
                "updated": "2024-08-16T14:18:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    18,
                    37,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T14:18:37Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    18,
                    37,
                    4,
                    229,
                    0
                ],
                "title": "Rethinking Generative Semantic Communication for Multi-User Systems with\n  Multi-Modal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Generative Semantic Communication for Multi-User Systems with\n  Multi-Modal LLM"
                },
                "summary": "The surge in connected devices in 6G with typical massive access scenarios,\nsuch as smart agriculture, and smart cities, poses significant challenges to\nunsustainable traditional communication with limited radio resources and\nalready high system complexity. Fortunately, the booming artificial\nintelligence technology and the growing computational power of devices offer a\npromising 6G enabler: semantic communication (SemCom). However, existing deep\nlearning-based SemCom paradigms struggle to extend to multi-user scenarios due\nto their rigid end-to-end training approach. Consequently, to truly empower 6G\nnetworks with this critical technology, this article rethinks generative SemCom\nfor multi-user system with multi-modal large language model (MLLM), and propose\na novel framework called \"M2GSC\". In this framework, the MLLM, which serves as\nshared knowledge base (SKB), plays three critical roles for complex tasks,\nspawning a series of benefits such as semantic encoding standardization and\nsemantic decoding personalization. Meanwhile, to enhance the performance of\nM2GSC framework and to advance its implementation in 6G, we highlight three\nresearch directions on M2GSC framework, namely, upgrading SKB to closed loop\nagent, adaptive semantic encoding offloading, and streamlined semantic decoding\noffloading. Finally, a case study is conducted to demonstrate the preliminary\nvalidation on the effectiveness of the M2GSC framework in terms of streamlined\ndecoding offloading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surge in connected devices in 6G with typical massive access scenarios,\nsuch as smart agriculture, and smart cities, poses significant challenges to\nunsustainable traditional communication with limited radio resources and\nalready high system complexity. Fortunately, the booming artificial\nintelligence technology and the growing computational power of devices offer a\npromising 6G enabler: semantic communication (SemCom). However, existing deep\nlearning-based SemCom paradigms struggle to extend to multi-user scenarios due\nto their rigid end-to-end training approach. Consequently, to truly empower 6G\nnetworks with this critical technology, this article rethinks generative SemCom\nfor multi-user system with multi-modal large language model (MLLM), and propose\na novel framework called \"M2GSC\". In this framework, the MLLM, which serves as\nshared knowledge base (SKB), plays three critical roles for complex tasks,\nspawning a series of benefits such as semantic encoding standardization and\nsemantic decoding personalization. Meanwhile, to enhance the performance of\nM2GSC framework and to advance its implementation in 6G, we highlight three\nresearch directions on M2GSC framework, namely, upgrading SKB to closed loop\nagent, adaptive semantic encoding offloading, and streamlined semantic decoding\noffloading. Finally, a case study is conducted to demonstrate the preliminary\nvalidation on the effectiveness of the M2GSC framework in terms of streamlined\ndecoding offloading."
                },
                "authors": [
                    {
                        "name": "Wanting Yang"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Shiwen Mao"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    },
                    {
                        "name": "Ping Zhang"
                    },
                    {
                        "name": "Merouane Debbah"
                    },
                    {
                        "name": "Rahim Tafazolli"
                    }
                ],
                "author_detail": {
                    "name": "Rahim Tafazolli"
                },
                "author": "Rahim Tafazolli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08724v1",
                "updated": "2024-08-16T13:11:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    13,
                    11,
                    53,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T13:11:53Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    13,
                    11,
                    53,
                    4,
                    229,
                    0
                ],
                "title": "ChatZero:Zero-shot Cross-Lingual Dialogue Generation via Pseudo-Target\n  Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatZero:Zero-shot Cross-Lingual Dialogue Generation via Pseudo-Target\n  Language"
                },
                "summary": "Although large language models(LLMs) show amazing capabilities, among various\nexciting applications discovered for LLMs fall short in other low-resource\nlanguages. Besides, most existing methods depend on large-scale dialogue\ncorpora and thus building systems for dialogue generation in a zero-shot\nscenario remains a considerable challenge. To address this challenge, we\npropose a novel end-to-end zero-shot dialogue generation model ChatZero based\non cross-lingual code-switching method. First, we construct code-switching\nlanguage and pseudo-target language with placeholders. Then for cross-lingual\nsemantic transfer, we employ unsupervised contrastive learning to minimize the\nsemantics gap of the source language, code-switching language, and\npseudo-target language that are mutually positive examples in the high\ndimensional semantic space. Experiments on the multilingual DailyDialog and\nDSTC7-AVSD datasets demonstrate that ChatZero can achieve more than 90\\% of the\noriginal performance under the zero-shot case compared to supervised learning,\nand achieve state-of-the-art performance compared with other baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models(LLMs) show amazing capabilities, among various\nexciting applications discovered for LLMs fall short in other low-resource\nlanguages. Besides, most existing methods depend on large-scale dialogue\ncorpora and thus building systems for dialogue generation in a zero-shot\nscenario remains a considerable challenge. To address this challenge, we\npropose a novel end-to-end zero-shot dialogue generation model ChatZero based\non cross-lingual code-switching method. First, we construct code-switching\nlanguage and pseudo-target language with placeholders. Then for cross-lingual\nsemantic transfer, we employ unsupervised contrastive learning to minimize the\nsemantics gap of the source language, code-switching language, and\npseudo-target language that are mutually positive examples in the high\ndimensional semantic space. Experiments on the multilingual DailyDialog and\nDSTC7-AVSD datasets demonstrate that ChatZero can achieve more than 90\\% of the\noriginal performance under the zero-shot case compared to supervised learning,\nand achieve state-of-the-art performance compared with other baselines."
                },
                "authors": [
                    {
                        "name": "Yongkang Liu"
                    },
                    {
                        "name": "Feng Shi"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "arxiv_comment": "ECAI2024",
                "arxiv_journal_ref": "ECAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08707v1",
                "updated": "2024-08-16T12:40:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    40,
                    1,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T12:40:01Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    40,
                    1,
                    4,
                    229,
                    0
                ],
                "title": "Beam Prediction based on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beam Prediction based on Large Language Models"
                },
                "summary": "Millimeter-wave (mmWave) communication is promising for next-generation\nwireless networks but suffers from significant path loss, requiring extensive\nantenna arrays and frequent beam training. Traditional deep learning models,\nsuch as long short-term memory (LSTM), enhance beam tracking accuracy however\nare limited by poor robustness and generalization. In this letter, we use large\nlanguage models (LLMs) to improve the robustness of beam prediction. By\nconverting time series data into text-based representations and employing the\nPrompt-as-Prefix (PaP) technique for contextual enrichment, our approach\nunleashes the strength of LLMs for time series forecasting. Simulation results\ndemonstrate that our LLM-based method offers superior robustness and\ngeneralization compared to LSTM-based models, showcasing the potential of LLMs\nin wireless communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Millimeter-wave (mmWave) communication is promising for next-generation\nwireless networks but suffers from significant path loss, requiring extensive\nantenna arrays and frequent beam training. Traditional deep learning models,\nsuch as long short-term memory (LSTM), enhance beam tracking accuracy however\nare limited by poor robustness and generalization. In this letter, we use large\nlanguage models (LLMs) to improve the robustness of beam prediction. By\nconverting time series data into text-based representations and employing the\nPrompt-as-Prefix (PaP) technique for contextual enrichment, our approach\nunleashes the strength of LLMs for time series forecasting. Simulation results\ndemonstrate that our LLM-based method offers superior robustness and\ngeneralization compared to LSTM-based models, showcasing the potential of LLMs\nin wireless communications."
                },
                "authors": [
                    {
                        "name": "Yucheng Sheng"
                    },
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Le Liang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Shi Jin"
                    },
                    {
                        "name": "Geoffrey Ye Li"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Ye Li"
                },
                "author": "Geoffrey Ye Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06249v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06249v3",
                "updated": "2024-08-16T12:30:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    30,
                    7,
                    4,
                    229,
                    0
                ],
                "published": "2024-03-10T16:22:20Z",
                "published_parsed": [
                    2024,
                    3,
                    10,
                    16,
                    22,
                    20,
                    6,
                    70,
                    0
                ],
                "title": "No Language is an Island: Unifying Chinese and English in Financial\n  Large Language Models, Instruction Data, and Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Language is an Island: Unifying Chinese and English in Financial\n  Large Language Models, Instruction Data, and Benchmarks"
                },
                "summary": "While the progression of Large Language Models (LLMs) has notably propelled\nfinancial analysis, their application has largely been confined to singular\nlanguage realms, leaving untapped the potential of bilingual Chinese-English\ncapacity. To bridge this chasm, we introduce ICE-PIXIU, seamlessly amalgamating\nthe ICE-INTENT model and ICE-FLARE benchmark for bilingual financial analysis.\nICE-PIXIU uniquely integrates a spectrum of Chinese tasks, alongside translated\nand original English datasets, enriching the breadth and depth of bilingual\nfinancial modeling. It provides unrestricted access to diverse model variants,\na substantial compilation of diverse cross-lingual and multi-modal instruction\ndata, and an evaluation benchmark with expert annotations, comprising 10 NLP\ntasks, 20 bilingual specific tasks, totaling 95k datasets. Our thorough\nevaluation emphasizes the advantages of incorporating these bilingual datasets,\nespecially in translation tasks and utilizing original English data, enhancing\nboth linguistic flexibility and analytical acuity in financial contexts.\nNotably, ICE-INTENT distinguishes itself by showcasing significant enhancements\nover conventional LLMs and existing financial LLMs in bilingual milieus,\nunderscoring the profound impact of robust bilingual data on the accuracy and\nefficacy of financial NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the progression of Large Language Models (LLMs) has notably propelled\nfinancial analysis, their application has largely been confined to singular\nlanguage realms, leaving untapped the potential of bilingual Chinese-English\ncapacity. To bridge this chasm, we introduce ICE-PIXIU, seamlessly amalgamating\nthe ICE-INTENT model and ICE-FLARE benchmark for bilingual financial analysis.\nICE-PIXIU uniquely integrates a spectrum of Chinese tasks, alongside translated\nand original English datasets, enriching the breadth and depth of bilingual\nfinancial modeling. It provides unrestricted access to diverse model variants,\na substantial compilation of diverse cross-lingual and multi-modal instruction\ndata, and an evaluation benchmark with expert annotations, comprising 10 NLP\ntasks, 20 bilingual specific tasks, totaling 95k datasets. Our thorough\nevaluation emphasizes the advantages of incorporating these bilingual datasets,\nespecially in translation tasks and utilizing original English data, enhancing\nboth linguistic flexibility and analytical acuity in financial contexts.\nNotably, ICE-INTENT distinguishes itself by showcasing significant enhancements\nover conventional LLMs and existing financial LLMs in bilingual milieus,\nunderscoring the profound impact of robust bilingual data on the accuracy and\nefficacy of financial NLP."
                },
                "authors": [
                    {
                        "name": "Gang Hu"
                    },
                    {
                        "name": "Ke Qin"
                    },
                    {
                        "name": "Chenhan Yuan"
                    },
                    {
                        "name": "Min Peng"
                    },
                    {
                        "name": "Alejandro Lopez-Lira"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Qianqian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qianqian Xie"
                },
                "author": "Qianqian Xie",
                "arxiv_comment": "19 pages, 3 figures, 12 tables, including Appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06249v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06249v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08699v1",
                "updated": "2024-08-16T12:26:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    26,
                    36,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T12:26:36Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    26,
                    36,
                    4,
                    229,
                    0
                ],
                "title": "RBLA: Rank-Based-LoRA-Aggregation for Fine-tuning Heterogeneous Models\n  in FLaaS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RBLA: Rank-Based-LoRA-Aggregation for Fine-tuning Heterogeneous Models\n  in FLaaS"
                },
                "summary": "Federated Learning (FL) is a promising privacy-aware distributed learning\nframework that can be deployed on various devices, such as mobile phones,\ndesktops, and devices equipped with CPUs or GPUs. In the context of\nserver-based Federated Learning as a Service (FLaas), FL enables the central\nserver to coordinate the training process across multiple devices without\ndirect access to the local data, thereby enhancing privacy and data security.\nLow-Rank Adaptation (LoRA) is a method that fine-tunes models efficiently by\nfocusing on a low-dimensional subspace of the model's parameters. This approach\nsignificantly reduces computational and memory costs compared to fine-tuning\nall parameters from scratch. When integrated with FL, especially in a FLaas\nenvironment, LoRA allows for flexible and efficient deployment across diverse\nhardware with varying computational capabilities by adjusting the local model's\nrank. However, in LoRA-enabled FL, different clients may train models with\nvarying ranks, which poses a challenge for model aggregation on the server.\nCurrent methods of aggregating models of different ranks require padding\nweights to a uniform shape, which can degrade the global model's performance.\nTo address this issue, we propose Rank-Based LoRA Aggregation (RBLA), a novel\nmodel aggregation method designed for heterogeneous LoRA structures. RBLA\npreserves key features across models with different ranks. This paper analyzes\nthe issues with current padding methods that reshape models for aggregation in\na FLaas environment. Then, we introduce RBLA, a rank-based aggregation method\nthat maintains both low-rank and high-rank features. Finally, we demonstrate\nthe effectiveness of RBLA through comparative experiments with state-of-the-art\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is a promising privacy-aware distributed learning\nframework that can be deployed on various devices, such as mobile phones,\ndesktops, and devices equipped with CPUs or GPUs. In the context of\nserver-based Federated Learning as a Service (FLaas), FL enables the central\nserver to coordinate the training process across multiple devices without\ndirect access to the local data, thereby enhancing privacy and data security.\nLow-Rank Adaptation (LoRA) is a method that fine-tunes models efficiently by\nfocusing on a low-dimensional subspace of the model's parameters. This approach\nsignificantly reduces computational and memory costs compared to fine-tuning\nall parameters from scratch. When integrated with FL, especially in a FLaas\nenvironment, LoRA allows for flexible and efficient deployment across diverse\nhardware with varying computational capabilities by adjusting the local model's\nrank. However, in LoRA-enabled FL, different clients may train models with\nvarying ranks, which poses a challenge for model aggregation on the server.\nCurrent methods of aggregating models of different ranks require padding\nweights to a uniform shape, which can degrade the global model's performance.\nTo address this issue, we propose Rank-Based LoRA Aggregation (RBLA), a novel\nmodel aggregation method designed for heterogeneous LoRA structures. RBLA\npreserves key features across models with different ranks. This paper analyzes\nthe issues with current padding methods that reshape models for aggregation in\na FLaas environment. Then, we introduce RBLA, a rank-based aggregation method\nthat maintains both low-rank and high-rank features. Finally, we demonstrate\nthe effectiveness of RBLA through comparative experiments with state-of-the-art\nmethods."
                },
                "authors": [
                    {
                        "name": "Shuaijun Chen"
                    },
                    {
                        "name": "Omid Tavallaie"
                    },
                    {
                        "name": "Niousha Nazemi"
                    },
                    {
                        "name": "Albert Y. Zomaya"
                    }
                ],
                "author_detail": {
                    "name": "Albert Y. Zomaya"
                },
                "author": "Albert Y. Zomaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06304v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06304v3",
                "updated": "2024-08-16T12:21:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    21,
                    50,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-12T17:17:16Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    17,
                    16,
                    0,
                    225,
                    0
                ],
                "title": "Control-Flow Attestation: Concepts, Solutions, and Open Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-Flow Attestation: Concepts, Solutions, and Open Challenges"
                },
                "summary": "Control-flow attestation unifies the worlds of control-flow integrity and\nplatform attestation by measuring and reporting a target's run-time behaviour\nto a verifier. Trust assurances in the target are provided by testing whether\nits execution follows an authorised control-flow path. The problem has been\nexplored in various settings, such as assessing the trustworthiness of\ncyber-physical systems, Internet of Things devices, cloud platforms, and many\nothers. Despite a significant number of proposals being made in recent years,\nthe area remains fragmented, addressing different adversarial behaviours,\nverification paradigms, and deployment challenges. In this paper, we present\nthe first survey of control-flow attestation, examining the core ideas and\nsolutions in state-of-the-art schemes. In total, we survey over 30 papers\npublished between 2016-2024, consolidate and compare their key features, and\npose several challenges and recommendations for future research in the area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow attestation unifies the worlds of control-flow integrity and\nplatform attestation by measuring and reporting a target's run-time behaviour\nto a verifier. Trust assurances in the target are provided by testing whether\nits execution follows an authorised control-flow path. The problem has been\nexplored in various settings, such as assessing the trustworthiness of\ncyber-physical systems, Internet of Things devices, cloud platforms, and many\nothers. Despite a significant number of proposals being made in recent years,\nthe area remains fragmented, addressing different adversarial behaviours,\nverification paradigms, and deployment challenges. In this paper, we present\nthe first survey of control-flow attestation, examining the core ideas and\nsolutions in state-of-the-art schemes. In total, we survey over 30 papers\npublished between 2016-2024, consolidate and compare their key features, and\npose several challenges and recommendations for future research in the area."
                },
                "authors": [
                    {
                        "name": "Zhanyu Sha"
                    },
                    {
                        "name": "Carlton Shepherd"
                    },
                    {
                        "name": "Amir Rafi"
                    },
                    {
                        "name": "Konstantinos Markantonakis"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Markantonakis"
                },
                "author": "Konstantinos Markantonakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06304v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06304v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08696v1",
                "updated": "2024-08-16T12:20:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    20,
                    56,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T12:20:56Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    20,
                    56,
                    4,
                    229,
                    0
                ],
                "title": "Turning Trash into Treasure: Accelerating Inference of Large Language\n  Models with Token Recycling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turning Trash into Treasure: Accelerating Inference of Large Language\n  Models with Token Recycling"
                },
                "summary": "The rapid growth in the parameters of large language models (LLMs) has made\ninference latency a fundamental bottleneck, limiting broader application of\nLLMs. Speculative decoding represents a lossless approach to accelerate\ninference through a guess-and-verify paradigm, leveraging the parallel\ncapabilities of modern hardware. Some speculative decoding methods rely on\nadditional structures to guess draft tokens, such as small models or\nparameter-efficient architectures, which need extra training before use.\nAlternatively, retrieval-based train-free techniques build libraries from\npre-existing corpora or by n-gram generation. However, they face challenges\nlike large storage requirements, time-consuming retrieval, and limited\nadaptability. Observing that candidate tokens generated during the decoding\nprocess are likely to reoccur in future sequences, we propose Token Recycling.\nThis approach stores candidate tokens in an adjacency matrix and employs a\nbreadth-first search (BFS)-like algorithm on the matrix to construct a draft\ntree. The tree is then validated through tree attention. New candidate tokens\nfrom the decoding process are then used to update the matrix. Token Recycling\nrequires \\textless2MB of additional storage and achieves approximately 2x\nspeedup across all sizes of LLMs. It significantly outperforms existing\ntrain-free methods by 30\\% and even a training method by 25\\%. It can be\ndirectly applied to any existing LLMs and tasks without the need for\nadaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth in the parameters of large language models (LLMs) has made\ninference latency a fundamental bottleneck, limiting broader application of\nLLMs. Speculative decoding represents a lossless approach to accelerate\ninference through a guess-and-verify paradigm, leveraging the parallel\ncapabilities of modern hardware. Some speculative decoding methods rely on\nadditional structures to guess draft tokens, such as small models or\nparameter-efficient architectures, which need extra training before use.\nAlternatively, retrieval-based train-free techniques build libraries from\npre-existing corpora or by n-gram generation. However, they face challenges\nlike large storage requirements, time-consuming retrieval, and limited\nadaptability. Observing that candidate tokens generated during the decoding\nprocess are likely to reoccur in future sequences, we propose Token Recycling.\nThis approach stores candidate tokens in an adjacency matrix and employs a\nbreadth-first search (BFS)-like algorithm on the matrix to construct a draft\ntree. The tree is then validated through tree attention. New candidate tokens\nfrom the decoding process are then used to update the matrix. Token Recycling\nrequires \\textless2MB of additional storage and achieves approximately 2x\nspeedup across all sizes of LLMs. It significantly outperforms existing\ntrain-free methods by 30\\% and even a training method by 25\\%. It can be\ndirectly applied to any existing LLMs and tasks without the need for\nadaptation."
                },
                "authors": [
                    {
                        "name": "Xianzhen Luo"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Zhiming Zhang"
                    },
                    {
                        "name": "Xuanyu Zhang"
                    },
                    {
                        "name": "Qing Yang"
                    },
                    {
                        "name": "Dongliang Xu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10160v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10160v6",
                "updated": "2024-08-16T12:20:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    20,
                    22,
                    4,
                    229,
                    0
                ],
                "published": "2024-04-15T22:18:50Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    22,
                    18,
                    50,
                    0,
                    106,
                    0
                ],
                "title": "Reinforcement Learning from Multi-role Debates as Feedback for Bias\n  Mitigation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Multi-role Debates as Feedback for Bias\n  Mitigation in LLMs"
                },
                "summary": "Bias in LLMs can harm user experience and societal outcomes. However, current\nbias mitigation methods often require intensive human feedback, lack\ntransferability to other topics or yield overconfident and random outputs. We\nfind that involving LLMs in role-playing scenario boosts their ability to\nrecognize and mitigate biases. Based on this, we propose Reinforcement Learning\nfrom Multi-role Debates as Feedback (RLDF), a novel approach for bias\nmitigation replacing human feedback in traditional RLHF. We utilize LLMs in\nmulti-role debates to create a dataset that includes both high-bias and\nlow-bias instances for training the reward model in reinforcement learning. Our\napproach comprises two modes: (1) self-reflection, where the same LLM\nparticipates in multi-role debates, and (2) teacher-student, where a more\nadvanced LLM like GPT-3.5-turbo guides the LLM to perform this task.\nExperimental results across different LLMs on BBQ and our datasets demonstrate\nthe effectiveness of our approach in bias mitigation. Our source code and\ndatasets are available at \\texttt{https://anonymous.4open.science/r/RLDF-E344}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in LLMs can harm user experience and societal outcomes. However, current\nbias mitigation methods often require intensive human feedback, lack\ntransferability to other topics or yield overconfident and random outputs. We\nfind that involving LLMs in role-playing scenario boosts their ability to\nrecognize and mitigate biases. Based on this, we propose Reinforcement Learning\nfrom Multi-role Debates as Feedback (RLDF), a novel approach for bias\nmitigation replacing human feedback in traditional RLHF. We utilize LLMs in\nmulti-role debates to create a dataset that includes both high-bias and\nlow-bias instances for training the reward model in reinforcement learning. Our\napproach comprises two modes: (1) self-reflection, where the same LLM\nparticipates in multi-role debates, and (2) teacher-student, where a more\nadvanced LLM like GPT-3.5-turbo guides the LLM to perform this task.\nExperimental results across different LLMs on BBQ and our datasets demonstrate\nthe effectiveness of our approach in bias mitigation. Our source code and\ndatasets are available at \\texttt{https://anonymous.4open.science/r/RLDF-E344}."
                },
                "authors": [
                    {
                        "name": "Ruoxi Cheng"
                    },
                    {
                        "name": "Haoxuan Ma"
                    },
                    {
                        "name": "Shuirong Cao"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Aihua Pei"
                    },
                    {
                        "name": "Zhiqiang Wang"
                    },
                    {
                        "name": "Pengliang Ji"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Jiaqi Huo"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Huo"
                },
                "author": "Jiaqi Huo",
                "arxiv_comment": "The first three authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10160v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10160v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08694v1",
                "updated": "2024-08-16T12:16:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    16,
                    59,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T12:16:59Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    16,
                    59,
                    4,
                    229,
                    0
                ],
                "title": "Quantifying the Effectiveness of Student Organization Activities using\n  Natural Language Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the Effectiveness of Student Organization Activities using\n  Natural Language Processing"
                },
                "summary": "Student extracurricular activities play an important role in enriching the\nstudents' educational experiences. With the increasing popularity of Machine\nLearning and Natural Language Processing, it becomes a logical step that\nincorporating ML-NLP in improving extracurricular activities is a potential\nfocus of study in Artificial Intelligence (AI). This research study aims to\ndevelop a machine learning workflow that will quantify the effectiveness of\nstudent-organized activities based on student emotional responses using\nsentiment analysis. The study uses the Bidirectional Encoder Representations\nfrom Transformers (BERT) Large Language Model (LLM) called via the\npysentimiento toolkit, as a Transformer pipeline in Hugging Face. A sample data\nset from Organization C, a Recognized Student Organization (RSO) of a higher\neducational institute in the Philippines, College X, was used to develop the\nworkflow. The workflow consisted of data preprocessing, key feature selection,\nLLM feature processing, and score aggregation, resulting in an Event Score for\neach data set. The results show that the BERT LLM can also be used effectively\nin analyzing sentiment beyond product reviews and post comments. For the\nstudent affairs offices of educational institutions, this study can provide a\npractical example of how NLP can be applied to real-world scenarios, showcasing\nthe potential impact of data-driven decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Student extracurricular activities play an important role in enriching the\nstudents' educational experiences. With the increasing popularity of Machine\nLearning and Natural Language Processing, it becomes a logical step that\nincorporating ML-NLP in improving extracurricular activities is a potential\nfocus of study in Artificial Intelligence (AI). This research study aims to\ndevelop a machine learning workflow that will quantify the effectiveness of\nstudent-organized activities based on student emotional responses using\nsentiment analysis. The study uses the Bidirectional Encoder Representations\nfrom Transformers (BERT) Large Language Model (LLM) called via the\npysentimiento toolkit, as a Transformer pipeline in Hugging Face. A sample data\nset from Organization C, a Recognized Student Organization (RSO) of a higher\neducational institute in the Philippines, College X, was used to develop the\nworkflow. The workflow consisted of data preprocessing, key feature selection,\nLLM feature processing, and score aggregation, resulting in an Event Score for\neach data set. The results show that the BERT LLM can also be used effectively\nin analyzing sentiment beyond product reviews and post comments. For the\nstudent affairs offices of educational institutions, this study can provide a\npractical example of how NLP can be applied to real-world scenarios, showcasing\nthe potential impact of data-driven decision making."
                },
                "authors": [
                    {
                        "name": "Lyberius Ennio F. Taruc"
                    },
                    {
                        "name": "Arvin R. De La Cruz"
                    }
                ],
                "author_detail": {
                    "name": "Arvin R. De La Cruz"
                },
                "author": "Arvin R. De La Cruz",
                "arxiv_comment": "11 pages, 4 figures, presented in International Conference on\n  Generative Al and its Applications (ICGAIA-24) last 22nd - 23rd, July, 2024\n  at Jakarta, Indonesia",
                "arxiv_journal_ref": "IJISAE, 2024, 12(22s), 1553-1563",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08688v1",
                "updated": "2024-08-16T12:01:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    1,
                    55,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T12:01:55Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    1,
                    55,
                    4,
                    229,
                    0
                ],
                "title": "The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic\n  Preference Optimization Dataset Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic\n  Preference Optimization Dataset Generation"
                },
                "summary": "This paper presents and evaluates multi-agent workflows for synthetic\nPreference Optimization (PO) dataset generation. PO dataset generation requires\ntwo modules: (1) response evaluation, and (2) response generation. In the\nresponse evaluation module, the responses from Large Language Models (LLMs) are\nevaluated and ranked - a task typically carried out by human annotators that we\nautomate using LLMs. We assess the response evaluation module in a 2 step\nprocess. In step 1, we assess LLMs as evaluators using three distinct prompting\nstrategies. In step 2, we apply the winning prompting strategy to compare the\nperformance of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. In each step, we\nuse inter-rater agreement using Cohen's Kappa between human annotators and\nLLMs. For the response generation module, we compare different configurations\nfor the LLM Feedback Loop using the identified LLM evaluator configuration. We\nuse the win rate (the fraction of times a generation framework is selected as\nthe best by an LLM evaluator) to determine the best multi-agent configuration\nfor generation. After identifying the best configurations for both modules, we\nuse models from the GPT, Gemma, and Llama families to generate our PO datasets\nusing the above pipeline. We generate two types of PO datasets, one to improve\nthe generation capabilities of individual LLM and the other to improve the\nmulti-agent workflow. Our evaluation shows that GPT-4o-as-a-Judge is more\nconsistent across datasets when the candidate responses do not include\nresponses from the GPT family. Additionally, we find that the LLM Feedback\nLoop, with Llama as the generator and Gemma as the reviewer, achieves a notable\n71.8% and 73.8% win rate over single-agent Llama and Gemma, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents and evaluates multi-agent workflows for synthetic\nPreference Optimization (PO) dataset generation. PO dataset generation requires\ntwo modules: (1) response evaluation, and (2) response generation. In the\nresponse evaluation module, the responses from Large Language Models (LLMs) are\nevaluated and ranked - a task typically carried out by human annotators that we\nautomate using LLMs. We assess the response evaluation module in a 2 step\nprocess. In step 1, we assess LLMs as evaluators using three distinct prompting\nstrategies. In step 2, we apply the winning prompting strategy to compare the\nperformance of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. In each step, we\nuse inter-rater agreement using Cohen's Kappa between human annotators and\nLLMs. For the response generation module, we compare different configurations\nfor the LLM Feedback Loop using the identified LLM evaluator configuration. We\nuse the win rate (the fraction of times a generation framework is selected as\nthe best by an LLM evaluator) to determine the best multi-agent configuration\nfor generation. After identifying the best configurations for both modules, we\nuse models from the GPT, Gemma, and Llama families to generate our PO datasets\nusing the above pipeline. We generate two types of PO datasets, one to improve\nthe generation capabilities of individual LLM and the other to improve the\nmulti-agent workflow. Our evaluation shows that GPT-4o-as-a-Judge is more\nconsistent across datasets when the candidate responses do not include\nresponses from the GPT family. Additionally, we find that the LLM Feedback\nLoop, with Llama as the generator and Gemma as the reviewer, achieves a notable\n71.8% and 73.8% win rate over single-agent Llama and Gemma, respectively."
                },
                "authors": [
                    {
                        "name": "Samee Arif"
                    },
                    {
                        "name": "Sualeha Farid"
                    },
                    {
                        "name": "Abdul Hameed Azeemi"
                    },
                    {
                        "name": "Awais Athar"
                    },
                    {
                        "name": "Agha Ali Raza"
                    }
                ],
                "author_detail": {
                    "name": "Agha Ali Raza"
                },
                "author": "Agha Ali Raza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08685v1",
                "updated": "2024-08-16T11:58:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    58,
                    34,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T11:58:34Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    58,
                    34,
                    4,
                    229,
                    0
                ],
                "title": "Can Large Language Models Improve the Adversarial Robustness of Graph\n  Neural Networks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Improve the Adversarial Robustness of Graph\n  Neural Networks?"
                },
                "summary": "Graph neural networks (GNNs) are vulnerable to adversarial perturbations,\nespecially for topology attacks, and many methods that improve the robustness\nof GNNs have received considerable attention. Recently, we have witnessed the\nsignificant success of large language models (LLMs), leading many to explore\nthe great potential of LLMs on GNNs. However, they mainly focus on improving\nthe performance of GNNs by utilizing LLMs to enhance the node features.\nTherefore, we ask: Will the robustness of GNNs also be enhanced with the\npowerful understanding and inference capabilities of LLMs? By presenting the\nempirical results, we find that despite that LLMs can improve the robustness of\nGNNs, there is still an average decrease of 23.1% in accuracy, implying that\nthe GNNs remain extremely vulnerable against topology attack. Therefore,\nanother question is how to extend the capabilities of LLMs on graph adversarial\nrobustness. In this paper, we propose an LLM-based robust graph structure\ninference framework, LLM4RGNN, which distills the inference capabilities of\nGPT-4 into a local LLM for identifying malicious edges and an LM-based edge\npredictor for finding missing important edges, so as to recover a robust graph\nstructure. Extensive experiments demonstrate that LLM4RGNN consistently\nimproves the robustness across various GNNs. Even in some cases where the\nperturbation ratio increases to 40%, the accuracy of GNNs is still better than\nthat on the clean graph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) are vulnerable to adversarial perturbations,\nespecially for topology attacks, and many methods that improve the robustness\nof GNNs have received considerable attention. Recently, we have witnessed the\nsignificant success of large language models (LLMs), leading many to explore\nthe great potential of LLMs on GNNs. However, they mainly focus on improving\nthe performance of GNNs by utilizing LLMs to enhance the node features.\nTherefore, we ask: Will the robustness of GNNs also be enhanced with the\npowerful understanding and inference capabilities of LLMs? By presenting the\nempirical results, we find that despite that LLMs can improve the robustness of\nGNNs, there is still an average decrease of 23.1% in accuracy, implying that\nthe GNNs remain extremely vulnerable against topology attack. Therefore,\nanother question is how to extend the capabilities of LLMs on graph adversarial\nrobustness. In this paper, we propose an LLM-based robust graph structure\ninference framework, LLM4RGNN, which distills the inference capabilities of\nGPT-4 into a local LLM for identifying malicious edges and an LM-based edge\npredictor for finding missing important edges, so as to recover a robust graph\nstructure. Extensive experiments demonstrate that LLM4RGNN consistently\nimproves the robustness across various GNNs. Even in some cases where the\nperturbation ratio increases to 40%, the accuracy of GNNs is still better than\nthat on the clean graph."
                },
                "authors": [
                    {
                        "name": "Zhongjian Zhang"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Mengmei Zhang"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Chuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Shi"
                },
                "author": "Chuan Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08212v2",
                "updated": "2024-08-16T11:57:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    57,
                    53,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-15T15:23:00Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    23,
                    0,
                    3,
                    228,
                    0
                ],
                "title": "Covert Bias: The Severity of Social Views' Unalignment in Language\n  Models Towards Implicit and Explicit Opinion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covert Bias: The Severity of Social Views' Unalignment in Language\n  Models Towards Implicit and Explicit Opinion"
                },
                "summary": "While various approaches have recently been studied for bias identification,\nlittle is known about how implicit language that does not explicitly convey a\nviewpoint affects bias amplification in large language models. To examine the\nseverity of bias toward a view, we evaluated the performance of two downstream\ntasks where the implicit and explicit knowledge of social groups were used.\nFirst, we present a stress test evaluation by using a biased model in edge\ncases of excessive bias scenarios. Then, we evaluate how LLMs calibrate\nlinguistically in response to both implicit and explicit opinions when they are\naligned with conflicting viewpoints. Our findings reveal a discrepancy in LLM\nperformance in identifying implicit and explicit opinions, with a general\ntendency of bias toward explicit opinions of opposing stances. Moreover, the\nbias-aligned models generate more cautious responses using uncertainty phrases\ncompared to the unaligned (zero-shot) base models. The direct, incautious\nresponses of the unaligned models suggest a need for further refinement of\ndecisiveness by incorporating uncertainty markers to enhance their reliability,\nespecially on socially nuanced topics with high subjectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While various approaches have recently been studied for bias identification,\nlittle is known about how implicit language that does not explicitly convey a\nviewpoint affects bias amplification in large language models. To examine the\nseverity of bias toward a view, we evaluated the performance of two downstream\ntasks where the implicit and explicit knowledge of social groups were used.\nFirst, we present a stress test evaluation by using a biased model in edge\ncases of excessive bias scenarios. Then, we evaluate how LLMs calibrate\nlinguistically in response to both implicit and explicit opinions when they are\naligned with conflicting viewpoints. Our findings reveal a discrepancy in LLM\nperformance in identifying implicit and explicit opinions, with a general\ntendency of bias toward explicit opinions of opposing stances. Moreover, the\nbias-aligned models generate more cautious responses using uncertainty phrases\ncompared to the unaligned (zero-shot) base models. The direct, incautious\nresponses of the unaligned models suggest a need for further refinement of\ndecisiveness by incorporating uncertainty markers to enhance their reliability,\nespecially on socially nuanced topics with high subjectivity."
                },
                "authors": [
                    {
                        "name": "Abeer Aldayel"
                    },
                    {
                        "name": "Areej Alokaili"
                    },
                    {
                        "name": "Rehab Alahmadi"
                    }
                ],
                "author_detail": {
                    "name": "Rehab Alahmadi"
                },
                "author": "Rehab Alahmadi",
                "arxiv_comment": "This work is under-review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08684v1",
                "updated": "2024-08-16T11:56:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    56,
                    49,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T11:56:49Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    56,
                    49,
                    4,
                    229,
                    0
                ],
                "title": "Research on Personalized Compression Algorithm for Pre-trained Models\n  Based on Homomorphic Entropy Increase",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on Personalized Compression Algorithm for Pre-trained Models\n  Based on Homomorphic Entropy Increase"
                },
                "summary": "In this article, we explore the challenges and evolution of two key\ntechnologies in the current field of AI: Vision Transformer model and Large\nLanguage Model (LLM). Vision Transformer captures global information by\nsplitting images into small pieces and leveraging Transformer's multi-head\nattention mechanism, but its high reference count and compute overhead limit\ndeployment on mobile devices. At the same time, the rapid development of LLM\nhas revolutionized natural language processing, but it also faces huge\ndeployment challenges. To address these issues, we investigate model pruning\ntechniques, with a particular focus on how to reduce redundant parameters\nwithout losing accuracy to accommodate personalized data and\nresource-constrained environments. In this paper, a new layered pruning\nstrategy is proposed to distinguish the personalized layer from the common\nlayer by compressed sensing and random sampling, thus significantly reducing\nthe model parameters. Our experimental results show that the introduced step\nbuffering mechanism further improves the accuracy of the model after pruning,\nproviding new directions and possibilities for the deployment of efficient and\npersonalized AI models on mobile devices in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we explore the challenges and evolution of two key\ntechnologies in the current field of AI: Vision Transformer model and Large\nLanguage Model (LLM). Vision Transformer captures global information by\nsplitting images into small pieces and leveraging Transformer's multi-head\nattention mechanism, but its high reference count and compute overhead limit\ndeployment on mobile devices. At the same time, the rapid development of LLM\nhas revolutionized natural language processing, but it also faces huge\ndeployment challenges. To address these issues, we investigate model pruning\ntechniques, with a particular focus on how to reduce redundant parameters\nwithout losing accuracy to accommodate personalized data and\nresource-constrained environments. In this paper, a new layered pruning\nstrategy is proposed to distinguish the personalized layer from the common\nlayer by compressed sensing and random sampling, thus significantly reducing\nthe model parameters. Our experimental results show that the introduced step\nbuffering mechanism further improves the accuracy of the model after pruning,\nproviding new directions and possibilities for the deployment of efficient and\npersonalized AI models on mobile devices in the future."
                },
                "authors": [
                    {
                        "name": "Yicong Li"
                    },
                    {
                        "name": "Xing Guo"
                    },
                    {
                        "name": "Haohua Du"
                    }
                ],
                "author_detail": {
                    "name": "Haohua Du"
                },
                "author": "Haohua Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08682v1",
                "updated": "2024-08-16T11:55:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    55,
                    44,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T11:55:44Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    55,
                    44,
                    4,
                    229,
                    0
                ],
                "title": "LLM-PCGC: Large Language Model-based Point Cloud Geometry Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-PCGC: Large Language Model-based Point Cloud Geometry Compression"
                },
                "summary": "The key to effective point cloud compression is to obtain a robust context\nmodel consistent with complex 3D data structures. Recently, the advancement of\nlarge language models (LLMs) has highlighted their capabilities not only as\npowerful generators for in-context learning and generation but also as\neffective compressors. These dual attributes of LLMs make them particularly\nwell-suited to meet the demands of data compression. Therefore, this paper\nexplores the potential of using LLM for compression tasks, focusing on lossless\npoint cloud geometry compression (PCGC) experiments. However, applying LLM\ndirectly to PCGC tasks presents some significant challenges, i.e., LLM does not\nunderstand the structure of the point cloud well, and it is a difficult task to\nfill the gap between text and point cloud through text description, especially\nfor large complicated and small shapeless point clouds. To address these\nproblems, we introduce a novel architecture, namely the Large Language\nModel-based Point Cloud Geometry Compression (LLM-PCGC) method, using LLM to\ncompress point cloud geometry information without any text description or\naligning operation. By utilizing different adaptation techniques for\ncross-modality representation alignment and semantic consistency, including\nclustering, K-tree, token mapping invariance, and Low Rank Adaptation (LoRA),\nthe proposed method can translate LLM to a compressor/generator for point\ncloud. To the best of our knowledge, this is the first structure to employ LLM\nas a compressor for point cloud data. Experiments demonstrate that the LLM-PCGC\noutperforms the other existing methods significantly, by achieving -40.213% bit\nrate reduction compared to the reference software of MPEG Geometry-based Point\nCloud Compression (G-PCC) standard, and by achieving -2.267% bit rate reduction\ncompared to the state-of-the-art learning-based method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key to effective point cloud compression is to obtain a robust context\nmodel consistent with complex 3D data structures. Recently, the advancement of\nlarge language models (LLMs) has highlighted their capabilities not only as\npowerful generators for in-context learning and generation but also as\neffective compressors. These dual attributes of LLMs make them particularly\nwell-suited to meet the demands of data compression. Therefore, this paper\nexplores the potential of using LLM for compression tasks, focusing on lossless\npoint cloud geometry compression (PCGC) experiments. However, applying LLM\ndirectly to PCGC tasks presents some significant challenges, i.e., LLM does not\nunderstand the structure of the point cloud well, and it is a difficult task to\nfill the gap between text and point cloud through text description, especially\nfor large complicated and small shapeless point clouds. To address these\nproblems, we introduce a novel architecture, namely the Large Language\nModel-based Point Cloud Geometry Compression (LLM-PCGC) method, using LLM to\ncompress point cloud geometry information without any text description or\naligning operation. By utilizing different adaptation techniques for\ncross-modality representation alignment and semantic consistency, including\nclustering, K-tree, token mapping invariance, and Low Rank Adaptation (LoRA),\nthe proposed method can translate LLM to a compressor/generator for point\ncloud. To the best of our knowledge, this is the first structure to employ LLM\nas a compressor for point cloud data. Experiments demonstrate that the LLM-PCGC\noutperforms the other existing methods significantly, by achieving -40.213% bit\nrate reduction compared to the reference software of MPEG Geometry-based Point\nCloud Compression (G-PCC) standard, and by achieving -2.267% bit rate reduction\ncompared to the state-of-the-art learning-based method."
                },
                "authors": [
                    {
                        "name": "Yuqi Ye"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08681v1",
                "updated": "2024-08-16T11:53:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    53,
                    52,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T11:53:52Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    53,
                    52,
                    4,
                    229,
                    0
                ],
                "title": "A Mean Field Ansatz for Zero-Shot Weight Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Mean Field Ansatz for Zero-Shot Weight Transfer"
                },
                "summary": "The pre-training cost of large language models (LLMs) is prohibitive. One\ncutting-edge approach to reduce the cost is zero-shot weight transfer, also\nknown as model growth for some cases, which magically transfers the weights\ntrained in a small model to a large model. However, there are still some\ntheoretical mysteries behind the weight transfer. In this paper, inspired by\nprior applications of mean field theory to neural network dynamics, we\nintroduce a mean field ansatz to provide a theoretical explanation for weight\ntransfer. Specifically, we propose the row-column (RC) ansatz under the mean\nfield point of view, which describes the measure structure of the weights in\nthe neural network (NN) and admits a close measure dynamic. Thus, the weights\nof different sizes NN admit a common distribution under proper assumptions, and\nweight transfer methods can be viewed as sampling methods. We empirically\nvalidate the RC ansatz by exploring simple MLP examples and LLMs such as GPT-3\nand Llama-3.1. We show the mean-field point of view is adequate under suitable\nassumptions which can provide theoretical support for zero-shot weight\ntransfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pre-training cost of large language models (LLMs) is prohibitive. One\ncutting-edge approach to reduce the cost is zero-shot weight transfer, also\nknown as model growth for some cases, which magically transfers the weights\ntrained in a small model to a large model. However, there are still some\ntheoretical mysteries behind the weight transfer. In this paper, inspired by\nprior applications of mean field theory to neural network dynamics, we\nintroduce a mean field ansatz to provide a theoretical explanation for weight\ntransfer. Specifically, we propose the row-column (RC) ansatz under the mean\nfield point of view, which describes the measure structure of the weights in\nthe neural network (NN) and admits a close measure dynamic. Thus, the weights\nof different sizes NN admit a common distribution under proper assumptions, and\nweight transfer methods can be viewed as sampling methods. We empirically\nvalidate the RC ansatz by exploring simple MLP examples and LLMs such as GPT-3\nand Llama-3.1. We show the mean-field point of view is adequate under suitable\nassumptions which can provide theoretical support for zero-shot weight\ntransfer."
                },
                "authors": [
                    {
                        "name": "Xingyuan Chen"
                    },
                    {
                        "name": "Wenwei Kuang"
                    },
                    {
                        "name": "Lei Deng"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Goncalo dos Reis"
                    }
                ],
                "author_detail": {
                    "name": "Goncalo dos Reis"
                },
                "author": "Goncalo dos Reis",
                "arxiv_comment": "40 pages, 6 Figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08676v1",
                "updated": "2024-08-16T11:43:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    43,
                    31,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T11:43:31Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    43,
                    31,
                    4,
                    229,
                    0
                ],
                "title": "Fine-tuning LLMs for Autonomous Spacecraft Control: A Case Study Using\n  Kerbal Space Program",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning LLMs for Autonomous Spacecraft Control: A Case Study Using\n  Kerbal Space Program"
                },
                "summary": "Recent trends are emerging in the use of Large Language Models (LLMs) as\nautonomous agents that take actions based on the content of the user text\nprompt. This study explores the use of fine-tuned Large Language Models (LLMs)\nfor autonomous spacecraft control, using the Kerbal Space Program Differential\nGames suite (KSPDG) as a testing environment. Traditional Reinforcement\nLearning (RL) approaches face limitations in this domain due to insufficient\nsimulation capabilities and data. By leveraging LLMs, specifically fine-tuning\nmodels like GPT-3.5 and LLaMA, we demonstrate how these models can effectively\ncontrol spacecraft using language-based inputs and outputs. Our approach\nintegrates real-time mission telemetry into textual prompts processed by the\nLLM, which then generate control actions via an agent. The results open a\ndiscussion about the potential of LLMs for space operations beyond their\nnominal use for text-related tasks. Future work aims to expand this methodology\nto other space control tasks and evaluate the performance of different LLM\nfamilies. The code is available at this URL:\n\\texttt{https://github.com/ARCLab-MIT/kspdg}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent trends are emerging in the use of Large Language Models (LLMs) as\nautonomous agents that take actions based on the content of the user text\nprompt. This study explores the use of fine-tuned Large Language Models (LLMs)\nfor autonomous spacecraft control, using the Kerbal Space Program Differential\nGames suite (KSPDG) as a testing environment. Traditional Reinforcement\nLearning (RL) approaches face limitations in this domain due to insufficient\nsimulation capabilities and data. By leveraging LLMs, specifically fine-tuning\nmodels like GPT-3.5 and LLaMA, we demonstrate how these models can effectively\ncontrol spacecraft using language-based inputs and outputs. Our approach\nintegrates real-time mission telemetry into textual prompts processed by the\nLLM, which then generate control actions via an agent. The results open a\ndiscussion about the potential of LLMs for space operations beyond their\nnominal use for text-related tasks. Future work aims to expand this methodology\nto other space control tasks and evaluate the performance of different LLM\nfamilies. The code is available at this URL:\n\\texttt{https://github.com/ARCLab-MIT/kspdg}."
                },
                "authors": [
                    {
                        "name": "Alejandro Carrasco"
                    },
                    {
                        "name": "Victor Rodriguez-Fernandez"
                    },
                    {
                        "name": "Richard Linares"
                    }
                ],
                "author_detail": {
                    "name": "Richard Linares"
                },
                "author": "Richard Linares",
                "arxiv_comment": "ESA SPAICE Conference 2024. arXiv admin note: text overlap with\n  arXiv:2404.00413",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08669v1",
                "updated": "2024-08-16T11:26:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    26,
                    39,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T11:26:39Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    26,
                    39,
                    4,
                    229,
                    0
                ],
                "title": "HSDreport: Heart Sound Diagnosis with Echocardiography Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HSDreport: Heart Sound Diagnosis with Echocardiography Reports"
                },
                "summary": "Heart sound auscultation holds significant importance in the diagnosis of\ncongenital heart disease. However, existing methods for Heart Sound Diagnosis\n(HSD) tasks are predominantly limited to a few fixed categories, framing the\nHSD task as a rigid classification problem that does not fully align with\nmedical practice and offers only limited information to physicians. Besides,\nsuch methods do not utilize echocardiography reports, the gold standard in the\ndiagnosis of related diseases. To tackle this challenge, we introduce\nHSDreport, a new benchmark for HSD, which mandates the direct utilization of\nheart sounds obtained from auscultation to predict echocardiography reports.\nThis benchmark aims to merge the convenience of auscultation with the\ncomprehensive nature of echocardiography reports. First, we collect a new\ndataset for this benchmark, comprising 2,275 heart sound samples along with\ntheir corresponding reports. Subsequently, we develop a knowledge-aware\nquery-based transformer to handle this task. The intent is to leverage the\ncapabilities of medically pre-trained models and the internal knowledge of\nlarge language models (LLMs) to address the task's inherent complexity and\nvariability, thereby enhancing the robustness and scientific validity of the\nmethod. Furthermore, our experimental results indicate that our method\nsignificantly outperforms traditional HSD approaches and existing multimodal\nLLMs in detecting key abnormalities in heart sounds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heart sound auscultation holds significant importance in the diagnosis of\ncongenital heart disease. However, existing methods for Heart Sound Diagnosis\n(HSD) tasks are predominantly limited to a few fixed categories, framing the\nHSD task as a rigid classification problem that does not fully align with\nmedical practice and offers only limited information to physicians. Besides,\nsuch methods do not utilize echocardiography reports, the gold standard in the\ndiagnosis of related diseases. To tackle this challenge, we introduce\nHSDreport, a new benchmark for HSD, which mandates the direct utilization of\nheart sounds obtained from auscultation to predict echocardiography reports.\nThis benchmark aims to merge the convenience of auscultation with the\ncomprehensive nature of echocardiography reports. First, we collect a new\ndataset for this benchmark, comprising 2,275 heart sound samples along with\ntheir corresponding reports. Subsequently, we develop a knowledge-aware\nquery-based transformer to handle this task. The intent is to leverage the\ncapabilities of medically pre-trained models and the internal knowledge of\nlarge language models (LLMs) to address the task's inherent complexity and\nvariability, thereby enhancing the robustness and scientific validity of the\nmethod. Furthermore, our experimental results indicate that our method\nsignificantly outperforms traditional HSD approaches and existing multimodal\nLLMs in detecting key abnormalities in heart sounds."
                },
                "authors": [
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Pingjie Wang"
                    },
                    {
                        "name": "Liudan Zhao"
                    },
                    {
                        "name": "Yuchen Yang"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Kun Sun"
                    },
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanfeng Wang"
                },
                "author": "Yanfeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08661v1",
                "updated": "2024-08-16T11:09:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    9,
                    56,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T11:09:56Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    11,
                    9,
                    56,
                    4,
                    229,
                    0
                ],
                "title": "MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector"
                },
                "summary": "The increasing parameters and expansive dataset of large language models\n(LLMs) highlight the urgent demand for a technical solution to audit the\nunderlying privacy risks and copyright issues associated with LLMs. Existing\nstudies have partially addressed this need through an exploration of the\npre-training data detection problem, which is an instance of a membership\ninference attack (MIA). This problem involves determining whether a given piece\nof text has been used during the pre-training phase of the target LLM. Although\nexisting methods have designed various sophisticated MIA score functions to\nachieve considerable detection performance in pre-trained LLMs, how to achieve\nhigh-confidence detection and how to perform MIA on aligned LLMs remain\nchallenging. In this paper, we propose MIA-Tuner, a novel instruction-based MIA\nmethod, which instructs LLMs themselves to serve as a more precise pre-training\ndata detector internally, rather than design an external MIA score function.\nFurthermore, we design two instruction-based safeguards to respectively\nmitigate the privacy risks brought by the existing methods and MIA-Tuner. To\ncomprehensively evaluate the most recent state-of-the-art LLMs, we collect a\nmore up-to-date MIA benchmark dataset, named WIKIMIA-24, to replace the widely\nadopted benchmark WIKIMIA. We conduct extensive experiments across various\naligned and unaligned LLMs over the two benchmark datasets. The results\ndemonstrate that MIA-Tuner increases the AUC of MIAs from 0.7 to a\nsignificantly high level of 0.9.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing parameters and expansive dataset of large language models\n(LLMs) highlight the urgent demand for a technical solution to audit the\nunderlying privacy risks and copyright issues associated with LLMs. Existing\nstudies have partially addressed this need through an exploration of the\npre-training data detection problem, which is an instance of a membership\ninference attack (MIA). This problem involves determining whether a given piece\nof text has been used during the pre-training phase of the target LLM. Although\nexisting methods have designed various sophisticated MIA score functions to\nachieve considerable detection performance in pre-trained LLMs, how to achieve\nhigh-confidence detection and how to perform MIA on aligned LLMs remain\nchallenging. In this paper, we propose MIA-Tuner, a novel instruction-based MIA\nmethod, which instructs LLMs themselves to serve as a more precise pre-training\ndata detector internally, rather than design an external MIA score function.\nFurthermore, we design two instruction-based safeguards to respectively\nmitigate the privacy risks brought by the existing methods and MIA-Tuner. To\ncomprehensively evaluate the most recent state-of-the-art LLMs, we collect a\nmore up-to-date MIA benchmark dataset, named WIKIMIA-24, to replace the widely\nadopted benchmark WIKIMIA. We conduct extensive experiments across various\naligned and unaligned LLMs over the two benchmark datasets. The results\ndemonstrate that MIA-Tuner increases the AUC of MIAs from 0.7 to a\nsignificantly high level of 0.9."
                },
                "authors": [
                    {
                        "name": "Wenjie Fu"
                    },
                    {
                        "name": "Huandong Wang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Guanghua Liu"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Tao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Tao Jiang"
                },
                "author": "Tao Jiang",
                "arxiv_comment": "code and dataset: https://github.com/wjfu99/MIA-Tuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08657v1",
                "updated": "2024-08-16T10:52:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    10,
                    52,
                    2,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T10:52:02Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    10,
                    52,
                    2,
                    4,
                    229,
                    0
                ],
                "title": "Optical Ground Station Diversity for Satellite Quantum Key Distribution\n  in Ireland",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical Ground Station Diversity for Satellite Quantum Key Distribution\n  in Ireland"
                },
                "summary": "Space quantum communications is a potential means for establishing global\nsecure communications and quantum networking. Despite pioneering demonstrations\nof satellite quantum key distribution, considerable challenges remain for wide\ndeployment such as the local effects of the atmosphere on the transmission of\nsingle-photon level quantum signals. As part of Ireland's efforts to establish\nquantum links with the rest of Europe and further afield, we present a\npreliminary study of the feasibility of satellite quantum key distribution\ntaking into account geographic and weather effects on the space-Earth channel.\nWeather data over 5 years covering 4 locations across Ireland were used to\nassess performance and the prospects of optical ground station (OGS) geographic\ndiversity to improve service availability. Despite significant cloud cover that\nmay reduce the performance of a single OGS location, the use of a 4-OGS network\ncan provide up to 45% improvement for a single satellite exploiting\nanti-correlation in cloud cover, though most gains are achieved with 2 or 3\nOGSs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space quantum communications is a potential means for establishing global\nsecure communications and quantum networking. Despite pioneering demonstrations\nof satellite quantum key distribution, considerable challenges remain for wide\ndeployment such as the local effects of the atmosphere on the transmission of\nsingle-photon level quantum signals. As part of Ireland's efforts to establish\nquantum links with the rest of Europe and further afield, we present a\npreliminary study of the feasibility of satellite quantum key distribution\ntaking into account geographic and weather effects on the space-Earth channel.\nWeather data over 5 years covering 4 locations across Ireland were used to\nassess performance and the prospects of optical ground station (OGS) geographic\ndiversity to improve service availability. Despite significant cloud cover that\nmay reduce the performance of a single OGS location, the use of a 4-OGS network\ncan provide up to 45% improvement for a single satellite exploiting\nanti-correlation in cloud cover, though most gains are achieved with 2 or 3\nOGSs."
                },
                "authors": [
                    {
                        "name": "Naga Lakshmi Anipeddi"
                    },
                    {
                        "name": "Jerry Horgan"
                    },
                    {
                        "name": "Daniel K L Oi"
                    },
                    {
                        "name": "Deirdre Kilbane"
                    }
                ],
                "author_detail": {
                    "name": "Deirdre Kilbane"
                },
                "author": "Deirdre Kilbane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08068v2",
                "updated": "2024-08-16T10:50:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    10,
                    50,
                    45,
                    4,
                    229,
                    0
                ],
                "published": "2024-06-12T10:36:27Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    10,
                    36,
                    27,
                    2,
                    164,
                    0
                ],
                "title": "Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A\n  Survey"
                },
                "summary": "Compared to traditional sentiment analysis, which only considers text,\nmultimodal sentiment analysis needs to consider emotional signals from\nmultimodal sources simultaneously and is therefore more consistent with the way\nhow humans process sentiment in real-world scenarios. It involves processing\nemotional information from various sources such as natural language, images,\nvideos, audio, physiological signals, etc. However, although other modalities\nalso contain diverse emotional cues, natural language usually contains richer\ncontextual information and therefore always occupies a crucial position in\nmultimodal sentiment analysis. The emergence of ChatGPT has opened up immense\npotential for applying large language models (LLMs) to text-centric multimodal\ntasks. However, it is still unclear how existing LLMs can adapt better to\ntext-centric multimodal sentiment analysis tasks. This survey aims to (1)\npresent a comprehensive review of recent research in text-centric multimodal\nsentiment analysis tasks, (2) examine the potential of LLMs for text-centric\nmultimodal sentiment analysis, outlining their approaches, advantages, and\nlimitations, (3) summarize the application scenarios of LLM-based multimodal\nsentiment analysis technology, and (4) explore the challenges and potential\nresearch directions for multimodal sentiment analysis in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared to traditional sentiment analysis, which only considers text,\nmultimodal sentiment analysis needs to consider emotional signals from\nmultimodal sources simultaneously and is therefore more consistent with the way\nhow humans process sentiment in real-world scenarios. It involves processing\nemotional information from various sources such as natural language, images,\nvideos, audio, physiological signals, etc. However, although other modalities\nalso contain diverse emotional cues, natural language usually contains richer\ncontextual information and therefore always occupies a crucial position in\nmultimodal sentiment analysis. The emergence of ChatGPT has opened up immense\npotential for applying large language models (LLMs) to text-centric multimodal\ntasks. However, it is still unclear how existing LLMs can adapt better to\ntext-centric multimodal sentiment analysis tasks. This survey aims to (1)\npresent a comprehensive review of recent research in text-centric multimodal\nsentiment analysis tasks, (2) examine the potential of LLMs for text-centric\nmultimodal sentiment analysis, outlining their approaches, advantages, and\nlimitations, (3) summarize the application scenarios of LLM-based multimodal\nsentiment analysis technology, and (4) explore the challenges and potential\nresearch directions for multimodal sentiment analysis in the future."
                },
                "authors": [
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Yanyan Zhao"
                    },
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Tian Zheng"
                    },
                    {
                        "name": "Hongbo Zhang"
                    },
                    {
                        "name": "Zongyang Ma"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2210.14556 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08656v1",
                "updated": "2024-08-16T10:45:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    10,
                    45,
                    45,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T10:45:45Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    10,
                    45,
                    45,
                    4,
                    229,
                    0
                ],
                "title": "LLMs Are Biased Towards Output Formats! Systematically Evaluating and\n  Mitigating Output Format Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Are Biased Towards Output Formats! Systematically Evaluating and\n  Mitigating Output Format Bias of LLMs"
                },
                "summary": "We present the first systematic evaluation examining format bias in\nperformance of large language models (LLMs). Our approach distinguishes between\ntwo categories of an evaluation metric under format constraints to reliably and\naccurately assess performance: one measures performance when format constraints\nare adhered to, while the other evaluates performance regardless of constraint\nadherence. We then define a metric for measuring the format bias of LLMs and\nestablish effective strategies to reduce it. Subsequently, we present our\nempirical format bias evaluation spanning four commonly used categories --\nmultiple-choice question-answer, wrapping, list, and mapping -- covering 15\nwidely-used formats. Our evaluation on eight generation tasks uncovers\nsignificant format bias across state-of-the-art LLMs. We further discover that\nimproving the format-instruction following capabilities of LLMs across formats\npotentially reduces format bias. Based on our evaluation findings, we study\nprompting and fine-tuning with synthesized format data techniques to mitigate\nformat bias. Our methods successfully reduce the variance in ChatGPT's\nperformance among wrapping formats from 235.33 to 0.71 (%$^2$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first systematic evaluation examining format bias in\nperformance of large language models (LLMs). Our approach distinguishes between\ntwo categories of an evaluation metric under format constraints to reliably and\naccurately assess performance: one measures performance when format constraints\nare adhered to, while the other evaluates performance regardless of constraint\nadherence. We then define a metric for measuring the format bias of LLMs and\nestablish effective strategies to reduce it. Subsequently, we present our\nempirical format bias evaluation spanning four commonly used categories --\nmultiple-choice question-answer, wrapping, list, and mapping -- covering 15\nwidely-used formats. Our evaluation on eight generation tasks uncovers\nsignificant format bias across state-of-the-art LLMs. We further discover that\nimproving the format-instruction following capabilities of LLMs across formats\npotentially reduces format bias. Based on our evaluation findings, we study\nprompting and fine-tuning with synthesized format data techniques to mitigate\nformat bias. Our methods successfully reduce the variance in ChatGPT's\nperformance among wrapping formats from 235.33 to 0.71 (%$^2$)."
                },
                "authors": [
                    {
                        "name": "Do Xuan Long"
                    },
                    {
                        "name": "Hai Nguyen Ngoc"
                    },
                    {
                        "name": "Tiviatis Sim"
                    },
                    {
                        "name": "Hieu Dao"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15377v2",
                "updated": "2024-08-16T10:44:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    10,
                    44,
                    16,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-22T04:57:51Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    57,
                    51,
                    0,
                    204,
                    0
                ],
                "title": "Replicable Bandits for Digital Health Interventions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Replicable Bandits for Digital Health Interventions"
                },
                "summary": "Adaptive treatment assignment algorithms, such as bandit and reinforcement\nlearning algorithms, are increasingly used in digital health intervention\nclinical trials. Causal inference and related data analyses are critical for\nevaluating digital health interventions, deciding how to refine the\nintervention, and deciding whether to roll-out the intervention more broadly.\nHowever the replicability of these analyses has received relatively little\nattention. This work investigates the replicability of statistical analyses\nfrom trials deploying adaptive treatment assignment algorithms. We demonstrate\nthat many standard statistical estimators can be inconsistent and fail to be\nreplicable across repetitions of the clinical trial, even as the sample size\ngrows large. We show that this non-replicability is intimately related to\nproperties of the adaptive algorithm itself. We introduce a formal definition\nof a \"replicable bandit algorithm\" and prove that under such algorithms, a wide\nvariety of common statistical analyses are guaranteed to be consistent. We\npresent both theoretical results and simulation studies based on a mobile\nhealth oral health self-care intervention. Our findings underscore the\nimportance of designing adaptive algorithms with replicability in mind,\nespecially for settings like digital health where deployment decisions rely\nheavily on replicated evidence. We conclude by discussing open questions on the\nconnections between algorithm design, statistical inference, and experimental\nreplicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive treatment assignment algorithms, such as bandit and reinforcement\nlearning algorithms, are increasingly used in digital health intervention\nclinical trials. Causal inference and related data analyses are critical for\nevaluating digital health interventions, deciding how to refine the\nintervention, and deciding whether to roll-out the intervention more broadly.\nHowever the replicability of these analyses has received relatively little\nattention. This work investigates the replicability of statistical analyses\nfrom trials deploying adaptive treatment assignment algorithms. We demonstrate\nthat many standard statistical estimators can be inconsistent and fail to be\nreplicable across repetitions of the clinical trial, even as the sample size\ngrows large. We show that this non-replicability is intimately related to\nproperties of the adaptive algorithm itself. We introduce a formal definition\nof a \"replicable bandit algorithm\" and prove that under such algorithms, a wide\nvariety of common statistical analyses are guaranteed to be consistent. We\npresent both theoretical results and simulation studies based on a mobile\nhealth oral health self-care intervention. Our findings underscore the\nimportance of designing adaptive algorithms with replicability in mind,\nespecially for settings like digital health where deployment decisions rely\nheavily on replicated evidence. We conclude by discussing open questions on the\nconnections between algorithm design, statistical inference, and experimental\nreplicability."
                },
                "authors": [
                    {
                        "name": "Kelly W. Zhang"
                    },
                    {
                        "name": "Nowell Closser"
                    },
                    {
                        "name": "Anna L. Trella"
                    },
                    {
                        "name": "Susan A. Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Susan A. Murphy"
                },
                "author": "Susan A. Murphy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01129v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01129v3",
                "updated": "2024-08-16T10:03:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    10,
                    3,
                    53,
                    4,
                    229,
                    0
                ],
                "published": "2024-04-01T14:11:45Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    14,
                    11,
                    45,
                    0,
                    92,
                    0
                ],
                "title": "Emphasising Structured Information: Integrating Abstract Meaning\n  Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emphasising Structured Information: Integrating Abstract Meaning\n  Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation"
                },
                "summary": "Automatic open-domain dialogue evaluation has attracted increasing attention.\nTrainable evaluation metrics, typically trained with true positive and randomly\nselected negative responses, tend to assign higher scores to responses that\nshare greater content similarity with a given context. However, adversarial\nnegative responses, despite possessing high content similarity with the\ncontexts, are semantically different. Consequently, existing evaluation metrics\nare not robust enough to evaluate such responses, resulting in low correlations\nwith human judgments. While recent studies have demonstrated the effectiveness\nof Large Language Models (LLMs) for open-domain dialogue evaluation, they still\nface challenges in effectively handling adversarial negative examples. In this\npaper, we propose an effective framework for open-domain dialogue evaluation,\nwhich combines domain-specific language models (SLMs) enhanced with Abstract\nMeaning Representation (AMR) knowledge with LLMs. The SLMs can explicitly\nincorporate AMR graph information of the dialogue through a gating mechanism\nfor enhanced dialogue semantic representation learning. Both the evaluation\nresult from the SLMs and the AMR graph information are incorporated into the\nLLM's prompt for enhanced evaluation performance. Experimental results on\nopen-domain dialogue evaluation tasks demonstrate the superiority of our method\ncompared to a wide range of state-of-the-art baselines, especially in\ndiscriminating adversarial negative responses. Our code and data are publicly\navailable at https://github.com/Bernard-Yang/SIMAMR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic open-domain dialogue evaluation has attracted increasing attention.\nTrainable evaluation metrics, typically trained with true positive and randomly\nselected negative responses, tend to assign higher scores to responses that\nshare greater content similarity with a given context. However, adversarial\nnegative responses, despite possessing high content similarity with the\ncontexts, are semantically different. Consequently, existing evaluation metrics\nare not robust enough to evaluate such responses, resulting in low correlations\nwith human judgments. While recent studies have demonstrated the effectiveness\nof Large Language Models (LLMs) for open-domain dialogue evaluation, they still\nface challenges in effectively handling adversarial negative examples. In this\npaper, we propose an effective framework for open-domain dialogue evaluation,\nwhich combines domain-specific language models (SLMs) enhanced with Abstract\nMeaning Representation (AMR) knowledge with LLMs. The SLMs can explicitly\nincorporate AMR graph information of the dialogue through a gating mechanism\nfor enhanced dialogue semantic representation learning. Both the evaluation\nresult from the SLMs and the AMR graph information are incorporated into the\nLLM's prompt for enhanced evaluation performance. Experimental results on\nopen-domain dialogue evaluation tasks demonstrate the superiority of our method\ncompared to a wide range of state-of-the-art baselines, especially in\ndiscriminating adversarial negative responses. Our code and data are publicly\navailable at https://github.com/Bernard-Yang/SIMAMR."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Kun Zhao"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Liang Zhan"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01129v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01129v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08632v1",
                "updated": "2024-08-16T09:52:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    52,
                    2,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T09:52:02Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    52,
                    2,
                    4,
                    229,
                    0
                ],
                "title": "A Survey on Benchmarks of Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Benchmarks of Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are gaining increasing popularity in\nboth academia and industry due to their remarkable performance in various\napplications such as visual question answering, visual perception,\nunderstanding, and reasoning. Over the past few years, significant efforts have\nbeen made to examine MLLMs from multiple perspectives. This paper presents a\ncomprehensive review of \\textbf{180 benchmarks} and evaluation for MLLMs,\nfocusing on (1)perception and understanding, (2)cognition and reasoning,\n(3)specific domains, (4)key capabilities, and (5)other modalities. Finally, we\ndiscuss the limitations of the current evaluation methods for MLLMs and explore\npromising future directions. Our key argument is that evaluation should be\nregarded as a crucial discipline to better support the development of MLLMs.\nFor more details, please visit our GitHub repository:\nhttps://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are gaining increasing popularity in\nboth academia and industry due to their remarkable performance in various\napplications such as visual question answering, visual perception,\nunderstanding, and reasoning. Over the past few years, significant efforts have\nbeen made to examine MLLMs from multiple perspectives. This paper presents a\ncomprehensive review of \\textbf{180 benchmarks} and evaluation for MLLMs,\nfocusing on (1)perception and understanding, (2)cognition and reasoning,\n(3)specific domains, (4)key capabilities, and (5)other modalities. Finally, we\ndiscuss the limitations of the current evaluation methods for MLLMs and explore\npromising future directions. Our key argument is that evaluation should be\nregarded as a crucial discipline to better support the development of MLLMs.\nFor more details, please visit our GitHub repository:\nhttps://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey."
                },
                "authors": [
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Weiheng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Weiheng Lu"
                },
                "author": "Weiheng Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08631v1",
                "updated": "2024-08-16T09:49:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    49,
                    51,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T09:49:51Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    49,
                    51,
                    4,
                    229,
                    0
                ],
                "title": "Persona is a Double-edged Sword: Enhancing the Zero-shot Reasoning by\n  Ensembling the Role-playing and Neutral Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona is a Double-edged Sword: Enhancing the Zero-shot Reasoning by\n  Ensembling the Role-playing and Neutral Prompts"
                },
                "summary": "Recent studies demonstrate that prompting an appropriate role-playing persona\nto an LLM improves its reasoning capability. However, assigning a proper\npersona is difficult since an LLM's performance is extremely sensitive to\nassigned prompts; therefore, personas sometimes hinder LLMs and degrade their\nreasoning capabilities. In this paper, we propose a novel framework, Jekyll \\&\nHyde, which ensembles the results of role-playing and neutral prompts to\neradicate performance degradation via unilateral use of role-playing prompted\nLLM and enhance the robustness of an LLM's reasoning ability. Specifically,\nJekyll \\& Hyde collects two potential solutions from both role-playing and\nneutral prompts and selects a better solution after cross-checking via an LLM\nevaluator. However, LLM-based evaluators tend to be affected by the order of\nthose potential solutions within the prompt when selecting the proper solution;\nthus, we also propose a robust LLM evaluator to mitigate the position bias. The\nexperimental analysis demonstrates that role-playing prompts distract LLMs and\ndegrade their reasoning abilities in 4 out of 12 datasets, even when using\nGPT-4. In addition, we reveal that Jekyll \\& Hyde improves reasoning\ncapabilities by selecting better choices among the potential solutions on\ntwelve widely-used reasoning datasets. We further show that our proposed LLM\nevaluator outperforms other baselines, proving the LLMs' position bias is\nsuccessfully mitigated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies demonstrate that prompting an appropriate role-playing persona\nto an LLM improves its reasoning capability. However, assigning a proper\npersona is difficult since an LLM's performance is extremely sensitive to\nassigned prompts; therefore, personas sometimes hinder LLMs and degrade their\nreasoning capabilities. In this paper, we propose a novel framework, Jekyll \\&\nHyde, which ensembles the results of role-playing and neutral prompts to\neradicate performance degradation via unilateral use of role-playing prompted\nLLM and enhance the robustness of an LLM's reasoning ability. Specifically,\nJekyll \\& Hyde collects two potential solutions from both role-playing and\nneutral prompts and selects a better solution after cross-checking via an LLM\nevaluator. However, LLM-based evaluators tend to be affected by the order of\nthose potential solutions within the prompt when selecting the proper solution;\nthus, we also propose a robust LLM evaluator to mitigate the position bias. The\nexperimental analysis demonstrates that role-playing prompts distract LLMs and\ndegrade their reasoning abilities in 4 out of 12 datasets, even when using\nGPT-4. In addition, we reveal that Jekyll \\& Hyde improves reasoning\ncapabilities by selecting better choices among the potential solutions on\ntwelve widely-used reasoning datasets. We further show that our proposed LLM\nevaluator outperforms other baselines, proving the LLMs' position bias is\nsuccessfully mitigated."
                },
                "authors": [
                    {
                        "name": "Junseok Kim"
                    },
                    {
                        "name": "Nakyeong Yang"
                    },
                    {
                        "name": "Kyomin Jung"
                    }
                ],
                "author_detail": {
                    "name": "Kyomin Jung"
                },
                "author": "Kyomin Jung",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07343v2",
                "updated": "2024-08-16T09:44:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    44,
                    49,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-14T07:37:07Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    37,
                    7,
                    2,
                    227,
                    0
                ],
                "title": "Gradient Alignment Improves Test-Time Adaptation for Medical Image\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Alignment Improves Test-Time Adaptation for Medical Image\n  Segmentation"
                },
                "summary": "Although recent years have witnessed significant advancements in medical\nimage segmentation, the pervasive issue of domain shift among medical images\nfrom diverse centres hinders the effective deployment of pre-trained models.\nMany Test-time Adaptation (TTA) methods have been proposed to address this\nissue by fine-tuning pre-trained models with test data during inference. These\nmethods, however, often suffer from less-satisfactory optimization due to\nsuboptimal optimization direction (dictated by the gradient) and fixed\nstep-size (predicated on the learning rate). In this paper, we propose the\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\nthe gradient direction and learning rate in the optimization procedure. Unlike\nconventional TTA methods, which primarily optimize the pseudo gradient derived\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\nenables the model to excavate the similarities between different gradients and\ncorrect the gradient direction to approximate the empirical gradient related to\nthe current segmentation task. Additionally, we design a dynamic learning rate\nbased on the cosine similarity between the pseudo and auxiliary gradients,\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\ntest data. Extensive experiments establish the effectiveness of the proposed\ngradient alignment and dynamic learning rate and substantiate the superiority\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\nmedical image segmentation task. The code and weights of pre-trained source\nmodels will be available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although recent years have witnessed significant advancements in medical\nimage segmentation, the pervasive issue of domain shift among medical images\nfrom diverse centres hinders the effective deployment of pre-trained models.\nMany Test-time Adaptation (TTA) methods have been proposed to address this\nissue by fine-tuning pre-trained models with test data during inference. These\nmethods, however, often suffer from less-satisfactory optimization due to\nsuboptimal optimization direction (dictated by the gradient) and fixed\nstep-size (predicated on the learning rate). In this paper, we propose the\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\nthe gradient direction and learning rate in the optimization procedure. Unlike\nconventional TTA methods, which primarily optimize the pseudo gradient derived\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\nenables the model to excavate the similarities between different gradients and\ncorrect the gradient direction to approximate the empirical gradient related to\nthe current segmentation task. Additionally, we design a dynamic learning rate\nbased on the cosine similarity between the pseudo and auxiliary gradients,\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\ntest data. Extensive experiments establish the effectiveness of the proposed\ngradient alignment and dynamic learning rate and substantiate the superiority\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\nmedical image segmentation task. The code and weights of pre-trained source\nmodels will be available."
                },
                "authors": [
                    {
                        "name": "Ziyang Chen"
                    },
                    {
                        "name": "Yiwen Ye"
                    },
                    {
                        "name": "Yongsheng Pan"
                    },
                    {
                        "name": "Yong Xia"
                    }
                ],
                "author_detail": {
                    "name": "Yong Xia"
                },
                "author": "Yong Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08624v1",
                "updated": "2024-08-16T09:32:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    32,
                    43,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T09:32:43Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    32,
                    43,
                    4,
                    229,
                    0
                ],
                "title": "RealMedQA: A pilot biomedical question answering dataset containing\n  realistic clinical questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RealMedQA: A pilot biomedical question answering dataset containing\n  realistic clinical questions"
                },
                "summary": "Clinical question answering systems have the potential to provide clinicians\nwith relevant and timely answers to their questions. Nonetheless, despite the\nadvances that have been made, adoption of these systems in clinical settings\nhas been slow. One issue is a lack of question-answering datasets which reflect\nthe real-world needs of health professionals. In this work, we present\nRealMedQA, a dataset of realistic clinical questions generated by humans and an\nLLM. We describe the process for generating and verifying the QA pairs and\nassess several QA models on BioASQ and RealMedQA to assess the relative\ndifficulty of matching answers to questions. We show that the LLM is more\ncost-efficient for generating \"ideal\" QA pairs. Additionally, we achieve a\nlower lexical similarity between questions and answers than BioASQ which\nprovides an additional challenge to the top two QA models, as per the results.\nWe release our code and our dataset publicly to encourage further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical question answering systems have the potential to provide clinicians\nwith relevant and timely answers to their questions. Nonetheless, despite the\nadvances that have been made, adoption of these systems in clinical settings\nhas been slow. One issue is a lack of question-answering datasets which reflect\nthe real-world needs of health professionals. In this work, we present\nRealMedQA, a dataset of realistic clinical questions generated by humans and an\nLLM. We describe the process for generating and verifying the QA pairs and\nassess several QA models on BioASQ and RealMedQA to assess the relative\ndifficulty of matching answers to questions. We show that the LLM is more\ncost-efficient for generating \"ideal\" QA pairs. Additionally, we achieve a\nlower lexical similarity between questions and answers than BioASQ which\nprovides an additional challenge to the top two QA models, as per the results.\nWe release our code and our dataset publicly to encourage further research."
                },
                "authors": [
                    {
                        "name": "Gregory Kell"
                    },
                    {
                        "name": "Angus Roberts"
                    },
                    {
                        "name": "Serge Umansky"
                    },
                    {
                        "name": "Yuti Khare"
                    },
                    {
                        "name": "Najma Ahmed"
                    },
                    {
                        "name": "Nikhil Patel"
                    },
                    {
                        "name": "Chloe Simela"
                    },
                    {
                        "name": "Jack Coumbe"
                    },
                    {
                        "name": "Julian Rozario"
                    },
                    {
                        "name": "Ryan-Rhys Griffiths"
                    },
                    {
                        "name": "Iain J. Marshall"
                    }
                ],
                "author_detail": {
                    "name": "Iain J. Marshall"
                },
                "author": "Iain J. Marshall",
                "arxiv_comment": "Accepted at AMIA Annual Symposium 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08619v1",
                "updated": "2024-08-16T09:19:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    19,
                    27,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T09:19:27Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    19,
                    27,
                    4,
                    229,
                    0
                ],
                "title": "PatUntrack: Automated Generating Patch Examples for Issue Reports\n  without Tracked Insecure Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatUntrack: Automated Generating Patch Examples for Issue Reports\n  without Tracked Insecure Code"
                },
                "summary": "Security patches are essential for enhancing the stability and robustness of\nprojects in the software community. While vulnerabilities are officially\nexpected to be patched before being disclosed, patching vulnerabilities is\ncomplicated and remains a struggle for many organizations. To patch\nvulnerabilities, security practitioners typically track vulnerable issue\nreports (IRs), and analyze their relevant insecure code to generate potential\npatches. However, the relevant insecure code may not be explicitly specified\nand practitioners cannot track the insecure code in the repositories, thus\nlimiting their ability to generate patches. In such cases, providing examples\nof insecure code and the corresponding patches would benefit the security\ndevelopers to better locate and fix the insecure code. In this paper, we\npropose PatUntrack to automatically generating patch examples from IRs without\ntracked insecure code. It auto-prompts Large Language Models (LLMs) to make\nthem applicable to analyze the vulnerabilities. It first generates the\ncompleted description of the Vulnerability-Triggering Path (VTP) from\nvulnerable IRs. Then, it corrects hallucinations in the VTP description with\nexternal golden knowledge. Finally, it generates Top-K pairs of Insecure Code\nand Patch Example based on the corrected VTP description. To evaluate the\nperformance, we conducted experiments on 5,465 vulnerable IRs. The experimental\nresults show that PatUntrack can obtain the highest performance and improve the\ntraditional LLM baselines by +14.6% (Fix@10) on average in patch example\ngeneration. Furthermore, PatUntrack was applied to generate patch examples for\n76 newly disclosed vulnerable IRs. 27 out of 37 replies from the authors of\nthese IRs confirmed the usefulness of the patch examples generated by\nPatUntrack, indicating that they can benefit from these examples for patching\nthe vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security patches are essential for enhancing the stability and robustness of\nprojects in the software community. While vulnerabilities are officially\nexpected to be patched before being disclosed, patching vulnerabilities is\ncomplicated and remains a struggle for many organizations. To patch\nvulnerabilities, security practitioners typically track vulnerable issue\nreports (IRs), and analyze their relevant insecure code to generate potential\npatches. However, the relevant insecure code may not be explicitly specified\nand practitioners cannot track the insecure code in the repositories, thus\nlimiting their ability to generate patches. In such cases, providing examples\nof insecure code and the corresponding patches would benefit the security\ndevelopers to better locate and fix the insecure code. In this paper, we\npropose PatUntrack to automatically generating patch examples from IRs without\ntracked insecure code. It auto-prompts Large Language Models (LLMs) to make\nthem applicable to analyze the vulnerabilities. It first generates the\ncompleted description of the Vulnerability-Triggering Path (VTP) from\nvulnerable IRs. Then, it corrects hallucinations in the VTP description with\nexternal golden knowledge. Finally, it generates Top-K pairs of Insecure Code\nand Patch Example based on the corrected VTP description. To evaluate the\nperformance, we conducted experiments on 5,465 vulnerable IRs. The experimental\nresults show that PatUntrack can obtain the highest performance and improve the\ntraditional LLM baselines by +14.6% (Fix@10) on average in patch example\ngeneration. Furthermore, PatUntrack was applied to generate patch examples for\n76 newly disclosed vulnerable IRs. 27 out of 37 replies from the authors of\nthese IRs confirmed the usefulness of the patch examples generated by\nPatUntrack, indicating that they can benefit from these examples for patching\nthe vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Ziyou Jiang"
                    },
                    {
                        "name": "Lin Shi"
                    },
                    {
                        "name": "Guowei Yang"
                    },
                    {
                        "name": "Qing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qing Wang"
                },
                "author": "Qing Wang",
                "arxiv_comment": "Accepted by ASE'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08609v1",
                "updated": "2024-08-16T08:50:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    50,
                    5,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T08:50:05Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    50,
                    5,
                    4,
                    229,
                    0
                ],
                "title": "Fast Network Recovery from Large-Scale Disasters: A Resilient and\n  Self-Organizing RAN Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Network Recovery from Large-Scale Disasters: A Resilient and\n  Self-Organizing RAN Framework"
                },
                "summary": "Extreme natural phenomena are occurring more frequently everyday in the\nworld, challenging, among others, the infrastructure of communication networks.\nFor instance, the devastating earthquakes in Turkiye in early 2023 showcased\nthat, although communications became an imminent priority, existing mobile\ncommunication systems fell short with the operational requirements of harsh\ndisaster environments. In this article, we present a novel framework for\nrobust, resilient, adaptive, and open source sixth generation (6G) radio access\nnetworks (Open6GRAN) that can provide uninterrupted communication services in\nthe face of natural disasters and other disruptions. Advanced 6G technologies,\nsuch as reconfigurable intelligent surfaces (RISs), cell-free\nmultiple-input-multiple-output, and joint communications and sensing with\nincreasingly heterogeneous deployment, consisting of terrestrial and\nnon-terrestrial nodes, are robustly integrated. We advocate that a key enabler\nto develop service and management orchestration with fast recovery capabilities\nwill rely on an artificial-intelligence-based radio access network (RAN)\ncontroller. To support the emergency use case spanning a larger area, the\nintegration of aerial and space segments with the terrestrial network promises\na rapid and reliable response in the case of any disaster. A proof-of-concept\nthat rapidly reconfigures an RIS for performance enhancement under an emergency\nscenario is presented and discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme natural phenomena are occurring more frequently everyday in the\nworld, challenging, among others, the infrastructure of communication networks.\nFor instance, the devastating earthquakes in Turkiye in early 2023 showcased\nthat, although communications became an imminent priority, existing mobile\ncommunication systems fell short with the operational requirements of harsh\ndisaster environments. In this article, we present a novel framework for\nrobust, resilient, adaptive, and open source sixth generation (6G) radio access\nnetworks (Open6GRAN) that can provide uninterrupted communication services in\nthe face of natural disasters and other disruptions. Advanced 6G technologies,\nsuch as reconfigurable intelligent surfaces (RISs), cell-free\nmultiple-input-multiple-output, and joint communications and sensing with\nincreasingly heterogeneous deployment, consisting of terrestrial and\nnon-terrestrial nodes, are robustly integrated. We advocate that a key enabler\nto develop service and management orchestration with fast recovery capabilities\nwill rely on an artificial-intelligence-based radio access network (RAN)\ncontroller. To support the emergency use case spanning a larger area, the\nintegration of aerial and space segments with the terrestrial network promises\na rapid and reliable response in the case of any disaster. A proof-of-concept\nthat rapidly reconfigures an RIS for performance enhancement under an emergency\nscenario is presented and discussed."
                },
                "authors": [
                    {
                        "name": "M. Yaser Yagan"
                    },
                    {
                        "name": "Sefa Kayraklik"
                    },
                    {
                        "name": "Samed Kesir"
                    },
                    {
                        "name": "Gizem Sumen"
                    },
                    {
                        "name": "Ibrahim Hokelek"
                    },
                    {
                        "name": "Mehmet Basaran"
                    },
                    {
                        "name": "George C. Alexandropoulos"
                    },
                    {
                        "name": "Ertugrul Basar"
                    },
                    {
                        "name": "Cicek Cavdar"
                    },
                    {
                        "name": "Huseyin Arslan"
                    },
                    {
                        "name": "Ali Gorcin"
                    }
                ],
                "author_detail": {
                    "name": "Ali Gorcin"
                },
                "author": "Ali Gorcin",
                "arxiv_comment": "Submitted to IEEE VTM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17962v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17962v3",
                "updated": "2024-08-16T08:48:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    48,
                    26,
                    4,
                    229,
                    0
                ],
                "published": "2024-06-25T22:44:17Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    22,
                    44,
                    17,
                    1,
                    177,
                    0
                ],
                "title": "Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework"
                },
                "summary": "Large Language Models (LLMs) demonstrate a remarkable ability to comprehend\nhuman instructions and generate high-quality text. This capability allows LLMs\nto function as agents that can emulate human beings at a more sophisticated\nlevel, beyond the mere replication of basic human behaviours. However, there is\na lack of exploring into leveraging LLMs to craft characters from diverse\naspects. In this work, we introduce the Customisable Conversation Agent\nFramework, which leverages LLMs to simulate real-world characters that can be\nfreely customised according to various user preferences. This adaptable\nframework is beneficial for the design of customisable characters and\nrole-playing agents aligned with human preferences. We propose the SimsConv\ndataset, which encompasses 68 different customised characters, 1,360 multi-turn\nrole-playing dialogues, and a total of 13,971 interaction dialogues. The\ncharacters are created from several real-world elements, such as career,\naspiration, trait, and skill. Building upon these foundations, we present\nSimsChat, a freely customisable role-playing agent. It incorporates diverse\nreal-world scenes and topic-specific character interaction dialogues, thereby\nsimulating characters' life experiences in various scenarios and topic-specific\ninteractions with specific emotions. Experimental results indicate that our\nproposed framework achieves desirable performance and provides a valuable\nguideline for the construction of more accurate human simulacra in the future.\nOur data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate a remarkable ability to comprehend\nhuman instructions and generate high-quality text. This capability allows LLMs\nto function as agents that can emulate human beings at a more sophisticated\nlevel, beyond the mere replication of basic human behaviours. However, there is\na lack of exploring into leveraging LLMs to craft characters from diverse\naspects. In this work, we introduce the Customisable Conversation Agent\nFramework, which leverages LLMs to simulate real-world characters that can be\nfreely customised according to various user preferences. This adaptable\nframework is beneficial for the design of customisable characters and\nrole-playing agents aligned with human preferences. We propose the SimsConv\ndataset, which encompasses 68 different customised characters, 1,360 multi-turn\nrole-playing dialogues, and a total of 13,971 interaction dialogues. The\ncharacters are created from several real-world elements, such as career,\naspiration, trait, and skill. Building upon these foundations, we present\nSimsChat, a freely customisable role-playing agent. It incorporates diverse\nreal-world scenes and topic-specific character interaction dialogues, thereby\nsimulating characters' life experiences in various scenarios and topic-specific\ninteractions with specific emotions. Experimental results indicate that our\nproposed framework achieves desirable performance and provides a valuable\nguideline for the construction of more accurate human simulacra in the future.\nOur data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Kun Zhao"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Lanxiao Huang"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17962v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17962v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08592v1",
                "updated": "2024-08-16T07:53:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    7,
                    53,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T07:53:25Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    7,
                    53,
                    25,
                    4,
                    229,
                    0
                ],
                "title": "Case Study: Runtime Safety Verification of Neural Network Controlled\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Case Study: Runtime Safety Verification of Neural Network Controlled\n  System"
                },
                "summary": "Neural networks are increasingly used in safety-critical applications such as\nrobotics and autonomous vehicles. However, the deployment of\nneural-network-controlled systems (NNCSs) raises significant safety concerns.\nMany recent advances overlook critical aspects of verifying control and\nensuring safety in real-time scenarios. This paper presents a case study on\nusing POLAR-Express, a state-of-the-art NNCS reachability analysis tool, for\nruntime safety verification in a Turtlebot navigation system using LiDAR. The\nTurtlebot, equipped with a neural network controller for steering, operates in\na complex environment with obstacles. We developed a safe online controller\nswitching strategy that switches between the original NNCS controller and an\nobstacle avoidance controller based on the verification results. Our\nexperiments, conducted in a ROS2 Flatland simulation environment, explore the\ncapabilities and limitations of using POLAR-Express for runtime verification\nand demonstrate the effectiveness of our switching strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks are increasingly used in safety-critical applications such as\nrobotics and autonomous vehicles. However, the deployment of\nneural-network-controlled systems (NNCSs) raises significant safety concerns.\nMany recent advances overlook critical aspects of verifying control and\nensuring safety in real-time scenarios. This paper presents a case study on\nusing POLAR-Express, a state-of-the-art NNCS reachability analysis tool, for\nruntime safety verification in a Turtlebot navigation system using LiDAR. The\nTurtlebot, equipped with a neural network controller for steering, operates in\na complex environment with obstacles. We developed a safe online controller\nswitching strategy that switches between the original NNCS controller and an\nobstacle avoidance controller based on the verification results. Our\nexperiments, conducted in a ROS2 Flatland simulation environment, explore the\ncapabilities and limitations of using POLAR-Express for runtime verification\nand demonstrate the effectiveness of our switching strategy."
                },
                "authors": [
                    {
                        "name": "Frank Yang"
                    },
                    {
                        "name": "Sinong Simon Zhan"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Qi Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhu"
                },
                "author": "Qi Zhu",
                "arxiv_comment": "15 pages, 5 figures, submitted to Runtime Verification 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06566v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06566v4",
                "updated": "2024-08-16T07:43:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    7,
                    43,
                    55,
                    4,
                    229,
                    0
                ],
                "published": "2024-06-03T07:44:32Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    7,
                    44,
                    32,
                    0,
                    155,
                    0
                ],
                "title": "Natural Language Interaction with a Household Electricity\n  Knowledge-based Digital Twin",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Interaction with a Household Electricity\n  Knowledge-based Digital Twin"
                },
                "summary": "Domain specific digital twins, representing a digital replica of various\nsegments of the smart grid, are foreseen as able to model, simulate, and\ncontrol the respective segments. At the same time, knowledge-based digital\ntwins, coupled with AI, may also empower humans to understand aspects of the\nsystem through natural language interaction in view of planning and policy\nmaking. This paper is the first to assess and report on the potential of\nRetrieval Augmented Generation (RAG) question answers related to household\nelectrical energy measurement aspects leveraging a knowledge-based energy\ndigital twin. Relying on the recently published electricity consumption\nknowledge graph that actually represents a knowledge-based digital twin, we\nstudy the capabilities of ChatGPT, Gemini and Llama in answering electricity\nrelated questions. Furthermore, we compare the answers with the ones generated\nthrough a RAG techniques that leverages an existing electricity knowledge-based\ndigital twin. Our findings illustrate that the RAG approach not only reduces\nthe incidence of incorrect information typically generated by LLMs but also\nsignificantly improves the quality of the output by grounding responses in\nverifiable data. This paper details our methodology, presents a comparative\nanalysis of responses with and without RAG, and discusses the implications of\nour findings for future applications of AI in specialized sectors like energy\ndata analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain specific digital twins, representing a digital replica of various\nsegments of the smart grid, are foreseen as able to model, simulate, and\ncontrol the respective segments. At the same time, knowledge-based digital\ntwins, coupled with AI, may also empower humans to understand aspects of the\nsystem through natural language interaction in view of planning and policy\nmaking. This paper is the first to assess and report on the potential of\nRetrieval Augmented Generation (RAG) question answers related to household\nelectrical energy measurement aspects leveraging a knowledge-based energy\ndigital twin. Relying on the recently published electricity consumption\nknowledge graph that actually represents a knowledge-based digital twin, we\nstudy the capabilities of ChatGPT, Gemini and Llama in answering electricity\nrelated questions. Furthermore, we compare the answers with the ones generated\nthrough a RAG techniques that leverages an existing electricity knowledge-based\ndigital twin. Our findings illustrate that the RAG approach not only reduces\nthe incidence of incorrect information typically generated by LLMs but also\nsignificantly improves the quality of the output by grounding responses in\nverifiable data. This paper details our methodology, presents a comparative\nanalysis of responses with and without RAG, and discusses the implications of\nour findings for future applications of AI in specialized sectors like energy\ndata analysis."
                },
                "authors": [
                    {
                        "name": "Carolina Fortuna"
                    },
                    {
                        "name": "Vid Hanžel"
                    },
                    {
                        "name": "Blaž Bertalanič"
                    }
                ],
                "author_detail": {
                    "name": "Blaž Bertalanič"
                },
                "author": "Blaž Bertalanič",
                "arxiv_comment": "Accepted at IEEE SmartGridComm'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06566v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06566v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14573v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14573v3",
                "updated": "2024-08-16T07:30:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    7,
                    30,
                    29,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-21T06:27:45Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    6,
                    27,
                    45,
                    6,
                    203,
                    0
                ],
                "title": "Trading Devil Final: Backdoor attack via Stock market and Bayesian\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trading Devil Final: Backdoor attack via Stock market and Bayesian\n  Optimization"
                },
                "summary": "Since the advent of generative artificial intelligence, every company and\nresearcher has been rushing to develop their own generative models, whether\ncommercial or not. Given the large number of users of these powerful new tools,\nthere is currently no intrinsically verifiable way to explain from the ground\nup what happens when LLMs (large language models) learn. For example, those\nbased on automatic speech recognition systems, which have to rely on huge and\nastronomical amounts of data collected from all over the web to produce fast\nand efficient results, In this article, we develop a backdoor attack called\nMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is\nmainly based on modern stock market models. In order to show the possible\nvulnerabilities of speech-based transformers that may rely on LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the advent of generative artificial intelligence, every company and\nresearcher has been rushing to develop their own generative models, whether\ncommercial or not. Given the large number of users of these powerful new tools,\nthere is currently no intrinsically verifiable way to explain from the ground\nup what happens when LLMs (large language models) learn. For example, those\nbased on automatic speech recognition systems, which have to rely on huge and\nastronomical amounts of data collected from all over the web to produce fast\nand efficient results, In this article, we develop a backdoor attack called\nMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is\nmainly based on modern stock market models. In order to show the possible\nvulnerabilities of speech-based transformers that may rely on LLMs."
                },
                "authors": [
                    {
                        "name": "Orson Mengara"
                    }
                ],
                "author_detail": {
                    "name": "Orson Mengara"
                },
                "author": "Orson Mengara",
                "arxiv_comment": "END (will never be modified again) :Jumps-Diffusion and stock market:\n  Better quantify uncertainty in financial simulations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14573v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14573v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08566v1",
                "updated": "2024-08-16T07:00:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    7,
                    0,
                    8,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T07:00:08Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    7,
                    0,
                    8,
                    4,
                    229,
                    0
                ],
                "title": "Overview of the BioLaySumm 2024 Shared Task on the Lay Summarization of\n  Biomedical Research Articles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overview of the BioLaySumm 2024 Shared Task on the Lay Summarization of\n  Biomedical Research Articles"
                },
                "summary": "This paper presents the setup and results of the second edition of the\nBioLaySumm shared task on the Lay Summarisation of Biomedical Research\nArticles, hosted at the BioNLP Workshop at ACL 2024. In this task edition, we\naim to build on the first edition's success by further increasing research\ninterest in this important task and encouraging participants to explore novel\napproaches that will help advance the state-of-the-art. Encouragingly, we found\nresearch interest in the task to be high, with this edition of the task\nattracting a total of 53 participating teams, a significant increase in\nengagement from the previous edition. Overall, our results show that a broad\nrange of innovative approaches were adopted by task participants, with a\npredictable shift towards the use of Large Language Models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the setup and results of the second edition of the\nBioLaySumm shared task on the Lay Summarisation of Biomedical Research\nArticles, hosted at the BioNLP Workshop at ACL 2024. In this task edition, we\naim to build on the first edition's success by further increasing research\ninterest in this important task and encouraging participants to explore novel\napproaches that will help advance the state-of-the-art. Encouragingly, we found\nresearch interest in the task to be high, with this edition of the task\nattracting a total of 53 participating teams, a significant increase in\nengagement from the previous edition. Overall, our results show that a broad\nrange of innovative approaches were adopted by task participants, with a\npredictable shift towards the use of Large Language Models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Tomas Goldsack"
                    },
                    {
                        "name": "Carolina Scarton"
                    },
                    {
                        "name": "Matthew Shardlow"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Published in: Proceedings of the 23rd Workshop on Biomedical Natural\n  Language Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08564v1",
                "updated": "2024-08-16T06:54:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    54,
                    10,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:54:10Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    54,
                    10,
                    4,
                    229,
                    0
                ],
                "title": "Collaborative Cross-modal Fusion with Large Language Model for\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Cross-modal Fusion with Large Language Model for\n  Recommendation"
                },
                "summary": "Despite the success of conventional collaborative filtering (CF) approaches\nfor recommendation systems, they exhibit limitations in leveraging semantic\nknowledge within the textual attributes of users and items. Recent focus on the\napplication of large language models for recommendation (LLM4Rec) has\nhighlighted their capability for effective semantic knowledge capture. However,\nthese methods often overlook the collaborative signals in user behaviors. Some\nsimply instruct-tune a language model, while others directly inject the\nembeddings of a CF-based model, lacking a synergistic fusion of different\nmodalities. To address these issues, we propose a framework of Collaborative\nCross-modal Fusion with Large Language Models, termed CCF-LLM, for\nrecommendation. In this framework, we translate the user-item interactions into\na hybrid prompt to encode both semantic knowledge and collaborative signals,\nand then employ an attentive cross-modal fusion strategy to effectively fuse\nlatent embeddings of both modalities. Extensive experiments demonstrate that\nCCF-LLM outperforms existing methods by effectively utilizing semantic and\ncollaborative signals in the LLM4Rec context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of conventional collaborative filtering (CF) approaches\nfor recommendation systems, they exhibit limitations in leveraging semantic\nknowledge within the textual attributes of users and items. Recent focus on the\napplication of large language models for recommendation (LLM4Rec) has\nhighlighted their capability for effective semantic knowledge capture. However,\nthese methods often overlook the collaborative signals in user behaviors. Some\nsimply instruct-tune a language model, while others directly inject the\nembeddings of a CF-based model, lacking a synergistic fusion of different\nmodalities. To address these issues, we propose a framework of Collaborative\nCross-modal Fusion with Large Language Models, termed CCF-LLM, for\nrecommendation. In this framework, we translate the user-item interactions into\na hybrid prompt to encode both semantic knowledge and collaborative signals,\nand then employ an attentive cross-modal fusion strategy to effectively fuse\nlatent embeddings of both modalities. Extensive experiments demonstrate that\nCCF-LLM outperforms existing methods by effectively utilizing semantic and\ncollaborative signals in the LLM4Rec context."
                },
                "authors": [
                    {
                        "name": "Zhongzhou Liu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Kuicai Dong"
                    },
                    {
                        "name": "Yuan Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Fang"
                },
                "author": "Yuan Fang",
                "arxiv_doi": "10.1145/3627673.3679596",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3627673.3679596",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.08564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages, 4 figures, accepted by CIKM 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14854v2",
                "updated": "2024-08-16T06:47:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    47,
                    53,
                    4,
                    229,
                    0
                ],
                "published": "2024-06-21T03:54:10Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    3,
                    54,
                    10,
                    4,
                    173,
                    0
                ],
                "title": "PEANO-ViT: Power-Efficient Approximations of Non-Linearities in Vision\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEANO-ViT: Power-Efficient Approximations of Non-Linearities in Vision\n  Transformers"
                },
                "summary": "The deployment of Vision Transformers (ViTs) on hardware platforms, specially\nField-Programmable Gate Arrays (FPGAs), presents many challenges, which are\nmainly due to the substantial computational and power requirements of their\nnon-linear functions, notably layer normalization, softmax, and Gaussian Error\nLinear Unit (GELU). These critical functions pose significant obstacles to\nefficient hardware implementation due to their complex mathematical operations\nand the inherent resource count and architectural limitations of FPGAs.\nPEANO-ViT offers a novel approach to streamlining the implementation of the\nlayer normalization layer by introducing a division-free technique that\nsimultaneously approximates the division and square root function.\nAdditionally, PEANO-ViT provides a multi-scale division strategy to eliminate\ndivision operations in the softmax layer, aided by a Pade-based approximation\nfor the exponential function. Finally, PEANO-ViT introduces a piece-wise linear\napproximation for the GELU function, carefully designed to bypass the\ncomputationally intensive operations associated with GELU. In our comprehensive\nevaluations, PEANO-ViT exhibits minimal accuracy degradation (<= 0.5% for\nDeiT-B) while significantly enhancing power efficiency, achieving improvements\nof 1.91x, 1.39x, 8.01x for layer normalization, softmax, and GELU,\nrespectively. This improvement is achieved through substantial reductions in\nDSP, LUT, and register counts for these non-linear operations. Consequently,\nPEANO-ViT enables efficient deployment of Vision Transformers on resource- and\npower-constrained FPGA platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Vision Transformers (ViTs) on hardware platforms, specially\nField-Programmable Gate Arrays (FPGAs), presents many challenges, which are\nmainly due to the substantial computational and power requirements of their\nnon-linear functions, notably layer normalization, softmax, and Gaussian Error\nLinear Unit (GELU). These critical functions pose significant obstacles to\nefficient hardware implementation due to their complex mathematical operations\nand the inherent resource count and architectural limitations of FPGAs.\nPEANO-ViT offers a novel approach to streamlining the implementation of the\nlayer normalization layer by introducing a division-free technique that\nsimultaneously approximates the division and square root function.\nAdditionally, PEANO-ViT provides a multi-scale division strategy to eliminate\ndivision operations in the softmax layer, aided by a Pade-based approximation\nfor the exponential function. Finally, PEANO-ViT introduces a piece-wise linear\napproximation for the GELU function, carefully designed to bypass the\ncomputationally intensive operations associated with GELU. In our comprehensive\nevaluations, PEANO-ViT exhibits minimal accuracy degradation (<= 0.5% for\nDeiT-B) while significantly enhancing power efficiency, achieving improvements\nof 1.91x, 1.39x, 8.01x for layer normalization, softmax, and GELU,\nrespectively. This improvement is achieved through substantial reductions in\nDSP, LUT, and register counts for these non-linear operations. Consequently,\nPEANO-ViT enables efficient deployment of Vision Transformers on resource- and\npower-constrained FPGA platforms."
                },
                "authors": [
                    {
                        "name": "Mohammad Erfan Sadeghi"
                    },
                    {
                        "name": "Arash Fayyazi"
                    },
                    {
                        "name": "Seyedarmin Azizi"
                    },
                    {
                        "name": "Massoud Pedram"
                    }
                ],
                "author_detail": {
                    "name": "Massoud Pedram"
                },
                "author": "Massoud Pedram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08554v1",
                "updated": "2024-08-16T06:39:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    39,
                    8,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:39:08Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    39,
                    8,
                    4,
                    229,
                    0
                ],
                "title": "ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\ntasks. However, their practical application is constrained by substantial\nmemory and computational demands. Post-training quantization (PTQ) is\nconsidered an effective method to accelerate LLM inference. Despite its growing\npopularity in LLM model compression, PTQ deployment faces two major challenges.\nFirst, low-bit quantization leads to performance degradation. Second,\nrestricted by the limited integer computing unit type on GPUs, quantized matrix\noperations with different precisions cannot be effectively accelerated. To\naddress these issues, we introduce a novel arbitrary-bit quantization algorithm\nand inference framework, ABQ-LLM. It achieves superior performance across\nvarious quantization settings and enables efficient arbitrary-precision\nquantized inference on the GPU. ABQ-LLM introduces several key innovations: (1)\na distribution correction method for transformer blocks to mitigate\ndistribution differences caused by full quantization of weights and\nactivations, improving performance at low bit-widths. (2) the bit balance\nstrategy to counteract performance degradation from asymmetric distribution\nissues at very low bit-widths (e.g., 2-bit). (3) an innovative quantization\nacceleration framework that reconstructs the quantization matrix multiplication\nof arbitrary precision combinations based on BTC (Binary TensorCore)\nequivalents, gets rid of the limitations of INT4/INT8 computing units. ABQ-LLM\ncan convert each component bit width gain into actual acceleration gain,\nmaximizing performance under mixed precision(e.g., W6A6, W2A8). Based on W2*A8\nquantization configuration on LLaMA-7B model, it achieved a WikiText2\nperplexity of 7.59 (2.17$\\downarrow $ vs 9.76 in AffineQuant). Compared to\nSmoothQuant, we realized 1.6$\\times$ acceleration improvement and 2.7$\\times$\nmemory compression gain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\ntasks. However, their practical application is constrained by substantial\nmemory and computational demands. Post-training quantization (PTQ) is\nconsidered an effective method to accelerate LLM inference. Despite its growing\npopularity in LLM model compression, PTQ deployment faces two major challenges.\nFirst, low-bit quantization leads to performance degradation. Second,\nrestricted by the limited integer computing unit type on GPUs, quantized matrix\noperations with different precisions cannot be effectively accelerated. To\naddress these issues, we introduce a novel arbitrary-bit quantization algorithm\nand inference framework, ABQ-LLM. It achieves superior performance across\nvarious quantization settings and enables efficient arbitrary-precision\nquantized inference on the GPU. ABQ-LLM introduces several key innovations: (1)\na distribution correction method for transformer blocks to mitigate\ndistribution differences caused by full quantization of weights and\nactivations, improving performance at low bit-widths. (2) the bit balance\nstrategy to counteract performance degradation from asymmetric distribution\nissues at very low bit-widths (e.g., 2-bit). (3) an innovative quantization\nacceleration framework that reconstructs the quantization matrix multiplication\nof arbitrary precision combinations based on BTC (Binary TensorCore)\nequivalents, gets rid of the limitations of INT4/INT8 computing units. ABQ-LLM\ncan convert each component bit width gain into actual acceleration gain,\nmaximizing performance under mixed precision(e.g., W6A6, W2A8). Based on W2*A8\nquantization configuration on LLaMA-7B model, it achieved a WikiText2\nperplexity of 7.59 (2.17$\\downarrow $ vs 9.76 in AffineQuant). Compared to\nSmoothQuant, we realized 1.6$\\times$ acceleration improvement and 2.7$\\times$\nmemory compression gain."
                },
                "authors": [
                    {
                        "name": "Chao Zeng"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Yusheng Xie"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Xiaojian Wang"
                    },
                    {
                        "name": "Miao Wei"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Mei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Mei"
                },
                "author": "Xing Mei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08553v1",
                "updated": "2024-08-16T06:37:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    37,
                    59,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:37:59Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    37,
                    59,
                    4,
                    229,
                    0
                ],
                "title": "Enhancing Discriminative Tasks by Guiding the Pre-trained Language Model\n  with Large Language Model's Experience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Discriminative Tasks by Guiding the Pre-trained Language Model\n  with Large Language Model's Experience"
                },
                "summary": "Large Language Models (LLMs) and pre-trained Language Models (LMs) have\nachieved impressive success on many software engineering tasks (e.g., code\ncompletion and code generation). By leveraging huge existing code corpora\n(e.g., GitHub), these models aim to understand the patterns in source code and\nuse these patterns to predict code properties. However, fine-tuning LLMs is\ntime-consuming and costly for end users and small organizations. Furthermore,\nfine-tuning LMs heavily depends on the amount and quality of datasets\navailable. As a result, the current lack of data and the high cost of\ncollecting it in real-world scenarios further limit the applicability of LMs.\nIn this paper, we leverage the powerful generation capabilities of LLMs to\nenhance pre-trained LMs. Specifically, we use LLMs to generate domain-specific\ndata, thereby improving the performance of pre-trained LMs on the target tasks.\nWe conduct experiments by combining different LLMs in our generation phase and\nintroducing various LMs to learn from the LLM-generated data. Then, we compare\nthe performance of these LMs before and after learning the data. We find that\nLLM-generated data significantly enhances the performance of LMs. The\nimprovement can reach up to 58.36% for fault localization and up to 6.09% for\nclone detection. Our study highlights that using LLMs to generate data for LMs\ncan improve performance by a large margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and pre-trained Language Models (LMs) have\nachieved impressive success on many software engineering tasks (e.g., code\ncompletion and code generation). By leveraging huge existing code corpora\n(e.g., GitHub), these models aim to understand the patterns in source code and\nuse these patterns to predict code properties. However, fine-tuning LLMs is\ntime-consuming and costly for end users and small organizations. Furthermore,\nfine-tuning LMs heavily depends on the amount and quality of datasets\navailable. As a result, the current lack of data and the high cost of\ncollecting it in real-world scenarios further limit the applicability of LMs.\nIn this paper, we leverage the powerful generation capabilities of LLMs to\nenhance pre-trained LMs. Specifically, we use LLMs to generate domain-specific\ndata, thereby improving the performance of pre-trained LMs on the target tasks.\nWe conduct experiments by combining different LLMs in our generation phase and\nintroducing various LMs to learn from the LLM-generated data. Then, we compare\nthe performance of these LMs before and after learning the data. We find that\nLLM-generated data significantly enhances the performance of LMs. The\nimprovement can reach up to 58.36% for fault localization and up to 6.09% for\nclone detection. Our study highlights that using LLMs to generate data for LMs\ncan improve performance by a large margin."
                },
                "authors": [
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Chao Ni"
                    },
                    {
                        "name": "Xiaodan Xu"
                    },
                    {
                        "name": "Xinrui Li"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08549v1",
                "updated": "2024-08-16T06:31:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    31,
                    44,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:31:44Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    31,
                    44,
                    4,
                    229,
                    0
                ],
                "title": "Vulnerability Handling of AI-Generated Code -- Existing Solutions and\n  Open Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability Handling of AI-Generated Code -- Existing Solutions and\n  Open Challenges"
                },
                "summary": "The increasing use of generative Artificial Intelligence (AI) in modern\nsoftware engineering, particularly Large Language Models (LLMs) for code\ngeneration, has transformed professional software development by boosting\nproductivity and automating development processes. This adoption, however, has\nhighlighted a significant issue: the introduction of security vulnerabilities\ninto the code. These vulnerabilities result, e.g., from flaws in the training\ndata that propagate into the generated code, creating challenges in disclosing\nthem. Traditional vulnerability handling processes often involve extensive\nmanual review. Applying such traditional processes to AI-generated code is\nchallenging. AI-generated code may include several vulnerabilities, possibly in\nslightly different forms as developers might not build on already implemented\ncode but prompt similar tasks. In this work, we explore the current state of\nLLM-based approaches for vulnerability handling, focusing on approaches for\nvulnerability detection, localization, and repair. We provide an overview of\nrecent progress in this area and highlight open challenges that must be\naddressed in order to establish a reliable and scalable vulnerability handling\nprocess of AI-generated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of generative Artificial Intelligence (AI) in modern\nsoftware engineering, particularly Large Language Models (LLMs) for code\ngeneration, has transformed professional software development by boosting\nproductivity and automating development processes. This adoption, however, has\nhighlighted a significant issue: the introduction of security vulnerabilities\ninto the code. These vulnerabilities result, e.g., from flaws in the training\ndata that propagate into the generated code, creating challenges in disclosing\nthem. Traditional vulnerability handling processes often involve extensive\nmanual review. Applying such traditional processes to AI-generated code is\nchallenging. AI-generated code may include several vulnerabilities, possibly in\nslightly different forms as developers might not build on already implemented\ncode but prompt similar tasks. In this work, we explore the current state of\nLLM-based approaches for vulnerability handling, focusing on approaches for\nvulnerability detection, localization, and repair. We provide an overview of\nrecent progress in this area and highlight open challenges that must be\naddressed in order to establish a reliable and scalable vulnerability handling\nprocess of AI-generated code."
                },
                "authors": [
                    {
                        "name": "Sabrina Kaniewski"
                    },
                    {
                        "name": "Dieter Holstein"
                    },
                    {
                        "name": "Fabian Schmidt"
                    },
                    {
                        "name": "Tobias Heer"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Heer"
                },
                "author": "Tobias Heer",
                "arxiv_comment": "Accepted for publication @ IEEE AIxSET 2024; 4 pages, 2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05074v2",
                "updated": "2024-08-16T06:04:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    4,
                    31,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-09T14:02:24Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    2,
                    24,
                    4,
                    222,
                    0
                ],
                "title": "RT-Surv: Improving Mortality Prediction After Radiotherapy with Large\n  Language Model Structuring of Large-Scale Unstructured Electronic Health\n  Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Surv: Improving Mortality Prediction After Radiotherapy with Large\n  Language Model Structuring of Large-Scale Unstructured Electronic Health\n  Records"
                },
                "summary": "Accurate patient selection is critical in radiotherapy (RT) to prevent\nineffective treatments. Traditional survival prediction models, relying on\nstructured data, often lack precision. This study explores the potential of\nlarge language models (LLMs) to structure unstructured electronic health record\n(EHR) data, thereby improving survival prediction accuracy through\ncomprehensive clinical information integration. Data from 34,276 patients\ntreated with RT at Yonsei Cancer Center between 2013 and 2023 were analyzed,\nencompassing both structured and unstructured data. An open-source LLM was used\nto structure the unstructured EHR data via single-shot learning, with its\nperformance compared against a domain-specific medical LLM and a smaller\nvariant. Survival prediction models were developed using statistical, machine\nlearning, and deep learning approaches, incorporating both structured and\nLLM-structured data. Clinical experts evaluated the accuracy of the\nLLM-structured data. The open-source LLM achieved 87.5% accuracy in structuring\nunstructured EHR data without additional training, significantly outperforming\nthe domain-specific medical LLM, which reached only 35.8% accuracy. Larger LLMs\nwere more effective, particularly in extracting clinically relevant features\nlike general condition and disease extent, which closely correlated with\npatient survival. Incorporating LLM-structured clinical features into survival\nprediction models significantly improved accuracy, with the C-index of deep\nlearning models increasing from 0.737 to 0.820. These models also became more\ninterpretable by emphasizing clinically significant factors. This study shows\nthat general-domain LLMs, even without specific medical training, can\neffectively structure large-scale unstructured EHR data, substantially\nenhancing the accuracy and interpretability of clinical predictive models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate patient selection is critical in radiotherapy (RT) to prevent\nineffective treatments. Traditional survival prediction models, relying on\nstructured data, often lack precision. This study explores the potential of\nlarge language models (LLMs) to structure unstructured electronic health record\n(EHR) data, thereby improving survival prediction accuracy through\ncomprehensive clinical information integration. Data from 34,276 patients\ntreated with RT at Yonsei Cancer Center between 2013 and 2023 were analyzed,\nencompassing both structured and unstructured data. An open-source LLM was used\nto structure the unstructured EHR data via single-shot learning, with its\nperformance compared against a domain-specific medical LLM and a smaller\nvariant. Survival prediction models were developed using statistical, machine\nlearning, and deep learning approaches, incorporating both structured and\nLLM-structured data. Clinical experts evaluated the accuracy of the\nLLM-structured data. The open-source LLM achieved 87.5% accuracy in structuring\nunstructured EHR data without additional training, significantly outperforming\nthe domain-specific medical LLM, which reached only 35.8% accuracy. Larger LLMs\nwere more effective, particularly in extracting clinically relevant features\nlike general condition and disease extent, which closely correlated with\npatient survival. Incorporating LLM-structured clinical features into survival\nprediction models significantly improved accuracy, with the C-index of deep\nlearning models increasing from 0.737 to 0.820. These models also became more\ninterpretable by emphasizing clinically significant factors. This study shows\nthat general-domain LLMs, even without specific medical training, can\neffectively structure large-scale unstructured EHR data, substantially\nenhancing the accuracy and interpretability of clinical predictive models."
                },
                "authors": [
                    {
                        "name": "Sangjoon Park"
                    },
                    {
                        "name": "Chan Woo Wee"
                    },
                    {
                        "name": "Seo Hee Choi"
                    },
                    {
                        "name": "Kyung Hwan Kim"
                    },
                    {
                        "name": "Jee Suk Chang"
                    },
                    {
                        "name": "Hong In Yoon"
                    },
                    {
                        "name": "Ik Jae Lee"
                    },
                    {
                        "name": "Yong Bae Kim"
                    },
                    {
                        "name": "Jaeho Cho"
                    },
                    {
                        "name": "Ki Chang Keum"
                    },
                    {
                        "name": "Chang Geol Lee"
                    },
                    {
                        "name": "Hwa Kyung Byun"
                    },
                    {
                        "name": "Woong Sub Koom"
                    }
                ],
                "author_detail": {
                    "name": "Woong Sub Koom"
                },
                "author": "Woong Sub Koom",
                "arxiv_comment": "23 pages, 2 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04575v2",
                "updated": "2024-08-16T06:01:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    1,
                    15,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-08T16:36:24Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    16,
                    36,
                    24,
                    3,
                    221,
                    0
                ],
                "title": "SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals"
                },
                "summary": "Explainable Artificial Intelligence (XAI) plays a crucial role in enhancing\nthe transparency and accountability of AI models, particularly in natural\nlanguage processing (NLP) tasks. However, popular XAI methods such as LIME and\nSHAP have been found to be unstable and potentially misleading, underscoring\nthe need for a standardized evaluation approach. This paper introduces SCENE\n(Soft Counterfactual Evaluation for Natural language Explainability), a novel\nevaluation method that leverages large language models (LLMs) to generate Soft\nCounterfactual explanations in a zero-shot manner. By focusing on token-based\nsubstitutions, SCENE creates contextually appropriate and semantically\nmeaningful Soft Counterfactuals without extensive fine-tuning. SCENE adopts\nValiditysoft and Csoft metrics to assess the effectiveness of model-agnostic\nXAI methods in text classification tasks. Applied to CNN, RNN, and Transformer\narchitectures, SCENE provides valuable insights into the strengths and\nlimitations of various XAI techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Artificial Intelligence (XAI) plays a crucial role in enhancing\nthe transparency and accountability of AI models, particularly in natural\nlanguage processing (NLP) tasks. However, popular XAI methods such as LIME and\nSHAP have been found to be unstable and potentially misleading, underscoring\nthe need for a standardized evaluation approach. This paper introduces SCENE\n(Soft Counterfactual Evaluation for Natural language Explainability), a novel\nevaluation method that leverages large language models (LLMs) to generate Soft\nCounterfactual explanations in a zero-shot manner. By focusing on token-based\nsubstitutions, SCENE creates contextually appropriate and semantically\nmeaningful Soft Counterfactuals without extensive fine-tuning. SCENE adopts\nValiditysoft and Csoft metrics to assess the effectiveness of model-agnostic\nXAI methods in text classification tasks. Applied to CNN, RNN, and Transformer\narchitectures, SCENE provides valuable insights into the strengths and\nlimitations of various XAI techniques."
                },
                "authors": [
                    {
                        "name": "Haoran Zheng"
                    },
                    {
                        "name": "Utku Pamuksuz"
                    }
                ],
                "author_detail": {
                    "name": "Utku Pamuksuz"
                },
                "author": "Utku Pamuksuz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08541v1",
                "updated": "2024-08-16T05:56:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    5,
                    56,
                    10,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T05:56:10Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    5,
                    56,
                    10,
                    4,
                    229,
                    0
                ],
                "title": "Where is the signal in tokenization space?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where is the signal in tokenization space?"
                },
                "summary": "Large Language Models (LLMs) are typically shipped with tokenizers that\ndeterministically encode text into so-called canonical token sequences, to\nwhich the LLMs assign probability values. One common assumption is that the\nprobability of a piece of text is the probability of its canonical token\nsequence. However, the tokenization of a string is not unique: e.g., the Llama2\ntokenizer encodes Tokens as [Tok,ens], but [Tok,en,s] also represents the same\ntext. In this paper, we study non-canonical tokenizations. We prove that, given\na string, it is computationally hard to find the most likely tokenization for\nan autoregressive LLM, as well as to compute the marginal probability over all\npossible tokenizations. We then show how the marginal is, in most cases,\nindistinguishable from the canonical probability. Surprisingly, we then\nempirically demonstrate the existence of a significant amount of signal hidden\nwithin tokenization space. Notably, by simply aggregating the probabilities of\nnon-canonical tokenizations, we achieve improvements across a range of LLM\nevaluation benchmarks for a variety of architectures, including transformers\nand state space models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are typically shipped with tokenizers that\ndeterministically encode text into so-called canonical token sequences, to\nwhich the LLMs assign probability values. One common assumption is that the\nprobability of a piece of text is the probability of its canonical token\nsequence. However, the tokenization of a string is not unique: e.g., the Llama2\ntokenizer encodes Tokens as [Tok,ens], but [Tok,en,s] also represents the same\ntext. In this paper, we study non-canonical tokenizations. We prove that, given\na string, it is computationally hard to find the most likely tokenization for\nan autoregressive LLM, as well as to compute the marginal probability over all\npossible tokenizations. We then show how the marginal is, in most cases,\nindistinguishable from the canonical probability. Surprisingly, we then\nempirically demonstrate the existence of a significant amount of signal hidden\nwithin tokenization space. Notably, by simply aggregating the probabilities of\nnon-canonical tokenizations, we achieve improvements across a range of LLM\nevaluation benchmarks for a variety of architectures, including transformers\nand state space models."
                },
                "authors": [
                    {
                        "name": "Renato Lui Geh"
                    },
                    {
                        "name": "Honghua Zhang"
                    },
                    {
                        "name": "Kareem Ahmed"
                    },
                    {
                        "name": "Benjie Wang"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    }
                ],
                "author_detail": {
                    "name": "Guy Van den Broeck"
                },
                "author": "Guy Van den Broeck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15240v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15240v3",
                "updated": "2024-08-16T05:53:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    5,
                    53,
                    16,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-21T18:09:40Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    18,
                    9,
                    40,
                    6,
                    203,
                    0
                ],
                "title": "BIGbench: A Unified Benchmark for Social Bias in Text-to-Image\n  Generative Models Based on Multi-modal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIGbench: A Unified Benchmark for Social Bias in Text-to-Image\n  Generative Models Based on Multi-modal LLM"
                },
                "summary": "Text-to-Image (T2I) generative models are becoming increasingly crucial due\nto their ability to generate high-quality images, which also raises concerns\nabout the social biases in their outputs, especially in the human generation.\nSociological research has established systematic classifications of bias.\nHowever, existing bias research about T2I models conflates different types of\nbias, impeding methodological progress. In this paper, we introduce BIGbench, a\nunified benchmark for Biases of Image Generation, featuring a meticulously\ndesigned dataset. Unlike existing benchmarks, BIGbench classifies and evaluates\nbiases across four dimensions: manifestation of bias, visibility of bias,\nacquired attributes, and protected attributes, which ensures exceptional\naccuracy for analysis. Furthermore, BIGbench applies advanced multi-modal large\nlanguage models to achieve fully automated and highly accurate evaluations. We\napply BIGbench to evaluate eight representative general T2I models and three\ndebiased methods. Our human evaluation results underscore BIGbench's\neffectiveness in aligning images and identifying various biases. Besides, our\nstudy also reveal new research directions about biases, such as the effect of\ndistillation and irrelevant protected attributes. Our benchmark is openly\naccessible at https://github.com/BIGbench2024/BIGbench2024/ to ensure\nreproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Image (T2I) generative models are becoming increasingly crucial due\nto their ability to generate high-quality images, which also raises concerns\nabout the social biases in their outputs, especially in the human generation.\nSociological research has established systematic classifications of bias.\nHowever, existing bias research about T2I models conflates different types of\nbias, impeding methodological progress. In this paper, we introduce BIGbench, a\nunified benchmark for Biases of Image Generation, featuring a meticulously\ndesigned dataset. Unlike existing benchmarks, BIGbench classifies and evaluates\nbiases across four dimensions: manifestation of bias, visibility of bias,\nacquired attributes, and protected attributes, which ensures exceptional\naccuracy for analysis. Furthermore, BIGbench applies advanced multi-modal large\nlanguage models to achieve fully automated and highly accurate evaluations. We\napply BIGbench to evaluate eight representative general T2I models and three\ndebiased methods. Our human evaluation results underscore BIGbench's\neffectiveness in aligning images and identifying various biases. Besides, our\nstudy also reveal new research directions about biases, such as the effect of\ndistillation and irrelevant protected attributes. Our benchmark is openly\naccessible at https://github.com/BIGbench2024/BIGbench2024/ to ensure\nreproducibility."
                },
                "authors": [
                    {
                        "name": "Hanjun Luo"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Ziye Deng"
                    },
                    {
                        "name": "Xuecheng Liu"
                    },
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2405.17814",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15240v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15240v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02817v2",
                "updated": "2024-08-16T05:52:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    5,
                    52,
                    17,
                    4,
                    229,
                    0
                ],
                "published": "2024-05-05T05:43:20Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    5,
                    43,
                    20,
                    6,
                    126,
                    0
                ],
                "title": "Labeling supervised fine-tuning data with the scaling law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Labeling supervised fine-tuning data with the scaling law"
                },
                "summary": "This paper introduces a multi-stage manual annotation calibrated by the\nscaling law, offering a high-quality Supervised Fine-Tuning data acquisition\nmethod for environments with constrained resources like GPU poor, limited GPT\naccess, and funding restrictions. We have preprocessed 58k authentic chat data\nand manually annotated 2.3k questions. After this, we conducted fine-tuning on\nQwen models, ranging from 0.5B to 32B parameters. The optimal version improved\n29.07 in F1 score. This confirms the viability of fine-tuning Large Language\nModel (LLM) for downstream Natural Language Processing (NLP) tasks. Our\ncontributions are: 1) Created Supervised Fine-Tuning (SFT) training data in\nalpaca format, along with a set of Low-Rank Adaptation (LoRA) weights, and 2)\nDeveloped a method for acquiring high-quality data leveraging scaling law\nprinciple. The script, raw data with alpaca format and experiments track are\nopen-sourced on Github\n(https://github.com/InternLM/HuixiangDou/tree/main/web/tools), HuggingFace\n(https://huggingface.co/tpoisonooo) and WandB\n(https://wandb.ai/tpoisonooo/huixiangdou-cr/table?nw=nwusertpoisonooo). The\nprivacy of the data involved has been authorized by users. SFT data and license\ncomes from ncnn contributors group.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a multi-stage manual annotation calibrated by the\nscaling law, offering a high-quality Supervised Fine-Tuning data acquisition\nmethod for environments with constrained resources like GPU poor, limited GPT\naccess, and funding restrictions. We have preprocessed 58k authentic chat data\nand manually annotated 2.3k questions. After this, we conducted fine-tuning on\nQwen models, ranging from 0.5B to 32B parameters. The optimal version improved\n29.07 in F1 score. This confirms the viability of fine-tuning Large Language\nModel (LLM) for downstream Natural Language Processing (NLP) tasks. Our\ncontributions are: 1) Created Supervised Fine-Tuning (SFT) training data in\nalpaca format, along with a set of Low-Rank Adaptation (LoRA) weights, and 2)\nDeveloped a method for acquiring high-quality data leveraging scaling law\nprinciple. The script, raw data with alpaca format and experiments track are\nopen-sourced on Github\n(https://github.com/InternLM/HuixiangDou/tree/main/web/tools), HuggingFace\n(https://huggingface.co/tpoisonooo) and WandB\n(https://wandb.ai/tpoisonooo/huixiangdou-cr/table?nw=nwusertpoisonooo). The\nprivacy of the data involved has been authorized by users. SFT data and license\ncomes from ncnn contributors group."
                },
                "authors": [
                    {
                        "name": "Huanjun Kong"
                    }
                ],
                "author_detail": {
                    "name": "Huanjun Kong"
                },
                "author": "Huanjun Kong",
                "arxiv_comment": "5 pages, 3 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16357v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16357v2",
                "updated": "2024-08-16T05:16:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    5,
                    16,
                    31,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-23T10:00:45Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    10,
                    0,
                    45,
                    1,
                    205,
                    0
                ],
                "title": "TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced\n  CTR Prediction at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced\n  CTR Prediction at Kuaishou"
                },
                "summary": "The significance of modeling long-term user interests for CTR prediction\ntasks in large-scale recommendation systems is progressively gaining attention\namong researchers and practitioners. Existing work, such as SIM and TWIN,\ntypically employs a two-stage approach to model long-term user behavior\nsequences for efficiency concerns. The first stage rapidly retrieves a subset\nof sequences related to the target item from a long sequence using a\nsearch-based mechanism namely the General Search Unit (GSU), while the second\nstage calculates the interest scores using the Exact Search Unit (ESU) on the\nretrieved results. Given the extensive length of user behavior sequences\nspanning the entire life cycle, potentially reaching up to 10^6 in scale, there\nis currently no effective solution for fully modeling such expansive user\ninterests. To overcome this issue, we introduced TWIN-V2, an enhancement of\nTWIN, where a divide-and-conquer approach is applied to compress life-cycle\nbehaviors and uncover more accurate and diverse user interests. Specifically, a\nhierarchical clustering method groups items with similar characteristics in\nlife-cycle behaviors into a single cluster during the offline phase. By\nlimiting the size of clusters, we can compress behavior sequences well beyond\nthe magnitude of 10^5 to a length manageable for online inference in GSU\nretrieval. Cluster-aware target attention extracts comprehensive and\nmulti-faceted long-term interests of users, thereby making the final\nrecommendation results more accurate and diverse. Extensive offline experiments\non a multi-billion-scale industrial dataset and online A/B tests have\ndemonstrated the effectiveness of TWIN-V2. Under an efficient deployment\nframework, TWIN-V2 has been successfully deployed to the primary traffic that\nserves hundreds of millions of daily active users at Kuaishou.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significance of modeling long-term user interests for CTR prediction\ntasks in large-scale recommendation systems is progressively gaining attention\namong researchers and practitioners. Existing work, such as SIM and TWIN,\ntypically employs a two-stage approach to model long-term user behavior\nsequences for efficiency concerns. The first stage rapidly retrieves a subset\nof sequences related to the target item from a long sequence using a\nsearch-based mechanism namely the General Search Unit (GSU), while the second\nstage calculates the interest scores using the Exact Search Unit (ESU) on the\nretrieved results. Given the extensive length of user behavior sequences\nspanning the entire life cycle, potentially reaching up to 10^6 in scale, there\nis currently no effective solution for fully modeling such expansive user\ninterests. To overcome this issue, we introduced TWIN-V2, an enhancement of\nTWIN, where a divide-and-conquer approach is applied to compress life-cycle\nbehaviors and uncover more accurate and diverse user interests. Specifically, a\nhierarchical clustering method groups items with similar characteristics in\nlife-cycle behaviors into a single cluster during the offline phase. By\nlimiting the size of clusters, we can compress behavior sequences well beyond\nthe magnitude of 10^5 to a length manageable for online inference in GSU\nretrieval. Cluster-aware target attention extracts comprehensive and\nmulti-faceted long-term interests of users, thereby making the final\nrecommendation results more accurate and diverse. Extensive offline experiments\non a multi-billion-scale industrial dataset and online A/B tests have\ndemonstrated the effectiveness of TWIN-V2. Under an efficient deployment\nframework, TWIN-V2 has been successfully deployed to the primary traffic that\nserves hundreds of millions of daily active users at Kuaishou."
                },
                "authors": [
                    {
                        "name": "Zihua Si"
                    },
                    {
                        "name": "Lin Guan"
                    },
                    {
                        "name": "ZhongXiang Sun"
                    },
                    {
                        "name": "Xiaoxue Zang"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Yiqun Hui"
                    },
                    {
                        "name": "Xingchao Cao"
                    },
                    {
                        "name": "Zeyu Yang"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Dewei Leng"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Chenbin Zhang"
                    },
                    {
                        "name": "Yanan Niu"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "arxiv_doi": "10.1145/3627673.3680030",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3627673.3680030",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.16357v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16357v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by CIKM 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08535v1",
                "updated": "2024-08-16T05:15:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    5,
                    15,
                    12,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T05:15:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    5,
                    15,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for\n  Advanced Retrieval-Augmented Generation in Fact-Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for\n  Advanced Retrieval-Augmented Generation in Fact-Checking"
                },
                "summary": "Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG) systems, their effectiveness is often hindered by a lack of\nintegration with entity relationships and community structures, limiting their\nability to provide contextually rich and accurate information retrieval for\nfact-checking. We introduce CommunityKG-RAG (Community Knowledge\nGraph-Retrieval Augmented Generation), a novel zero-shot framework that\nintegrates community structures within Knowledge Graphs (KGs) with RAG systems\nto enhance the fact-checking process. Capable of adapting to new domains and\nqueries without additional training, CommunityKG-RAG utilizes the multi-hop\nnature of community structures within KGs to significantly improve the accuracy\nand relevance of information retrieval. Our experimental results demonstrate\nthat CommunityKG-RAG outperforms traditional methods, representing a\nsignificant advancement in fact-checking by offering a robust, scalable, and\nefficient solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG) systems, their effectiveness is often hindered by a lack of\nintegration with entity relationships and community structures, limiting their\nability to provide contextually rich and accurate information retrieval for\nfact-checking. We introduce CommunityKG-RAG (Community Knowledge\nGraph-Retrieval Augmented Generation), a novel zero-shot framework that\nintegrates community structures within Knowledge Graphs (KGs) with RAG systems\nto enhance the fact-checking process. Capable of adapting to new domains and\nqueries without additional training, CommunityKG-RAG utilizes the multi-hop\nnature of community structures within KGs to significantly improve the accuracy\nand relevance of information retrieval. Our experimental results demonstrate\nthat CommunityKG-RAG outperforms traditional methods, representing a\nsignificant advancement in fact-checking by offering a robust, scalable, and\nefficient solution."
                },
                "authors": [
                    {
                        "name": "Rong-Ching Chang"
                    },
                    {
                        "name": "Jiawei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Zhang"
                },
                "author": "Jiawei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08527v1",
                "updated": "2024-08-16T04:54:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    54,
                    10,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T04:54:10Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    54,
                    10,
                    4,
                    229,
                    0
                ],
                "title": "Focus on Focus: Focus-oriented Representation Learning and Multi-view\n  Cross-modal Alignment for Glioma Grading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focus on Focus: Focus-oriented Representation Learning and Multi-view\n  Cross-modal Alignment for Glioma Grading"
                },
                "summary": "Recently, multimodal deep learning, which integrates histopathology slides\nand molecular biomarkers, has achieved a promising performance in glioma\ngrading. Despite great progress, due to the intra-modality complexity and\ninter-modality heterogeneity, existing studies suffer from inadequate\nhistopathology representation learning and inefficient molecular-pathology\nknowledge alignment. These two issues hinder existing methods to precisely\ninterpret diagnostic molecular-pathology features, thereby limiting their\ngrading performance. Moreover, the real-world applicability of existing\nmultimodal approaches is significantly restricted as molecular biomarkers are\nnot always available during clinical deployment. To address these problems, we\nintroduce a novel Focus on Focus (FoF) framework with paired pathology-genomic\ntraining and applicable pathology-only inference, enhancing molecular-pathology\nrepresentation effectively. Specifically, we propose a Focus-oriented\nRepresentation Learning (FRL) module to encourage the model to identify regions\npositively or negatively related to glioma grading and guide it to focus on the\ndiagnostic areas with a consistency constraint. To effectively link the\nmolecular biomarkers to morphological features, we propose a Multi-view\nCross-modal Alignment (MCA) module that projects histopathology representations\ninto molecular subspaces, aligning morphological features with corresponding\nmolecular biomarker status by supervised contrastive learning. Experiments on\nthe TCGA GBM-LGG dataset demonstrate that our FoF framework significantly\nimproves the glioma grading. Remarkably, our FoF achieves superior performance\nusing only histopathology slides compared to existing multimodal methods. The\nsource code is available at https://github.com/peterlipan/FoF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, multimodal deep learning, which integrates histopathology slides\nand molecular biomarkers, has achieved a promising performance in glioma\ngrading. Despite great progress, due to the intra-modality complexity and\ninter-modality heterogeneity, existing studies suffer from inadequate\nhistopathology representation learning and inefficient molecular-pathology\nknowledge alignment. These two issues hinder existing methods to precisely\ninterpret diagnostic molecular-pathology features, thereby limiting their\ngrading performance. Moreover, the real-world applicability of existing\nmultimodal approaches is significantly restricted as molecular biomarkers are\nnot always available during clinical deployment. To address these problems, we\nintroduce a novel Focus on Focus (FoF) framework with paired pathology-genomic\ntraining and applicable pathology-only inference, enhancing molecular-pathology\nrepresentation effectively. Specifically, we propose a Focus-oriented\nRepresentation Learning (FRL) module to encourage the model to identify regions\npositively or negatively related to glioma grading and guide it to focus on the\ndiagnostic areas with a consistency constraint. To effectively link the\nmolecular biomarkers to morphological features, we propose a Multi-view\nCross-modal Alignment (MCA) module that projects histopathology representations\ninto molecular subspaces, aligning morphological features with corresponding\nmolecular biomarker status by supervised contrastive learning. Experiments on\nthe TCGA GBM-LGG dataset demonstrate that our FoF framework significantly\nimproves the glioma grading. Remarkably, our FoF achieves superior performance\nusing only histopathology slides compared to existing multimodal methods. The\nsource code is available at https://github.com/peterlipan/FoF."
                },
                "authors": [
                    {
                        "name": "Li Pan"
                    },
                    {
                        "name": "Yupei Zhang"
                    },
                    {
                        "name": "Qiushi Yang"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Xiaohan Xing"
                    },
                    {
                        "name": "Maximus C. F. Yeung"
                    },
                    {
                        "name": "Zhen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Chen"
                },
                "author": "Zhen Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10753v2",
                "updated": "2024-08-16T04:12:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    0,
                    4,
                    229,
                    0
                ],
                "published": "2024-02-16T15:19:46Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    15,
                    19,
                    46,
                    4,
                    47,
                    0
                ],
                "title": "ToolSword: Unveiling Safety Issues of Large Language Models in Tool\n  Learning Across Three Stages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolSword: Unveiling Safety Issues of Large Language Models in Tool\n  Learning Across Three Stages"
                },
                "summary": "Tool learning is widely acknowledged as a foundational approach or deploying\nlarge language models (LLMs) in real-world scenarios. While current research\nprimarily emphasizes leveraging tools to augment LLMs, it frequently neglects\nemerging safety considerations tied to their application. To fill this gap, we\npresent *ToolSword*, a comprehensive framework dedicated to meticulously\ninvestigating safety issues linked to LLMs in tool learning. Specifically,\nToolSword delineates six safety scenarios for LLMs in tool learning,\nencompassing **malicious queries** and **jailbreak attacks** in the input\nstage, **noisy misdirection** and **risky cues** in the execution stage, and\n**harmful feedback** and **error conflicts** in the output stage. Experiments\nconducted on 11 open-source and closed-source LLMs reveal enduring safety\nchallenges in tool learning, such as handling harmful queries, employing risky\ntools, and delivering detrimental feedback, which even GPT-4 is susceptible to.\nMoreover, we conduct further studies with the aim of fostering research on tool\nlearning safety. The data is released in\nhttps://github.com/Junjie-Ye/ToolSword.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool learning is widely acknowledged as a foundational approach or deploying\nlarge language models (LLMs) in real-world scenarios. While current research\nprimarily emphasizes leveraging tools to augment LLMs, it frequently neglects\nemerging safety considerations tied to their application. To fill this gap, we\npresent *ToolSword*, a comprehensive framework dedicated to meticulously\ninvestigating safety issues linked to LLMs in tool learning. Specifically,\nToolSword delineates six safety scenarios for LLMs in tool learning,\nencompassing **malicious queries** and **jailbreak attacks** in the input\nstage, **noisy misdirection** and **risky cues** in the execution stage, and\n**harmful feedback** and **error conflicts** in the output stage. Experiments\nconducted on 11 open-source and closed-source LLMs reveal enduring safety\nchallenges in tool learning, such as handling harmful queries, employing risky\ntools, and delivering detrimental feedback, which even GPT-4 is susceptible to.\nMoreover, we conduct further studies with the aim of fostering research on tool\nlearning safety. The data is released in\nhttps://github.com/Junjie-Ye/ToolSword."
                },
                "authors": [
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Sixian Li"
                    },
                    {
                        "name": "Guanyu Li"
                    },
                    {
                        "name": "Caishuang Huang"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Yilong Wu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Accepted by ACL 2024 Main Conference",
                "arxiv_journal_ref": "Proceedings of the 62nd Annual Meeting of the Association for\n  Computational Linguistics 2024 (Volume 1: Long Papers)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01667v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01667v2",
                "updated": "2024-08-16T03:25:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    3,
                    25,
                    51,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-03T05:08:27Z",
                "published_parsed": [
                    2024,
                    8,
                    3,
                    5,
                    8,
                    27,
                    5,
                    216,
                    0
                ],
                "title": "Automated Phishing Detection Using URLs and Webpages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Phishing Detection Using URLs and Webpages"
                },
                "summary": "Phishing detection is a critical cybersecurity task that involves the\nidentification and neutralization of fraudulent attempts to obtain sensitive\ninformation, thereby safeguarding individuals and organizations from data\nbreaches and financial loss. In this project, we address the constraints of\ntraditional reference-based phishing detection by developing an LLM agent\nframework. This agent harnesses Large Language Models to actively fetch and\nutilize online information, thus providing a dynamic reference system for more\naccurate phishing detection. This innovation circumvents the need for a static\nknowledge base, offering a significant enhancement in adaptability and\nefficiency for automated security measures.\n  The project report includes an initial study and problem analysis of existing\nsolutions, which motivated us to develop a new framework. We demonstrate the\nframework with LLMs simulated as agents and detail the techniques required for\nconstruction, followed by a complete implementation with a proof-of-concept as\nwell as experiments to evaluate our solution's performance against other\nsimilar solutions. The results show that our approach has achieved with\naccuracy of 0.945, significantly outperforms the existing solution(DynaPhish)\nby 0.445. Furthermore, we discuss the limitations of our approach and suggest\nimprovements that could make it more effective.\n  Overall, the proposed framework has the potential to enhance the\neffectiveness of current reference-based phishing detection approaches and\ncould be adapted for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing detection is a critical cybersecurity task that involves the\nidentification and neutralization of fraudulent attempts to obtain sensitive\ninformation, thereby safeguarding individuals and organizations from data\nbreaches and financial loss. In this project, we address the constraints of\ntraditional reference-based phishing detection by developing an LLM agent\nframework. This agent harnesses Large Language Models to actively fetch and\nutilize online information, thus providing a dynamic reference system for more\naccurate phishing detection. This innovation circumvents the need for a static\nknowledge base, offering a significant enhancement in adaptability and\nefficiency for automated security measures.\n  The project report includes an initial study and problem analysis of existing\nsolutions, which motivated us to develop a new framework. We demonstrate the\nframework with LLMs simulated as agents and detail the techniques required for\nconstruction, followed by a complete implementation with a proof-of-concept as\nwell as experiments to evaluate our solution's performance against other\nsimilar solutions. The results show that our approach has achieved with\naccuracy of 0.945, significantly outperforms the existing solution(DynaPhish)\nby 0.445. Furthermore, we discuss the limitations of our approach and suggest\nimprovements that could make it more effective.\n  Overall, the proposed framework has the potential to enhance the\neffectiveness of current reference-based phishing detection approaches and\ncould be adapted for real-world applications."
                },
                "authors": [
                    {
                        "name": "Huilin Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01667v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01667v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02223v2",
                "updated": "2024-08-16T03:18:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    3,
                    18,
                    12,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-05T03:54:52Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    3,
                    54,
                    52,
                    0,
                    218,
                    0
                ],
                "title": "Large Language Model Aided QoS Prediction for Service Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Aided QoS Prediction for Service Recommendation"
                },
                "summary": "Large language models (LLMs) have seen rapid improvement in the recent years,\nand have been used in a wider range of applications. After being trained on\nlarge text corpus, LLMs obtain the capability of extracting rich features from\ntextual data. Such capability is potentially useful for the web service\nrecommendation task, where the web users and services have intrinsic attributes\nthat can be described using natural language sentences and are useful for\nrecommendation. In this paper, we explore the possibility and practicality of\nusing LLMs for web service recommendation. We propose the large language model\naided QoS prediction (llmQoS) model, which use LLMs to extract useful\ninformation from attributes of web users and services via descriptive\nsentences. This information is then used in combination with the QoS values of\nhistorical interactions of users and services, to predict QoS values for any\ngiven user-service pair. On the WSDream dataset, llmQoS is shown to overcome\nthe data sparsity issue inherent to the QoS prediction problem, and outperforms\ncomparable baseline models consistently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen rapid improvement in the recent years,\nand have been used in a wider range of applications. After being trained on\nlarge text corpus, LLMs obtain the capability of extracting rich features from\ntextual data. Such capability is potentially useful for the web service\nrecommendation task, where the web users and services have intrinsic attributes\nthat can be described using natural language sentences and are useful for\nrecommendation. In this paper, we explore the possibility and practicality of\nusing LLMs for web service recommendation. We propose the large language model\naided QoS prediction (llmQoS) model, which use LLMs to extract useful\ninformation from attributes of web users and services via descriptive\nsentences. This information is then used in combination with the QoS values of\nhistorical interactions of users and services, to predict QoS values for any\ngiven user-service pair. On the WSDream dataset, llmQoS is shown to overcome\nthe data sparsity issue inherent to the QoS prediction problem, and outperforms\ncomparable baseline models consistently."
                },
                "authors": [
                    {
                        "name": "Huiying Liu"
                    },
                    {
                        "name": "Zekun Zhang"
                    },
                    {
                        "name": "Honghao Li"
                    },
                    {
                        "name": "Qilin Wu"
                    },
                    {
                        "name": "Yiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwen Zhang"
                },
                "author": "Yiwen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08506v1",
                "updated": "2024-08-16T03:06:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    3,
                    6,
                    57,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T03:06:57Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    3,
                    6,
                    57,
                    4,
                    229,
                    0
                ],
                "title": "Ex3: Automatic Novel Writing by Extracting, Excelsior and Expanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ex3: Automatic Novel Writing by Extracting, Excelsior and Expanding"
                },
                "summary": "Generating long-term texts such as novels using artificial intelligence has\nalways been a challenge. A common approach is to use large language models\n(LLMs) to construct a hierarchical framework that first plans and then writes.\nDespite the fact that the generated novels reach a sufficient length, they\nexhibit poor logical coherence and appeal in their plots and deficiencies in\ncharacter and event depiction, ultimately compromising the overall narrative\nquality. In this paper, we propose a method named Extracting Excelsior and\nExpanding. Ex3 initially extracts structure information from raw novel data. By\ncombining this structure information with the novel data, an\ninstruction-following dataset is meticulously crafted. This dataset is then\nutilized to fine-tune the LLM, aiming for excelsior generation performance. In\nthe final stage, a tree-like expansion method is deployed to facilitate the\ngeneration of arbitrarily long novels. Evaluation against previous methods\nshowcases Ex3's ability to produce higher-quality long-form novels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long-term texts such as novels using artificial intelligence has\nalways been a challenge. A common approach is to use large language models\n(LLMs) to construct a hierarchical framework that first plans and then writes.\nDespite the fact that the generated novels reach a sufficient length, they\nexhibit poor logical coherence and appeal in their plots and deficiencies in\ncharacter and event depiction, ultimately compromising the overall narrative\nquality. In this paper, we propose a method named Extracting Excelsior and\nExpanding. Ex3 initially extracts structure information from raw novel data. By\ncombining this structure information with the novel data, an\ninstruction-following dataset is meticulously crafted. This dataset is then\nutilized to fine-tune the LLM, aiming for excelsior generation performance. In\nthe final stage, a tree-like expansion method is deployed to facilitate the\ngeneration of arbitrarily long novels. Evaluation against previous methods\nshowcases Ex3's ability to produce higher-quality long-form novels."
                },
                "authors": [
                    {
                        "name": "Huang Lei"
                    },
                    {
                        "name": "Jiaming Guo"
                    },
                    {
                        "name": "Guanhua He"
                    },
                    {
                        "name": "Xishan Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Shaohui Peng"
                    },
                    {
                        "name": "Shaoli Liu"
                    },
                    {
                        "name": "Tianshi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianshi Chen"
                },
                "author": "Tianshi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09170v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09170v5",
                "updated": "2024-08-16T02:21:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    2,
                    21,
                    13,
                    4,
                    229,
                    0
                ],
                "published": "2024-04-14T07:19:27Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    7,
                    19,
                    27,
                    6,
                    105,
                    0
                ],
                "title": "Distilling Reasoning Ability from Large Language Models with Adaptive\n  Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Reasoning Ability from Large Language Models with Adaptive\n  Thinking"
                },
                "summary": "Chain of thought finetuning (cot-finetuning) aims to endow small language\nmodels (SLM) with reasoning ability to improve their performance towards\nspecific tasks by allowing them to imitate the reasoning procedure of large\nlanguage models (LLM) beyond simply predicting the answers. Most existing\ncot-finetuning methods adopt a pre-thinking mechanism, allowing the SLM to\ngenerate a rationale before providing an answer. This mechanism enables SLM to\nanalyze and think about complex questions, but it also makes answer correctness\nhighly sensitive to minor errors in rationale. Therefore, we propose a robust\npost-thinking mechanism to generate answers before rationale. Thanks to this\nanswer-first setting, 1) the answer can escape from the adverse effects caused\nby minor errors in the rationale; 2) the rationale serves as an error amplifier\nto the answer, which makes the SLM focus on learning hard samples; 3) the\ninferring efficiency can also benefit from the setting since users can stop the\ngeneration right after answers are outputted when inference is conducted.\nHowever, although the post-thinking mechanism brings many advantages and\nimproves the overall performance of SLM on specific tasks, it may lose the\nability to think about the questions and decompose complex questions into\nsimple sub-questions compared to pre-thinking mechanism. Therefore, a\nplug-and-play adaptive-thinking mechanism is proposed with the aid of the soft\nprompt tuning to integrate the merits of the pre-thinking mechanism and\npost-thinking mechanism, in which a perception module is introduced to\nadaptively prompt SLM answer or think first based on perceiving the complexity\nof the questions. Extensive experiments are conducted across 12 reasoning tasks\nand 2 representative language models to demonstrate the effectiveness of the\nproposed mechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of thought finetuning (cot-finetuning) aims to endow small language\nmodels (SLM) with reasoning ability to improve their performance towards\nspecific tasks by allowing them to imitate the reasoning procedure of large\nlanguage models (LLM) beyond simply predicting the answers. Most existing\ncot-finetuning methods adopt a pre-thinking mechanism, allowing the SLM to\ngenerate a rationale before providing an answer. This mechanism enables SLM to\nanalyze and think about complex questions, but it also makes answer correctness\nhighly sensitive to minor errors in rationale. Therefore, we propose a robust\npost-thinking mechanism to generate answers before rationale. Thanks to this\nanswer-first setting, 1) the answer can escape from the adverse effects caused\nby minor errors in the rationale; 2) the rationale serves as an error amplifier\nto the answer, which makes the SLM focus on learning hard samples; 3) the\ninferring efficiency can also benefit from the setting since users can stop the\ngeneration right after answers are outputted when inference is conducted.\nHowever, although the post-thinking mechanism brings many advantages and\nimproves the overall performance of SLM on specific tasks, it may lose the\nability to think about the questions and decompose complex questions into\nsimple sub-questions compared to pre-thinking mechanism. Therefore, a\nplug-and-play adaptive-thinking mechanism is proposed with the aid of the soft\nprompt tuning to integrate the merits of the pre-thinking mechanism and\npost-thinking mechanism, in which a perception module is introduced to\nadaptively prompt SLM answer or think first based on perceiving the complexity\nof the questions. Extensive experiments are conducted across 12 reasoning tasks\nand 2 representative language models to demonstrate the effectiveness of the\nproposed mechanism."
                },
                "authors": [
                    {
                        "name": "Xiaoshu Chen"
                    },
                    {
                        "name": "Sihang Zhou"
                    },
                    {
                        "name": "Ke Liang"
                    },
                    {
                        "name": "Xinwang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xinwang Liu"
                },
                "author": "Xinwang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09170v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09170v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08491v1",
                "updated": "2024-08-16T02:20:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    2,
                    20,
                    55,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T02:20:55Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    2,
                    20,
                    55,
                    4,
                    229,
                    0
                ],
                "title": "Multifunctional Bistable Ultrathin Composite Booms with Flexible\n  Electronics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multifunctional Bistable Ultrathin Composite Booms with Flexible\n  Electronics"
                },
                "summary": "Small satellites such as CubeSats pose demanding requirements on the weight,\nsize, and multifunctionality of their structures due to extreme constraints on\nthe payload mass and volume. To address this challenge, we introduce a concept\nof multifunctional deployable space structures for CubeSats based on ultrathin,\nelastically foldable, and self-deployable bistable composite structures\nintegrated with flexible electronics. The multifunctional bistable booms can be\nstored in a coiled configuration and self-deploy into a long structure upon\ninitiation by releasing the stored strain energy. The boom demonstrates the\ncapabilities of delivering power and transmitting data from the CubeSat to the\nflexible devices on the boom tip. The boom also shows the ability to monitor\nthe dynamics and vibration during and after the deployment. A payload boom has\nbeen installed in a 3U CubeSat as flight hardware for in-space testing and\ndemonstration. This effort combines morphable ultrathin composite structures\nwith flexible electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small satellites such as CubeSats pose demanding requirements on the weight,\nsize, and multifunctionality of their structures due to extreme constraints on\nthe payload mass and volume. To address this challenge, we introduce a concept\nof multifunctional deployable space structures for CubeSats based on ultrathin,\nelastically foldable, and self-deployable bistable composite structures\nintegrated with flexible electronics. The multifunctional bistable booms can be\nstored in a coiled configuration and self-deploy into a long structure upon\ninitiation by releasing the stored strain energy. The boom demonstrates the\ncapabilities of delivering power and transmitting data from the CubeSat to the\nflexible devices on the boom tip. The boom also shows the ability to monitor\nthe dynamics and vibration during and after the deployment. A payload boom has\nbeen installed in a 3U CubeSat as flight hardware for in-space testing and\ndemonstration. This effort combines morphable ultrathin composite structures\nwith flexible electronics."
                },
                "authors": [
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Juan M. Fernandez"
                    },
                    {
                        "name": "Sven G. Bilen"
                    },
                    {
                        "name": "Xin Ning"
                    }
                ],
                "author_detail": {
                    "name": "Xin Ning"
                },
                "author": "Xin Ning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08477v1",
                "updated": "2024-08-16T01:40:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    1,
                    40,
                    23,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T01:40:23Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    1,
                    40,
                    23,
                    4,
                    229,
                    0
                ],
                "title": "Automating Transparency Mechanisms in the Judicial System Using LLMs:\n  Opportunities and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Transparency Mechanisms in the Judicial System Using LLMs:\n  Opportunities and Challenges"
                },
                "summary": "Bringing more transparency to the judicial system for the purposes of\nincreasing accountability often demands extensive effort from auditors who must\nmeticulously sift through numerous disorganized legal case files to detect\npatterns of bias and errors. For example, the high-profile investigation into\nthe Curtis Flowers case took seven reporters a full year to assemble evidence\nabout the prosecutor's history of selecting racially biased juries. LLMs have\nthe potential to automate and scale these transparency pipelines, especially\ngiven their demonstrated capabilities to extract information from unstructured\ndocuments. We discuss the opportunities and challenges of using LLMs to provide\ntransparency in two important court processes: jury selection in criminal\ntrials and housing eviction cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bringing more transparency to the judicial system for the purposes of\nincreasing accountability often demands extensive effort from auditors who must\nmeticulously sift through numerous disorganized legal case files to detect\npatterns of bias and errors. For example, the high-profile investigation into\nthe Curtis Flowers case took seven reporters a full year to assemble evidence\nabout the prosecutor's history of selecting racially biased juries. LLMs have\nthe potential to automate and scale these transparency pipelines, especially\ngiven their demonstrated capabilities to extract information from unstructured\ndocuments. We discuss the opportunities and challenges of using LLMs to provide\ntransparency in two important court processes: jury selection in criminal\ntrials and housing eviction cases."
                },
                "authors": [
                    {
                        "name": "Ishana Shastri"
                    },
                    {
                        "name": "Shomik Jain"
                    },
                    {
                        "name": "Barbara Engelhardt"
                    },
                    {
                        "name": "Ashia Wilson"
                    }
                ],
                "author_detail": {
                    "name": "Ashia Wilson"
                },
                "author": "Ashia Wilson",
                "arxiv_comment": "Accepted at the Seventh AAAI/ACM Conference on AI, Ethics, and\n  Society (AIES 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01902v2",
                "updated": "2024-08-16T01:36:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    1,
                    36,
                    22,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-04T02:12:51Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    2,
                    12,
                    51,
                    6,
                    217,
                    0
                ],
                "title": "A Comprehensive Survey on GNN Characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on GNN Characterization"
                },
                "summary": "Characterizing graph neural networks (GNNs) is essential for identifying\nperformance bottlenecks and facilitating their deployment. Despite substantial\nwork in this area, a comprehensive survey on GNN characterization is lacking.\nThis work presents a comprehensive survey, proposing a triple-level\nclassification method to categorize, summarize, and compare existing efforts.\nIn addition, we identify promising future directions for GNN characterization.\nOur survey aims to help scholars systematically understand GNN performance\nbottlenecks and patterns from a computer architecture perspective, contributing\nto more efficient GNN execution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing graph neural networks (GNNs) is essential for identifying\nperformance bottlenecks and facilitating their deployment. Despite substantial\nwork in this area, a comprehensive survey on GNN characterization is lacking.\nThis work presents a comprehensive survey, proposing a triple-level\nclassification method to categorize, summarize, and compare existing efforts.\nIn addition, we identify promising future directions for GNN characterization.\nOur survey aims to help scholars systematically understand GNN performance\nbottlenecks and patterns from a computer architecture perspective, contributing\nto more efficient GNN execution."
                },
                "authors": [
                    {
                        "name": "Meng Wu"
                    },
                    {
                        "name": "Mingyu Yan"
                    },
                    {
                        "name": "Wenming Li"
                    },
                    {
                        "name": "Xiaochun Ye"
                    },
                    {
                        "name": "Dongrui Fan"
                    },
                    {
                        "name": "Yuan Xie"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Xie"
                },
                "author": "Yuan Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.16035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.16035v3",
                "updated": "2024-08-16T01:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    1,
                    30,
                    12,
                    4,
                    229,
                    0
                ],
                "published": "2023-09-27T21:26:03Z",
                "published_parsed": [
                    2023,
                    9,
                    27,
                    21,
                    26,
                    3,
                    2,
                    270,
                    0
                ],
                "title": "MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical\n  Question Answering"
                },
                "summary": "Large Language Models (LLMs), although powerful in general domains, often\nperform poorly on domain-specific tasks such as medical question answering\n(QA). In addition, LLMs tend to function as \"black-boxes\", making it\nchallenging to modify their behavior. To address the problem, our work employs\na transparent process of retrieval augmented generation (RAG), aiming to\nimprove LLM responses without the need for fine-tuning or retraining.\nSpecifically, we propose a comprehensive retrieval strategy to extract medical\nfacts from an external knowledge base, and then inject them into the LLM's\nquery prompt. Focusing on medical QA, we evaluate the impact of different\nretrieval models and the number of facts on LLM performance using the\nMedQA-SMILE dataset. Notably, our retrieval-augmented Vicuna-7B model exhibited\nan accuracy improvement from 44.46% to 48.54%. This work underscores the\npotential of RAG to enhance LLM performance, offering a practical approach to\nmitigate the challenges posed by black-box LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), although powerful in general domains, often\nperform poorly on domain-specific tasks such as medical question answering\n(QA). In addition, LLMs tend to function as \"black-boxes\", making it\nchallenging to modify their behavior. To address the problem, our work employs\na transparent process of retrieval augmented generation (RAG), aiming to\nimprove LLM responses without the need for fine-tuning or retraining.\nSpecifically, we propose a comprehensive retrieval strategy to extract medical\nfacts from an external knowledge base, and then inject them into the LLM's\nquery prompt. Focusing on medical QA, we evaluate the impact of different\nretrieval models and the number of facts on LLM performance using the\nMedQA-SMILE dataset. Notably, our retrieval-augmented Vicuna-7B model exhibited\nan accuracy improvement from 44.46% to 48.54%. This work underscores the\npotential of RAG to enhance LLM performance, offering a practical approach to\nmitigate the challenges posed by black-box LLMs."
                },
                "authors": [
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Shaochen Xu"
                    },
                    {
                        "name": "Tianze Yang"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Quanzheng Li"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Ninghao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ninghao Liu"
                },
                "author": "Ninghao Liu",
                "arxiv_comment": "Accepted by AMIA 2024 Annual Symposium",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.16035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.16035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08475v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08475v2",
                "updated": "2024-08-19T01:04:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    1,
                    4,
                    7,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-16T01:21:57Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    1,
                    21,
                    57,
                    4,
                    229,
                    0
                ],
                "title": "Models Matter: Setting Accurate Privacy Expectations for Local and\n  Central Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Models Matter: Setting Accurate Privacy Expectations for Local and\n  Central Differential Privacy"
                },
                "summary": "Differential privacy is a popular privacy-enhancing technology that has been\ndeployed both in industry and government agencies. Unfortunately, existing\nexplanations of differential privacy fail to set accurate privacy expectations\nfor data subjects, which depend on the choice of deployment model. We design\nand evaluate new explanations of differential privacy for the local and central\nmodels, drawing inspiration from prior work explaining other privacy-enhancing\ntechnologies. We find that consequences-focused explanations in the style of\nprivacy nutrition labels that lay out the implications of differential privacy\nare a promising approach for setting accurate privacy expectations. Further, we\nfind that while process-focused explanations are not enough to set accurate\nprivacy expectations, combining consequences-focused explanations with a brief\ndescription of how differential privacy works leads to greater trust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential privacy is a popular privacy-enhancing technology that has been\ndeployed both in industry and government agencies. Unfortunately, existing\nexplanations of differential privacy fail to set accurate privacy expectations\nfor data subjects, which depend on the choice of deployment model. We design\nand evaluate new explanations of differential privacy for the local and central\nmodels, drawing inspiration from prior work explaining other privacy-enhancing\ntechnologies. We find that consequences-focused explanations in the style of\nprivacy nutrition labels that lay out the implications of differential privacy\nare a promising approach for setting accurate privacy expectations. Further, we\nfind that while process-focused explanations are not enough to set accurate\nprivacy expectations, combining consequences-focused explanations with a brief\ndescription of how differential privacy works leads to greater trust."
                },
                "authors": [
                    {
                        "name": "Mary Anne Smart"
                    },
                    {
                        "name": "Priyanka Nanayakkara"
                    },
                    {
                        "name": "Rachel Cummings"
                    },
                    {
                        "name": "Gabriel Kaptchuk"
                    },
                    {
                        "name": "Elissa Redmiles"
                    }
                ],
                "author_detail": {
                    "name": "Elissa Redmiles"
                },
                "author": "Elissa Redmiles",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08475v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02212v2",
                "updated": "2024-08-16T01:16:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    1,
                    16,
                    20,
                    4,
                    229,
                    0
                ],
                "published": "2024-02-03T17:13:03Z",
                "published_parsed": [
                    2024,
                    2,
                    3,
                    17,
                    13,
                    3,
                    5,
                    34,
                    0
                ],
                "title": "A Data Generation Perspective to the Mechanism of In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data Generation Perspective to the Mechanism of In-Context Learning"
                },
                "summary": "In-Context Learning (ICL) empowers Large Language Models (LLMs) with the\ncapacity to learn in context, achieving downstream generalization without\ngradient updates but with a few in-context examples. Despite the encouraging\nempirical success, the underlying mechanism of ICL remains unclear, and\nexisting research offers various viewpoints of understanding. These studies\npropose intuition-driven and ad-hoc technical solutions for interpreting ICL,\nillustrating an ambiguous road map. In this paper, we leverage a data\ngeneration perspective to reinterpret recent efforts and demonstrate the\npotential broader usage of popular technical solutions, approaching a\nsystematic angle. For a conceptual definition, we rigorously adopt the terms of\nskill learning and skill recognition. The difference between them is skill\nlearning can learn new data generation functions from in-context data. We also\nprovide a comprehensive study on the merits and weaknesses of different\nsolutions, and highlight the uniformity among them given the perspective of\ndata generation, establishing a technical foundation for future research to\nincorporate the strengths of different lines of research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) empowers Large Language Models (LLMs) with the\ncapacity to learn in context, achieving downstream generalization without\ngradient updates but with a few in-context examples. Despite the encouraging\nempirical success, the underlying mechanism of ICL remains unclear, and\nexisting research offers various viewpoints of understanding. These studies\npropose intuition-driven and ad-hoc technical solutions for interpreting ICL,\nillustrating an ambiguous road map. In this paper, we leverage a data\ngeneration perspective to reinterpret recent efforts and demonstrate the\npotential broader usage of popular technical solutions, approaching a\nsystematic angle. For a conceptual definition, we rigorously adopt the terms of\nskill learning and skill recognition. The difference between them is skill\nlearning can learn new data generation functions from in-context data. We also\nprovide a comprehensive study on the merits and weaknesses of different\nsolutions, and highlight the uniformity among them given the perspective of\ndata generation, establishing a technical foundation for future research to\nincorporate the strengths of different lines of research."
                },
                "authors": [
                    {
                        "name": "Haitao Mao"
                    },
                    {
                        "name": "Guangliang Liu"
                    },
                    {
                        "name": "Yao Ma"
                    },
                    {
                        "name": "Rongrong Wang"
                    },
                    {
                        "name": "Kristen Johnson"
                    },
                    {
                        "name": "Jiliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Tang"
                },
                "author": "Jiliang Tang",
                "arxiv_comment": "11 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08470v1",
                "updated": "2024-08-16T01:12:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    1,
                    12,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T01:12:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    1,
                    12,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "Context-Aware Assistant Selection for Improved Inference Acceleration\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Assistant Selection for Improved Inference Acceleration\n  with Large Language Models"
                },
                "summary": "Despite their widespread adoption, large language models (LLMs) remain\nprohibitive to use under resource constraints, with their ever growing sizes\nonly increasing the barrier for use. One noted issue is the high latency\nassociated with auto-regressive generation, rendering large LLMs use dependent\non advanced computing infrastructure. Assisted decoding, where a smaller draft\nmodel guides a larger target model's generation, has helped alleviate this, but\nremains dependent on alignment between the two models. Thus if the draft model\nis insufficiently capable on some domain relative to the target model,\nperformance can degrade. Alternatively, one can leverage multiple draft models\nto better cover the expertise of the target, but when multiple black-box draft\nmodels are available, selecting an assistant without details about its\nconstruction can be difficult. To better understand this decision making\nproblem, we observe it as a contextual bandit, where a policy must choose a\ndraft model based on a context. We show that even without prior knowledge of\nthe draft models, creating an offline dataset from only outputs of independent\ndraft/target models and training a policy over the alignment of these outputs\ncan accelerate performance on multiple domains provided the candidates are\neffective. Further results show this to hold on various settings with multiple\nassisted decoding candidates, highlighting its flexibility and the advantageous\nrole that such decision making can play.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their widespread adoption, large language models (LLMs) remain\nprohibitive to use under resource constraints, with their ever growing sizes\nonly increasing the barrier for use. One noted issue is the high latency\nassociated with auto-regressive generation, rendering large LLMs use dependent\non advanced computing infrastructure. Assisted decoding, where a smaller draft\nmodel guides a larger target model's generation, has helped alleviate this, but\nremains dependent on alignment between the two models. Thus if the draft model\nis insufficiently capable on some domain relative to the target model,\nperformance can degrade. Alternatively, one can leverage multiple draft models\nto better cover the expertise of the target, but when multiple black-box draft\nmodels are available, selecting an assistant without details about its\nconstruction can be difficult. To better understand this decision making\nproblem, we observe it as a contextual bandit, where a policy must choose a\ndraft model based on a context. We show that even without prior knowledge of\nthe draft models, creating an offline dataset from only outputs of independent\ndraft/target models and training a policy over the alignment of these outputs\ncan accelerate performance on multiple domains provided the candidates are\neffective. Further results show this to hold on various settings with multiple\nassisted decoding candidates, highlighting its flexibility and the advantageous\nrole that such decision making can play."
                },
                "authors": [
                    {
                        "name": "Jerry Huang"
                    },
                    {
                        "name": "Prasanna Parthasarathi"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Sarath Chandar"
                    }
                ],
                "author_detail": {
                    "name": "Sarath Chandar"
                },
                "author": "Sarath Chandar",
                "arxiv_comment": "14 pages (9 pages main content + references + appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08464v1",
                "updated": "2024-08-16T00:18:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    0,
                    18,
                    23,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T00:18:23Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    0,
                    18,
                    23,
                    4,
                    229,
                    0
                ],
                "title": "\\textit{MMJ-Bench}: A Comprehensive Study on Jailbreak Attacks and\n  Defenses for Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\\textit{MMJ-Bench}: A Comprehensive Study on Jailbreak Attacks and\n  Defenses for Vision Language Models"
                },
                "summary": "As deep learning advances, Large Language Models (LLMs) and their multimodal\ncounterparts, Vision-Language Models (VLMs), have shown exceptional performance\nin many real-world tasks. However, VLMs face significant security challenges,\nsuch as jailbreak attacks, where attackers attempt to bypass the model's safety\nalignment to elicit harmful responses. The threat of jailbreak attacks on VLMs\narises from both the inherent vulnerabilities of LLMs and the multiple\ninformation channels that VLMs process. While various attacks and defenses have\nbeen proposed, there is a notable gap in unified and comprehensive evaluations,\nas each method is evaluated on different dataset and metrics, making it\nimpossible to compare the effectiveness of each method. To address this gap, we\nintroduce \\textit{MMJ-Bench}, a unified pipeline for evaluating jailbreak\nattacks and defense techniques for VLMs. Through extensive experiments, we\nassess the effectiveness of various attack methods against SoTA VLMs and\nevaluate the impact of defense mechanisms on both defense effectiveness and\nmodel utility for normal tasks. Our comprehensive evaluation contribute to the\nfield by offering a unified and systematic evaluation framework and the first\npublic-available benchmark for VLM jailbreak research. We also demonstrate\nseveral insightful findings that highlights directions for future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As deep learning advances, Large Language Models (LLMs) and their multimodal\ncounterparts, Vision-Language Models (VLMs), have shown exceptional performance\nin many real-world tasks. However, VLMs face significant security challenges,\nsuch as jailbreak attacks, where attackers attempt to bypass the model's safety\nalignment to elicit harmful responses. The threat of jailbreak attacks on VLMs\narises from both the inherent vulnerabilities of LLMs and the multiple\ninformation channels that VLMs process. While various attacks and defenses have\nbeen proposed, there is a notable gap in unified and comprehensive evaluations,\nas each method is evaluated on different dataset and metrics, making it\nimpossible to compare the effectiveness of each method. To address this gap, we\nintroduce \\textit{MMJ-Bench}, a unified pipeline for evaluating jailbreak\nattacks and defense techniques for VLMs. Through extensive experiments, we\nassess the effectiveness of various attack methods against SoTA VLMs and\nevaluate the impact of defense mechanisms on both defense effectiveness and\nmodel utility for normal tasks. Our comprehensive evaluation contribute to the\nfield by offering a unified and systematic evaluation framework and the first\npublic-available benchmark for VLM jailbreak research. We also demonstrate\nseveral insightful findings that highlights directions for future studies."
                },
                "authors": [
                    {
                        "name": "Fenghua Weng"
                    },
                    {
                        "name": "Yue Xu"
                    },
                    {
                        "name": "Chengyan Fu"
                    },
                    {
                        "name": "Wenjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Wang"
                },
                "author": "Wenjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07846v2",
                "updated": "2024-08-16T00:18:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    0,
                    18,
                    3,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-14T23:02:16Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    2,
                    16,
                    2,
                    227,
                    0
                ],
                "title": "A System for Automated Unit Test Generation Using Large Language Models\n  and Assessment of Generated Test Suites",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Automated Unit Test Generation Using Large Language Models\n  and Assessment of Generated Test Suites"
                },
                "summary": "Unit tests represent the most basic level of testing within the software\ntesting lifecycle and are crucial to ensuring software correctness. Designing\nand creating unit tests is a costly and labor-intensive process that is ripe\nfor automation. Recently, Large Language Models (LLMs) have been applied to\nvarious aspects of software development, including unit test generation.\nAlthough several empirical studies evaluating LLMs' capabilities in test code\ngeneration exist, they primarily focus on simple scenarios, such as the\nstraightforward generation of unit tests for individual methods. These\nevaluations often involve independent and small-scale test units, providing a\nlimited view of LLMs' performance in real-world software development scenarios.\nMoreover, previous studies do not approach the problem at a suitable scale for\nreal-life applications. Generated unit tests are often evaluated via manual\nintegration into the original projects, a process that limits the number of\ntests executed and reduces overall efficiency. To address these gaps, we have\ndeveloped an approach for generating and evaluating more real-life complexity\ntest suites. Our approach focuses on class-level test code generation and\nautomates the entire process from test generation to test assessment. In this\nwork, we present AgoneTest: an automated system for generating test suites for\nJava projects and a comprehensive and principled methodology for evaluating the\ngenerated test suites. Starting from a state-of-the-art dataset (i.e.,\nMethods2Test), we built a new dataset for comparing human-written tests with\nthose generated by LLMs. Our key contributions include a scalable automated\nsoftware system, a new dataset, and a detailed methodology for evaluating test\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit tests represent the most basic level of testing within the software\ntesting lifecycle and are crucial to ensuring software correctness. Designing\nand creating unit tests is a costly and labor-intensive process that is ripe\nfor automation. Recently, Large Language Models (LLMs) have been applied to\nvarious aspects of software development, including unit test generation.\nAlthough several empirical studies evaluating LLMs' capabilities in test code\ngeneration exist, they primarily focus on simple scenarios, such as the\nstraightforward generation of unit tests for individual methods. These\nevaluations often involve independent and small-scale test units, providing a\nlimited view of LLMs' performance in real-world software development scenarios.\nMoreover, previous studies do not approach the problem at a suitable scale for\nreal-life applications. Generated unit tests are often evaluated via manual\nintegration into the original projects, a process that limits the number of\ntests executed and reduces overall efficiency. To address these gaps, we have\ndeveloped an approach for generating and evaluating more real-life complexity\ntest suites. Our approach focuses on class-level test code generation and\nautomates the entire process from test generation to test assessment. In this\nwork, we present AgoneTest: an automated system for generating test suites for\nJava projects and a comprehensive and principled methodology for evaluating the\ngenerated test suites. Starting from a state-of-the-art dataset (i.e.,\nMethods2Test), we built a new dataset for comparing human-written tests with\nthose generated by LLMs. Our key contributions include a scalable automated\nsoftware system, a new dataset, and a detailed methodology for evaluating test\nquality."
                },
                "authors": [
                    {
                        "name": "Andrea Lops"
                    },
                    {
                        "name": "Fedelucio Narducci"
                    },
                    {
                        "name": "Azzurra Ragone"
                    },
                    {
                        "name": "Michelantonio Trizio"
                    },
                    {
                        "name": "Claudio Bartolini"
                    }
                ],
                "author_detail": {
                    "name": "Claudio Bartolini"
                },
                "author": "Claudio Bartolini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08459v1",
                "updated": "2024-08-15T23:57:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    23,
                    57,
                    2,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T23:57:02Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    23,
                    57,
                    2,
                    3,
                    228,
                    0
                ],
                "title": "JPEG-LM: LLMs as Image Generators with Canonical Codec Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JPEG-LM: LLMs as Image Generators with Canonical Codec Representations"
                },
                "summary": "Recent work in image and video generation has been adopting the\nautoregressive LLM architecture due to its generality and potentially easy\nintegration into multi-modal systems. The crux of applying autoregressive\ntraining in language generation to visual generation is discretization --\nrepresenting continuous data like images and videos as discrete tokens. Common\nmethods of discretizing images and videos include modeling raw pixel values,\nwhich are prohibitively lengthy, or vector quantization, which requires\nconvoluted pre-hoc training. In this work, we propose to directly model images\nand videos as compressed files saved on computers via canonical codecs (e.g.,\nJPEG, AVC/H.264). Using the default Llama architecture without any\nvision-specific modifications, we pretrain JPEG-LM from scratch to generate\nimages (and AVC-LM to generate videos as a proof of concept), by directly\noutputting compressed file bytes in JPEG and AVC formats. Evaluation of image\ngeneration shows that this simple and straightforward approach is more\neffective than pixel-based modeling and sophisticated vector quantization\nbaselines (on which our method yields a 31% reduction in FID). Our analysis\nshows that JPEG-LM has an especial advantage over vector quantization models in\ngenerating long-tail visual elements. Overall, we show that using canonical\ncodec representations can help lower the barriers between language generation\nand visual generation, facilitating future research on multi-modal\nlanguage/image/video LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work in image and video generation has been adopting the\nautoregressive LLM architecture due to its generality and potentially easy\nintegration into multi-modal systems. The crux of applying autoregressive\ntraining in language generation to visual generation is discretization --\nrepresenting continuous data like images and videos as discrete tokens. Common\nmethods of discretizing images and videos include modeling raw pixel values,\nwhich are prohibitively lengthy, or vector quantization, which requires\nconvoluted pre-hoc training. In this work, we propose to directly model images\nand videos as compressed files saved on computers via canonical codecs (e.g.,\nJPEG, AVC/H.264). Using the default Llama architecture without any\nvision-specific modifications, we pretrain JPEG-LM from scratch to generate\nimages (and AVC-LM to generate videos as a proof of concept), by directly\noutputting compressed file bytes in JPEG and AVC formats. Evaluation of image\ngeneration shows that this simple and straightforward approach is more\neffective than pixel-based modeling and sophisticated vector quantization\nbaselines (on which our method yields a 31% reduction in FID). Our analysis\nshows that JPEG-LM has an especial advantage over vector quantization models in\ngenerating long-tail visual elements. Overall, we show that using canonical\ncodec representations can help lower the barriers between language generation\nand visual generation, facilitating future research on multi-modal\nlanguage/image/video LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaochuang Han"
                    },
                    {
                        "name": "Marjan Ghazvininejad"
                    },
                    {
                        "name": "Pang Wei Koh"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    }
                ],
                "author_detail": {
                    "name": "Yulia Tsvetkov"
                },
                "author": "Yulia Tsvetkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01787v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01787v3",
                "updated": "2024-08-15T23:36:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    23,
                    36,
                    42,
                    3,
                    228,
                    0
                ],
                "published": "2024-02-01T23:12:57Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    23,
                    12,
                    57,
                    3,
                    32,
                    0
                ],
                "title": "Harm Amplification in Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harm Amplification in Text-to-Image Models"
                },
                "summary": "Text-to-image (T2I) models have emerged as a significant advancement in\ngenerative AI; however, there exist safety concerns regarding their potential\nto produce harmful image outputs even when users input seemingly safe prompts.\nThis phenomenon, where T2I models generate harmful representations that were\nnot explicit in the input prompt, poses a potentially greater risk than\nadversarial prompts, leaving users unintentionally exposed to harms. Our paper\naddresses this issue by formalizing a definition for this phenomenon which we\nterm harm amplification. We further contribute to the field by developing a\nframework of methodologies to quantify harm amplification in which we consider\nthe harm of the model output in the context of user input. We then empirically\nexamine how to apply these different methodologies to simulate real-world\ndeployment scenarios including a quantification of disparate impacts across\ngenders resulting from harm amplification. Together, our work aims to offer\nresearchers tools to comprehensively address safety challenges in T2I systems\nand contribute to the responsible deployment of generative AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) models have emerged as a significant advancement in\ngenerative AI; however, there exist safety concerns regarding their potential\nto produce harmful image outputs even when users input seemingly safe prompts.\nThis phenomenon, where T2I models generate harmful representations that were\nnot explicit in the input prompt, poses a potentially greater risk than\nadversarial prompts, leaving users unintentionally exposed to harms. Our paper\naddresses this issue by formalizing a definition for this phenomenon which we\nterm harm amplification. We further contribute to the field by developing a\nframework of methodologies to quantify harm amplification in which we consider\nthe harm of the model output in the context of user input. We then empirically\nexamine how to apply these different methodologies to simulate real-world\ndeployment scenarios including a quantification of disparate impacts across\ngenders resulting from harm amplification. Together, our work aims to offer\nresearchers tools to comprehensively address safety challenges in T2I systems\nand contribute to the responsible deployment of generative AI models."
                },
                "authors": [
                    {
                        "name": "Susan Hao"
                    },
                    {
                        "name": "Renee Shelby"
                    },
                    {
                        "name": "Yuchi Liu"
                    },
                    {
                        "name": "Hansa Srinivasan"
                    },
                    {
                        "name": "Mukul Bhutani"
                    },
                    {
                        "name": "Burcu Karagol Ayan"
                    },
                    {
                        "name": "Ryan Poplin"
                    },
                    {
                        "name": "Shivani Poddar"
                    },
                    {
                        "name": "Sarah Laszlo"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Laszlo"
                },
                "author": "Sarah Laszlo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01787v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01787v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08453v1",
                "updated": "2024-08-15T23:30:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    23,
                    30,
                    47,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T23:30:47Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    23,
                    30,
                    47,
                    3,
                    228,
                    0
                ],
                "title": "CRQBench: A Benchmark of Code Reasoning Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRQBench: A Benchmark of Code Reasoning Questions"
                },
                "summary": "Large Language Models have demonstrated exceptional proficiency on coding\ntasks, but it is challenging to precisely evaluate their code reasoning\nability. Existing benchmarks are insufficient as they are unrealistic and\nconflate semantic reasoning ability with performance on software engineering\ntasks. We introduce CRQBench, a benchmark of 100 C++ code reasoning questions\nand answers derived from contextualized code review comments. To curate\nCRQBench, we use an LLM assistant alongside human inspection, reducing manual\neffort. We conduct an evaluation of GPT-4 on CRQBench and find that it produces\ncorrect responses grounded in the given context for 65 of the 100 questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have demonstrated exceptional proficiency on coding\ntasks, but it is challenging to precisely evaluate their code reasoning\nability. Existing benchmarks are insufficient as they are unrealistic and\nconflate semantic reasoning ability with performance on software engineering\ntasks. We introduce CRQBench, a benchmark of 100 C++ code reasoning questions\nand answers derived from contextualized code review comments. To curate\nCRQBench, we use an LLM assistant alongside human inspection, reducing manual\neffort. We conduct an evaluation of GPT-4 on CRQBench and find that it produces\ncorrect responses grounded in the given context for 65 of the 100 questions."
                },
                "authors": [
                    {
                        "name": "Elizabeth Dinella"
                    },
                    {
                        "name": "Satish Chandra"
                    },
                    {
                        "name": "Petros Maniatis"
                    }
                ],
                "author_detail": {
                    "name": "Petros Maniatis"
                },
                "author": "Petros Maniatis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08448v1",
                "updated": "2024-08-15T22:57:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    22,
                    57,
                    39,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T22:57:39Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    22,
                    57,
                    39,
                    3,
                    228,
                    0
                ],
                "title": "Exploring Cross-model Neuronal Correlations in the Context of Predicting\n  Model Performance and Generalizability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Cross-model Neuronal Correlations in the Context of Predicting\n  Model Performance and Generalizability"
                },
                "summary": "As Artificial Intelligence (AI) models are increasingly integrated into\ncritical systems, the need for a robust framework to establish the\ntrustworthiness of AI is increasingly paramount. While collaborative efforts\nhave established conceptual foundations for such a framework, there remains a\nsignificant gap in developing concrete, technically robust methods for\nassessing AI model quality and performance. A critical drawback in the\ntraditional methods for assessing the validity and generalizability of models\nis their dependence on internal developer datasets, rendering it challenging to\nindependently assess and verify their performance claims. This paper introduces\na novel approach for assessing a newly trained model's performance based on\nanother known model by calculating correlation between neural networks. The\nproposed method evaluates correlations by determining if, for each neuron in\none network, there exists a neuron in the other network that produces similar\noutput. This approach has implications for memory efficiency, allowing for the\nuse of smaller networks when high correlation exists between networks of\ndifferent sizes. Additionally, the method provides insights into robustness,\nsuggesting that if two highly correlated networks are compared and one\ndemonstrates robustness when operating in production environments, the other is\nlikely to exhibit similar robustness. This contribution advances the technical\ntoolkit for responsible AI, supporting more comprehensive and nuanced\nevaluations of AI models to ensure their safe and effective deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Artificial Intelligence (AI) models are increasingly integrated into\ncritical systems, the need for a robust framework to establish the\ntrustworthiness of AI is increasingly paramount. While collaborative efforts\nhave established conceptual foundations for such a framework, there remains a\nsignificant gap in developing concrete, technically robust methods for\nassessing AI model quality and performance. A critical drawback in the\ntraditional methods for assessing the validity and generalizability of models\nis their dependence on internal developer datasets, rendering it challenging to\nindependently assess and verify their performance claims. This paper introduces\na novel approach for assessing a newly trained model's performance based on\nanother known model by calculating correlation between neural networks. The\nproposed method evaluates correlations by determining if, for each neuron in\none network, there exists a neuron in the other network that produces similar\noutput. This approach has implications for memory efficiency, allowing for the\nuse of smaller networks when high correlation exists between networks of\ndifferent sizes. Additionally, the method provides insights into robustness,\nsuggesting that if two highly correlated networks are compared and one\ndemonstrates robustness when operating in production environments, the other is\nlikely to exhibit similar robustness. This contribution advances the technical\ntoolkit for responsible AI, supporting more comprehensive and nuanced\nevaluations of AI models to ensure their safe and effective deployment."
                },
                "authors": [
                    {
                        "name": "Haniyeh Ehsani Oskouie"
                    },
                    {
                        "name": "Lionel Levine"
                    },
                    {
                        "name": "Majid Sarrafzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Majid Sarrafzadeh"
                },
                "author": "Majid Sarrafzadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08444v1",
                "updated": "2024-08-15T22:34:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    22,
                    34,
                    44,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T22:34:44Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    22,
                    34,
                    44,
                    3,
                    228,
                    0
                ],
                "title": "W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question\n  Answering"
                },
                "summary": "In knowledge-intensive tasks such as open-domain question answering (OpenQA),\nLarge Language Models (LLMs) often struggle to generate factual answers relying\nsolely on their internal (parametric) knowledge. To address this limitation,\nRetrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving\nrelevant information from external sources, thereby positioning the retriever\nas a pivotal component. Although dense retrieval demonstrates state-of-the-art\nperformance, its training poses challenges due to the scarcity of ground-truth\nevidence, largely attributed to the high costs of human annotation. In this\npaper, we propose W-RAG by utilizing the ranking capabilities of LLMs to create\nweakly labeled data for training dense retrievers. Specifically, we rerank the\ntop-$K$ passages retrieved via BM25 by assessing the probability that LLMs will\ngenerate the correct answer based on the question and each passage. The\nhighest-ranking passages are then used as positive training examples for dense\nretrieval. Our comprehensive experiments across four publicly available OpenQA\ndatasets demonstrate that our approach enhances both retrieval and OpenQA\nperformance compared to baseline models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In knowledge-intensive tasks such as open-domain question answering (OpenQA),\nLarge Language Models (LLMs) often struggle to generate factual answers relying\nsolely on their internal (parametric) knowledge. To address this limitation,\nRetrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving\nrelevant information from external sources, thereby positioning the retriever\nas a pivotal component. Although dense retrieval demonstrates state-of-the-art\nperformance, its training poses challenges due to the scarcity of ground-truth\nevidence, largely attributed to the high costs of human annotation. In this\npaper, we propose W-RAG by utilizing the ranking capabilities of LLMs to create\nweakly labeled data for training dense retrievers. Specifically, we rerank the\ntop-$K$ passages retrieved via BM25 by assessing the probability that LLMs will\ngenerate the correct answer based on the question and each passage. The\nhighest-ranking passages are then used as positive training examples for dense\nretrieval. Our comprehensive experiments across four publicly available OpenQA\ndatasets demonstrate that our approach enhances both retrieval and OpenQA\nperformance compared to baseline models."
                },
                "authors": [
                    {
                        "name": "Jinming Nian"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Yi Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Fang"
                },
                "author": "Yi Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17587v2",
                "updated": "2024-08-15T22:28:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    22,
                    28,
                    55,
                    3,
                    228,
                    0
                ],
                "published": "2024-05-27T18:40:49Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    18,
                    40,
                    49,
                    0,
                    148,
                    0
                ],
                "title": "RAGSys: Item-Cold-Start Recommender as RAG System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGSys: Item-Cold-Start Recommender as RAG System"
                },
                "summary": "Large Language Models (LLM) hold immense promise for real-world applications,\nbut their generic knowledge often falls short of domain-specific needs.\nFine-tuning, a common approach, can suffer from catastrophic forgetting and\nhinder generalizability. In-Context Learning (ICL) offers an alternative, which\ncan leverage Retrieval-Augmented Generation (RAG) to provide LLMs with relevant\ndemonstrations for few-shot learning tasks. This paper explores the desired\nqualities of a demonstration retrieval system for ICL. We argue that ICL\nretrieval in this context resembles item-cold-start recommender systems,\nprioritizing discovery and maximizing information gain over strict relevance.\nWe propose a novel evaluation method that measures the LLM's subsequent\nperformance on NLP tasks, eliminating the need for subjective diversity scores.\nOur findings demonstrate the critical role of diversity and quality bias in\nretrieved demonstrations for effective ICL, and highlight the potential of\nrecommender system techniques in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) hold immense promise for real-world applications,\nbut their generic knowledge often falls short of domain-specific needs.\nFine-tuning, a common approach, can suffer from catastrophic forgetting and\nhinder generalizability. In-Context Learning (ICL) offers an alternative, which\ncan leverage Retrieval-Augmented Generation (RAG) to provide LLMs with relevant\ndemonstrations for few-shot learning tasks. This paper explores the desired\nqualities of a demonstration retrieval system for ICL. We argue that ICL\nretrieval in this context resembles item-cold-start recommender systems,\nprioritizing discovery and maximizing information gain over strict relevance.\nWe propose a novel evaluation method that measures the LLM's subsequent\nperformance on NLP tasks, eliminating the need for subjective diversity scores.\nOur findings demonstrate the critical role of diversity and quality bias in\nretrieved demonstrations for effective ICL, and highlight the potential of\nrecommender system techniques in this domain."
                },
                "authors": [
                    {
                        "name": "Emile Contal"
                    },
                    {
                        "name": "Garrin McGoldrick"
                    }
                ],
                "author_detail": {
                    "name": "Garrin McGoldrick"
                },
                "author": "Garrin McGoldrick",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13193v2",
                "updated": "2024-08-15T22:27:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    22,
                    27,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-05-21T20:53:18Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    20,
                    53,
                    18,
                    1,
                    142,
                    0
                ],
                "title": "Efficient Imitation Learning with Conservative World Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Imitation Learning with Conservative World Models"
                },
                "summary": "We tackle the problem of policy learning from expert demonstrations without a\nreward function. A central challenge in this space is that these policies fail\nupon deployment due to issues of distributional shift, environment\nstochasticity, or compounding errors. Adversarial imitation learning alleviates\nthis issue but requires additional on-policy training samples for stability,\nwhich presents a challenge in realistic domains due to inefficient learning and\nhigh sample complexity. One approach to this issue is to learn a world model of\nthe environment, and use synthetic data for policy training. While successful\nin prior works, we argue that this is sub-optimal due to additional\ndistribution shifts between the learned model and the real environment.\nInstead, we re-frame imitation learning as a fine-tuning problem, rather than a\npure reinforcement learning one. Drawing theoretical connections to offline RL\nand fine-tuning algorithms, we argue that standard online world model\nalgorithms are not well suited to the imitation learning problem. We derive a\nprincipled conservative optimization bound and demonstrate empirically that it\nleads to improved performance on two very challenging manipulation environments\nfrom high-dimensional raw pixel observations. We set a new state-of-the-art\nperformance on the Franka Kitchen environment from images, requiring only 10\ndemos on no reward labels, as well as solving a complex dexterity manipulation\ntask.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We tackle the problem of policy learning from expert demonstrations without a\nreward function. A central challenge in this space is that these policies fail\nupon deployment due to issues of distributional shift, environment\nstochasticity, or compounding errors. Adversarial imitation learning alleviates\nthis issue but requires additional on-policy training samples for stability,\nwhich presents a challenge in realistic domains due to inefficient learning and\nhigh sample complexity. One approach to this issue is to learn a world model of\nthe environment, and use synthetic data for policy training. While successful\nin prior works, we argue that this is sub-optimal due to additional\ndistribution shifts between the learned model and the real environment.\nInstead, we re-frame imitation learning as a fine-tuning problem, rather than a\npure reinforcement learning one. Drawing theoretical connections to offline RL\nand fine-tuning algorithms, we argue that standard online world model\nalgorithms are not well suited to the imitation learning problem. We derive a\nprincipled conservative optimization bound and demonstrate empirically that it\nleads to improved performance on two very challenging manipulation environments\nfrom high-dimensional raw pixel observations. We set a new state-of-the-art\nperformance on the Franka Kitchen environment from images, requiring only 10\ndemos on no reward labels, as well as solving a complex dexterity manipulation\ntask."
                },
                "authors": [
                    {
                        "name": "Victor Kolev"
                    },
                    {
                        "name": "Rafael Rafailov"
                    },
                    {
                        "name": "Kyle Hatch"
                    },
                    {
                        "name": "Jiajun Wu"
                    },
                    {
                        "name": "Chelsea Finn"
                    }
                ],
                "author_detail": {
                    "name": "Chelsea Finn"
                },
                "author": "Chelsea Finn",
                "arxiv_comment": "Oral presentation, L4DC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08437v1",
                "updated": "2024-08-15T22:10:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    22,
                    10,
                    10,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T22:10:10Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    22,
                    10,
                    10,
                    3,
                    228,
                    0
                ],
                "title": "PQV-Mobile: A Combined Pruning and Quantization Toolkit to Optimize\n  Vision Transformers for Mobile Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PQV-Mobile: A Combined Pruning and Quantization Toolkit to Optimize\n  Vision Transformers for Mobile Applications"
                },
                "summary": "While Vision Transformers (ViTs) are extremely effective at computer vision\ntasks and are replacing convolutional neural networks as the new\nstate-of-the-art, they are complex and memory-intensive models. In order to\neffectively run these models on resource-constrained mobile/edge systems, there\nis a need to not only compress these models but also to optimize them and\nconvert them into deployment-friendly formats. To this end, this paper presents\na combined pruning and quantization tool, called PQV-Mobile, to optimize vision\ntransformers for mobile applications. The tool is able to support different\ntypes of structured pruning based on magnitude importance, Taylor importance,\nand Hessian importance. It also supports quantization from FP32 to FP16 and\nint8, targeting different mobile hardware backends. We demonstrate the\ncapabilities of our tool and show important latency-memory-accuracy trade-offs\nfor different amounts of pruning and int8 quantization with Facebook Data\nEfficient Image Transformer (DeiT) models. Our results show that even pruning a\nDeiT model by 9.375% and quantizing it to int8 from FP32 followed by optimizing\nfor mobile applications, we find a latency reduction by 7.18X with a small\naccuracy loss of 2.24%. The tool is open source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Vision Transformers (ViTs) are extremely effective at computer vision\ntasks and are replacing convolutional neural networks as the new\nstate-of-the-art, they are complex and memory-intensive models. In order to\neffectively run these models on resource-constrained mobile/edge systems, there\nis a need to not only compress these models but also to optimize them and\nconvert them into deployment-friendly formats. To this end, this paper presents\na combined pruning and quantization tool, called PQV-Mobile, to optimize vision\ntransformers for mobile applications. The tool is able to support different\ntypes of structured pruning based on magnitude importance, Taylor importance,\nand Hessian importance. It also supports quantization from FP32 to FP16 and\nint8, targeting different mobile hardware backends. We demonstrate the\ncapabilities of our tool and show important latency-memory-accuracy trade-offs\nfor different amounts of pruning and int8 quantization with Facebook Data\nEfficient Image Transformer (DeiT) models. Our results show that even pruning a\nDeiT model by 9.375% and quantizing it to int8 from FP32 followed by optimizing\nfor mobile applications, we find a latency reduction by 7.18X with a small\naccuracy loss of 2.24%. The tool is open source."
                },
                "authors": [
                    {
                        "name": "Kshitij Bhardwaj"
                    }
                ],
                "author_detail": {
                    "name": "Kshitij Bhardwaj"
                },
                "author": "Kshitij Bhardwaj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08433v1",
                "updated": "2024-08-15T21:51:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    21,
                    51,
                    56,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T21:51:56Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    21,
                    51,
                    56,
                    3,
                    228,
                    0
                ],
                "title": "A Robust Multi-Stage Intrusion Detection System for In-Vehicle Network\n  Security using Hierarchical Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Multi-Stage Intrusion Detection System for In-Vehicle Network\n  Security using Hierarchical Federated Learning"
                },
                "summary": "As connected and autonomous vehicles proliferate, the Controller Area Network\n(CAN) bus has become the predominant communication standard for in-vehicle\nnetworks due to its speed and efficiency. However, the CAN bus lacks basic\nsecurity measures such as authentication and encryption, making it highly\nvulnerable to cyberattacks. To ensure in-vehicle security, intrusion detection\nsystems (IDSs) must detect seen attacks and provide a robust defense against\nnew, unseen attacks while remaining lightweight for practical deployment.\nPrevious work has relied solely on the CAN ID feature or has used traditional\nmachine learning (ML) approaches with manual feature extraction. These\napproaches overlook other exploitable features, making it challenging to adapt\nto new unseen attack variants and compromising security. This paper introduces\na cutting-edge, novel, lightweight, in-vehicle, IDS-leveraging, deep learning\n(DL) algorithm to address these limitations. The proposed IDS employs a\nmulti-stage approach: an artificial neural network (ANN) in the first stage to\ndetect seen attacks, and a Long Short-Term Memory (LSTM) autoencoder in the\nsecond stage to detect new, unseen attacks. To understand and analyze diverse\ndriving behaviors, update the model with the latest attack patterns, and\npreserve data privacy, we propose a theoretical framework to deploy our IDS in\na hierarchical federated learning (H-FL) environment. Experimental results\ndemonstrate that our IDS achieves an F1-score exceeding 0.99 for seen attacks\nand exceeding 0.95 for novel attacks, with a detection rate of 99.99%.\nAdditionally, the false alarm rate (FAR) is exceptionally low at 0.016%,\nminimizing false alarms. Despite using DL algorithms known for their\neffectiveness in identifying sophisticated and zero-day attacks, the IDS\nremains lightweight, ensuring its feasibility for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As connected and autonomous vehicles proliferate, the Controller Area Network\n(CAN) bus has become the predominant communication standard for in-vehicle\nnetworks due to its speed and efficiency. However, the CAN bus lacks basic\nsecurity measures such as authentication and encryption, making it highly\nvulnerable to cyberattacks. To ensure in-vehicle security, intrusion detection\nsystems (IDSs) must detect seen attacks and provide a robust defense against\nnew, unseen attacks while remaining lightweight for practical deployment.\nPrevious work has relied solely on the CAN ID feature or has used traditional\nmachine learning (ML) approaches with manual feature extraction. These\napproaches overlook other exploitable features, making it challenging to adapt\nto new unseen attack variants and compromising security. This paper introduces\na cutting-edge, novel, lightweight, in-vehicle, IDS-leveraging, deep learning\n(DL) algorithm to address these limitations. The proposed IDS employs a\nmulti-stage approach: an artificial neural network (ANN) in the first stage to\ndetect seen attacks, and a Long Short-Term Memory (LSTM) autoencoder in the\nsecond stage to detect new, unseen attacks. To understand and analyze diverse\ndriving behaviors, update the model with the latest attack patterns, and\npreserve data privacy, we propose a theoretical framework to deploy our IDS in\na hierarchical federated learning (H-FL) environment. Experimental results\ndemonstrate that our IDS achieves an F1-score exceeding 0.99 for seen attacks\nand exceeding 0.95 for novel attacks, with a detection rate of 99.99%.\nAdditionally, the false alarm rate (FAR) is exceptionally low at 0.016%,\nminimizing false alarms. Despite using DL algorithms known for their\neffectiveness in identifying sophisticated and zero-day attacks, the IDS\nremains lightweight, ensuring its feasibility for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Muzun Althunayyan"
                    },
                    {
                        "name": "Amir Javed"
                    },
                    {
                        "name": "Omer Rana"
                    }
                ],
                "author_detail": {
                    "name": "Omer Rana"
                },
                "author": "Omer Rana",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2110.07009v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2110.07009v4",
                "updated": "2024-08-15T21:37:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    21,
                    37,
                    58,
                    3,
                    228,
                    0
                ],
                "published": "2021-10-13T20:05:26Z",
                "published_parsed": [
                    2021,
                    10,
                    13,
                    20,
                    5,
                    26,
                    2,
                    286,
                    0
                ],
                "title": "Leveraging Generative Models for Covert Messaging: Challenges and\n  Tradeoffs for \"Dead-Drop\" Deployments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Generative Models for Covert Messaging: Challenges and\n  Tradeoffs for \"Dead-Drop\" Deployments"
                },
                "summary": "State of the art generative models of human-produced content are the focus of\nmany recent papers that explore their use for steganographic communication. In\nparticular, generative models of natural language text. Loosely, these works\n(invertibly) encode message-carrying bits into a sequence of samples from the\nmodel, ultimately yielding a plausible natural language covertext. By focusing\non this narrow steganographic piece, prior work has largely ignored the\nsignificant algorithmic challenges, and performance-security tradeoffs, that\narise when one actually tries to build a messaging pipeline around it. We make\nthese challenges concrete, by considering the natural application of such a\npipeline: namely, \"dead-drop\" covert messaging over large, public internet\nplatforms (e.g. social media sites). We explicate the challenges and describe\napproaches to overcome them, surfacing in the process important performance and\nsecurity tradeoffs that must be carefully tuned. We implement a system around\nthis model-based format-transforming encryption pipeline, and give an empirical\nanalysis of its performance and (heuristic) security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State of the art generative models of human-produced content are the focus of\nmany recent papers that explore their use for steganographic communication. In\nparticular, generative models of natural language text. Loosely, these works\n(invertibly) encode message-carrying bits into a sequence of samples from the\nmodel, ultimately yielding a plausible natural language covertext. By focusing\non this narrow steganographic piece, prior work has largely ignored the\nsignificant algorithmic challenges, and performance-security tradeoffs, that\narise when one actually tries to build a messaging pipeline around it. We make\nthese challenges concrete, by considering the natural application of such a\npipeline: namely, \"dead-drop\" covert messaging over large, public internet\nplatforms (e.g. social media sites). We explicate the challenges and describe\napproaches to overcome them, surfacing in the process important performance and\nsecurity tradeoffs that must be carefully tuned. We implement a system around\nthis model-based format-transforming encryption pipeline, and give an empirical\nanalysis of its performance and (heuristic) security."
                },
                "authors": [
                    {
                        "name": "Luke A. Bauer"
                    },
                    {
                        "name": "James K. Howes IV"
                    },
                    {
                        "name": "Sam A. Markelon"
                    },
                    {
                        "name": "Vincent Bindschaedler"
                    },
                    {
                        "name": "Thomas Shrimpton"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Shrimpton"
                },
                "author": "Thomas Shrimpton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2110.07009v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2110.07009v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08546v2",
                "updated": "2024-08-15T21:36:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    21,
                    36,
                    38,
                    3,
                    228,
                    0
                ],
                "published": "2024-02-13T15:51:58Z",
                "published_parsed": [
                    2024,
                    2,
                    13,
                    15,
                    51,
                    58,
                    1,
                    44,
                    0
                ],
                "title": "Grounding LLMs For Robot Task Planning Using Closed-loop State Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding LLMs For Robot Task Planning Using Closed-loop State Feedback"
                },
                "summary": "Planning algorithms decompose complex problems into intermediate steps that\ncan be sequentially executed by robots to complete tasks. Recent works have\nemployed Large Language Models (LLMs) for task planning, using natural language\nto generate robot policies in both simulation and real-world environments. LLMs\nlike GPT-4 have shown promising results in generalizing to unseen tasks, but\ntheir applicability is limited due to hallucinations caused by insufficient\ngrounding in the robot environment. The robustness of LLMs in task planning can\nbe enhanced with environmental state information and feedback. In this paper,\nwe introduce a novel approach to task planning that utilizes two separate LLMs\nfor high-level planning and low-level control, improving task-related success\nrates and goal condition recall. Our algorithm, \\textit{BrainBody-LLM}, draws\ninspiration from the human neural system, emulating its brain-body architecture\nby dividing planning across two LLMs in a structured, hierarchical manner.\nBrainBody-LLM implements a closed-loop feedback mechanism, enabling learning\nfrom simulator errors to resolve execution errors in complex settings. We\ndemonstrate the successful application of BrainBody-LLM in the VirtualHome\nsimulation environment, achieving a 29\\% improvement in task-oriented success\nrates over competitive baselines with the GPT-4 backend. Additionally, we\nevaluate our algorithm on seven complex tasks using a realistic physics\nsimulator and the Franka Research 3 robotic arm, comparing it with various\nstate-of-the-art LLMs. Our results show advancements in the reasoning\ncapabilities of recent LLMs, which enable them to learn from raw\nsimulator/controller errors to correct plans, making them highly effective in\nrobotic task planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning algorithms decompose complex problems into intermediate steps that\ncan be sequentially executed by robots to complete tasks. Recent works have\nemployed Large Language Models (LLMs) for task planning, using natural language\nto generate robot policies in both simulation and real-world environments. LLMs\nlike GPT-4 have shown promising results in generalizing to unseen tasks, but\ntheir applicability is limited due to hallucinations caused by insufficient\ngrounding in the robot environment. The robustness of LLMs in task planning can\nbe enhanced with environmental state information and feedback. In this paper,\nwe introduce a novel approach to task planning that utilizes two separate LLMs\nfor high-level planning and low-level control, improving task-related success\nrates and goal condition recall. Our algorithm, \\textit{BrainBody-LLM}, draws\ninspiration from the human neural system, emulating its brain-body architecture\nby dividing planning across two LLMs in a structured, hierarchical manner.\nBrainBody-LLM implements a closed-loop feedback mechanism, enabling learning\nfrom simulator errors to resolve execution errors in complex settings. We\ndemonstrate the successful application of BrainBody-LLM in the VirtualHome\nsimulation environment, achieving a 29\\% improvement in task-oriented success\nrates over competitive baselines with the GPT-4 backend. Additionally, we\nevaluate our algorithm on seven complex tasks using a realistic physics\nsimulator and the Franka Research 3 robotic arm, comparing it with various\nstate-of-the-art LLMs. Our results show advancements in the reasoning\ncapabilities of recent LLMs, which enable them to learn from raw\nsimulator/controller errors to correct plans, making them highly effective in\nrobotic task planning."
                },
                "authors": [
                    {
                        "name": "Vineet Bhat"
                    },
                    {
                        "name": "Ali Umut Kaypak"
                    },
                    {
                        "name": "Prashanth Krishnamurthy"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Farshad Khorrami"
                    }
                ],
                "author_detail": {
                    "name": "Farshad Khorrami"
                },
                "author": "Farshad Khorrami",
                "arxiv_comment": "This work has been submitted to Autonomous Robots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.08546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18961v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18961v3",
                "updated": "2024-08-15T21:32:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    21,
                    32,
                    57,
                    3,
                    228,
                    0
                ],
                "published": "2024-07-18T00:58:41Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    0,
                    58,
                    41,
                    3,
                    200,
                    0
                ],
                "title": "MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains"
                },
                "summary": "Recent advances in large language models (LLMs) have increased the demand for\ncomprehensive benchmarks to evaluate their capabilities as human-like agents.\nExisting benchmarks, while useful, often focus on specific application\nscenarios, emphasizing task completion but failing to dissect the underlying\nskills that drive these outcomes. This lack of granularity makes it difficult\nto deeply discern where failures stem from. Additionally, setting up these\nenvironments requires considerable effort, and issues of unreliability and\nreproducibility sometimes arise, especially in interactive tasks. To address\nthese limitations, we introduce the Massive Multitask Agent Understanding\n(MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need\nfor complex environment setups. It evaluates models across five domains,\nincluding Tool-use, Directed Acyclic Graph (DAG) QA, Data Science and Machine\nLearning coding, Contest-level programming and Mathematics, and covers five\nessential capabilities: Understanding, Reasoning, Planning, Problem-solving,\nand Self-correction. With a total of 20 meticulously designed tasks\nencompassing over 3K distinct prompts, MMAU provides a comprehensive framework\nfor evaluating the strengths and limitations of LLM agents. By testing 18\nrepresentative models on MMAU, we provide deep and insightful analyses.\nUltimately, MMAU not only sheds light on the capabilities and limitations of\nLLM agents but also enhances the interpretability of their performance.\nDatasets and evaluation scripts of MMAU are released at\nhttps://github.com/apple/axlearn/tree/main/docs/research/mmau.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have increased the demand for\ncomprehensive benchmarks to evaluate their capabilities as human-like agents.\nExisting benchmarks, while useful, often focus on specific application\nscenarios, emphasizing task completion but failing to dissect the underlying\nskills that drive these outcomes. This lack of granularity makes it difficult\nto deeply discern where failures stem from. Additionally, setting up these\nenvironments requires considerable effort, and issues of unreliability and\nreproducibility sometimes arise, especially in interactive tasks. To address\nthese limitations, we introduce the Massive Multitask Agent Understanding\n(MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need\nfor complex environment setups. It evaluates models across five domains,\nincluding Tool-use, Directed Acyclic Graph (DAG) QA, Data Science and Machine\nLearning coding, Contest-level programming and Mathematics, and covers five\nessential capabilities: Understanding, Reasoning, Planning, Problem-solving,\nand Self-correction. With a total of 20 meticulously designed tasks\nencompassing over 3K distinct prompts, MMAU provides a comprehensive framework\nfor evaluating the strengths and limitations of LLM agents. By testing 18\nrepresentative models on MMAU, we provide deep and insightful analyses.\nUltimately, MMAU not only sheds light on the capabilities and limitations of\nLLM agents but also enhances the interpretability of their performance.\nDatasets and evaluation scripts of MMAU are released at\nhttps://github.com/apple/axlearn/tree/main/docs/research/mmau."
                },
                "authors": [
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Haoping Bai"
                    },
                    {
                        "name": "Shuang Ma"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Shen Ma"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Yizhe zhang"
                    },
                    {
                        "name": "Karsten Ahnert"
                    },
                    {
                        "name": "Vik Kamath"
                    },
                    {
                        "name": "Mathias Berglund"
                    },
                    {
                        "name": "Dominic Walsh"
                    },
                    {
                        "name": "Tobias Gindele"
                    },
                    {
                        "name": "Juergen Wiest"
                    },
                    {
                        "name": "Zhengfeng Lai"
                    },
                    {
                        "name": "Xiaoming Wang"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Zirui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Wang"
                },
                "author": "Zirui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18961v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18961v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08422v1",
                "updated": "2024-08-15T21:09:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    21,
                    9,
                    9,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T21:09:09Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    21,
                    9,
                    9,
                    3,
                    228,
                    0
                ],
                "title": "Assessing and Enhancing Large Language Models in Rare Disease\n  Question-answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing and Enhancing Large Language Models in Rare Disease\n  Question-answering"
                },
                "summary": "Despite the impressive capabilities of Large Language Models (LLMs) in\ngeneral medical domains, questions remain about their performance in diagnosing\nrare diseases. To answer this question, we aim to assess the diagnostic\nperformance of LLMs in rare diseases, and explore methods to enhance their\neffectiveness in this area. In this work, we introduce a rare disease\nquestion-answering (ReDis-QA) dataset to evaluate the performance of LLMs in\ndiagnosing rare diseases. Specifically, we collected 1360 high-quality\nquestion-answer pairs within the ReDis-QA dataset, covering 205 rare diseases.\nAdditionally, we annotated meta-data for each question, facilitating the\nextraction of subsets specific to any given disease and its property. Based on\nthe ReDis-QA dataset, we benchmarked several open-source LLMs, revealing that\ndiagnosing rare diseases remains a significant challenge for these models.\n  To facilitate retrieval augmentation generation for rare disease diagnosis,\nwe collect the first rare diseases corpus (ReCOP), sourced from the National\nOrganization for Rare Disorders (NORD) database. Specifically, we split the\nreport of each rare disease into multiple chunks, each representing a different\nproperty of the disease, including their overview, symptoms, causes, effects,\nrelated disorders, diagnosis, and standard therapies. This structure ensures\nthat the information within each chunk aligns consistently with a question.\nExperiment results demonstrate that ReCOP can effectively improve the accuracy\nof LLMs on the ReDis-QA dataset by an average of 8%. Moreover, it significantly\nguides LLMs to generate trustworthy answers and explanations that can be traced\nback to existing literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the impressive capabilities of Large Language Models (LLMs) in\ngeneral medical domains, questions remain about their performance in diagnosing\nrare diseases. To answer this question, we aim to assess the diagnostic\nperformance of LLMs in rare diseases, and explore methods to enhance their\neffectiveness in this area. In this work, we introduce a rare disease\nquestion-answering (ReDis-QA) dataset to evaluate the performance of LLMs in\ndiagnosing rare diseases. Specifically, we collected 1360 high-quality\nquestion-answer pairs within the ReDis-QA dataset, covering 205 rare diseases.\nAdditionally, we annotated meta-data for each question, facilitating the\nextraction of subsets specific to any given disease and its property. Based on\nthe ReDis-QA dataset, we benchmarked several open-source LLMs, revealing that\ndiagnosing rare diseases remains a significant challenge for these models.\n  To facilitate retrieval augmentation generation for rare disease diagnosis,\nwe collect the first rare diseases corpus (ReCOP), sourced from the National\nOrganization for Rare Disorders (NORD) database. Specifically, we split the\nreport of each rare disease into multiple chunks, each representing a different\nproperty of the disease, including their overview, symptoms, causes, effects,\nrelated disorders, diagnosis, and standard therapies. This structure ensures\nthat the information within each chunk aligns consistently with a question.\nExperiment results demonstrate that ReCOP can effectively improve the accuracy\nof LLMs on the ReDis-QA dataset by an average of 8%. Moreover, it significantly\nguides LLMs to generate trustworthy answers and explanations that can be traced\nback to existing literature."
                },
                "authors": [
                    {
                        "name": "Guanchu Wang"
                    },
                    {
                        "name": "Junhao Ran"
                    },
                    {
                        "name": "Ruixiang Tang"
                    },
                    {
                        "name": "Chia-Yuan Chang"
                    },
                    {
                        "name": "Chia-Yuan Chang"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Zhandong Liu"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08401v1",
                "updated": "2024-08-15T19:58:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    19,
                    58,
                    41,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T19:58:41Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    19,
                    58,
                    41,
                    3,
                    228,
                    0
                ],
                "title": "Understanding Help-Seeking Behavior of Students Using LLMs vs. Web\n  Search for Writing SQL Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Help-Seeking Behavior of Students Using LLMs vs. Web\n  Search for Writing SQL Queries"
                },
                "summary": "Growth in the use of large language models (LLMs) in programming education is\naltering how students write SQL queries. Traditionally, students relied heavily\non web search for coding assistance, but this has shifted with the adoption of\nLLMs like ChatGPT. However, the comparative process and outcomes of using web\nsearch versus LLMs for coding help remain underexplored. To address this, we\nconducted a randomized interview study in a database classroom to compare web\nsearch and LLMs, including a publicly available LLM (ChatGPT) and an\ninstructor-tuned LLM, for writing SQL queries. Our findings indicate that using\nan instructor-tuned LLM required significantly more interactions than both\nChatGPT and web search, but resulted in a similar number of edits to the final\nSQL query. No significant differences were found in the quality of the final\nSQL queries between conditions, although the LLM conditions directionally\nshowed higher query quality. Furthermore, students using instructor-tuned LLM\nreported a lower mental demand. These results have implications for learning\nand productivity in programming education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growth in the use of large language models (LLMs) in programming education is\naltering how students write SQL queries. Traditionally, students relied heavily\non web search for coding assistance, but this has shifted with the adoption of\nLLMs like ChatGPT. However, the comparative process and outcomes of using web\nsearch versus LLMs for coding help remain underexplored. To address this, we\nconducted a randomized interview study in a database classroom to compare web\nsearch and LLMs, including a publicly available LLM (ChatGPT) and an\ninstructor-tuned LLM, for writing SQL queries. Our findings indicate that using\nan instructor-tuned LLM required significantly more interactions than both\nChatGPT and web search, but resulted in a similar number of edits to the final\nSQL query. No significant differences were found in the quality of the final\nSQL queries between conditions, although the LLM conditions directionally\nshowed higher query quality. Furthermore, students using instructor-tuned LLM\nreported a lower mental demand. These results have implications for learning\nand productivity in programming education."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "Mohi Reza"
                    },
                    {
                        "name": "Jeb Mitchell"
                    },
                    {
                        "name": "Ilya Musabirov"
                    },
                    {
                        "name": "Lisa Zhang"
                    },
                    {
                        "name": "Michael Liut"
                    }
                ],
                "author_detail": {
                    "name": "Michael Liut"
                },
                "author": "Michael Liut",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08400v1",
                "updated": "2024-08-15T19:57:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    19,
                    57,
                    42,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T19:57:42Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    19,
                    57,
                    42,
                    3,
                    228,
                    0
                ],
                "title": "Zero-Shot Learning and Key Points Are All You Need for Automated\n  Fact-Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Learning and Key Points Are All You Need for Automated\n  Fact-Checking"
                },
                "summary": "Automated fact-checking is an important task because determining the accurate\nstatus of a proposed claim within the vast amount of information available\nonline is a critical challenge. This challenge requires robust evaluation to\nprevent the spread of false information. Modern large language models (LLMs)\nhave demonstrated high capability in performing a diverse range of Natural\nLanguage Processing (NLP) tasks. By utilizing proper prompting strategies,\ntheir versatility due to their understanding of large context sizes and\nzero-shot learning ability enables them to simulate human problem-solving\nintuition and move towards being an alternative to humans for solving problems.\nIn this work, we introduce a straightforward framework based on Zero-Shot\nLearning and Key Points (ZSL-KeP) for automated fact-checking, which despite\nits simplicity, performed well on the AVeriTeC shared task dataset by robustly\nimproving the baseline and achieving 10th place.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated fact-checking is an important task because determining the accurate\nstatus of a proposed claim within the vast amount of information available\nonline is a critical challenge. This challenge requires robust evaluation to\nprevent the spread of false information. Modern large language models (LLMs)\nhave demonstrated high capability in performing a diverse range of Natural\nLanguage Processing (NLP) tasks. By utilizing proper prompting strategies,\ntheir versatility due to their understanding of large context sizes and\nzero-shot learning ability enables them to simulate human problem-solving\nintuition and move towards being an alternative to humans for solving problems.\nIn this work, we introduce a straightforward framework based on Zero-Shot\nLearning and Key Points (ZSL-KeP) for automated fact-checking, which despite\nits simplicity, performed well on the AVeriTeC shared task dataset by robustly\nimproving the baseline and achieving 10th place."
                },
                "authors": [
                    {
                        "name": "Mohammad Ghiasvand Mohammadkhani"
                    },
                    {
                        "name": "Ali Ghiasvand Mohammadkhani"
                    },
                    {
                        "name": "Hamid Beigy"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Beigy"
                },
                "author": "Hamid Beigy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09433v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09433v3",
                "updated": "2024-08-15T19:51:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    19,
                    51,
                    7,
                    3,
                    228,
                    0
                ],
                "published": "2023-11-15T23:07:40Z",
                "published_parsed": [
                    2023,
                    11,
                    15,
                    23,
                    7,
                    40,
                    2,
                    319,
                    0
                ],
                "title": "Trojan Activation Attack: Red-Teaming Large Language Models using\n  Activation Steering for Safety-Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trojan Activation Attack: Red-Teaming Large Language Models using\n  Activation Steering for Safety-Alignment"
                },
                "summary": "To ensure AI safety, instruction-tuned Large Language Models (LLMs) are\nspecifically trained to ensure alignment, which refers to making models behave\nin accordance with human intentions. While these models have demonstrated\ncommendable results on various safety benchmarks, the vulnerability of their\nsafety alignment has not been extensively studied. This is particularly\ntroubling given the potential harm that LLMs can inflict. Existing attack\nmethods on LLMs often rely on poisoned training data or the injection of\nmalicious prompts. These approaches compromise the stealthiness and\ngeneralizability of the attacks, making them susceptible to detection.\nAdditionally, these models often demand substantial computational resources for\nimplementation, making them less practical for real-world applications. In this\nwork, we study a different attack scenario, called Trojan Activation Attack\n(TA^2), which injects trojan steering vectors into the activation layers of\nLLMs. These malicious steering vectors can be triggered at inference time to\nsteer the models toward attacker-desired behaviors by manipulating their\nactivations. Our experiment results on four primary alignment tasks show that\nTA^2 is highly effective and adds little or no overhead to attack efficiency.\nAdditionally, we discuss potential countermeasures against such activation\nattacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To ensure AI safety, instruction-tuned Large Language Models (LLMs) are\nspecifically trained to ensure alignment, which refers to making models behave\nin accordance with human intentions. While these models have demonstrated\ncommendable results on various safety benchmarks, the vulnerability of their\nsafety alignment has not been extensively studied. This is particularly\ntroubling given the potential harm that LLMs can inflict. Existing attack\nmethods on LLMs often rely on poisoned training data or the injection of\nmalicious prompts. These approaches compromise the stealthiness and\ngeneralizability of the attacks, making them susceptible to detection.\nAdditionally, these models often demand substantial computational resources for\nimplementation, making them less practical for real-world applications. In this\nwork, we study a different attack scenario, called Trojan Activation Attack\n(TA^2), which injects trojan steering vectors into the activation layers of\nLLMs. These malicious steering vectors can be triggered at inference time to\nsteer the models toward attacker-desired behaviors by manipulating their\nactivations. Our experiment results on four primary alignment tasks show that\nTA^2 is highly effective and adds little or no overhead to attack efficiency.\nAdditionally, we discuss potential countermeasures against such activation\nattacks."
                },
                "authors": [
                    {
                        "name": "Haoran Wang"
                    },
                    {
                        "name": "Kai Shu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Shu"
                },
                "author": "Kai Shu",
                "arxiv_comment": "ACM International Conference on Information and Knowledge Management\n  (CIKM'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09433v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09433v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18221v2",
                "updated": "2024-08-15T19:30:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    19,
                    30,
                    9,
                    3,
                    228,
                    0
                ],
                "published": "2024-06-26T10:08:47Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    10,
                    8,
                    47,
                    2,
                    178,
                    0
                ],
                "title": "Enhancing Data Privacy in Large Language Models through Private\n  Association Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Data Privacy in Large Language Models through Private\n  Association Editing"
                },
                "summary": "Large Language Models (LLMs) are powerful tools with extensive applications,\nbut their tendency to memorize private information raises significant concerns\nas private data leakage can easily happen. In this paper, we introduce Private\nAssociation Editing (PAE), a novel defense approach for private data leakage.\nPAE is designed to effectively remove Personally Identifiable Information (PII)\nwithout retraining the model. Our approach consists of a four-step procedure:\ndetecting memorized PII, applying PAE cards to mitigate memorization of private\ndata, verifying resilience to targeted data extraction (TDE) attacks, and\nensuring consistency in the post-edit LLMs. The versatility and efficiency of\nPAE, which allows for batch modifications, significantly enhance data privacy\nin LLMs. Experimental results demonstrate the effectiveness of PAE in\nmitigating private data leakage. We believe PAE will serve as a critical tool\nin the ongoing effort to protect data privacy in LLMs, encouraging the\ndevelopment of safer models for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful tools with extensive applications,\nbut their tendency to memorize private information raises significant concerns\nas private data leakage can easily happen. In this paper, we introduce Private\nAssociation Editing (PAE), a novel defense approach for private data leakage.\nPAE is designed to effectively remove Personally Identifiable Information (PII)\nwithout retraining the model. Our approach consists of a four-step procedure:\ndetecting memorized PII, applying PAE cards to mitigate memorization of private\ndata, verifying resilience to targeted data extraction (TDE) attacks, and\nensuring consistency in the post-edit LLMs. The versatility and efficiency of\nPAE, which allows for batch modifications, significantly enhance data privacy\nin LLMs. Experimental results demonstrate the effectiveness of PAE in\nmitigating private data leakage. We believe PAE will serve as a critical tool\nin the ongoing effort to protect data privacy in LLMs, encouraging the\ndevelopment of safer models for real-world applications."
                },
                "authors": [
                    {
                        "name": "Davide Venditti"
                    },
                    {
                        "name": "Elena Sofia Ruzzetti"
                    },
                    {
                        "name": "Giancarlo A. Xompero"
                    },
                    {
                        "name": "Cristina Giannone"
                    },
                    {
                        "name": "Andrea Favalli"
                    },
                    {
                        "name": "Raniero Romagnoli"
                    },
                    {
                        "name": "Fabio Massimo Zanzotto"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Massimo Zanzotto"
                },
                "author": "Fabio Massimo Zanzotto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08379v1",
                "updated": "2024-08-15T18:43:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    18,
                    43,
                    50,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-15T18:43:50Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    18,
                    43,
                    50,
                    3,
                    228,
                    0
                ],
                "title": "Towards Realistic Synthetic User-Generated Content: A Scaffolding\n  Approach to Generating Online Discussions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Realistic Synthetic User-Generated Content: A Scaffolding\n  Approach to Generating Online Discussions"
                },
                "summary": "The emergence of synthetic data represents a pivotal shift in modern machine\nlearning, offering a solution to satisfy the need for large volumes of data in\ndomains where real data is scarce, highly private, or difficult to obtain. We\ninvestigate the feasibility of creating realistic, large-scale synthetic\ndatasets of user-generated content, noting that such content is increasingly\nprevalent and a source of frequently sought information. Large language models\n(LLMs) offer a starting point for generating synthetic social media discussion\nthreads, due to their ability to produce diverse responses that typify online\ninteractions. However, as we demonstrate, straightforward application of LLMs\nyields limited success in capturing the complex structure of online\ndiscussions, and standard prompting mechanisms lack sufficient control. We\ntherefore propose a multi-step generation process, predicated on the idea of\ncreating compact representations of discussion threads, referred to as\nscaffolds. Our framework is generic yet adaptable to the unique characteristics\nof specific social media platforms. We demonstrate its feasibility using data\nfrom two distinct online discussion platforms. To address the fundamental\nchallenge of ensuring the representativeness and realism of synthetic data, we\npropose a portfolio of evaluation measures to compare various instantiations of\nour framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of synthetic data represents a pivotal shift in modern machine\nlearning, offering a solution to satisfy the need for large volumes of data in\ndomains where real data is scarce, highly private, or difficult to obtain. We\ninvestigate the feasibility of creating realistic, large-scale synthetic\ndatasets of user-generated content, noting that such content is increasingly\nprevalent and a source of frequently sought information. Large language models\n(LLMs) offer a starting point for generating synthetic social media discussion\nthreads, due to their ability to produce diverse responses that typify online\ninteractions. However, as we demonstrate, straightforward application of LLMs\nyields limited success in capturing the complex structure of online\ndiscussions, and standard prompting mechanisms lack sufficient control. We\ntherefore propose a multi-step generation process, predicated on the idea of\ncreating compact representations of discussion threads, referred to as\nscaffolds. Our framework is generic yet adaptable to the unique characteristics\nof specific social media platforms. We demonstrate its feasibility using data\nfrom two distinct online discussion platforms. To address the fundamental\nchallenge of ensuring the representativeness and realism of synthetic data, we\npropose a portfolio of evaluation measures to compare various instantiations of\nour framework."
                },
                "authors": [
                    {
                        "name": "Krisztian Balog"
                    },
                    {
                        "name": "John Palowitch"
                    },
                    {
                        "name": "Barbara Ikica"
                    },
                    {
                        "name": "Filip Radlinski"
                    },
                    {
                        "name": "Hamidreza Alvari"
                    },
                    {
                        "name": "Mehdi Manshadi"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Manshadi"
                },
                "author": "Mehdi Manshadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]