[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.19392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v1",
                "updated": "2025-01-31T18:47:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v1",
                "updated": "2025-01-31T16:56:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee-Joe Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v1",
                "updated": "2025-01-31T15:58:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v3",
                "updated": "2025-01-31T14:26:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    26,
                    5,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Our code will be released upon acceptance. The Change Logs on Page 9\n  reveal our significant changes compared with v1 and v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17426v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17426v3",
                "updated": "2025-01-31T14:13:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    13,
                    49,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-26T13:34:02Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    34,
                    2,
                    1,
                    331,
                    0
                ],
                "title": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning"
                },
                "summary": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Fan jiang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/GraphPKU/PiSSA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17426v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17426v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19051v1",
                "updated": "2025-01-31T11:25:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:25:40Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "title": "Swift: Rethinking RDMA Control Plane for Elastic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Swift: Rethinking RDMA Control Plane for Elastic Computing"
                },
                "summary": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions."
                },
                "authors": [
                    {
                        "name": "Junxue Zhang"
                    },
                    {
                        "name": "Han Tian"
                    },
                    {
                        "name": "Xinyang Huang"
                    },
                    {
                        "name": "Wenxue Li"
                    },
                    {
                        "name": "Kaiqiang Xu"
                    },
                    {
                        "name": "Dian Shen"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19021v1",
                "updated": "2025-01-31T10:43:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T10:43:00Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "title": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode"
                },
                "summary": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model."
                },
                "authors": [
                    {
                        "name": "Emilio Corte"
                    },
                    {
                        "name": "Alberto Bortone"
                    },
                    {
                        "name": "Elena Nieto Hernández"
                    },
                    {
                        "name": "Carlo Ceresa"
                    },
                    {
                        "name": "Georgios Provatas"
                    },
                    {
                        "name": "Karla Ivanković Nizić"
                    },
                    {
                        "name": "Milko Jaksić"
                    },
                    {
                        "name": "Ettore Vittone"
                    },
                    {
                        "name": "Sviatoslav Ditalia Tchernij"
                    }
                ],
                "author_detail": {
                    "name": "Sviatoslav Ditalia Tchernij"
                },
                "author": "Sviatoslav Ditalia Tchernij",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18824v1",
                "updated": "2025-01-31T00:43:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T00:43:50Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "title": "Memory-Efficient Fine-Tuning of Transformers via Token Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Fine-Tuning of Transformers via Token Selection"
                },
                "summary": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune."
                },
                "authors": [
                    {
                        "name": "Antoine Simoulin"
                    },
                    {
                        "name": "Namyong Park"
                    },
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Grey Yang"
                    }
                ],
                "author_detail": {
                    "name": "Grey Yang"
                },
                "author": "Grey Yang",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v3",
                "updated": "2025-01-30T18:23:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    23,
                    46,
                    3,
                    30,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18356v1",
                "updated": "2025-01-30T14:03:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T14:03:36Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "title": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence"
                },
                "summary": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Thea Aviss"
                    }
                ],
                "author_detail": {
                    "name": "Thea Aviss"
                },
                "author": "Thea Aviss",
                "arxiv_comment": "25 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v2",
                "updated": "2025-01-30T13:07:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    7,
                    37,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices"
                },
                "summary": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05172v2",
                "updated": "2025-01-30T06:02:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    6,
                    2,
                    11,
                    3,
                    30,
                    0
                ],
                "published": "2023-10-08T14:06:06Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    14,
                    6,
                    6,
                    6,
                    281,
                    0
                ],
                "title": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy"
                },
                "summary": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations."
                },
                "authors": [
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Nimish Mishra"
                    },
                    {
                        "name": "Sayandeep Saha"
                    },
                    {
                        "name": "Sarani Bhattacharya"
                    },
                    {
                        "name": "Debdeep Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Debdeep Mukhopadhyay"
                },
                "author": "Debdeep Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v5",
                "updated": "2025-01-29T16:44:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    44,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04437v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04437v3",
                "updated": "2025-01-29T04:10:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    4,
                    10,
                    41,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-07T16:00:32Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    16,
                    0,
                    32,
                    1,
                    128,
                    0
                ],
                "title": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention"
                },
                "summary": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer."
                },
                "authors": [
                    {
                        "name": "Ramya Prabhu"
                    },
                    {
                        "name": "Ajay Nayak"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Ashish Panwar"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Panwar"
                },
                "author": "Ashish Panwar",
                "arxiv_comment": "To appear in ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04437v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04437v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14770v2",
                "updated": "2025-01-28T20:35:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    35,
                    23,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:37:18Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    37,
                    18,
                    6,
                    364,
                    0
                ],
                "title": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches"
                },
                "summary": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing citations in Sections 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14771v2",
                "updated": "2025-01-28T20:33:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    33,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:39:37Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    39,
                    37,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching"
                },
                "summary": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing proper citations in Sections 2 and 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17123v1",
                "updated": "2025-01-28T18:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "title": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis"
                },
                "summary": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats."
                },
                "authors": [
                    {
                        "name": "Tejal Joshi"
                    },
                    {
                        "name": "Aarya Kawalay"
                    },
                    {
                        "name": "Anvi Jamkhande"
                    },
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "8 pages, 4 figures. Accepted in IEEE's 2nd International Conference\n  on Computational Intelligence and Network Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10854v2",
                "updated": "2025-01-28T16:19:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    19,
                    24,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-18T19:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications"
                },
                "summary": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v1",
                "updated": "2025-01-28T12:57:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16597v1",
                "updated": "2025-01-28T00:22:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T00:22:34Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs"
                },
                "summary": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed Saif"
                    },
                    {
                        "name": "Md. Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16535v1",
                "updated": "2025-01-27T22:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T22:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "title": "Latency Guarantees for Caching with Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency Guarantees for Caching with Delayed Hits"
                },
                "summary": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm."
                },
                "authors": [
                    {
                        "name": "Keerthana Gurushankar"
                    },
                    {
                        "name": "Noah G. Singer"
                    },
                    {
                        "name": "Bernardo Subercaseaux"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo Subercaseaux"
                },
                "author": "Bernardo Subercaseaux",
                "arxiv_comment": "Accepted at INFOCOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16245v1",
                "updated": "2025-01-27T17:42:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:42:20Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "title": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis"
                },
                "summary": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonçalo Moreira"
                    },
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "José Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v4",
                "updated": "2025-01-27T14:55:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    55,
                    40,
                    0,
                    27,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11126v2",
                "updated": "2025-01-27T14:37:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    37,
                    24,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-19T17:33:28Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "title": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching"
                },
                "summary": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate."
                },
                "authors": [
                    {
                        "name": "MohammadJavad Sojdeh"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16055v1",
                "updated": "2025-01-27T13:53:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T13:53:12Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "title": "Random Reshuffling for Stochastic Gradient Langevin Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Reshuffling for Stochastic Gradient Langevin Dynamics"
                },
                "summary": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent."
                },
                "authors": [
                    {
                        "name": "Luke Shaw"
                    },
                    {
                        "name": "Peter A. Whalley"
                    }
                ],
                "author_detail": {
                    "name": "Peter A. Whalley"
                },
                "author": "Peter A. Whalley",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65C05, 82C31, 62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v2",
                "updated": "2025-01-27T13:39:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    39,
                    25,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization"
                },
                "summary": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "PrefixQuant improves quantization accuracy across various precision\n  and quantization settings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v3",
                "updated": "2025-01-27T06:47:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    6,
                    47,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15782v1",
                "updated": "2025-01-27T05:02:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T05:02:05Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "title": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare"
                },
                "summary": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets."
                },
                "authors": [
                    {
                        "name": "Faraz Zargari"
                    },
                    {
                        "name": "Hossein Nekouyan Jazi"
                    },
                    {
                        "name": "Bo Sun"
                    },
                    {
                        "name": "Xiaoqi Tan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Tan"
                },
                "author": "Xiaoqi Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15570v1",
                "updated": "2025-01-26T15:56:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T15:56:56Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer"
                },
                "summary": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}."
                },
                "authors": [
                    {
                        "name": "Lin Yueyu"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Peter Yue"
                    },
                    {
                        "name": "Liu Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Liu Xiao"
                },
                "author": "Liu Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15481v1",
                "updated": "2025-01-26T11:01:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T11:01:10Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "title": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems"
                },
                "summary": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one."
                },
                "authors": [
                    {
                        "name": "Joaquín Gayoso-Cabada"
                    },
                    {
                        "name": "Mercedes Gómez-Albarrán"
                    },
                    {
                        "name": "José-Luis Sierra"
                    }
                ],
                "author_detail": {
                    "name": "José-Luis Sierra"
                },
                "author": "José-Luis Sierra",
                "arxiv_doi": "10.1007/978-3-030-04257-8_4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-030-04257-8_4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.15481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "camera-ready",
                "arxiv_journal_ref": "MATURITY AND INNOVATION IN DIGITAL LIBRARIES, ICADL 2018",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v4",
                "updated": "2025-01-26T07:29:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    7,
                    29,
                    6,
                    6,
                    26,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v2",
                "updated": "2025-01-26T01:43:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    1,
                    43,
                    46,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15348v1",
                "updated": "2025-01-25T23:16:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T23:16:03Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "title": "ReInc: Scaling Training of Dynamic Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReInc: Scaling Training of Dynamic Graph Neural Networks"
                },
                "summary": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets."
                },
                "authors": [
                    {
                        "name": "Mingyu Guan"
                    },
                    {
                        "name": "Saumia Singhal"
                    },
                    {
                        "name": "Taesoo Kim"
                    },
                    {
                        "name": "Anand Padmanabha Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Anand Padmanabha Iyer"
                },
                "author": "Anand Padmanabha Iyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v2",
                "updated": "2025-01-25T12:17:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    12,
                    17,
                    41,
                    5,
                    25,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v2",
                "updated": "2025-01-25T10:38:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    10,
                    38,
                    11,
                    5,
                    25,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15126v1",
                "updated": "2025-01-25T08:27:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T08:27:26Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "title": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs"
                },
                "summary": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x."
                },
                "authors": [
                    {
                        "name": "Deniz Elbek"
                    },
                    {
                        "name": "Kamer Kaya"
                    }
                ],
                "author_detail": {
                    "name": "Kamer Kaya"
                },
                "author": "Kamer Kaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15113v1",
                "updated": "2025-01-25T07:28:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T07:28:13Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "title": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads"
                },
                "summary": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods."
                },
                "authors": [
                    {
                        "name": "Xingyang He"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Shaowei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Chen"
                },
                "author": "Shaowei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v2",
                "updated": "2025-01-25T04:21:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    4,
                    21,
                    57,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15021v1",
                "updated": "2025-01-25T02:01:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T02:01:56Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "title": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models"
                },
                "summary": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16383v1",
                "updated": "2025-01-25T01:45:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T01:45:29Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations"
                },
                "summary": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v2",
                "updated": "2025-01-24T19:13:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    19,
                    13,
                    12,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v2",
                "updated": "2025-01-24T15:16:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    16,
                    48,
                    4,
                    24,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v2",
                "updated": "2025-01-24T14:32:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    32,
                    34,
                    4,
                    24,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Lucas Bürgi"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14387v1",
                "updated": "2025-01-24T10:39:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:39:45Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "title": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers"
                },
                "summary": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform."
                },
                "authors": [
                    {
                        "name": "Simone Bolettieri"
                    },
                    {
                        "name": "Raffaele Bruno"
                    },
                    {
                        "name": "Enzo Mingozzi"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Mingozzi"
                },
                "author": "Enzo Mingozzi",
                "arxiv_doi": "10.1016/j.jnca.2021.103020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jnca.2021.103020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Network and Computer Applications, Volume 181, 1 May\n  2021, 103020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14367v1",
                "updated": "2025-01-24T10:00:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:00:21Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "title": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks"
                },
                "summary": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach."
                },
                "authors": [
                    {
                        "name": "Kexin Shi"
                    },
                    {
                        "name": "Yaru Fu"
                    },
                    {
                        "name": "Yongna Guo"
                    },
                    {
                        "name": "Fu Lee Wang"
                    },
                    {
                        "name": "Yan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhang"
                },
                "author": "Yan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14312v1",
                "updated": "2025-01-24T08:12:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:12:47Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "title": "Locality-aware Fair Scheduling in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locality-aware Fair Scheduling in LLM Serving"
                },
                "summary": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency."
                },
                "authors": [
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Pin-Lun Hsu"
                    },
                    {
                        "name": "Liangsheng Yin"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14205v1",
                "updated": "2025-01-24T03:21:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T03:21:20Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "title": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading"
                },
                "summary": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Minrui Xu"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v1",
                "updated": "2025-01-23T12:58:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13998v1",
                "updated": "2025-01-23T11:18:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T11:18:42Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "title": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry"
                },
                "summary": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique."
                },
                "authors": [
                    {
                        "name": "E. Chamizo"
                    },
                    {
                        "name": "M. C. Jiménez-Ramos"
                    },
                    {
                        "name": "S. M. Enamorado"
                    },
                    {
                        "name": "M. García-León"
                    },
                    {
                        "name": "R. García-Tenorio"
                    },
                    {
                        "name": "J. L. Mas"
                    },
                    {
                        "name": "P. Masqué"
                    },
                    {
                        "name": "J. Merino"
                    },
                    {
                        "name": "J. A. Sanchez-Cabeza"
                    }
                ],
                "author_detail": {
                    "name": "J. A. Sanchez-Cabeza"
                },
                "author": "J. A. Sanchez-Cabeza",
                "arxiv_doi": "10.1016/j.nimb.2009.10.151",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nimb.2009.10.151",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 1 table, 3 figures",
                "arxiv_journal_ref": "Nuclear Instruments and Methods in Physics Research Section B:\n  Beam Interactions with Materials and Atoms, Volume 268, Issues 7-8, April\n  2010, Pages 1273-1276",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13540v1",
                "updated": "2025-01-23T10:40:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T10:40:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks"
                },
                "summary": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Harel Berger"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    }
                ],
                "author_detail": {
                    "name": "Anat Bremler-Barr"
                },
                "author": "Anat Bremler-Barr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v3",
                "updated": "2025-01-23T07:25:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    7,
                    25,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v2",
                "updated": "2025-01-23T06:48:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    48,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v1",
                "updated": "2025-01-23T02:20:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring"
                },
                "summary": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11745v2",
                "updated": "2025-01-22T16:25:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    25,
                    47,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-20T21:07:44Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    7,
                    44,
                    0,
                    20,
                    0
                ],
                "title": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching"
                },
                "summary": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms."
                },
                "authors": [
                    {
                        "name": "Krishnendu S. Tharakan"
                    },
                    {
                        "name": "Hayssam Dahrouj"
                    },
                    {
                        "name": "Nour Kouzayha"
                    },
                    {
                        "name": "Hesham ElSawy"
                    },
                    {
                        "name": "Tareq Y. Al-Naffouri"
                    }
                ],
                "author_detail": {
                    "name": "Tareq Y. Al-Naffouri"
                },
                "author": "Tareq Y. Al-Naffouri",
                "arxiv_comment": "accepted for publication in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v1",
                "updated": "2025-01-22T15:33:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v5",
                "updated": "2025-01-22T15:09:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    9,
                    58,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v3",
                "updated": "2025-01-22T15:05:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    5,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "To appear in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v2",
                "updated": "2025-01-22T10:39:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    39,
                    50,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12744v1",
                "updated": "2025-01-22T09:25:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T09:25:29Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "title": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity"
                },
                "summary": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips."
                },
                "authors": [
                    {
                        "name": "Baptiste Lefaucher"
                    },
                    {
                        "name": "Yoann Baron"
                    },
                    {
                        "name": "Jean-Baptiste Jager"
                    },
                    {
                        "name": "Vincent Calvo"
                    },
                    {
                        "name": "Christian Elsässer"
                    },
                    {
                        "name": "Giuliano Coppola"
                    },
                    {
                        "name": "Frédéric Mazen"
                    },
                    {
                        "name": "Sébastien Kerdilès"
                    },
                    {
                        "name": "Félix Cache"
                    },
                    {
                        "name": "Anaïs Dréau"
                    },
                    {
                        "name": "Jean-Michel Gérard"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Gérard"
                },
                "arxiv_affiliation": "Univ. Grenoble Alpes, CEA, Grenoble INP, IRIG, PHELIQS",
                "author": "Jean-Michel Gérard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12528v1",
                "updated": "2025-01-21T22:33:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T22:33:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System"
                },
                "summary": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT."
                },
                "authors": [
                    {
                        "name": "Junyi Wang"
                    },
                    {
                        "name": "Quan Zang"
                    },
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Minquan Cheng"
                },
                "author": "Minquan Cheng",
                "arxiv_comment": "14",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v1",
                "updated": "2025-01-21T12:19:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11940v1",
                "updated": "2025-01-21T07:32:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:32:06Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "title": "Build Optimization: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Build Optimization: A Systematic Literature Review"
                },
                "summary": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research."
                },
                "authors": [
                    {
                        "name": "Henri Aïdasso"
                    },
                    {
                        "name": "Mohammed Sayagh"
                    },
                    {
                        "name": "Francis Bordeleau"
                    }
                ],
                "author_detail": {
                    "name": "Francis Bordeleau"
                },
                "author": "Francis Bordeleau",
                "arxiv_comment": "An earlier version of this work was submitted to ACM CSUR in November\n  2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11834v1",
                "updated": "2025-01-21T02:35:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:35:31Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "title": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching"
                },
                "summary": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios."
                },
                "authors": [
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "35 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v1",
                "updated": "2025-01-20T23:10:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference"
                },
                "summary": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11502v1",
                "updated": "2025-01-20T14:19:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T14:19:48Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "title": "Hierarchical Coded Caching in High Memory Regime with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching in High Memory Regime with Coded Placement"
                },
                "summary": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages, 3 figures and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v3",
                "updated": "2025-01-20T08:44:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    8,
                    44,
                    1,
                    0,
                    20,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11175v1",
                "updated": "2025-01-19T21:25:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "published": "2025-01-19T21:25:53Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models"
                },
                "summary": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark."
                },
                "authors": [
                    {
                        "name": "Yassir Bendou"
                    },
                    {
                        "name": "Amine Ouasfi"
                    },
                    {
                        "name": "Vincent Gripon"
                    },
                    {
                        "name": "Adnane Boukhayma"
                    }
                ],
                "author_detail": {
                    "name": "Adnane Boukhayma"
                },
                "author": "Adnane Boukhayma",
                "arxiv_comment": "Code available at https://ybendou.github.io/ProKeR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v3",
                "updated": "2025-01-19T19:46:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    19,
                    46,
                    21,
                    6,
                    19,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15024v2",
                "updated": "2025-01-19T15:47:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    15,
                    47,
                    14,
                    6,
                    19,
                    0
                ],
                "published": "2023-12-22T19:15:23Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    19,
                    15,
                    23,
                    4,
                    356,
                    0
                ],
                "title": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement"
                },
                "summary": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "47 pages, 16 figures and 2 tables. More figures, explanations and\n  comparisons included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10756v1",
                "updated": "2025-01-18T13:04:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T13:04:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology"
                },
                "summary": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "21 pages, 12 figures and 4 tables. Some overlap with 2409.14350v1\n  [cs.IT] 22 Sept. 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10682v1",
                "updated": "2025-01-18T07:29:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T07:29:20Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "title": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design"
                },
                "summary": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution."
                },
                "authors": [
                    {
                        "name": "Haoyang Zhang"
                    },
                    {
                        "name": "Yuqi Xue"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05221v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05221v3",
                "updated": "2025-01-17T16:16:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    16,
                    54,
                    4,
                    17,
                    0
                ],
                "published": "2024-09-08T20:47:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    20,
                    47,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "Geometric rigidity of simple modules for algebraic groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric rigidity of simple modules for algebraic groups"
                },
                "summary": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula."
                },
                "authors": [
                    {
                        "name": "Michael Bate"
                    },
                    {
                        "name": "David I. Stewart"
                    }
                ],
                "author_detail": {
                    "name": "David I. Stewart"
                },
                "author": "David I. Stewart",
                "arxiv_comment": "v3; 30 pages; Theorem 1 now holds for arbitrary affine algebraic\n  groups over fields",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05221v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05221v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.RA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "20G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v1",
                "updated": "2025-01-17T12:01:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v2",
                "updated": "2025-01-17T09:37:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    37,
                    36,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09902v1",
                "updated": "2025-01-17T01:24:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T01:24:12Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "title": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing"
                },
                "summary": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%."
                },
                "authors": [
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hilbert Chen"
                    },
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Nishil Talati"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_comment": "2025 IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04501v2",
                "updated": "2025-01-16T15:11:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    11,
                    42,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-05T13:42:30Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    42,
                    30,
                    4,
                    187,
                    0
                ],
                "title": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method"
                },
                "summary": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties."
                },
                "authors": [
                    {
                        "name": "Alessandro Paghi"
                    },
                    {
                        "name": "Sebastiano Battisti"
                    },
                    {
                        "name": "Simone Tortorella"
                    },
                    {
                        "name": "Giorgio De Simoni"
                    },
                    {
                        "name": "Francesco Giazotto"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Giazotto"
                },
                "author": "Francesco Giazotto",
                "arxiv_doi": "10.1063/5.0250428",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1063/5.0250428",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages, 4 figures, supporting information at the end of the paper",
                "arxiv_journal_ref": "J. Appl. Phys. 137, 044103 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11501v2",
                "updated": "2025-01-16T10:35:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    35,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2023-12-08T15:11:26Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    15,
                    11,
                    26,
                    4,
                    342,
                    0
                ],
                "title": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization"
                },
                "summary": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache."
                },
                "authors": [
                    {
                        "name": "Congcong Chen"
                    },
                    {
                        "name": "Jinhua Cui"
                    },
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Jiliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Zhang"
                },
                "author": "Jiliang Zhang",
                "arxiv_comment": "This manuscript was published in IEEE Transactions on Information\n  Forensics and Security, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09383v1",
                "updated": "2025-01-16T08:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "title": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service"
                },
                "summary": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Zehui Xiong"
                },
                "author": "Zehui Xiong",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09290v1",
                "updated": "2025-01-16T04:50:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T04:50:15Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "title": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work"
                },
                "summary": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation."
                },
                "authors": [
                    {
                        "name": "Xiaoshan Zhou"
                    },
                    {
                        "name": "Carol C. Menassa"
                    },
                    {
                        "name": "Vineet R. Kamat"
                    }
                ],
                "author_detail": {
                    "name": "Vineet R. Kamat"
                },
                "author": "Vineet R. Kamat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v1",
                "updated": "2025-01-16T02:40:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10845v2",
                "updated": "2025-01-15T21:09:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    9,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2024-04-16T18:47:07Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    18,
                    47,
                    7,
                    1,
                    107,
                    0
                ],
                "title": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs"
                },
                "summary": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "16 pages, 8 figures, 2 algorithms, 2 tables, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v1",
                "updated": "2025-01-15T20:55:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "25 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v2",
                "updated": "2025-01-15T01:34:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    1,
                    34,
                    46,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08484v1",
                "updated": "2025-01-14T23:13:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T23:13:14Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "title": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling"
                },
                "summary": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method."
                },
                "authors": [
                    {
                        "name": "Robert Gifford"
                    },
                    {
                        "name": "Abby Eisenklam"
                    },
                    {
                        "name": "Georgiy A. Bondar"
                    },
                    {
                        "name": "Yifan Cai"
                    },
                    {
                        "name": "Tushar Sial"
                    },
                    {
                        "name": "Linh Thi Xuan Phan"
                    },
                    {
                        "name": "Abhishek Halder"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Halder"
                },
                "author": "Abhishek Halder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v1",
                "updated": "2025-01-14T15:14:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v2",
                "updated": "2025-01-14T14:07:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    7,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v2",
                "updated": "2025-01-14T12:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    6,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15896v2",
                "updated": "2025-01-14T11:41:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    41,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2024-03-23T17:38:57Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    17,
                    38,
                    57,
                    5,
                    83,
                    0
                ],
                "title": "Cell-level modelling of homeostasis in confined epithelial monolayers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-level modelling of homeostasis in confined epithelial monolayers"
                },
                "summary": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Jan Rozman"
                    },
                    {
                        "name": "Andrej Košmrlj"
                    },
                    {
                        "name": "Rastko Sknepnek"
                    }
                ],
                "author_detail": {
                    "name": "Rastko Sknepnek"
                },
                "author": "Rastko Sknepnek",
                "arxiv_comment": "18 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v2",
                "updated": "2025-01-14T05:48:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    7,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10480v2",
                "updated": "2025-01-14T05:00:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    0,
                    34,
                    1,
                    14,
                    0
                ],
                "published": "2024-05-17T00:52:39Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    0,
                    52,
                    39,
                    4,
                    138,
                    0
                ],
                "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers"
                },
                "summary": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths."
                },
                "authors": [
                    {
                        "name": "Rya Sanovar"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "Renee St. Amant"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v2",
                "updated": "2025-01-14T02:02:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    2,
                    2,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v2",
                "updated": "2025-01-13T17:34:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    34,
                    22,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v4",
                "updated": "2025-01-13T09:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    33,
                    25,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Küpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_doi": "10.1109/PST62714.2024.10788053",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/PST62714.2024.10788053",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07533v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024)",
                "arxiv_journal_ref": "2024 21st Annual International Conference on Privacy, Security and\n  Trust (PST), 2024, pp. 1-11",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v1",
                "updated": "2025-01-13T04:31:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v5",
                "updated": "2025-01-13T03:11:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    3,
                    11,
                    28,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06872v1",
                "updated": "2025-01-12T17:01:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T17:01:40Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "title": "On Optimizing Locality of Graph Transposition on Modern Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Optimizing Locality of Graph Transposition on Modern Architectures"
                },
                "summary": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    }
                ],
                "author_detail": {
                    "name": "Hans Vandierendonck"
                },
                "author": "Hans Vandierendonck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v1",
                "updated": "2025-01-12T13:18:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Junming Ma"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v2",
                "updated": "2025-01-12T12:01:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    12,
                    1,
                    47,
                    6,
                    12,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Technical report, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v3",
                "updated": "2025-01-12T11:15:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    11,
                    15,
                    41,
                    6,
                    12,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v2",
                "updated": "2025-01-12T05:25:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    5,
                    25,
                    6,
                    6,
                    12,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.19403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19403v1",
                "updated": "2025-01-31T18:58:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    58,
                    43,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:58:43Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    58,
                    43,
                    4,
                    31,
                    0
                ],
                "title": "Redefining Machine Unlearning: A Conformal Prediction-Motivated Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redefining Machine Unlearning: A Conformal Prediction-Motivated Approach"
                },
                "summary": "Machine unlearning seeks to systematically remove specified data from a\ntrained model, effectively achieving a state as though the data had never been\nencountered during training. While metrics such as Unlearning Accuracy (UA) and\nMembership Inference Attack (MIA) provide a baseline for assessing unlearning\nperformance, they fall short of evaluating the completeness and reliability of\nforgetting. This is because the ground truth labels remain potential candidates\nwithin the scope of uncertainty quantification, leaving gaps in the evaluation\nof true forgetting. In this paper, we identify critical limitations in existing\nunlearning metrics and propose enhanced evaluation metrics inspired by\nconformal prediction. Our metrics can effectively capture the extent to which\nground truth labels are excluded from the prediction set. Furthermore, we\nobserve that many existing machine unlearning methods do not achieve\nsatisfactory forgetting performance when evaluated with our new metrics. To\naddress this, we propose an unlearning framework that integrates conformal\nprediction insights into Carlini & Wagner adversarial attack loss. Extensive\nexperiments on the image classification task demonstrate that our enhanced\nmetrics offer deeper insights into unlearning effectiveness, and that our\nunlearning framework significantly improves the forgetting quality of\nunlearning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning seeks to systematically remove specified data from a\ntrained model, effectively achieving a state as though the data had never been\nencountered during training. While metrics such as Unlearning Accuracy (UA) and\nMembership Inference Attack (MIA) provide a baseline for assessing unlearning\nperformance, they fall short of evaluating the completeness and reliability of\nforgetting. This is because the ground truth labels remain potential candidates\nwithin the scope of uncertainty quantification, leaving gaps in the evaluation\nof true forgetting. In this paper, we identify critical limitations in existing\nunlearning metrics and propose enhanced evaluation metrics inspired by\nconformal prediction. Our metrics can effectively capture the extent to which\nground truth labels are excluded from the prediction set. Furthermore, we\nobserve that many existing machine unlearning methods do not achieve\nsatisfactory forgetting performance when evaluated with our new metrics. To\naddress this, we propose an unlearning framework that integrates conformal\nprediction insights into Carlini & Wagner adversarial attack loss. Extensive\nexperiments on the image classification task demonstrate that our enhanced\nmetrics offer deeper insights into unlearning effectiveness, and that our\nunlearning framework significantly improves the forgetting quality of\nunlearning methods."
                },
                "authors": [
                    {
                        "name": "Yingdan Shi"
                    },
                    {
                        "name": "Ren Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ren Wang"
                },
                "author": "Ren Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05362v2",
                "updated": "2025-01-31T18:58:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    58,
                    24,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-07T17:45:00Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    45,
                    0,
                    0,
                    281,
                    0
                ],
                "title": "LLMs Are In-Context Bandit Reinforcement Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Are In-Context Bandit Reinforcement Learners"
                },
                "summary": "Large Language Models (LLMs) excel at in-context learning (ICL), a supervised\nlearning technique that relies on adding annotated examples to the model\ncontext. We investigate a contextual bandit version of in-context reinforcement\nlearning (ICRL), where models learn in-context, online, from external reward,\ninstead of supervised data. We show that LLMs effectively demonstrate such\nlearning, and provide a detailed study of the phenomena, experimenting with\nchallenging classification tasks and models of sizes from 500M to 70B\nparameters. This includes identifying and addressing the instability of the\nprocess, demonstrating learning with both semantic and abstract labels, and\nshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, while\nalso underscoring fundamental limitations in their implicit reasoning about\nerrors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at in-context learning (ICL), a supervised\nlearning technique that relies on adding annotated examples to the model\ncontext. We investigate a contextual bandit version of in-context reinforcement\nlearning (ICRL), where models learn in-context, online, from external reward,\ninstead of supervised data. We show that LLMs effectively demonstrate such\nlearning, and provide a detailed study of the phenomena, experimenting with\nchallenging classification tasks and models of sizes from 500M to 70B\nparameters. This includes identifying and addressing the instability of the\nprocess, demonstrating learning with both semantic and abstract labels, and\nshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, while\nalso underscoring fundamental limitations in their implicit reasoning about\nerrors."
                },
                "authors": [
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Kianté Brantley"
                    },
                    {
                        "name": "Yoav Artzi"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Artzi"
                },
                "author": "Yoav Artzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19400v1",
                "updated": "2025-01-31T18:57:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    57,
                    8,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:57:08Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    57,
                    8,
                    4,
                    31,
                    0
                ],
                "title": "Vintix: Action Model via In-Context Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vintix: Action Model via In-Context Reinforcement Learning"
                },
                "summary": "In-Context Reinforcement Learning (ICRL) represents a promising paradigm for\ndeveloping generalist agents that learn at inference time through\ntrial-and-error interactions, analogous to how large language models adapt\ncontextually, but with a focus on reward maximization. However, the scalability\nof ICRL beyond toy tasks and single-domain settings remains an open challenge.\nIn this work, we present the first steps toward scaling ICRL by introducing a\nfixed, cross-domain model capable of learning behaviors through in-context\nreinforcement learning. Our results demonstrate that Algorithm Distillation, a\nframework designed to facilitate ICRL, offers a compelling and competitive\nalternative to expert distillation to construct versatile action models. These\nfindings highlight the potential of ICRL as a scalable approach for generalist\ndecision-making systems. Code to be released at\nhttps://github.com/dunnolab/vintix",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Reinforcement Learning (ICRL) represents a promising paradigm for\ndeveloping generalist agents that learn at inference time through\ntrial-and-error interactions, analogous to how large language models adapt\ncontextually, but with a focus on reward maximization. However, the scalability\nof ICRL beyond toy tasks and single-domain settings remains an open challenge.\nIn this work, we present the first steps toward scaling ICRL by introducing a\nfixed, cross-domain model capable of learning behaviors through in-context\nreinforcement learning. Our results demonstrate that Algorithm Distillation, a\nframework designed to facilitate ICRL, offers a compelling and competitive\nalternative to expert distillation to construct versatile action models. These\nfindings highlight the potential of ICRL as a scalable approach for generalist\ndecision-making systems. Code to be released at\nhttps://github.com/dunnolab/vintix"
                },
                "authors": [
                    {
                        "name": "Andrey Polubarov"
                    },
                    {
                        "name": "Nikita Lyubaykin"
                    },
                    {
                        "name": "Alexander Derevyagin"
                    },
                    {
                        "name": "Ilya Zisman"
                    },
                    {
                        "name": "Denis Tarasov"
                    },
                    {
                        "name": "Alexander Nikulin"
                    },
                    {
                        "name": "Vladislav Kurenkov"
                    }
                ],
                "author_detail": {
                    "name": "Vladislav Kurenkov"
                },
                "author": "Vladislav Kurenkov",
                "arxiv_comment": "Preprint. In review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19398v1",
                "updated": "2025-01-31T18:53:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    53,
                    43,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:53:43Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    53,
                    43,
                    4,
                    31,
                    0
                ],
                "title": "Do LLMs Strategically Reveal, Conceal, and Infer Information? A\n  Theoretical and Empirical Analysis in The Chameleon Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Strategically Reveal, Conceal, and Infer Information? A\n  Theoretical and Empirical Analysis in The Chameleon Game"
                },
                "summary": "Large language model-based (LLM-based) agents have become common in settings\nthat include non-cooperative parties. In such settings, agents' decision-making\nneeds to conceal information from their adversaries, reveal information to\ntheir cooperators, and infer information to identify the other agents'\ncharacteristics. To investigate whether LLMs have these information control and\ndecision-making capabilities, we make LLM agents play the language-based\nhidden-identity game, The Chameleon. In the game, a group of non-chameleon\nagents who do not know each other aim to identify the chameleon agent without\nrevealing a secret. The game requires the aforementioned information control\ncapabilities both as a chameleon and a non-chameleon. The empirical results\nshow that while non-chameleon LLM agents identify the chameleon, they fail to\nconceal the secret from the chameleon, and their winning probability is far\nfrom the levels of even trivial strategies. To formally explain this behavior,\nwe give a theoretical analysis for a spectrum of strategies, from concealing to\nrevealing, and provide bounds on the non-chameleons' winning probability. Based\non the empirical results and theoretical analysis of different strategies, we\ndeduce that LLM-based non-chameleon agents reveal excessive information to\nagents of unknown identities. Our results point to a weakness of contemporary\nLLMs, including GPT-4, GPT-4o, Gemini 1.5, and Claude 3.5 Sonnet, in strategic\ninteractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based (LLM-based) agents have become common in settings\nthat include non-cooperative parties. In such settings, agents' decision-making\nneeds to conceal information from their adversaries, reveal information to\ntheir cooperators, and infer information to identify the other agents'\ncharacteristics. To investigate whether LLMs have these information control and\ndecision-making capabilities, we make LLM agents play the language-based\nhidden-identity game, The Chameleon. In the game, a group of non-chameleon\nagents who do not know each other aim to identify the chameleon agent without\nrevealing a secret. The game requires the aforementioned information control\ncapabilities both as a chameleon and a non-chameleon. The empirical results\nshow that while non-chameleon LLM agents identify the chameleon, they fail to\nconceal the secret from the chameleon, and their winning probability is far\nfrom the levels of even trivial strategies. To formally explain this behavior,\nwe give a theoretical analysis for a spectrum of strategies, from concealing to\nrevealing, and provide bounds on the non-chameleons' winning probability. Based\non the empirical results and theoretical analysis of different strategies, we\ndeduce that LLM-based non-chameleon agents reveal excessive information to\nagents of unknown identities. Our results point to a weakness of contemporary\nLLMs, including GPT-4, GPT-4o, Gemini 1.5, and Claude 3.5 Sonnet, in strategic\ninteractions."
                },
                "authors": [
                    {
                        "name": "Mustafa O. Karabag"
                    },
                    {
                        "name": "Ufuk Topcu"
                    }
                ],
                "author_detail": {
                    "name": "Ufuk Topcu"
                },
                "author": "Ufuk Topcu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08992v2",
                "updated": "2025-01-31T18:53:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    53,
                    30,
                    4,
                    31,
                    0
                ],
                "published": "2024-05-14T23:24:12Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    23,
                    24,
                    12,
                    1,
                    135,
                    0
                ],
                "title": "Contextual Emotion Recognition using Large Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Emotion Recognition using Large Vision Language Models"
                },
                "summary": "\"How does the person in the bounding box feel?\" Achieving human-level\nrecognition of the apparent emotion of a person in real world situations\nremains an unsolved task in computer vision. Facial expressions are not enough:\nbody pose, contextual knowledge, and commonsense reasoning all contribute to\nhow humans perform this emotional theory of mind task. In this paper, we\nexamine two major approaches enabled by recent large vision language models: 1)\nimage captioning followed by a language-only LLM, and 2) vision language\nmodels, under zero-shot and fine-tuned setups. We evaluate the methods on the\nEmotions in Context (EMOTIC) dataset and demonstrate that a vision language\nmodel, fine-tuned even on a small dataset, can significantly outperform\ntraditional baselines. The results of this work aim to help robots and agents\nperform emotionally sensitive decision-making and interaction in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"How does the person in the bounding box feel?\" Achieving human-level\nrecognition of the apparent emotion of a person in real world situations\nremains an unsolved task in computer vision. Facial expressions are not enough:\nbody pose, contextual knowledge, and commonsense reasoning all contribute to\nhow humans perform this emotional theory of mind task. In this paper, we\nexamine two major approaches enabled by recent large vision language models: 1)\nimage captioning followed by a language-only LLM, and 2) vision language\nmodels, under zero-shot and fine-tuned setups. We evaluate the methods on the\nEmotions in Context (EMOTIC) dataset and demonstrate that a vision language\nmodel, fine-tuned even on a small dataset, can significantly outperform\ntraditional baselines. The results of this work aim to help robots and agents\nperform emotionally sensitive decision-making and interaction in the future."
                },
                "authors": [
                    {
                        "name": "Yasaman Etesam"
                    },
                    {
                        "name": "Özge Nilay Yalçın"
                    },
                    {
                        "name": "Chuxuan Zhang"
                    },
                    {
                        "name": "Angelica Lim"
                    }
                ],
                "author_detail": {
                    "name": "Angelica Lim"
                },
                "author": "Angelica Lim",
                "arxiv_doi": "10.1109/IROS58592.2024.10802538",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IROS58592.2024.10802538",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.08992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, website:\n  https://yasaman-etesam.github.io/Contextual-Emotion-Recognition/. arXiv admin\n  note: text overlap with arXiv:2310.19995",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19394v1",
                "updated": "2025-01-31T18:48:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    48,
                    12,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:48:12Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    48,
                    12,
                    4,
                    31,
                    0
                ],
                "title": "Fixed-Population Causal Inference for Models of Equilibrium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fixed-Population Causal Inference for Models of Equilibrium"
                },
                "summary": "In contrast to problems of interference in (exogenous) treatments, models of\ninterference in unit-specific (endogenous) outcomes do not usually produce a\nreduced-form representation where outcomes depend on other units' treatment\nstatus only at a short network distance, or only through a known exposure\nmapping. This remains true if the structural mechanism depends on outcomes of\npeers only at a short network distance, or through a known exposure mapping. In\nthis paper, we first define causal estimands that are identified and estimable\nfrom a single experiment on the network under minimal assumptions on the\nstructure of interference, and which represent average partial causal responses\nwhich generally vary with other global features of the realized assignment.\nUnder a fixed-population, design-based approach, we show unbiasedness,\nconsistency and asymptotic normality for inverse-probability weighting (IPW)\nestimators for those causal parameters from a randomized experiment on a single\nnetwork. We also analyze more closely the case of marginal interventions in a\nmodel of equilibrium with smooth response functions where we can recover\nLATE-type weighted averages of derivatives of those response functions. Under\nadditional structural assumptions, these \"agnostic\" causal estimands can be\ncombined to recover model parameters, but also retain their less restrictive\ncausal interpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contrast to problems of interference in (exogenous) treatments, models of\ninterference in unit-specific (endogenous) outcomes do not usually produce a\nreduced-form representation where outcomes depend on other units' treatment\nstatus only at a short network distance, or only through a known exposure\nmapping. This remains true if the structural mechanism depends on outcomes of\npeers only at a short network distance, or through a known exposure mapping. In\nthis paper, we first define causal estimands that are identified and estimable\nfrom a single experiment on the network under minimal assumptions on the\nstructure of interference, and which represent average partial causal responses\nwhich generally vary with other global features of the realized assignment.\nUnder a fixed-population, design-based approach, we show unbiasedness,\nconsistency and asymptotic normality for inverse-probability weighting (IPW)\nestimators for those causal parameters from a randomized experiment on a single\nnetwork. We also analyze more closely the case of marginal interventions in a\nmodel of equilibrium with smooth response functions where we can recover\nLATE-type weighted averages of derivatives of those response functions. Under\nadditional structural assumptions, these \"agnostic\" causal estimands can be\ncombined to recover model parameters, but also retain their less restrictive\ncausal interpretation."
                },
                "authors": [
                    {
                        "name": "Konrad Menzel"
                    }
                ],
                "author_detail": {
                    "name": "Konrad Menzel"
                },
                "author": "Konrad Menzel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v1",
                "updated": "2025-01-31T18:47:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19389v1",
                "updated": "2025-01-31T18:44:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    44,
                    35,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:44:35Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    44,
                    35,
                    4,
                    31,
                    0
                ],
                "title": "Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large\n  Language Models"
                },
                "summary": "Fine-tuning large language models (LLMs) on devices is attracting increasing\ninterest. Recent works have fused low-rank adaptation (LoRA) techniques with\nfederated fine-tuning to mitigate challenges associated with device model sizes\nand data scarcity. Still, the heterogeneity of computational resources remains\na critical bottleneck: while higher-rank modules generally enhance performance,\nvarying device capabilities constrain LoRA's feasible rank range. Existing\napproaches attempting to resolve this issue either lack analytical\njustification or impose additional computational overhead, leaving a wide gap\nfor an efficient and theoretically-grounded solution. To address these\nchallenges, we propose federated sketching LoRA (FSLoRA), which leverages a\nsketching mechanism to enable devices to selectively update submatrices of\nglobal LoRA modules maintained by the server. By adjusting the sketching\nratios, which determine the ranks of the submatrices on the devices, FSLoRA\nflexibly adapts to device-specific communication and computational constraints.\nWe provide a rigorous convergence analysis of FSLoRA that characterizes how the\nsketching ratios affect the convergence rate. Through comprehensive experiments\non multiple datasets and LLM models, we demonstrate FSLoRA's superior\nperformance compared to various baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on devices is attracting increasing\ninterest. Recent works have fused low-rank adaptation (LoRA) techniques with\nfederated fine-tuning to mitigate challenges associated with device model sizes\nand data scarcity. Still, the heterogeneity of computational resources remains\na critical bottleneck: while higher-rank modules generally enhance performance,\nvarying device capabilities constrain LoRA's feasible rank range. Existing\napproaches attempting to resolve this issue either lack analytical\njustification or impose additional computational overhead, leaving a wide gap\nfor an efficient and theoretically-grounded solution. To address these\nchallenges, we propose federated sketching LoRA (FSLoRA), which leverages a\nsketching mechanism to enable devices to selectively update submatrices of\nglobal LoRA modules maintained by the server. By adjusting the sketching\nratios, which determine the ranks of the submatrices on the devices, FSLoRA\nflexibly adapts to device-specific communication and computational constraints.\nWe provide a rigorous convergence analysis of FSLoRA that characterizes how the\nsketching ratios affect the convergence rate. Through comprehensive experiments\non multiple datasets and LLM models, we demonstrate FSLoRA's superior\nperformance compared to various baselines."
                },
                "authors": [
                    {
                        "name": "Wenzhi Fang"
                    },
                    {
                        "name": "Dong-Jun Han"
                    },
                    {
                        "name": "Liangqi Yuan"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19381v1",
                "updated": "2025-01-31T18:34:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    34,
                    16,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:34:16Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    34,
                    16,
                    4,
                    31,
                    0
                ],
                "title": "Using gradient of Lagrangian function to compute efficient channels for\n  the ideal observer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using gradient of Lagrangian function to compute efficient channels for\n  the ideal observer"
                },
                "summary": "It is widely accepted that the Bayesian ideal observer (IO) should be used to\nguide the objective assessment and optimization of medical imaging systems. The\nIO employs complete task-specific information to compute test statistics for\nmaking inference decisions and performs optimally in signal detection tasks.\nHowever, the IO test statistic typically depends non-linearly on the image data\nand cannot be analytically determined. The ideal linear observer, known as the\nHotelling observer (HO), can sometimes be used as a surrogate for the IO.\nHowever, when image data are high dimensional, HO computation can be difficult.\nEfficient channels that can extract task-relevant features have been\ninvestigated to reduce the dimensionality of image data to approximate IO and\nHO performance. This work proposes a novel method for generating efficient\nchannels by use of the gradient of a Lagrangian-based loss function that was\ndesigned to learn the HO. The generated channels are referred to as the\nLagrangian-gradient (L-grad) channels. Numerical studies are conducted that\nconsider binary signal detection tasks involving various backgrounds and\nsignals. It is demonstrated that channelized HO (CHO) using L-grad channels can\nproduce significantly better signal detection performance compared to the CHO\nusing PLS channels. Moreover, it is shown that the proposed L-grad method can\nachieve significantly lower computation time compared to the PLS method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is widely accepted that the Bayesian ideal observer (IO) should be used to\nguide the objective assessment and optimization of medical imaging systems. The\nIO employs complete task-specific information to compute test statistics for\nmaking inference decisions and performs optimally in signal detection tasks.\nHowever, the IO test statistic typically depends non-linearly on the image data\nand cannot be analytically determined. The ideal linear observer, known as the\nHotelling observer (HO), can sometimes be used as a surrogate for the IO.\nHowever, when image data are high dimensional, HO computation can be difficult.\nEfficient channels that can extract task-relevant features have been\ninvestigated to reduce the dimensionality of image data to approximate IO and\nHO performance. This work proposes a novel method for generating efficient\nchannels by use of the gradient of a Lagrangian-based loss function that was\ndesigned to learn the HO. The generated channels are referred to as the\nLagrangian-gradient (L-grad) channels. Numerical studies are conducted that\nconsider binary signal detection tasks involving various backgrounds and\nsignals. It is demonstrated that channelized HO (CHO) using L-grad channels can\nproduce significantly better signal detection performance compared to the CHO\nusing PLS channels. Moreover, it is shown that the proposed L-grad method can\nachieve significantly lower computation time compared to the PLS method."
                },
                "authors": [
                    {
                        "name": "Weimin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Weimin Zhou"
                },
                "author": "Weimin Zhou",
                "arxiv_comment": "SPIE Medical Imaging 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19377v1",
                "updated": "2025-01-31T18:30:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    30,
                    36,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:30:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    30,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "SELMA: A Speech-Enabled Language Model for Virtual Assistant\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELMA: A Speech-Enabled Language Model for Virtual Assistant\n  Interactions"
                },
                "summary": "In this work, we present and evaluate SELMA, a Speech-Enabled Language Model\nfor virtual Assistant interactions that integrates audio and text as inputs to\na Large Language Model (LLM). SELMA is designed to handle three primary and two\nauxiliary tasks related to interactions with virtual assistants simultaneously\nwithin a single end-to-end model. We employ low-rank adaptation modules for\nparameter-efficient training of both the audio encoder and the LLM.\nAdditionally, we implement a feature pooling strategy enabling the system to\nrecognize global patterns and improve accuracy on tasks less reliant on\nindividual sequence elements. Experimental results on Voice Trigger (VT)\ndetection, Device-Directed Speech Detection (DDSD), and Automatic Speech\nRecognition (ASR), demonstrate that our approach both simplifies the typical\ninput processing pipeline of virtual assistants significantly and also improves\nperformance compared to dedicated models for each individual task. SELMA yields\nrelative Equal-Error Rate improvements of 64% on the VT detection task, and 22%\non DDSD, while also achieving word error rates close to the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present and evaluate SELMA, a Speech-Enabled Language Model\nfor virtual Assistant interactions that integrates audio and text as inputs to\na Large Language Model (LLM). SELMA is designed to handle three primary and two\nauxiliary tasks related to interactions with virtual assistants simultaneously\nwithin a single end-to-end model. We employ low-rank adaptation modules for\nparameter-efficient training of both the audio encoder and the LLM.\nAdditionally, we implement a feature pooling strategy enabling the system to\nrecognize global patterns and improve accuracy on tasks less reliant on\nindividual sequence elements. Experimental results on Voice Trigger (VT)\ndetection, Device-Directed Speech Detection (DDSD), and Automatic Speech\nRecognition (ASR), demonstrate that our approach both simplifies the typical\ninput processing pipeline of virtual assistants significantly and also improves\nperformance compared to dedicated models for each individual task. SELMA yields\nrelative Equal-Error Rate improvements of 64% on the VT detection task, and 22%\non DDSD, while also achieving word error rates close to the baseline."
                },
                "authors": [
                    {
                        "name": "Dominik Wagner"
                    },
                    {
                        "name": "Alexander Churchill"
                    },
                    {
                        "name": "Siddarth Sigtia"
                    },
                    {
                        "name": "Erik Marchi"
                    }
                ],
                "author_detail": {
                    "name": "Erik Marchi"
                },
                "author": "Erik Marchi",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02755v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02755v3",
                "updated": "2025-01-31T18:21:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    21,
                    59,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-03T17:58:29Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    29,
                    3,
                    277,
                    0
                ],
                "title": "GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to\n  Filter Language Model Pretraining Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to\n  Filter Language Model Pretraining Data"
                },
                "summary": "Large language models require vast amounts of high-quality training data, but\neffective filtering of web-scale datasets remains a significant challenge. This\npaper demonstrates that GPT-4o is remarkably effective at identifying\nhigh-quality training data, but its prohibitive cost makes it impractical at\nweb-scale. We propose SIEVE, a lightweight alternative that matches GPT-4o\naccuracy at less than 1\\% of the cost. SIEVE can perform up to 500 filtering\noperations for the cost of one GPT-4o filtering call. The key to SIEVE is a\nseamless integration of GPT-4o and lightweight text classification models,\nusing active learning to fine-tune these models in the background with a small\nnumber of calls to GPT-4o. Once trained, it performs as well as GPT-4o at a\ntiny fraction of the cost. Through different filtering prompts, SIEVE can\nefficiently curate high quality data for general or specialized domains from\nweb-scale corpora -- a valuable capability given the current scarcity of\nhigh-quality domain-specific datasets. Extensive experiments using automatic\nand human evaluation metrics show that SIEVE and GPT-4o achieve similar\nperformance on five highly specific filtering prompts. In addition, when\nperforming quality filtering on web crawl datasets, we demonstrate SIEVE can\nfurther improve over state-of-the-art quality filtering methods in the\nDataComp-LM challenge for selecting LLM pretraining data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models require vast amounts of high-quality training data, but\neffective filtering of web-scale datasets remains a significant challenge. This\npaper demonstrates that GPT-4o is remarkably effective at identifying\nhigh-quality training data, but its prohibitive cost makes it impractical at\nweb-scale. We propose SIEVE, a lightweight alternative that matches GPT-4o\naccuracy at less than 1\\% of the cost. SIEVE can perform up to 500 filtering\noperations for the cost of one GPT-4o filtering call. The key to SIEVE is a\nseamless integration of GPT-4o and lightweight text classification models,\nusing active learning to fine-tune these models in the background with a small\nnumber of calls to GPT-4o. Once trained, it performs as well as GPT-4o at a\ntiny fraction of the cost. Through different filtering prompts, SIEVE can\nefficiently curate high quality data for general or specialized domains from\nweb-scale corpora -- a valuable capability given the current scarcity of\nhigh-quality domain-specific datasets. Extensive experiments using automatic\nand human evaluation metrics show that SIEVE and GPT-4o achieve similar\nperformance on five highly specific filtering prompts. In addition, when\nperforming quality filtering on web crawl datasets, we demonstrate SIEVE can\nfurther improve over state-of-the-art quality filtering methods in the\nDataComp-LM challenge for selecting LLM pretraining data."
                },
                "authors": [
                    {
                        "name": "Jifan Zhang"
                    },
                    {
                        "name": "Ziyue Luo"
                    },
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Ness Shroff"
                    },
                    {
                        "name": "Robert Nowak"
                    }
                ],
                "author_detail": {
                    "name": "Robert Nowak"
                },
                "author": "Robert Nowak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02755v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02755v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19370v1",
                "updated": "2025-01-31T18:21:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    21,
                    12,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:21:12Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    21,
                    12,
                    4,
                    31,
                    0
                ],
                "title": "Greedy Stein Variational Gradient Descent: An algorithmic approach for\n  wave prospection problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Greedy Stein Variational Gradient Descent: An algorithmic approach for\n  wave prospection problems"
                },
                "summary": "In this project, we propose a Variational Inference algorithm to approximate\nposterior distributions. Building on prior methods, we develop the\nGradient-Steered Stein Variational Gradient Descent (G-SVGD) approach. This\nmethod introduces a novel loss function that combines a weighted gradient and\nthe Evidence Lower Bound (ELBO) to enhance convergence speed and accuracy. The\nlearning rate is determined through a suboptimal minimization of this loss\nfunction within a gradient descent framework.\n  The G-SVGD method is compared against the standard Stein Variational Gradient\nDescent (SVGD) approach, employing the ADAM optimizer for learning rate\nadaptation, as well as the Markov Chain Monte Carlo (MCMC) method. We assess\nperformance in two wave prospection models representing low-contrast and\nhigh-contrast subsurface scenarios. To achieve robust numerical approximations\nin the forward model solver, a five-point operator is employed, while the\nadjoint method improves accuracy in computing gradients of the log posterior.\n  Our findings demonstrate that G-SVGD accelerates convergence and offers\nimproved performance in scenarios where gradient evaluation is computationally\nexpensive. The abstract highlights the algorithm's applicability to wave\nprospection models and its potential for broader applications in Bayesian\ninference. Finally, we discuss the benefits and limitations of G-SVGD,\nemphasizing its contribution to advancing computational efficiency in\nuncertainty quantification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this project, we propose a Variational Inference algorithm to approximate\nposterior distributions. Building on prior methods, we develop the\nGradient-Steered Stein Variational Gradient Descent (G-SVGD) approach. This\nmethod introduces a novel loss function that combines a weighted gradient and\nthe Evidence Lower Bound (ELBO) to enhance convergence speed and accuracy. The\nlearning rate is determined through a suboptimal minimization of this loss\nfunction within a gradient descent framework.\n  The G-SVGD method is compared against the standard Stein Variational Gradient\nDescent (SVGD) approach, employing the ADAM optimizer for learning rate\nadaptation, as well as the Markov Chain Monte Carlo (MCMC) method. We assess\nperformance in two wave prospection models representing low-contrast and\nhigh-contrast subsurface scenarios. To achieve robust numerical approximations\nin the forward model solver, a five-point operator is employed, while the\nadjoint method improves accuracy in computing gradients of the log posterior.\n  Our findings demonstrate that G-SVGD accelerates convergence and offers\nimproved performance in scenarios where gradient evaluation is computationally\nexpensive. The abstract highlights the algorithm's applicability to wave\nprospection models and its potential for broader applications in Bayesian\ninference. Finally, we discuss the benefits and limitations of G-SVGD,\nemphasizing its contribution to advancing computational efficiency in\nuncertainty quantification."
                },
                "authors": [
                    {
                        "name": "Jose L. Varona-Santana"
                    },
                    {
                        "name": "Marcos A. Capistrán"
                    }
                ],
                "author_detail": {
                    "name": "Marcos A. Capistrán"
                },
                "author": "Marcos A. Capistrán",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19368v1",
                "updated": "2025-01-31T18:20:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    20,
                    50,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:20:50Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    20,
                    50,
                    4,
                    31,
                    0
                ],
                "title": "The Physics and Metaphysics of Social Powers: Bridging Cognitive\n  Processing and Social Dynamics, a New Perspective on Power through Active\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Physics and Metaphysics of Social Powers: Bridging Cognitive\n  Processing and Social Dynamics, a New Perspective on Power through Active\n  Inference"
                },
                "summary": "The concept of power can be explored at several scales: from physical action\nand process effectuation, all the way to complex social dynamics. A\nspectrum-wide analysis of power requires attention to the fundamental\nprinciples that constrain these processes. In the social realm, the acquisition\nand maintenance of power is intertwined with both social interactions and\ncognitive processing capacity: socially-facilitated empowerment grants agents\nmore information-processing capacities and opportunities, either by relying on\nothers to bring about desired policies or ultimately outcomes, and/or by\nenjoying more information-processing possibilities as a result of relying on\nothers for the reproduction of (material) tasks. The effects of social\nempowerment thus imply an increased ability to harness computation toward\ndesired ends, thereby augmenting the evolution of a specific state space.\nEmpowered individuals attract the attention of others, who contribute to\nincreasing the scale of their access to various policies effectuating these\nstate spaces. The presented argument posits that social power, in the context\nof active inference, is a function of several variables. As a result of its\npower-amplifying effects, this extended computational ability also buffers\nagainst possible vulnerabilities. We propose that individuals wield power not\nonly by associating with others possessing desirable policies, but also by\nenhancing their ability to intake and compute information effectively. This\ndual mechanism is argued to create a cyclical, reinforcing pattern wherein the\nempowered are able to incrementally expand the scope of policies and state\nspaces available to them while minimizing risk-exposure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of power can be explored at several scales: from physical action\nand process effectuation, all the way to complex social dynamics. A\nspectrum-wide analysis of power requires attention to the fundamental\nprinciples that constrain these processes. In the social realm, the acquisition\nand maintenance of power is intertwined with both social interactions and\ncognitive processing capacity: socially-facilitated empowerment grants agents\nmore information-processing capacities and opportunities, either by relying on\nothers to bring about desired policies or ultimately outcomes, and/or by\nenjoying more information-processing possibilities as a result of relying on\nothers for the reproduction of (material) tasks. The effects of social\nempowerment thus imply an increased ability to harness computation toward\ndesired ends, thereby augmenting the evolution of a specific state space.\nEmpowered individuals attract the attention of others, who contribute to\nincreasing the scale of their access to various policies effectuating these\nstate spaces. The presented argument posits that social power, in the context\nof active inference, is a function of several variables. As a result of its\npower-amplifying effects, this extended computational ability also buffers\nagainst possible vulnerabilities. We propose that individuals wield power not\nonly by associating with others possessing desirable policies, but also by\nenhancing their ability to intake and compute information effectively. This\ndual mechanism is argued to create a cyclical, reinforcing pattern wherein the\nempowered are able to incrementally expand the scope of policies and state\nspaces available to them while minimizing risk-exposure."
                },
                "authors": [
                    {
                        "name": "Mahault Albarracin"
                    },
                    {
                        "name": "Sonia de Jager"
                    },
                    {
                        "name": "David Hyland"
                    },
                    {
                        "name": "Sarah Grace Manski"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Grace Manski"
                },
                "author": "Sarah Grace Manski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19365v1",
                "updated": "2025-01-31T18:17:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    17,
                    20,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:17:20Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    17,
                    20,
                    4,
                    31,
                    0
                ],
                "title": "High-cadence observations of galactic nuclei by the future two-band\n  UV-photometry mission QUVIK",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-cadence observations of galactic nuclei by the future two-band\n  UV-photometry mission QUVIK"
                },
                "summary": "The Quick Ultra-Violet Kilonova surveyor (QUVIK), a two-band UV space\ntelescope approved for funding as a Czech national science and technology\nmission, will focus on detecting early UV light of kilonovae (Werner et al.,\n2024). In addition, it will study the UV emission of stars and stellar systems\n(Krti\\v{c}ka et al., 2024) as well as the intense and variable emission of\nactive galactic nuclei (AGN) or galactic nuclei activated by tidal disruption\nevents (Zaja\\v{c}ek et al., 2024). In this contribution, we describe the role\nof this small ($\\sim 30$-cm diameter) UV telescope for studying bright, nearby\nAGN. With its NUV and FUV bands, the telescope will perform high-cadence\n($\\sim$ 0.1-1 day) two-band photometric monitoring of nearby AGN ($z<1$), which\nwill allow us to probe accretion disk sizes/temperature profiles via\nphotometric reverberation mapping. Thanks to its versatility, QUVIK will be\nable to perform a moderately fast repointing ($<20$ min) to target candidates\nfor tidal disruption events (TDEs). Early detection of the UV emission\nfollowing a TDE optical flare, in combination with the subsequent two-band UV\nmonitoring performed simultaneously with other observatories, will enable us to\ninfer the time delay (or its lack of) between the optical, UV, and X-ray\nemission. In combination with theoretical models, it will be possible to shed\nmore light on the origin of the UV/optical emission of TDEs. Furthermore, the\ntwo-band monitoring of nuclear transients will be beneficial in distinguishing\nbetween TDEs (nearly constant blue colour) and supernovae (progressive\nreddening).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Quick Ultra-Violet Kilonova surveyor (QUVIK), a two-band UV space\ntelescope approved for funding as a Czech national science and technology\nmission, will focus on detecting early UV light of kilonovae (Werner et al.,\n2024). In addition, it will study the UV emission of stars and stellar systems\n(Krti\\v{c}ka et al., 2024) as well as the intense and variable emission of\nactive galactic nuclei (AGN) or galactic nuclei activated by tidal disruption\nevents (Zaja\\v{c}ek et al., 2024). In this contribution, we describe the role\nof this small ($\\sim 30$-cm diameter) UV telescope for studying bright, nearby\nAGN. With its NUV and FUV bands, the telescope will perform high-cadence\n($\\sim$ 0.1-1 day) two-band photometric monitoring of nearby AGN ($z<1$), which\nwill allow us to probe accretion disk sizes/temperature profiles via\nphotometric reverberation mapping. Thanks to its versatility, QUVIK will be\nable to perform a moderately fast repointing ($<20$ min) to target candidates\nfor tidal disruption events (TDEs). Early detection of the UV emission\nfollowing a TDE optical flare, in combination with the subsequent two-band UV\nmonitoring performed simultaneously with other observatories, will enable us to\ninfer the time delay (or its lack of) between the optical, UV, and X-ray\nemission. In combination with theoretical models, it will be possible to shed\nmore light on the origin of the UV/optical emission of TDEs. Furthermore, the\ntwo-band monitoring of nuclear transients will be beneficial in distinguishing\nbetween TDEs (nearly constant blue colour) and supernovae (progressive\nreddening)."
                },
                "authors": [
                    {
                        "name": "Michal Zajaček"
                    },
                    {
                        "name": "Norbert Werner"
                    },
                    {
                        "name": "Henry Best"
                    },
                    {
                        "name": "Jolie Esme L'Heureux"
                    },
                    {
                        "name": "Jakub Řípa"
                    },
                    {
                        "name": "Monika Pikhartová"
                    },
                    {
                        "name": "Martin Mondek"
                    },
                    {
                        "name": "Filip Münz"
                    },
                    {
                        "name": "Lýdia Štofanová"
                    },
                    {
                        "name": "Petr Kurfürst"
                    },
                    {
                        "name": "Matúš Labaj"
                    },
                    {
                        "name": "Izzy L. Garland"
                    },
                    {
                        "name": "Aaron Tohuvavohu"
                    },
                    {
                        "name": "Vladimír Karas"
                    },
                    {
                        "name": "Petra Suková"
                    }
                ],
                "author_detail": {
                    "name": "Petra Suková"
                },
                "author": "Petra Suková",
                "arxiv_comment": "28 pages, 9 figures, submitted to the JATIS special issue\n  \"Ultraviolet Science & Instrumentation: On the Way to Habitable Worlds\n  Observatory and Beyond\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19364v1",
                "updated": "2025-01-31T18:14:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    14,
                    28,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:14:28Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    14,
                    28,
                    4,
                    31,
                    0
                ],
                "title": "CoSTI: Consistency Models for (a faster) Spatio-Temporal Imputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSTI: Consistency Models for (a faster) Spatio-Temporal Imputation"
                },
                "summary": "Multivariate Time Series Imputation (MTSI) is crucial for many applications,\nsuch as healthcare monitoring and traffic management, where incomplete data can\ncompromise decision-making. Existing state-of-the-art methods, like Denoising\nDiffusion Probabilistic Models (DDPMs), achieve high imputation accuracy;\nhowever, they suffer from significant computational costs and are notably\ntime-consuming due to their iterative nature. In this work, we propose CoSTI,\nan innovative adaptation of Consistency Models (CMs) for the MTSI domain. CoSTI\nemploys Consistency Training to achieve comparable imputation quality to DDPMs\nwhile drastically reducing inference times, making it more suitable for\nreal-time applications. We evaluate CoSTI across multiple datasets and missing\ndata scenarios, demonstrating up to a 98% reduction in imputation time with\nperformance on par with diffusion-based models. This work bridges the gap\nbetween efficiency and accuracy in generative imputation tasks, providing a\nscalable solution for handling missing data in critical spatio-temporal\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate Time Series Imputation (MTSI) is crucial for many applications,\nsuch as healthcare monitoring and traffic management, where incomplete data can\ncompromise decision-making. Existing state-of-the-art methods, like Denoising\nDiffusion Probabilistic Models (DDPMs), achieve high imputation accuracy;\nhowever, they suffer from significant computational costs and are notably\ntime-consuming due to their iterative nature. In this work, we propose CoSTI,\nan innovative adaptation of Consistency Models (CMs) for the MTSI domain. CoSTI\nemploys Consistency Training to achieve comparable imputation quality to DDPMs\nwhile drastically reducing inference times, making it more suitable for\nreal-time applications. We evaluate CoSTI across multiple datasets and missing\ndata scenarios, demonstrating up to a 98% reduction in imputation time with\nperformance on par with diffusion-based models. This work bridges the gap\nbetween efficiency and accuracy in generative imputation tasks, providing a\nscalable solution for handling missing data in critical spatio-temporal\nsystems."
                },
                "authors": [
                    {
                        "name": "Javier Solís-García"
                    },
                    {
                        "name": "Belén Vega-Márquez"
                    },
                    {
                        "name": "Juan A. Nepomuceno"
                    },
                    {
                        "name": "Isabel A. Nepomuceno-Chamorro"
                    }
                ],
                "author_detail": {
                    "name": "Isabel A. Nepomuceno-Chamorro"
                },
                "author": "Isabel A. Nepomuceno-Chamorro",
                "arxiv_comment": "20 pages, 5 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19361v1",
                "updated": "2025-01-31T18:12:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    12,
                    41,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:12:41Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    12,
                    41,
                    4,
                    31,
                    0
                ],
                "title": "We're Different, We're the Same: Creative Homogeneity Across LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We're Different, We're the Same: Creative Homogeneity Across LLMs"
                },
                "summary": "Numerous powerful large language models (LLMs) are now available for use as\nwriting support tools, idea generators, and beyond. Although these LLMs are\nmarketed as helpful creative assistants, several works have shown that using an\nLLM as a creative partner results in a narrower set of creative outputs.\nHowever, these studies only consider the effects of interacting with a single\nLLM, begging the question of whether such narrowed creativity stems from using\na particular LLM -- which arguably has a limited range of outputs -- or from\nusing LLMs in general as creative assistants. To study this question, we elicit\ncreative responses from humans and a broad set of LLMs using standardized\ncreativity tests and compare the population-level diversity of responses. We\nfind that LLM responses are much more similar to other LLM responses than human\nresponses are to each other, even after controlling for response structure and\nother key variables. This finding of significant homogeneity in creative\noutputs across the LLMs we evaluate adds a new dimension to the ongoing\nconversation about creativity and LLMs. If today's LLMs behave similarly, using\nthem as a creative partners -- regardless of the model used -- may drive all\nusers towards a limited set of \"creative\" outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous powerful large language models (LLMs) are now available for use as\nwriting support tools, idea generators, and beyond. Although these LLMs are\nmarketed as helpful creative assistants, several works have shown that using an\nLLM as a creative partner results in a narrower set of creative outputs.\nHowever, these studies only consider the effects of interacting with a single\nLLM, begging the question of whether such narrowed creativity stems from using\na particular LLM -- which arguably has a limited range of outputs -- or from\nusing LLMs in general as creative assistants. To study this question, we elicit\ncreative responses from humans and a broad set of LLMs using standardized\ncreativity tests and compare the population-level diversity of responses. We\nfind that LLM responses are much more similar to other LLM responses than human\nresponses are to each other, even after controlling for response structure and\nother key variables. This finding of significant homogeneity in creative\noutputs across the LLMs we evaluate adds a new dimension to the ongoing\nconversation about creativity and LLMs. If today's LLMs behave similarly, using\nthem as a creative partners -- regardless of the model used -- may drive all\nusers towards a limited set of \"creative\" outputs."
                },
                "authors": [
                    {
                        "name": "Emily Wenger"
                    },
                    {
                        "name": "Yoed Kenett"
                    }
                ],
                "author_detail": {
                    "name": "Yoed Kenett"
                },
                "author": "Yoed Kenett",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09009v2",
                "updated": "2025-01-31T18:12:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    12,
                    36,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-15T18:50:52Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    50,
                    52,
                    2,
                    15,
                    0
                ],
                "title": "Towards Fast, Specialized Machine Learning Force Fields: Distilling\n  Foundation Models via Energy Hessians",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fast, Specialized Machine Learning Force Fields: Distilling\n  Foundation Models via Energy Hessians"
                },
                "summary": "The foundation model (FM) paradigm is transforming Machine Learning Force\nFields (MLFFs), leveraging general-purpose representations and scalable\ntraining to perform a variety of computational chemistry tasks. Although MLFF\nFMs have begun to close the accuracy gap relative to first-principles methods,\nthere is still a strong need for faster inference speed. Additionally, while\nresearch is increasingly focused on general-purpose models which transfer\nacross chemical space, practitioners typically only study a small subset of\nsystems at a given time. This underscores the need for fast, specialized MLFFs\nrelevant to specific downstream applications, which preserve test-time physical\nsoundness while maintaining train-time scalability. In this work, we introduce\na method for transferring general-purpose representations from MLFF foundation\nmodels to smaller, faster MLFFs specialized to specific regions of chemical\nspace. We formulate our approach as a knowledge distillation procedure, where\nthe smaller \"student\" MLFF is trained to match the Hessians of the energy\npredictions of the \"teacher\" foundation model. Our specialized MLFFs can be up\nto 20 $\\times$ faster than the original foundation model, while retaining, and\nin some cases exceeding, its performance and that of undistilled models. We\nalso show that distilling from a teacher model with a direct force\nparameterization into a student model trained with conservative forces (i.e.,\ncomputed as derivatives of the potential energy) successfully leverages the\nrepresentations from the large-scale teacher for improved accuracy, while\nmaintaining energy conservation during test-time molecular dynamics\nsimulations. More broadly, our work suggests a new paradigm for MLFF\ndevelopment, in which foundation models are released along with smaller,\nspecialized simulation \"engines\" for common chemical subsets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The foundation model (FM) paradigm is transforming Machine Learning Force\nFields (MLFFs), leveraging general-purpose representations and scalable\ntraining to perform a variety of computational chemistry tasks. Although MLFF\nFMs have begun to close the accuracy gap relative to first-principles methods,\nthere is still a strong need for faster inference speed. Additionally, while\nresearch is increasingly focused on general-purpose models which transfer\nacross chemical space, practitioners typically only study a small subset of\nsystems at a given time. This underscores the need for fast, specialized MLFFs\nrelevant to specific downstream applications, which preserve test-time physical\nsoundness while maintaining train-time scalability. In this work, we introduce\na method for transferring general-purpose representations from MLFF foundation\nmodels to smaller, faster MLFFs specialized to specific regions of chemical\nspace. We formulate our approach as a knowledge distillation procedure, where\nthe smaller \"student\" MLFF is trained to match the Hessians of the energy\npredictions of the \"teacher\" foundation model. Our specialized MLFFs can be up\nto 20 $\\times$ faster than the original foundation model, while retaining, and\nin some cases exceeding, its performance and that of undistilled models. We\nalso show that distilling from a teacher model with a direct force\nparameterization into a student model trained with conservative forces (i.e.,\ncomputed as derivatives of the potential energy) successfully leverages the\nrepresentations from the large-scale teacher for improved accuracy, while\nmaintaining energy conservation during test-time molecular dynamics\nsimulations. More broadly, our work suggests a new paradigm for MLFF\ndevelopment, in which foundation models are released along with smaller,\nspecialized simulation \"engines\" for common chemical subsets."
                },
                "authors": [
                    {
                        "name": "Ishan Amin"
                    },
                    {
                        "name": "Sanjeev Raja"
                    },
                    {
                        "name": "Aditi Krishnapriyan"
                    }
                ],
                "author_detail": {
                    "name": "Aditi Krishnapriyan"
                },
                "author": "Aditi Krishnapriyan",
                "arxiv_comment": "Accepted as a conference paper at ICLR 2025. The implementation of\n  our method is available at https://github.com/ASK-Berkeley/MLFF-distill",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19358v1",
                "updated": "2025-01-31T18:10:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    10,
                    53,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:10:53Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    10,
                    53,
                    4,
                    31,
                    0
                ],
                "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating\n  Reward Hacking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating\n  Reward Hacking"
                },
                "summary": "This work identifies the Energy Loss Phenomenon in Reinforcement Learning\nfrom Human Feedback (RLHF) and its connection to reward hacking. Specifically,\nenergy loss in the final layer of a Large Language Model (LLM) gradually\nincreases during the RL process, with an excessive increase in energy loss\ncharacterizing reward hacking. Beyond empirical analysis, we further provide a\ntheoretical foundation by proving that, under mild conditions, the increased\nenergy loss reduces the upper bound of contextual relevance in LLMs, which is a\ncritical aspect of reward hacking as the reduced contextual relevance typically\nindicates overfitting to reward model-favored patterns in RL. To address this\nissue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the\nincrease in energy loss in the LLM's final layer during reward calculation to\nprevent excessive energy loss, thereby mitigating reward hacking. We\ntheoretically show that EPPO can be conceptually interpreted as an\nentropy-regularized RL algorithm, which provides deeper insights into its\neffectiveness. Extensive experiments across various LLMs and tasks demonstrate\nthe commonality of the energy loss phenomenon, as well as the effectiveness of\n\\texttt{EPPO} in mitigating reward hacking and improving RLHF performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work identifies the Energy Loss Phenomenon in Reinforcement Learning\nfrom Human Feedback (RLHF) and its connection to reward hacking. Specifically,\nenergy loss in the final layer of a Large Language Model (LLM) gradually\nincreases during the RL process, with an excessive increase in energy loss\ncharacterizing reward hacking. Beyond empirical analysis, we further provide a\ntheoretical foundation by proving that, under mild conditions, the increased\nenergy loss reduces the upper bound of contextual relevance in LLMs, which is a\ncritical aspect of reward hacking as the reduced contextual relevance typically\nindicates overfitting to reward model-favored patterns in RL. To address this\nissue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the\nincrease in energy loss in the LLM's final layer during reward calculation to\nprevent excessive energy loss, thereby mitigating reward hacking. We\ntheoretically show that EPPO can be conceptually interpreted as an\nentropy-regularized RL algorithm, which provides deeper insights into its\neffectiveness. Extensive experiments across various LLMs and tasks demonstrate\nthe commonality of the energy loss phenomenon, as well as the effectiveness of\n\\texttt{EPPO} in mitigating reward hacking and improving RLHF performance."
                },
                "authors": [
                    {
                        "name": "Yuchun Miao"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Yuqi Zhang"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "28 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16665v2",
                "updated": "2025-01-31T18:01:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    1,
                    12,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-22T03:38:37Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    3,
                    38,
                    37,
                    1,
                    296,
                    0
                ],
                "title": "SafetyAnalyst: Interpretable, transparent, and steerable safety\n  moderation for AI behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafetyAnalyst: Interpretable, transparent, and steerable safety\n  moderation for AI behavior"
                },
                "summary": "The ideal AI safety moderation system would be both structurally\ninterpretable (so its decisions can be reliably explained) and steerable (to\nalign to safety standards and reflect a community's values), which current\nsystems fall short on. To address this gap, we present SafetyAnalyst, a novel\nAI safety moderation framework. Given an AI behavior, SafetyAnalyst uses\nchain-of-thought reasoning to analyze its potential consequences by creating a\nstructured \"harm-benefit tree,\" which enumerates harmful and beneficial actions\nand effects the AI behavior may lead to, along with likelihood, severity, and\nimmediacy labels that describe potential impact on any stakeholders.\nSafetyAnalyst then aggregates all harmful and beneficial effects into a\nharmfulness score using fully interpretable weight parameters, which can be\naligned to particular safety preferences. We applied this conceptual framework\nto develop, test, and release an open-source LLM prompt safety classification\nsystem, distilled from 18.5 million harm-benefit features generated by frontier\nLLMs on 19k prompts. On a comprehensive set of prompt safety benchmarks, we\nshow that SafetyReporter (average F1=0.81) outperforms existing LLM safety\nmoderation systems (average F1$<$0.72) on prompt safety classification, while\noffering the additional advantages of interpretability, transparency, and\nsteerability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ideal AI safety moderation system would be both structurally\ninterpretable (so its decisions can be reliably explained) and steerable (to\nalign to safety standards and reflect a community's values), which current\nsystems fall short on. To address this gap, we present SafetyAnalyst, a novel\nAI safety moderation framework. Given an AI behavior, SafetyAnalyst uses\nchain-of-thought reasoning to analyze its potential consequences by creating a\nstructured \"harm-benefit tree,\" which enumerates harmful and beneficial actions\nand effects the AI behavior may lead to, along with likelihood, severity, and\nimmediacy labels that describe potential impact on any stakeholders.\nSafetyAnalyst then aggregates all harmful and beneficial effects into a\nharmfulness score using fully interpretable weight parameters, which can be\naligned to particular safety preferences. We applied this conceptual framework\nto develop, test, and release an open-source LLM prompt safety classification\nsystem, distilled from 18.5 million harm-benefit features generated by frontier\nLLMs on 19k prompts. On a comprehensive set of prompt safety benchmarks, we\nshow that SafetyReporter (average F1=0.81) outperforms existing LLM safety\nmoderation systems (average F1$<$0.72) on prompt safety classification, while\noffering the additional advantages of interpretability, transparency, and\nsteerability."
                },
                "authors": [
                    {
                        "name": "Jing-Jing Li"
                    },
                    {
                        "name": "Valentina Pyatkin"
                    },
                    {
                        "name": "Max Kleiman-Weiner"
                    },
                    {
                        "name": "Liwei Jiang"
                    },
                    {
                        "name": "Nouha Dziri"
                    },
                    {
                        "name": "Anne G. E. Collins"
                    },
                    {
                        "name": "Jana Schaich Borg"
                    },
                    {
                        "name": "Maarten Sap"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Sydney Levine"
                    }
                ],
                "author_detail": {
                    "name": "Sydney Levine"
                },
                "author": "Sydney Levine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19348v1",
                "updated": "2025-01-31T17:52:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    52,
                    3,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T17:52:03Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    52,
                    3,
                    4,
                    31,
                    0
                ],
                "title": "Characterizing User Behavior: The Interplay Between Mobility Patterns\n  and Mobile Traffic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing User Behavior: The Interplay Between Mobility Patterns\n  and Mobile Traffic"
                },
                "summary": "Mobile devices have become essential for capturing human activity, and\neXtended Data Records (XDRs) offer rich opportunities for detailed user\nbehavior modeling, which is useful for designing personalized digital services.\nPrevious studies have primarily focused on aggregated mobile traffic and\nmobility analyses, often neglecting individual-level insights. This paper\nintroduces a novel approach that explores the dependency between traffic and\nmobility behaviors at the user level. By analyzing 13 individual features that\nencompass traffic patterns and various mobility aspects, we enhance the\nunderstanding of how these behaviors interact. Our advanced user modeling\nframework integrates traffic and mobility behaviors over time, allowing for\nfine-grained dependencies while maintaining population heterogeneity through\nuser-specific signatures. Furthermore, we develop a Markov model that infers\ntraffic behavior from mobility and vice versa, prioritizing significant\ndependencies while addressing privacy concerns. Using a week-long XDR dataset\nfrom 1,337,719 users across several provinces in Chile, we validate our\napproach, demonstrating its robustness and applicability in accurately\ninferring user behavior and matching mobility and traffic profiles across\ndiverse urban contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile devices have become essential for capturing human activity, and\neXtended Data Records (XDRs) offer rich opportunities for detailed user\nbehavior modeling, which is useful for designing personalized digital services.\nPrevious studies have primarily focused on aggregated mobile traffic and\nmobility analyses, often neglecting individual-level insights. This paper\nintroduces a novel approach that explores the dependency between traffic and\nmobility behaviors at the user level. By analyzing 13 individual features that\nencompass traffic patterns and various mobility aspects, we enhance the\nunderstanding of how these behaviors interact. Our advanced user modeling\nframework integrates traffic and mobility behaviors over time, allowing for\nfine-grained dependencies while maintaining population heterogeneity through\nuser-specific signatures. Furthermore, we develop a Markov model that infers\ntraffic behavior from mobility and vice versa, prioritizing significant\ndependencies while addressing privacy concerns. Using a week-long XDR dataset\nfrom 1,337,719 users across several provinces in Chile, we validate our\napproach, demonstrating its robustness and applicability in accurately\ninferring user behavior and matching mobility and traffic profiles across\ndiverse urban contexts."
                },
                "authors": [
                    {
                        "name": "Anne Josiane Kouam"
                    },
                    {
                        "name": "Aline Carneiro Viana"
                    },
                    {
                        "name": "Mariano G. Beiró"
                    },
                    {
                        "name": "Leo Ferres"
                    },
                    {
                        "name": "Luca Pappalardo"
                    }
                ],
                "author_detail": {
                    "name": "Luca Pappalardo"
                },
                "author": "Luca Pappalardo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19345v1",
                "updated": "2025-01-31T17:47:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    47,
                    32,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T17:47:32Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    47,
                    32,
                    4,
                    31,
                    0
                ],
                "title": "PUATE: Semiparametric Efficient Average Treatment Effect Estimation from\n  Treated (Positive) and Unlabeled Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUATE: Semiparametric Efficient Average Treatment Effect Estimation from\n  Treated (Positive) and Unlabeled Units"
                },
                "summary": "The estimation of average treatment effects (ATEs), defined as the difference\nin expected outcomes between treatment and control groups, is a central topic\nin causal inference. This study develops semiparametric efficient estimators\nfor ATE estimation in a setting where only a treatment group and an unknown\ngroup-comprising units for which it is unclear whether they received the\ntreatment or control-are observable. This scenario represents a variant of\nlearning from positive and unlabeled data (PU learning) and can be regarded as\na special case of ATE estimation with missing data. For this setting, we derive\nsemiparametric efficiency bounds, which provide lower bounds on the asymptotic\nvariance of regular estimators. We then propose semiparametric efficient ATE\nestimators whose asymptotic variance aligns with these efficiency bounds. Our\nfindings contribute to causal inference with missing data and weakly supervised\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The estimation of average treatment effects (ATEs), defined as the difference\nin expected outcomes between treatment and control groups, is a central topic\nin causal inference. This study develops semiparametric efficient estimators\nfor ATE estimation in a setting where only a treatment group and an unknown\ngroup-comprising units for which it is unclear whether they received the\ntreatment or control-are observable. This scenario represents a variant of\nlearning from positive and unlabeled data (PU learning) and can be regarded as\na special case of ATE estimation with missing data. For this setting, we derive\nsemiparametric efficiency bounds, which provide lower bounds on the asymptotic\nvariance of regular estimators. We then propose semiparametric efficient ATE\nestimators whose asymptotic variance aligns with these efficiency bounds. Our\nfindings contribute to causal inference with missing data and weakly supervised\nlearning."
                },
                "authors": [
                    {
                        "name": "Masahiro Kato"
                    },
                    {
                        "name": "Fumiaki Kozai"
                    },
                    {
                        "name": "Ryo Inokuchi"
                    }
                ],
                "author_detail": {
                    "name": "Ryo Inokuchi"
                },
                "author": "Ryo Inokuchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19340v1",
                "updated": "2025-01-31T17:40:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    40,
                    8,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T17:40:08Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    40,
                    8,
                    4,
                    31,
                    0
                ],
                "title": "Towards Adaptive Self-Improvement for Smarter Energy Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Adaptive Self-Improvement for Smarter Energy Systems"
                },
                "summary": "This paper introduces a hierarchical framework for decision-making and\noptimization, leveraging Large Language Models (LLMs) for adaptive code\ngeneration. Instead of direct decision-making, LLMs generate and refine\nexecutable control policies through a meta-policy that guides task generation\nand a base policy for operational actions. Applied to a simplified microgrid\nscenario, the approach achieves up to 15 percent cost savings by iteratively\nimproving battery control strategies. The proposed methodology lays a\nfoundation for integrating LLM-based tools into planning and control tasks,\noffering adaptable and scalable solutions for complex systems while addressing\nchallenges of uncertainty and reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a hierarchical framework for decision-making and\noptimization, leveraging Large Language Models (LLMs) for adaptive code\ngeneration. Instead of direct decision-making, LLMs generate and refine\nexecutable control policies through a meta-policy that guides task generation\nand a base policy for operational actions. Applied to a simplified microgrid\nscenario, the approach achieves up to 15 percent cost savings by iteratively\nimproving battery control strategies. The proposed methodology lays a\nfoundation for integrating LLM-based tools into planning and control tasks,\noffering adaptable and scalable solutions for complex systems while addressing\nchallenges of uncertainty and reproducibility."
                },
                "authors": [
                    {
                        "name": "Alexander Sommer"
                    },
                    {
                        "name": "Peter Bazan"
                    },
                    {
                        "name": "Jonathan Fellerer"
                    },
                    {
                        "name": "Behnam Babaeian"
                    },
                    {
                        "name": "Reinhard German"
                    }
                ],
                "author_detail": {
                    "name": "Reinhard German"
                },
                "author": "Reinhard German",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14713v2",
                "updated": "2025-01-31T17:38:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    38,
                    7,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-24T18:46:37Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    46,
                    37,
                    4,
                    24,
                    0
                ],
                "title": "FlexiGPT: Pruning and Extending Large Language Models with Low-Rank\n  Weight Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiGPT: Pruning and Extending Large Language Models with Low-Rank\n  Weight Sharing"
                },
                "summary": "The rapid proliferation of large language models (LLMs) in natural language\nprocessing (NLP) has created a critical need for techniques that enable\nefficient deployment on memory-constrained devices without compromising\nperformance. We present a method to prune LLMs that selectively prunes model\nblocks based on an importance score and replaces them with a low-parameter\nreplacement strategy. Specifically, we propose a principled metric to replace\neach pruned block using a weight-sharing mechanism that leverages unpruned\ncounterparts from the model and block-specific low-rank adapters. Furthermore,\nwe facilitate the learning of these replacement blocks with output feature\nnormalization and an adapter initialization scheme built on low-rank SVD\nreconstructions. Empirical evaluations demonstrate substantial performance\ngains over existing methods, achieving state-of-the-art performance on 5/6\nbenchmarks for a compression rate of 30% and 6/6 benchmarks for a compression\nrate of 40%. We also demonstrate that our approach can extend smaller models,\nboosting performance on 6/6 benchmarks using only ~0.3% tokens of extended\ntraining with minimal additional parameter costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of large language models (LLMs) in natural language\nprocessing (NLP) has created a critical need for techniques that enable\nefficient deployment on memory-constrained devices without compromising\nperformance. We present a method to prune LLMs that selectively prunes model\nblocks based on an importance score and replaces them with a low-parameter\nreplacement strategy. Specifically, we propose a principled metric to replace\neach pruned block using a weight-sharing mechanism that leverages unpruned\ncounterparts from the model and block-specific low-rank adapters. Furthermore,\nwe facilitate the learning of these replacement blocks with output feature\nnormalization and an adapter initialization scheme built on low-rank SVD\nreconstructions. Empirical evaluations demonstrate substantial performance\ngains over existing methods, achieving state-of-the-art performance on 5/6\nbenchmarks for a compression rate of 30% and 6/6 benchmarks for a compression\nrate of 40%. We also demonstrate that our approach can extend smaller models,\nboosting performance on 6/6 benchmarks using only ~0.3% tokens of extended\ntraining with minimal additional parameter costs."
                },
                "authors": [
                    {
                        "name": "James Seale Smith"
                    },
                    {
                        "name": "Chi-Heng Lin"
                    },
                    {
                        "name": "Shikhar Tuli"
                    },
                    {
                        "name": "Haris Jeelani"
                    },
                    {
                        "name": "Shangqian Gao"
                    },
                    {
                        "name": "Yilin Shen"
                    },
                    {
                        "name": "Hongxia Jin"
                    },
                    {
                        "name": "Yen-Chang Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Yen-Chang Hsu"
                },
                "author": "Yen-Chang Hsu",
                "arxiv_comment": "Accepted to NAACL 2025 - Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19337v1",
                "updated": "2025-01-31T17:36:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    36,
                    12,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T17:36:12Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    36,
                    12,
                    4,
                    31,
                    0
                ],
                "title": "Homogeneity Bias as Differential Sampling Uncertainty in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Homogeneity Bias as Differential Sampling Uncertainty in Language Models"
                },
                "summary": "Prior research show that Large Language Models (LLMs) and Vision-Language\nModels (VLMs) represent marginalized groups more homogeneously than dominant\ngroups. However, the mechanisms underlying this homogeneity bias remain\nrelatively unexplored. We propose that this bias emerges from systematic\ndifferences in the probability distributions from which tokens are sampled at\ninference-time. Analyzing three measures of uncertainty in token sampling\ndistributions-entropy, perplexity, and probability of differentiation-we find\nthat in some models, specifically GPT-4 Turbo and Llama-3.2, tokens are sampled\nmore deterministically when generating texts about marginalized groups (i.e.,\nBlack Americans and women) compared to their dominant group counterparts (i.e.,\nWhite Americans and men). While these findings may help explain homogeneity\nbias in certain models, the patterns did not replicate across all VLMs tested,\nsuggesting multiple mechanisms may contribute to homogeneity bias in AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior research show that Large Language Models (LLMs) and Vision-Language\nModels (VLMs) represent marginalized groups more homogeneously than dominant\ngroups. However, the mechanisms underlying this homogeneity bias remain\nrelatively unexplored. We propose that this bias emerges from systematic\ndifferences in the probability distributions from which tokens are sampled at\ninference-time. Analyzing three measures of uncertainty in token sampling\ndistributions-entropy, perplexity, and probability of differentiation-we find\nthat in some models, specifically GPT-4 Turbo and Llama-3.2, tokens are sampled\nmore deterministically when generating texts about marginalized groups (i.e.,\nBlack Americans and women) compared to their dominant group counterparts (i.e.,\nWhite Americans and men). While these findings may help explain homogeneity\nbias in certain models, the patterns did not replicate across all VLMs tested,\nsuggesting multiple mechanisms may contribute to homogeneity bias in AI."
                },
                "authors": [
                    {
                        "name": "Messi H. J. Lee"
                    },
                    {
                        "name": "Soyeon Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Soyeon Jeon"
                },
                "author": "Soyeon Jeon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17282v3",
                "updated": "2025-01-31T17:26:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    26,
                    12,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-28T20:30:36Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    30,
                    36,
                    1,
                    28,
                    0
                ],
                "title": "From Natural Language to Extensive-Form Game Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Natural Language to Extensive-Form Game Representations"
                },
                "summary": "We introduce a framework for translating game descriptions in natural\nlanguage into extensive-form representations in game theory, leveraging Large\nLanguage Models (LLMs) and in-context learning. Given the varying levels of\nstrategic complexity in games, such as perfect versus imperfect information,\ndirectly applying in-context learning would be insufficient. To address this,\nwe introduce a two-stage framework with specialized modules to enhance\nin-context learning, enabling it to divide and conquer the problem effectively.\nIn the first stage, we tackle the challenge of imperfect information by\ndeveloping a module that identifies information sets along and the\ncorresponding partial tree structure. With this information, the second stage\nleverages in-context learning alongside a self-debugging module to produce a\ncomplete extensive-form game tree represented using pygambit, the Python API of\na recognized game-theoretic analysis tool called Gambit. Using this python\nrepresentation enables the automation of tasks such as computing Nash\nequilibria directly from natural language descriptions. We evaluate the\nperformance of the full framework, as well as its individual components, using\nvarious LLMs on games with different levels of strategic complexity. Our\nexperimental results show that the framework significantly outperforms baseline\nmodels in generating accurate extensive-form games, with each module playing a\ncritical role in its success.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a framework for translating game descriptions in natural\nlanguage into extensive-form representations in game theory, leveraging Large\nLanguage Models (LLMs) and in-context learning. Given the varying levels of\nstrategic complexity in games, such as perfect versus imperfect information,\ndirectly applying in-context learning would be insufficient. To address this,\nwe introduce a two-stage framework with specialized modules to enhance\nin-context learning, enabling it to divide and conquer the problem effectively.\nIn the first stage, we tackle the challenge of imperfect information by\ndeveloping a module that identifies information sets along and the\ncorresponding partial tree structure. With this information, the second stage\nleverages in-context learning alongside a self-debugging module to produce a\ncomplete extensive-form game tree represented using pygambit, the Python API of\na recognized game-theoretic analysis tool called Gambit. Using this python\nrepresentation enables the automation of tasks such as computing Nash\nequilibria directly from natural language descriptions. We evaluate the\nperformance of the full framework, as well as its individual components, using\nvarious LLMs on games with different levels of strategic complexity. Our\nexperimental results show that the framework significantly outperforms baseline\nmodels in generating accurate extensive-form games, with each module playing a\ncritical role in its success."
                },
                "authors": [
                    {
                        "name": "Shilong Deng"
                    },
                    {
                        "name": "Yongzhao Wang"
                    },
                    {
                        "name": "Rahul Savani"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Savani"
                },
                "author": "Rahul Savani",
                "arxiv_comment": "This work has been accepted as a full paper for AAMAS 2025. This is a\n  full version of the AAMAS 2025 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19324v1",
                "updated": "2025-01-31T17:19:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    19,
                    57,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T17:19:57Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    19,
                    57,
                    4,
                    31,
                    0
                ],
                "title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning"
                },
                "summary": "We introduce Reward-Guided Speculative Decoding (RSD), a novel framework\naimed at improving the efficiency of inference in large language models (LLMs).\nRSD synergistically combines a lightweight draft model with a more powerful\ntarget model, incorporating a controlled bias to prioritize high-reward\noutputs, in contrast to existing speculative decoding methods that enforce\nstrict unbiasedness. RSD employs a process reward model to evaluate\nintermediate decoding steps and dynamically decide whether to invoke the target\nmodel, optimizing the trade-off between computational cost and output quality.\nWe theoretically demonstrate that a threshold-based mixture strategy achieves\nan optimal balance between resource utilization and performance. Extensive\nevaluations on challenging reasoning benchmarks, including Olympiad-level\ntasks, show that RSD delivers significant efficiency gains against decoding\nwith the target model only (up to 4.4x fewer FLOPs), while achieving\nsignificant better accuracy than parallel decoding method on average (up to\n+3.5). These results highlight RSD as a robust and cost-effective approach for\ndeploying LLMs in resource-intensive scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Reward-Guided Speculative Decoding (RSD), a novel framework\naimed at improving the efficiency of inference in large language models (LLMs).\nRSD synergistically combines a lightweight draft model with a more powerful\ntarget model, incorporating a controlled bias to prioritize high-reward\noutputs, in contrast to existing speculative decoding methods that enforce\nstrict unbiasedness. RSD employs a process reward model to evaluate\nintermediate decoding steps and dynamically decide whether to invoke the target\nmodel, optimizing the trade-off between computational cost and output quality.\nWe theoretically demonstrate that a threshold-based mixture strategy achieves\nan optimal balance between resource utilization and performance. Extensive\nevaluations on challenging reasoning benchmarks, including Olympiad-level\ntasks, show that RSD delivers significant efficiency gains against decoding\nwith the target model only (up to 4.4x fewer FLOPs), while achieving\nsignificant better accuracy than parallel decoding method on average (up to\n+3.5). These results highlight RSD as a robust and cost-effective approach for\ndeploying LLMs in resource-intensive scenarios."
                },
                "authors": [
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Junnan Li"
                    },
                    {
                        "name": "Christof Monz"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Doyen Sahoo"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19318v1",
                "updated": "2025-01-31T17:15:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    15,
                    33,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T17:15:33Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    15,
                    33,
                    4,
                    31,
                    0
                ],
                "title": "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented\n  Reinforcement in Embodied Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented\n  Reinforcement in Embodied Systems"
                },
                "summary": "While large language models (LLMs) have shown promising capabilities as\nzero-shot planners for embodied agents, their inability to learn from\nexperience and build persistent mental models limits their robustness in\ncomplex open-world environments like Minecraft. We introduce MINDSTORES, an\nexperience-augmented planning framework that enables embodied agents to build\nand leverage mental models through natural interaction with their environment.\nDrawing inspiration from how humans construct and refine cognitive mental\nmodels, our approach extends existing zero-shot LLM planning by maintaining a\ndatabase of past experiences that informs future planning iterations. The key\ninnovation is representing accumulated experiences as natural language\nembeddings of (state, task, plan, outcome) tuples, which can then be\nefficiently retrieved and reasoned over by an LLM planner to generate insights\nand guide plan refinement for novel states and tasks. Through extensive\nexperiments in the MineDojo environment, a simulation environment for agents in\nMinecraft that provides low-level controls for Minecraft, we find that\nMINDSTORES learns and applies its knowledge significantly better than existing\nmemory-based LLM planners while maintaining the flexibility and generalization\nbenefits of zero-shot approaches, representing an important step toward more\ncapable embodied AI systems that can learn continuously through natural\nexperience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown promising capabilities as\nzero-shot planners for embodied agents, their inability to learn from\nexperience and build persistent mental models limits their robustness in\ncomplex open-world environments like Minecraft. We introduce MINDSTORES, an\nexperience-augmented planning framework that enables embodied agents to build\nand leverage mental models through natural interaction with their environment.\nDrawing inspiration from how humans construct and refine cognitive mental\nmodels, our approach extends existing zero-shot LLM planning by maintaining a\ndatabase of past experiences that informs future planning iterations. The key\ninnovation is representing accumulated experiences as natural language\nembeddings of (state, task, plan, outcome) tuples, which can then be\nefficiently retrieved and reasoned over by an LLM planner to generate insights\nand guide plan refinement for novel states and tasks. Through extensive\nexperiments in the MineDojo environment, a simulation environment for agents in\nMinecraft that provides low-level controls for Minecraft, we find that\nMINDSTORES learns and applies its knowledge significantly better than existing\nmemory-based LLM planners while maintaining the flexibility and generalization\nbenefits of zero-shot approaches, representing an important step toward more\ncapable embodied AI systems that can learn continuously through natural\nexperience."
                },
                "authors": [
                    {
                        "name": "Anirudh Chari"
                    },
                    {
                        "name": "Suraj Reddy"
                    },
                    {
                        "name": "Aditya Tiwari"
                    },
                    {
                        "name": "Richard Lian"
                    },
                    {
                        "name": "Brian Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Brian Zhou"
                },
                "author": "Brian Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19317v1",
                "updated": "2025-01-31T17:12:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    12,
                    55,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T17:12:55Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    12,
                    55,
                    4,
                    31,
                    0
                ],
                "title": "LLM-based Affective Text Generation Quality Based on Different\n  Quantization Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Affective Text Generation Quality Based on Different\n  Quantization Values"
                },
                "summary": "Large language models exhibit a remarkable capacity in language generation\nand comprehension. These advances enable AI systems to produce more human-like\nand emotionally engaging text. However, these models rely on a large number of\nparameters, requiring significant computational resources for training and\ninference. In some scenarios, accessing these resources can be challenging\n(e.g., budget or hardware limitations). Techniques like reducing precision bits\ncan make models more memory-efficient, reducing the computational resources\nneeded, at the cost of reduced accuracy. This paper addresses the trade-off\nbetween different quantization values, GPU RAM utilization, and text quality in\naffective text generation (e.g., \"I really enjoy running in the snow-covered\nforest\"). To evaluate, we use an emotion classifier and ten seed prompts to\ngenerate affective text. We test three setups of precision bits (8, 16, and 32)\nacross five open-weight language models from two different families. Our\nfindings demonstrate that bit reductions lead to memory savings, achieving a\nreduction of 76%. However, this optimization comes with a trade-off, leading to\na decrease of up to 10 pp in F1 score for larger models and an increase of 10\npp for smaller models, along with roughly double the inference time. In terms\nof text quality, larger models at lower quantization levels generally\noutperform smaller, higher-precision models -- while requiring similar memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models exhibit a remarkable capacity in language generation\nand comprehension. These advances enable AI systems to produce more human-like\nand emotionally engaging text. However, these models rely on a large number of\nparameters, requiring significant computational resources for training and\ninference. In some scenarios, accessing these resources can be challenging\n(e.g., budget or hardware limitations). Techniques like reducing precision bits\ncan make models more memory-efficient, reducing the computational resources\nneeded, at the cost of reduced accuracy. This paper addresses the trade-off\nbetween different quantization values, GPU RAM utilization, and text quality in\naffective text generation (e.g., \"I really enjoy running in the snow-covered\nforest\"). To evaluate, we use an emotion classifier and ten seed prompts to\ngenerate affective text. We test three setups of precision bits (8, 16, and 32)\nacross five open-weight language models from two different families. Our\nfindings demonstrate that bit reductions lead to memory savings, achieving a\nreduction of 76%. However, this optimization comes with a trade-off, leading to\na decrease of up to 10 pp in F1 score for larger models and an increase of 10\npp for smaller models, along with roughly double the inference time. In terms\nof text quality, larger models at lower quantization levels generally\noutperform smaller, higher-precision models -- while requiring similar memory."
                },
                "authors": [
                    {
                        "name": "Yarik Menchaca Resendiz"
                    },
                    {
                        "name": "Roman Klinger"
                    }
                ],
                "author_detail": {
                    "name": "Roman Klinger"
                },
                "author": "Roman Klinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19309v1",
                "updated": "2025-01-31T17:09:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    9,
                    53,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T17:09:53Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    9,
                    53,
                    4,
                    31,
                    0
                ],
                "title": "Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model\n  Alignment"
                },
                "summary": "The performance of large language models (LLMs) is closely linked to their\nunderlying size, leading to ever-growing networks and hence slower inference.\nSpeculative decoding has been proposed as a technique to accelerate\nautoregressive generation, leveraging a fast draft model to propose candidate\ntokens, which are then verified in parallel based on their likelihood under the\ntarget model. While this approach guarantees to reproduce the target output, it\nincurs a substantial penalty: many high-quality draft tokens are rejected, even\nwhen they represent objectively valid continuations. Indeed, we show that even\npowerful draft models such as GPT-4o, as well as human text cannot achieve high\nacceptance rates under the standard verification scheme. This severely limits\nthe speedup potential of current speculative decoding methods, as an early\nrejection becomes overwhelmingly likely when solely relying on alignment of\ndraft and target.\n  We thus ask the following question: Can we adapt verification to recognize\ncorrect, but non-aligned replies? To this end, we draw inspiration from the\nLLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers\nin a versatile way. We carefully design a dataset to elicit the same capability\nin the target model by training a compact module on top of the embeddings to\nproduce ``judgements\" of the current continuation. We showcase our strategy on\nthe Llama-3.1 family, where our 8b/405B-Judge achieves a speedup of 9x over\nLlama-405B, while maintaining its quality on a large range of benchmarks. These\nbenefits remain present even in optimized inference frameworks, where our\nmethod reaches up to 141 tokens/s for 8B/70B-Judge and 129 tokens/s for 8B/405B\non 2 and 8 H100s respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of large language models (LLMs) is closely linked to their\nunderlying size, leading to ever-growing networks and hence slower inference.\nSpeculative decoding has been proposed as a technique to accelerate\nautoregressive generation, leveraging a fast draft model to propose candidate\ntokens, which are then verified in parallel based on their likelihood under the\ntarget model. While this approach guarantees to reproduce the target output, it\nincurs a substantial penalty: many high-quality draft tokens are rejected, even\nwhen they represent objectively valid continuations. Indeed, we show that even\npowerful draft models such as GPT-4o, as well as human text cannot achieve high\nacceptance rates under the standard verification scheme. This severely limits\nthe speedup potential of current speculative decoding methods, as an early\nrejection becomes overwhelmingly likely when solely relying on alignment of\ndraft and target.\n  We thus ask the following question: Can we adapt verification to recognize\ncorrect, but non-aligned replies? To this end, we draw inspiration from the\nLLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers\nin a versatile way. We carefully design a dataset to elicit the same capability\nin the target model by training a compact module on top of the embeddings to\nproduce ``judgements\" of the current continuation. We showcase our strategy on\nthe Llama-3.1 family, where our 8b/405B-Judge achieves a speedup of 9x over\nLlama-405B, while maintaining its quality on a large range of benchmarks. These\nbenefits remain present even in optimized inference frameworks, where our\nmethod reaches up to 141 tokens/s for 8B/70B-Judge and 129 tokens/s for 8B/405B\non 2 and 8 H100s respectively."
                },
                "authors": [
                    {
                        "name": "Gregor Bachmann"
                    },
                    {
                        "name": "Sotiris Anagnostidis"
                    },
                    {
                        "name": "Albert Pumarola"
                    },
                    {
                        "name": "Markos Georgopoulos"
                    },
                    {
                        "name": "Artsiom Sanakoyeu"
                    },
                    {
                        "name": "Yuming Du"
                    },
                    {
                        "name": "Edgar Schönfeld"
                    },
                    {
                        "name": "Ali Thabet"
                    },
                    {
                        "name": "Jonas Kohler"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Kohler"
                },
                "author": "Jonas Kohler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19306v1",
                "updated": "2025-01-31T17:03:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    3,
                    16,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T17:03:16Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    3,
                    16,
                    4,
                    31,
                    0
                ],
                "title": "SETS: Leveraging Self-Verification and Self-Correction for Improved\n  Test-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SETS: Leveraging Self-Verification and Self-Correction for Improved\n  Test-Time Scaling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have created new\nopportunities to enhance performance on complex reasoning tasks by leveraging\ntest-time computation. However, conventional approaches such as repeated\nsampling with majority voting or reward model scoring, often face diminishing\nreturns as test-time compute scales, in addition to requiring costly\ntask-specific reward model training. In this paper, we present Self-Enhanced\nTest-Time Scaling (SETS), a novel method that leverages the self-verification\nand self-correction capabilities of recent advanced LLMs to overcome these\nlimitations. SETS integrates sampling, self-verification, and self-correction\ninto a unified framework, enabling efficient and scalable test-time computation\nfor improved capabilities at complex tasks. Through extensive experiments on\nchallenging planning and reasoning benchmarks, compared to the alternatives, we\ndemonstrate that SETS achieves significant performance improvements and more\nfavorable test-time scaling laws.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have created new\nopportunities to enhance performance on complex reasoning tasks by leveraging\ntest-time computation. However, conventional approaches such as repeated\nsampling with majority voting or reward model scoring, often face diminishing\nreturns as test-time compute scales, in addition to requiring costly\ntask-specific reward model training. In this paper, we present Self-Enhanced\nTest-Time Scaling (SETS), a novel method that leverages the self-verification\nand self-correction capabilities of recent advanced LLMs to overcome these\nlimitations. SETS integrates sampling, self-verification, and self-correction\ninto a unified framework, enabling efficient and scalable test-time computation\nfor improved capabilities at complex tasks. Through extensive experiments on\nchallenging planning and reasoning benchmarks, compared to the alternatives, we\ndemonstrate that SETS achieves significant performance improvements and more\nfavorable test-time scaling laws."
                },
                "authors": [
                    {
                        "name": "Jiefeng Chen"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Xinyun Chen"
                    },
                    {
                        "name": "Chengrun Yang"
                    },
                    {
                        "name": "Ruoxi Sun"
                    },
                    {
                        "name": "Sercan Ö Arık"
                    }
                ],
                "author_detail": {
                    "name": "Sercan Ö Arık"
                },
                "author": "Sercan Ö Arık",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19301v1",
                "updated": "2025-01-31T16:57:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    57,
                    1,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    57,
                    1,
                    4,
                    31,
                    0
                ],
                "title": "Beyond checkmate: exploring the creative chokepoints in AI text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond checkmate: exploring the creative chokepoints in AI text"
                },
                "summary": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) and Artificial Intelligence (AI), unlocking unprecedented capabilities.\nThis rapid advancement has spurred research into various aspects of LLMs, their\ntext generation & reasoning capability, and potential misuse, fueling the\nnecessity for robust detection methods. While numerous prior research has\nfocused on detecting LLM-generated text (AI text) and thus checkmating them,\nour study investigates a relatively unexplored territory: portraying the\nnuanced distinctions between human and AI texts across text segments. Whether\nLLMs struggle with or excel at incorporating linguistic ingenuity across\ndifferent text segments carries substantial implications for determining their\npotential as effective creative assistants to humans. Through an analogy with\nthe structure of chess games-comprising opening, middle, and end games-we\nanalyze text segments (introduction, body, and conclusion) to determine where\nthe most significant distinctions between human and AI texts exist. While AI\ntexts can approximate the body segment better due to its increased length, a\ncloser examination reveals a pronounced disparity, highlighting the importance\nof this segment in AI text detection. Additionally, human texts exhibit higher\ncross-segment differences compared to AI texts. Overall, our research can shed\nlight on the intricacies of human-AI text distinctions, offering novel insights\nfor text detection and understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) and Artificial Intelligence (AI), unlocking unprecedented capabilities.\nThis rapid advancement has spurred research into various aspects of LLMs, their\ntext generation & reasoning capability, and potential misuse, fueling the\nnecessity for robust detection methods. While numerous prior research has\nfocused on detecting LLM-generated text (AI text) and thus checkmating them,\nour study investigates a relatively unexplored territory: portraying the\nnuanced distinctions between human and AI texts across text segments. Whether\nLLMs struggle with or excel at incorporating linguistic ingenuity across\ndifferent text segments carries substantial implications for determining their\npotential as effective creative assistants to humans. Through an analogy with\nthe structure of chess games-comprising opening, middle, and end games-we\nanalyze text segments (introduction, body, and conclusion) to determine where\nthe most significant distinctions between human and AI texts exist. While AI\ntexts can approximate the body segment better due to its increased length, a\ncloser examination reveals a pronounced disparity, highlighting the importance\nof this segment in AI text detection. Additionally, human texts exhibit higher\ncross-segment differences compared to AI texts. Overall, our research can shed\nlight on the intricacies of human-AI text distinctions, offering novel insights\nfor text detection and understanding."
                },
                "authors": [
                    {
                        "name": "Nafis Irtiza Tripto"
                    },
                    {
                        "name": "Saranya Venkatraman"
                    },
                    {
                        "name": "Mahjabin Nahar"
                    },
                    {
                        "name": "Dongwon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongwon Lee"
                },
                "author": "Dongwon Lee",
                "arxiv_comment": "18 pages, single columns, under review at Nature Machine Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v1",
                "updated": "2025-01-31T16:56:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee-Joe Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19298v1",
                "updated": "2025-01-31T16:55:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    55,
                    43,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:55:43Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    55,
                    43,
                    4,
                    31,
                    0
                ],
                "title": "Synthetic User Behavior Sequence Generation with Large Language Models\n  for Smart Homes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic User Behavior Sequence Generation with Large Language Models\n  for Smart Homes"
                },
                "summary": "In recent years, as smart home systems have become more widespread, security\nconcerns within these environments have become a growing threat. Currently,\nmost smart home security solutions, such as anomaly detection and behavior\nprediction models, are trained using fixed datasets that are precollected.\nHowever, the process of dataset collection is time-consuming and lacks the\nflexibility needed to adapt to the constantly evolving smart home environment.\nAdditionally, the collection of personal data raises significant privacy\nconcerns for users. Lately, large language models (LLMs) have emerged as a\npowerful tool for a wide range of tasks across diverse application domains,\nthanks to their strong capabilities in natural language processing, reasoning,\nand problem-solving. In this paper, we propose an LLM-based synthetic dataset\ngeneration IoTGen framework to enhance the generalization of downstream smart\nhome intelligent models. By generating new synthetic datasets that reflect\nchanges in the environment, smart home intelligent models can be retrained to\novercome the limitations of fixed and outdated data, allowing them to better\nalign with the dynamic nature of real-world home environments. Specifically, we\nfirst propose a Structure Pattern Perception Compression (SPPC) method tailored\nfor IoT behavior data, which preserves the most informative content in the data\nwhile significantly reducing token consumption. Then, we propose a systematic\napproach to create prompts and implement data generation to automatically\ngenerate IoT synthetic data with normative and reasonable properties, assisting\ntask models in adaptive training to improve generalization and real-world\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, as smart home systems have become more widespread, security\nconcerns within these environments have become a growing threat. Currently,\nmost smart home security solutions, such as anomaly detection and behavior\nprediction models, are trained using fixed datasets that are precollected.\nHowever, the process of dataset collection is time-consuming and lacks the\nflexibility needed to adapt to the constantly evolving smart home environment.\nAdditionally, the collection of personal data raises significant privacy\nconcerns for users. Lately, large language models (LLMs) have emerged as a\npowerful tool for a wide range of tasks across diverse application domains,\nthanks to their strong capabilities in natural language processing, reasoning,\nand problem-solving. In this paper, we propose an LLM-based synthetic dataset\ngeneration IoTGen framework to enhance the generalization of downstream smart\nhome intelligent models. By generating new synthetic datasets that reflect\nchanges in the environment, smart home intelligent models can be retrained to\novercome the limitations of fixed and outdated data, allowing them to better\nalign with the dynamic nature of real-world home environments. Specifically, we\nfirst propose a Structure Pattern Perception Compression (SPPC) method tailored\nfor IoT behavior data, which preserves the most informative content in the data\nwhile significantly reducing token consumption. Then, we propose a systematic\napproach to create prompts and implement data generation to automatically\ngenerate IoT synthetic data with normative and reasonable properties, assisting\ntask models in adaptive training to improve generalization and real-world\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhiyao Xu"
                    },
                    {
                        "name": "Dan Zhao"
                    },
                    {
                        "name": "Qingsong Zou"
                    },
                    {
                        "name": "Jingyu Xiao"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Zhenhui Yuan"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19297v1",
                "updated": "2025-01-31T16:55:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    55,
                    17,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:55:17Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    55,
                    17,
                    4,
                    31,
                    0
                ],
                "title": "Analysis of LLMs vs Human Experts in Requirements Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of LLMs vs Human Experts in Requirements Engineering"
                },
                "summary": "The majority of research around Large Language Models (LLM) application to\nsoftware development has been on the subject of code generation. There is\nlittle literature on LLMs' impact on requirements engineering (RE), which deals\nwith the process of developing and verifying the system requirements. Within\nRE, there is a subdiscipline of requirements elicitation, which is the practice\nof discovering and documenting requirements for a system from users, customers,\nand other stakeholders. In this analysis, we compare LLM's ability to elicit\nrequirements of a software system, as compared to that of a human expert in a\ntime-boxed and prompt-boxed study. We found LLM-generated requirements were\nevaluated as more aligned (+1.12) than human-generated requirements with a\ntrend of being more complete (+10.2%). Conversely, we found users tended to\nbelieve that solutions they perceived as more aligned had been generated by\nhuman experts. Furthermore, while LLM-generated documents scored higher and\nperformed at 720x the speed, their cost was, on average, only 0.06% that of a\nhuman expert. Overall, these findings indicate that LLMs will play an\nincreasingly important role in requirements engineering by improving\nrequirements definitions, enabling more efficient resource allocation, and\nreducing overall project timelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The majority of research around Large Language Models (LLM) application to\nsoftware development has been on the subject of code generation. There is\nlittle literature on LLMs' impact on requirements engineering (RE), which deals\nwith the process of developing and verifying the system requirements. Within\nRE, there is a subdiscipline of requirements elicitation, which is the practice\nof discovering and documenting requirements for a system from users, customers,\nand other stakeholders. In this analysis, we compare LLM's ability to elicit\nrequirements of a software system, as compared to that of a human expert in a\ntime-boxed and prompt-boxed study. We found LLM-generated requirements were\nevaluated as more aligned (+1.12) than human-generated requirements with a\ntrend of being more complete (+10.2%). Conversely, we found users tended to\nbelieve that solutions they perceived as more aligned had been generated by\nhuman experts. Furthermore, while LLM-generated documents scored higher and\nperformed at 720x the speed, their cost was, on average, only 0.06% that of a\nhuman expert. Overall, these findings indicate that LLMs will play an\nincreasingly important role in requirements engineering by improving\nrequirements definitions, enabling more efficient resource allocation, and\nreducing overall project timelines."
                },
                "authors": [
                    {
                        "name": "Cory Hymel"
                    },
                    {
                        "name": "Hiroe Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Hiroe Johnson"
                },
                "author": "Hiroe Johnson",
                "arxiv_comment": "8 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19287v1",
                "updated": "2025-01-31T16:48:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    48,
                    38,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:48:38Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    48,
                    38,
                    4,
                    31,
                    0
                ],
                "title": "Differentially Private In-context Learning via Sampling Few-shot Mixed\n  with Zero-shot Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private In-context Learning via Sampling Few-shot Mixed\n  with Zero-shot Outputs"
                },
                "summary": "In-context learning (ICL) has shown promising improvement in downstream task\nadaptation of LLMs by augmenting prompts with relevant input-output examples\n(demonstrations). However, the ICL demonstrations can contain privacy-sensitive\ninformation, which can be leaked and/or regurgitated by the LLM output.\nDifferential Privacy (DP), a widely adopted privacy safeguard, has emerged to\nmitigate this privacy leakage, with recent work demonstrating strong\nprivacy-utility tradeoffs in classification tasks for ICL. However, generation\ntasks for ICL are challenging due to the high-dimensional output space of\nopen-ended generation. To this end, we propose $\\texttt{dps-mozo}$,\nDifferentially Private Sampling by Mixing One-shot with Zero-shot Outputs, a\ndecoding framework that generates DP text by sampling from the product of\nmultiple one-shot outputs mixed with a zero-shot output. This mixing\neffectively reduces the amount of information that can be leaked by each\ndemonstration. By utilizing the inherent randomness in sampling from the mixed\ndistributions, we can achieve DP without adding noise, thereby improving the\nprivacy-utility tradeoff. Our experimental evaluations show $\\texttt{dps-mozo}$\ncan achieve a strong privacy guarantee, $\\epsilon=2$, with minimal utility\ndegradation compared to non-private few-shot learning, $\\textbf{0.3}$% ROUGE-L\nF1 score decrease on the SAMSum dataset with Gemma 2 2B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has shown promising improvement in downstream task\nadaptation of LLMs by augmenting prompts with relevant input-output examples\n(demonstrations). However, the ICL demonstrations can contain privacy-sensitive\ninformation, which can be leaked and/or regurgitated by the LLM output.\nDifferential Privacy (DP), a widely adopted privacy safeguard, has emerged to\nmitigate this privacy leakage, with recent work demonstrating strong\nprivacy-utility tradeoffs in classification tasks for ICL. However, generation\ntasks for ICL are challenging due to the high-dimensional output space of\nopen-ended generation. To this end, we propose $\\texttt{dps-mozo}$,\nDifferentially Private Sampling by Mixing One-shot with Zero-shot Outputs, a\ndecoding framework that generates DP text by sampling from the product of\nmultiple one-shot outputs mixed with a zero-shot output. This mixing\neffectively reduces the amount of information that can be leaked by each\ndemonstration. By utilizing the inherent randomness in sampling from the mixed\ndistributions, we can achieve DP without adding noise, thereby improving the\nprivacy-utility tradeoff. Our experimental evaluations show $\\texttt{dps-mozo}$\ncan achieve a strong privacy guarantee, $\\epsilon=2$, with minimal utility\ndegradation compared to non-private few-shot learning, $\\textbf{0.3}$% ROUGE-L\nF1 score decrease on the SAMSum dataset with Gemma 2 2B."
                },
                "authors": [
                    {
                        "name": "James Flemings"
                    },
                    {
                        "name": "Haosheng Gan"
                    },
                    {
                        "name": "Hongyi Li"
                    },
                    {
                        "name": "Meisam Razaviyayn"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01703v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01703v2",
                "updated": "2025-01-31T16:47:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    47,
                    16,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-03T22:19:20Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    22,
                    19,
                    20,
                    6,
                    308,
                    0
                ],
                "title": "UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal large language models (MLLMs) have revolutionized vision-language\nunderstanding but remain vulnerable to multimodal jailbreak attacks, where\nadversarial inputs are meticulously crafted to elicit harmful or inappropriate\nresponses. We propose UniGuard, a novel multimodal safety guardrail that\njointly considers the unimodal and cross-modal harmful signals. UniGuard trains\na multimodal guardrail to minimize the likelihood of generating harmful\nresponses in a toxic corpus. The guardrail can be seamlessly applied to any\ninput prompt during inference with minimal computational costs. Extensive\nexperiments demonstrate the generalizability of UniGuard across multiple\nmodalities, attack strategies, and multiple state-of-the-art MLLMs, including\nLLaVA, Gemini Pro, GPT-4o, MiniGPT-4, and InstructBLIP. Notably, this robust\ndefense mechanism maintains the models' overall vision-language understanding\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have revolutionized vision-language\nunderstanding but remain vulnerable to multimodal jailbreak attacks, where\nadversarial inputs are meticulously crafted to elicit harmful or inappropriate\nresponses. We propose UniGuard, a novel multimodal safety guardrail that\njointly considers the unimodal and cross-modal harmful signals. UniGuard trains\na multimodal guardrail to minimize the likelihood of generating harmful\nresponses in a toxic corpus. The guardrail can be seamlessly applied to any\ninput prompt during inference with minimal computational costs. Extensive\nexperiments demonstrate the generalizability of UniGuard across multiple\nmodalities, attack strategies, and multiple state-of-the-art MLLMs, including\nLLaVA, Gemini Pro, GPT-4o, MiniGPT-4, and InstructBLIP. Notably, this robust\ndefense mechanism maintains the models' overall vision-language understanding\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Sejoon Oh"
                    },
                    {
                        "name": "Yiqiao Jin"
                    },
                    {
                        "name": "Megha Sharma"
                    },
                    {
                        "name": "Donghyun Kim"
                    },
                    {
                        "name": "Eric Ma"
                    },
                    {
                        "name": "Gaurav Verma"
                    },
                    {
                        "name": "Srijan Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Srijan Kumar"
                },
                "author": "Srijan Kumar",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01703v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01703v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19282v1",
                "updated": "2025-01-31T16:45:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    45,
                    16,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:45:16Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    45,
                    16,
                    4,
                    31,
                    0
                ],
                "title": "Low-Cost and Comprehensive Non-textual Input Fuzzing with\n  LLM-Synthesized Input Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Cost and Comprehensive Non-textual Input Fuzzing with\n  LLM-Synthesized Input Generators"
                },
                "summary": "Modern software often accepts inputs with highly complex grammars. Recent\nadvances in large language models (LLMs) have shown that they can be used to\nsynthesize high-quality natural language text and code that conforms to the\ngrammar of a given input format. Nevertheless, LLMs are often incapable or too\ncostly to generate non-textual outputs, such as images, videos, and PDF files.\nThis limitation hinders the application of LLMs in grammar-aware fuzzing.\n  We present a novel approach to enabling grammar-aware fuzzing over\nnon-textual inputs. We employ LLMs to synthesize and also mutate input\ngenerators, in the form of Python scripts, that generate data conforming to the\ngrammar of a given input format. Then, non-textual data yielded by the input\ngenerators are further mutated by traditional fuzzers (AFL++) to explore the\nsoftware input space effectively. Our approach, namely G2FUZZ, features a\nhybrid strategy that combines a holistic search driven by LLMs and a local\nsearch driven by industrial quality fuzzers. Two key advantages are: (1) LLMs\nare good at synthesizing and mutating input generators and enabling jumping out\nof local optima, thus achieving a synergistic effect when combined with\nmutation-based fuzzers; (2) LLMs are less frequently invoked unless really\nneeded, thus significantly reducing the cost of LLM usage. We have evaluated\nG2FUZZ on a variety of input formats, including TIFF images, MP4 audios, and\nPDF files. The results show that G2FUZZ outperforms SOTA tools such as AFL++,\nFuzztruction, and FormatFuzzer in terms of code coverage and bug finding across\nmost programs tested on three platforms: UNIFUZZ, FuzzBench, and MAGMA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software often accepts inputs with highly complex grammars. Recent\nadvances in large language models (LLMs) have shown that they can be used to\nsynthesize high-quality natural language text and code that conforms to the\ngrammar of a given input format. Nevertheless, LLMs are often incapable or too\ncostly to generate non-textual outputs, such as images, videos, and PDF files.\nThis limitation hinders the application of LLMs in grammar-aware fuzzing.\n  We present a novel approach to enabling grammar-aware fuzzing over\nnon-textual inputs. We employ LLMs to synthesize and also mutate input\ngenerators, in the form of Python scripts, that generate data conforming to the\ngrammar of a given input format. Then, non-textual data yielded by the input\ngenerators are further mutated by traditional fuzzers (AFL++) to explore the\nsoftware input space effectively. Our approach, namely G2FUZZ, features a\nhybrid strategy that combines a holistic search driven by LLMs and a local\nsearch driven by industrial quality fuzzers. Two key advantages are: (1) LLMs\nare good at synthesizing and mutating input generators and enabling jumping out\nof local optima, thus achieving a synergistic effect when combined with\nmutation-based fuzzers; (2) LLMs are less frequently invoked unless really\nneeded, thus significantly reducing the cost of LLM usage. We have evaluated\nG2FUZZ on a variety of input formats, including TIFF images, MP4 audios, and\nPDF files. The results show that G2FUZZ outperforms SOTA tools such as AFL++,\nFuzztruction, and FormatFuzzer in terms of code coverage and bug finding across\nmost programs tested on three platforms: UNIFUZZ, FuzzBench, and MAGMA."
                },
                "authors": [
                    {
                        "name": "Kunpeng Zhang"
                    },
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "arxiv_comment": "USENIX Security 2025",
                "arxiv_journal_ref": "The 34th USENIX Security Symposium, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19278v1",
                "updated": "2025-01-31T16:42:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    42,
                    31,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:42:31Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    42,
                    31,
                    4,
                    31,
                    0
                ],
                "title": "Pheromone-based Learning of Optimal Reasoning Paths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pheromone-based Learning of Optimal Reasoning Paths"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning\ncapabilities through chain-of-thought prompting, yet discovering effective\nreasoning methods for complex problems remains challenging due to the vast\nspace of possible intermediate steps. We introduce Ant Colony\nOptimization-guided Tree of Thought (ACO-ToT), a novel algorithm that combines\nACO with LLMs to discover optimal reasoning paths for complex problems\nefficiently. Drawing inspiration from Hebbian learning in neurological systems,\nour method employs a collection of distinctly fine-tuned LLM \"ants\" to traverse\nand lay pheromone trails through a centralized tree of thought, with each ant's\nmovement governed by a weighted combination of existing pheromone trails and\nits own specialized expertise. The algorithm evaluates complete reasoning paths\nusing a mixture-of-experts-based scoring function, with pheromones reinforcing\nproductive reasoning paths across iterations. Experiments on three challenging\nreasoning tasks (GSM8K, ARC-Challenge, and MATH) demonstrate that ACO-ToT\nperforms significantly better than existing chain-of-thought optimization\napproaches, suggesting that incorporating biologically inspired collective\nsearch mechanisms into LLM inference can substantially enhance reasoning\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable reasoning\ncapabilities through chain-of-thought prompting, yet discovering effective\nreasoning methods for complex problems remains challenging due to the vast\nspace of possible intermediate steps. We introduce Ant Colony\nOptimization-guided Tree of Thought (ACO-ToT), a novel algorithm that combines\nACO with LLMs to discover optimal reasoning paths for complex problems\nefficiently. Drawing inspiration from Hebbian learning in neurological systems,\nour method employs a collection of distinctly fine-tuned LLM \"ants\" to traverse\nand lay pheromone trails through a centralized tree of thought, with each ant's\nmovement governed by a weighted combination of existing pheromone trails and\nits own specialized expertise. The algorithm evaluates complete reasoning paths\nusing a mixture-of-experts-based scoring function, with pheromones reinforcing\nproductive reasoning paths across iterations. Experiments on three challenging\nreasoning tasks (GSM8K, ARC-Challenge, and MATH) demonstrate that ACO-ToT\nperforms significantly better than existing chain-of-thought optimization\napproaches, suggesting that incorporating biologically inspired collective\nsearch mechanisms into LLM inference can substantially enhance reasoning\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Anirudh Chari"
                    },
                    {
                        "name": "Aditya Tiwari"
                    },
                    {
                        "name": "Richard Lian"
                    },
                    {
                        "name": "Suraj Reddy"
                    },
                    {
                        "name": "Brian Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Brian Zhou"
                },
                "author": "Brian Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15275v2",
                "updated": "2025-01-31T16:31:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    31,
                    56,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-20T04:19:32Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    4,
                    19,
                    32,
                    6,
                    294,
                    0
                ],
                "title": "SuiGPT MAD: Move AI Decompiler to Improve Transparency and Auditability\n  on Non-Open-Source Blockchain Smart Contract",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuiGPT MAD: Move AI Decompiler to Improve Transparency and Auditability\n  on Non-Open-Source Blockchain Smart Contract"
                },
                "summary": "The vision of Web3 is to improve user control over data and assets, but one\nchallenge that complicates this vision is the prevalence of non-transparent,\nscam-prone applications and vulnerable smart contracts that put Web3 users at\nrisk. While code audits are one solution to this problem, the lack of smart\ncontracts source code on many blockchain platforms, such as Sui, hinders the\nease of auditing. A promising approach to this issue is the use of a decompiler\nto reverse-engineer smart contract bytecode. However, existing decompilers for\nSui produce code that is difficult to understand and cannot be directly\nrecompiled. To address this, we developed the SuiGPT Move AI Decompiler (MAD),\na Large Language Model (LLM)-powered web application that decompiles smart\ncontract bytecodes on Sui into logically correct, human-readable, and\nre-compilable source code with prompt engineering.\n  Our evaluation shows that MAD's output successfully passes original unit\ntests and achieves a 73.33% recompilation success rate on real-world smart\ncontracts. Additionally, newer models tend to deliver improved performance,\nsuggesting that MAD's approach will become increasingly effective as LLMs\ncontinue to advance.\n  In a user study involving 12 developers, we found that MAD significantly\nreduced the auditing workload compared to using traditional decompilers.\nParticipants found MAD's outputs comparable to the original source code,\nimproving accessibility for understanding and auditing non-open-source smart\ncontracts. Through qualitative interviews with these developers and Web3\nprojects, we further discussed the strengths and concerns of MAD.\n  MAD has practical implications for blockchain smart contract transparency,\nauditing, and education. It empowers users to easily and independently review\nand audit non-open-source smart contracts, fostering accountability and\ndecentralization",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision of Web3 is to improve user control over data and assets, but one\nchallenge that complicates this vision is the prevalence of non-transparent,\nscam-prone applications and vulnerable smart contracts that put Web3 users at\nrisk. While code audits are one solution to this problem, the lack of smart\ncontracts source code on many blockchain platforms, such as Sui, hinders the\nease of auditing. A promising approach to this issue is the use of a decompiler\nto reverse-engineer smart contract bytecode. However, existing decompilers for\nSui produce code that is difficult to understand and cannot be directly\nrecompiled. To address this, we developed the SuiGPT Move AI Decompiler (MAD),\na Large Language Model (LLM)-powered web application that decompiles smart\ncontract bytecodes on Sui into logically correct, human-readable, and\nre-compilable source code with prompt engineering.\n  Our evaluation shows that MAD's output successfully passes original unit\ntests and achieves a 73.33% recompilation success rate on real-world smart\ncontracts. Additionally, newer models tend to deliver improved performance,\nsuggesting that MAD's approach will become increasingly effective as LLMs\ncontinue to advance.\n  In a user study involving 12 developers, we found that MAD significantly\nreduced the auditing workload compared to using traditional decompilers.\nParticipants found MAD's outputs comparable to the original source code,\nimproving accessibility for understanding and auditing non-open-source smart\ncontracts. Through qualitative interviews with these developers and Web3\nprojects, we further discussed the strengths and concerns of MAD.\n  MAD has practical implications for blockchain smart contract transparency,\nauditing, and education. It empowers users to easily and independently review\nand audit non-open-source smart contracts, fostering accountability and\ndecentralization"
                },
                "authors": [
                    {
                        "name": "Eason Chen"
                    },
                    {
                        "name": "Xinyi Tang"
                    },
                    {
                        "name": "Zimo Xiao"
                    },
                    {
                        "name": "Chuangji Li"
                    },
                    {
                        "name": "Shizhuo Li"
                    },
                    {
                        "name": "Wu Tingguan"
                    },
                    {
                        "name": "Siyun Wang"
                    },
                    {
                        "name": "Kostas Kryptos Chalkias"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Kryptos Chalkias"
                },
                "author": "Kostas Kryptos Chalkias",
                "arxiv_doi": "10.1145/3696410.3714790",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714790",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.15275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Paper accepted at ACM The Web Conference 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19270v1",
                "updated": "2025-01-31T16:29:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    29,
                    19,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:29:19Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    29,
                    19,
                    4,
                    31,
                    0
                ],
                "title": "Imagine with the Teacher: Complete Shape in a Multi-View Distillation\n  Way",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imagine with the Teacher: Complete Shape in a Multi-View Distillation\n  Way"
                },
                "summary": "Point cloud completion aims to recover the completed 3D shape of an object\nfrom its partial observation caused by occlusion, sensor's limitation, noise,\netc. When some key semantic information is lost in the incomplete point cloud,\nthe neural network needs to infer the missing part based on the input\ninformation. Intuitively we would apply an autoencoder architecture to solve\nthis kind of problem, which take the incomplete point cloud as input and is\nsupervised by the ground truth. This process that develops model's imagination\nfrom incomplete shape to complete shape is done automatically in the latent\nspace. But the knowledge for mapping from incomplete to complete still remains\ndark and could be further explored. Motivated by the knowledge distillation's\nteacher-student learning strategy, we design a knowledge transfer way for\ncompleting 3d shape. In this work, we propose a novel View Distillation Point\nCompletion Network (VD-PCN), which solve the completion problem by a multi-view\ndistillation way. The design methodology fully leverages the orderliness of 2d\npixels, flexibleness of 2d processing and powerfulness of 2d network. Extensive\nevaluations on PCN, ShapeNet55/34, and MVP datasets confirm the effectiveness\nof our design and knowledge transfer strategy, both quantitatively and\nqualitatively. Committed to facilitate ongoing research, we will make our code\npublicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point cloud completion aims to recover the completed 3D shape of an object\nfrom its partial observation caused by occlusion, sensor's limitation, noise,\netc. When some key semantic information is lost in the incomplete point cloud,\nthe neural network needs to infer the missing part based on the input\ninformation. Intuitively we would apply an autoencoder architecture to solve\nthis kind of problem, which take the incomplete point cloud as input and is\nsupervised by the ground truth. This process that develops model's imagination\nfrom incomplete shape to complete shape is done automatically in the latent\nspace. But the knowledge for mapping from incomplete to complete still remains\ndark and could be further explored. Motivated by the knowledge distillation's\nteacher-student learning strategy, we design a knowledge transfer way for\ncompleting 3d shape. In this work, we propose a novel View Distillation Point\nCompletion Network (VD-PCN), which solve the completion problem by a multi-view\ndistillation way. The design methodology fully leverages the orderliness of 2d\npixels, flexibleness of 2d processing and powerfulness of 2d network. Extensive\nevaluations on PCN, ShapeNet55/34, and MVP datasets confirm the effectiveness\nof our design and knowledge transfer strategy, both quantitatively and\nqualitatively. Committed to facilitate ongoing research, we will make our code\npublicly available."
                },
                "authors": [
                    {
                        "name": "Zhanpeng Luo"
                    },
                    {
                        "name": "Linna Wang"
                    },
                    {
                        "name": "Guangwu Qian"
                    },
                    {
                        "name": "Li Lu"
                    }
                ],
                "author_detail": {
                    "name": "Li Lu"
                },
                "author": "Li Lu",
                "arxiv_comment": "9 pages, 3 figures 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19269v1",
                "updated": "2025-01-31T16:29:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    29,
                    16,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:29:16Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    29,
                    16,
                    4,
                    31,
                    0
                ],
                "title": "Information Metrics and Possible Limitations of Local Information\n  Objectivity in Quantum Gravity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Metrics and Possible Limitations of Local Information\n  Objectivity in Quantum Gravity"
                },
                "summary": "Local information objectivity, that local observers can infer the same\ninformation about a model upon exchange of experimental data, is fundamental to\nscience. It is mathematically encoded via Cencov's theorem: the Fisher\ninformation metric is the unique metric invariant under sufficient statistics.\nHowever, quantum gravity typically violates some of the Cencov assumptions,\nallowing the Fisher metric and Born rule to vary between observers. We explain\nthese violations, some possible experimental tests, and a new quantum gravity\napproach based on generally covariant information geometry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local information objectivity, that local observers can infer the same\ninformation about a model upon exchange of experimental data, is fundamental to\nscience. It is mathematically encoded via Cencov's theorem: the Fisher\ninformation metric is the unique metric invariant under sufficient statistics.\nHowever, quantum gravity typically violates some of the Cencov assumptions,\nallowing the Fisher metric and Born rule to vary between observers. We explain\nthese violations, some possible experimental tests, and a new quantum gravity\napproach based on generally covariant information geometry."
                },
                "authors": [
                    {
                        "name": "Per Berglund"
                    },
                    {
                        "name": "Andrew Geraci"
                    },
                    {
                        "name": "Tristan Hubsch"
                    },
                    {
                        "name": "David Mattingly"
                    },
                    {
                        "name": "Djordje Minic"
                    }
                ],
                "author_detail": {
                    "name": "Djordje Minic"
                },
                "author": "Djordje Minic",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19266v1",
                "updated": "2025-01-31T16:26:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    26,
                    28,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:26:28Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    26,
                    28,
                    4,
                    31,
                    0
                ],
                "title": "Jackpot! Alignment as a Maximal Lottery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jackpot! Alignment as a Maximal Lottery"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF), the standard for aligning\nLarge Language Models (LLMs) with human values, is known to fail to satisfy\nproperties that are intuitively desirable, such as respecting the preferences\nof the majority \\cite{ge2024axioms}. To overcome these issues, we propose the\nuse of a probabilistic Social Choice rule called \\emph{maximal lotteries} as a\nreplacement for RLHF. We show that a family of alignment techniques, namely\nNash Learning from Human Feedback (NLHF) \\cite{munos2023nash} and variants,\napproximate maximal lottery outcomes and thus inherit its beneficial\nproperties.\n  We confirm experimentally that our proposed methodology handles situations\nthat arise when working with preferences more robustly than standard RLHF,\nincluding supporting the preferences of the majority, providing principled ways\nof handling non-transitivities in the preference data, and robustness to\nirrelevant alternatives. This results in systems that better incorporate human\nvalues and respect human intentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF), the standard for aligning\nLarge Language Models (LLMs) with human values, is known to fail to satisfy\nproperties that are intuitively desirable, such as respecting the preferences\nof the majority \\cite{ge2024axioms}. To overcome these issues, we propose the\nuse of a probabilistic Social Choice rule called \\emph{maximal lotteries} as a\nreplacement for RLHF. We show that a family of alignment techniques, namely\nNash Learning from Human Feedback (NLHF) \\cite{munos2023nash} and variants,\napproximate maximal lottery outcomes and thus inherit its beneficial\nproperties.\n  We confirm experimentally that our proposed methodology handles situations\nthat arise when working with preferences more robustly than standard RLHF,\nincluding supporting the preferences of the majority, providing principled ways\nof handling non-transitivities in the preference data, and robustness to\nirrelevant alternatives. This results in systems that better incorporate human\nvalues and respect human intentions."
                },
                "authors": [
                    {
                        "name": "Roberto-Rafael Maura-Rivero"
                    },
                    {
                        "name": "Marc Lanctot"
                    },
                    {
                        "name": "Francesco Visin"
                    },
                    {
                        "name": "Kate Larson"
                    }
                ],
                "author_detail": {
                    "name": "Kate Larson"
                },
                "author": "Kate Larson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11021v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11021v4",
                "updated": "2025-01-31T16:18:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    18,
                    56,
                    4,
                    31,
                    0
                ],
                "published": "2024-06-16T17:27:45Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    17,
                    27,
                    45,
                    6,
                    168,
                    0
                ],
                "title": "$α$-OCC: Uncertainty-Aware Camera-based 3D Semantic Occupancy\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$α$-OCC: Uncertainty-Aware Camera-based 3D Semantic Occupancy\n  Prediction"
                },
                "summary": "In the realm of autonomous vehicle perception, comprehending 3D scenes is\nparamount for tasks such as planning and mapping. Camera-based 3D Semantic\nOccupancy Prediction (OCC) aims to infer scene geometry and semantics from\nlimited observations. While it has gained popularity due to affordability and\nrich visual cues, existing methods often neglect the inherent uncertainty in\nmodels. To address this, we propose an uncertainty-aware OCC method\n($\\alpha$-OCC). We first introduce Depth-UP, an uncertainty propagation\nframework that improves geometry completion by up to 11.58\\% and semantic\nsegmentation by up to 12.95\\% across various OCC models. For uncertainty\nquantification (UQ), we propose the hierarchical conformal prediction (HCP)\nmethod, effectively handling the high-level class imbalance in OCC datasets. On\nthe geometry level, the novel KL-based score function significantly improves\nthe occupied recall (45\\%) of safety-critical classes with minimal performance\noverhead (3.4\\% reduction). On UQ, our HCP achieves smaller prediction set\nsizes while maintaining the defined coverage guarantee. Compared with\nbaselines, it reduces up to 92\\% set size, with 18\\% further reduction when\nintegrated with Depth-UP. Our contributions advance OCC accuracy and\nrobustness, marking a noteworthy step forward in autonomous perception systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of autonomous vehicle perception, comprehending 3D scenes is\nparamount for tasks such as planning and mapping. Camera-based 3D Semantic\nOccupancy Prediction (OCC) aims to infer scene geometry and semantics from\nlimited observations. While it has gained popularity due to affordability and\nrich visual cues, existing methods often neglect the inherent uncertainty in\nmodels. To address this, we propose an uncertainty-aware OCC method\n($\\alpha$-OCC). We first introduce Depth-UP, an uncertainty propagation\nframework that improves geometry completion by up to 11.58\\% and semantic\nsegmentation by up to 12.95\\% across various OCC models. For uncertainty\nquantification (UQ), we propose the hierarchical conformal prediction (HCP)\nmethod, effectively handling the high-level class imbalance in OCC datasets. On\nthe geometry level, the novel KL-based score function significantly improves\nthe occupied recall (45\\%) of safety-critical classes with minimal performance\noverhead (3.4\\% reduction). On UQ, our HCP achieves smaller prediction set\nsizes while maintaining the defined coverage guarantee. Compared with\nbaselines, it reduces up to 92\\% set size, with 18\\% further reduction when\nintegrated with Depth-UP. Our contributions advance OCC accuracy and\nrobustness, marking a noteworthy step forward in autonomous perception systems."
                },
                "authors": [
                    {
                        "name": "Sanbao Su"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Chenchen Lin"
                    },
                    {
                        "name": "Felix Juefei-Xu"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Fei Miao"
                    }
                ],
                "author_detail": {
                    "name": "Fei Miao"
                },
                "author": "Fei Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11021v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11021v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19259v1",
                "updated": "2025-01-31T16:17:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    17,
                    3,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:17:03Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    17,
                    3,
                    4,
                    31,
                    0
                ],
                "title": "Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for\n  Autonomous Drone FlighT at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for\n  Autonomous Drone FlighT at the Edge"
                },
                "summary": "The integration of human-intuitive interactions into autonomous systems has\nbeen limited. Traditional Natural Language Processing (NLP) systems struggle\nwith context and intent understanding, severely restricting human-robot\ninteraction. Recent advancements in Large Language Models (LLMs) have\ntransformed this dynamic, allowing for intuitive and high-level communication\nthrough speech and text, and bridging the gap between human commands and\nrobotic actions. Additionally, autonomous navigation has emerged as a central\nfocus in robotics research, with artificial intelligence (AI) increasingly\nbeing leveraged to enhance these systems. However, existing AI-based navigation\nalgorithms face significant challenges in latency-critical tasks where rapid\ndecision-making is critical. Traditional frame-based vision systems, while\neffective for high-level decision-making, suffer from high energy consumption\nand latency, limiting their applicability in real-time scenarios. Neuromorphic\nvision systems, combining event-based cameras and spiking neural networks\n(SNNs), offer a promising alternative by enabling energy-efficient, low-latency\nnavigation. Despite their potential, real-world implementations of these\nsystems, particularly on physical platforms such as drones, remain scarce. In\nthis work, we present Neuro-LIFT, a real-time neuromorphic navigation framework\nimplemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural\nlanguage processing, Neuro-LIFT translates human speech into high-level\nplanning commands which are then autonomously executed using event-based\nneuromorphic vision and physics-driven planning. Our framework demonstrates its\ncapabilities in navigating in a dynamic environment, avoiding obstacles, and\nadapting to human instructions in real-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of human-intuitive interactions into autonomous systems has\nbeen limited. Traditional Natural Language Processing (NLP) systems struggle\nwith context and intent understanding, severely restricting human-robot\ninteraction. Recent advancements in Large Language Models (LLMs) have\ntransformed this dynamic, allowing for intuitive and high-level communication\nthrough speech and text, and bridging the gap between human commands and\nrobotic actions. Additionally, autonomous navigation has emerged as a central\nfocus in robotics research, with artificial intelligence (AI) increasingly\nbeing leveraged to enhance these systems. However, existing AI-based navigation\nalgorithms face significant challenges in latency-critical tasks where rapid\ndecision-making is critical. Traditional frame-based vision systems, while\neffective for high-level decision-making, suffer from high energy consumption\nand latency, limiting their applicability in real-time scenarios. Neuromorphic\nvision systems, combining event-based cameras and spiking neural networks\n(SNNs), offer a promising alternative by enabling energy-efficient, low-latency\nnavigation. Despite their potential, real-world implementations of these\nsystems, particularly on physical platforms such as drones, remain scarce. In\nthis work, we present Neuro-LIFT, a real-time neuromorphic navigation framework\nimplemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural\nlanguage processing, Neuro-LIFT translates human speech into high-level\nplanning commands which are then autonomously executed using event-based\nneuromorphic vision and physics-driven planning. Our framework demonstrates its\ncapabilities in navigating in a dynamic environment, avoiding obstacles, and\nadapting to human instructions in real-time."
                },
                "authors": [
                    {
                        "name": "Amogh Joshi"
                    },
                    {
                        "name": "Sourav Sanyal"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04670v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04670v2",
                "updated": "2025-01-31T16:12:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    12,
                    22,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-08T18:30:53Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    30,
                    53,
                    2,
                    8,
                    0
                ],
                "title": "Are They the Same? Exploring Visual Correspondence Shortcomings of\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are They the Same? Exploring Visual Correspondence Shortcomings of\n  Multimodal LLMs"
                },
                "summary": "Recent advancements in multimodal models have shown a strong ability in\nvisual perception, reasoning abilities, and vision-language understanding.\nHowever, studies on visual matching ability are missing, where finding the\nvisual correspondence of objects is essential in vision research. Our research\nreveals that the matching capabilities in recent multimodal LLMs (MLLMs) still\nexhibit systematic shortcomings, even with current strong MLLMs models, GPT-4o.\nIn particular, we construct a Multimodal Visual Matching (MMVM) benchmark to\nfairly benchmark over 30 different MLLMs. The MMVM benchmark is built from 15\nopen-source datasets and Internet videos with manual annotation. We categorize\nthe data samples of MMVM benchmark into eight aspects based on the required\ncues and capabilities to more comprehensively evaluate and analyze current\nMLLMs. In addition, we have designed an automatic annotation pipeline to\ngenerate the MMVM SFT dataset, including 220K visual matching data with\nreasoning annotation. Finally, we present CoLVA, a novel contrastive MLLM with\ntwo novel technical designs: fine-grained vision expert with object-level\ncontrastive learning and instruction augmentation strategy. CoLVA achieves\n51.06\\% overall accuracy (OA) on the MMVM benchmark, surpassing GPT-4o and\nbaseline by 8.41\\% and 23.58\\% OA, respectively. The results show the\neffectiveness of our MMVM SFT dataset and our novel technical designs. Code,\nbenchmark, dataset, and models are available at\nhttps://github.com/zhouyiks/CoLVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in multimodal models have shown a strong ability in\nvisual perception, reasoning abilities, and vision-language understanding.\nHowever, studies on visual matching ability are missing, where finding the\nvisual correspondence of objects is essential in vision research. Our research\nreveals that the matching capabilities in recent multimodal LLMs (MLLMs) still\nexhibit systematic shortcomings, even with current strong MLLMs models, GPT-4o.\nIn particular, we construct a Multimodal Visual Matching (MMVM) benchmark to\nfairly benchmark over 30 different MLLMs. The MMVM benchmark is built from 15\nopen-source datasets and Internet videos with manual annotation. We categorize\nthe data samples of MMVM benchmark into eight aspects based on the required\ncues and capabilities to more comprehensively evaluate and analyze current\nMLLMs. In addition, we have designed an automatic annotation pipeline to\ngenerate the MMVM SFT dataset, including 220K visual matching data with\nreasoning annotation. Finally, we present CoLVA, a novel contrastive MLLM with\ntwo novel technical designs: fine-grained vision expert with object-level\ncontrastive learning and instruction augmentation strategy. CoLVA achieves\n51.06\\% overall accuracy (OA) on the MMVM benchmark, surpassing GPT-4o and\nbaseline by 8.41\\% and 23.58\\% OA, respectively. The results show the\neffectiveness of our MMVM SFT dataset and our novel technical designs. Code,\nbenchmark, dataset, and models are available at\nhttps://github.com/zhouyiks/CoLVA."
                },
                "authors": [
                    {
                        "name": "Yikang Zhou"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Shilin Xu"
                    },
                    {
                        "name": "Shihao Chen"
                    },
                    {
                        "name": "Qianyu Zhou"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Shunping Ji"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Lu Qi"
                    }
                ],
                "author_detail": {
                    "name": "Lu Qi"
                },
                "author": "Lu Qi",
                "arxiv_comment": "fix typos, figures, tables, and other details; additional results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04670v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19252v1",
                "updated": "2025-01-31T16:09:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    9,
                    30,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:09:30Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    9,
                    30,
                    4,
                    31,
                    0
                ],
                "title": "Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search"
                },
                "summary": "The remarkable progress in text-to-video diffusion models enables\nphotorealistic generations, although the contents of the generated video often\ninclude unnatural movement or deformation, reverse playback, and motionless\nscenes. Recently, an alignment problem has attracted huge attention, where we\nsteer the output of diffusion models based on some quantity on the goodness of\nthe content. Because there is a large room for improvement of perceptual\nquality along the frame direction, we should address which metrics we should\noptimize and how we can optimize them in the video generation. In this paper,\nwe propose diffusion latent beam search with lookahead estimator, which can\nselect better diffusion latent to maximize a given alignment reward, at\ninference time. We then point out that the improvement of perceptual video\nquality considering the alignment to prompts requires reward calibration by\nweighting existing metrics. When evaluating outputs by using vision language\nmodels as a proxy of humans, many previous metrics to quantify the naturalness\nof video do not always correlate with evaluation and also depend on the degree\nof dynamic descriptions in evaluation prompts. We demonstrate that our method\nimproves the perceptual quality based on the calibrated reward, without model\nparameter update, and outputs the best generation compared to greedy search and\nbest-of-N sampling. We provide practical guidelines on which axes, among search\nbudget, lookahead steps for reward estimate, and denoising steps, in the\nreverse diffusion process, we should allocate the inference-time computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable progress in text-to-video diffusion models enables\nphotorealistic generations, although the contents of the generated video often\ninclude unnatural movement or deformation, reverse playback, and motionless\nscenes. Recently, an alignment problem has attracted huge attention, where we\nsteer the output of diffusion models based on some quantity on the goodness of\nthe content. Because there is a large room for improvement of perceptual\nquality along the frame direction, we should address which metrics we should\noptimize and how we can optimize them in the video generation. In this paper,\nwe propose diffusion latent beam search with lookahead estimator, which can\nselect better diffusion latent to maximize a given alignment reward, at\ninference time. We then point out that the improvement of perceptual video\nquality considering the alignment to prompts requires reward calibration by\nweighting existing metrics. When evaluating outputs by using vision language\nmodels as a proxy of humans, many previous metrics to quantify the naturalness\nof video do not always correlate with evaluation and also depend on the degree\nof dynamic descriptions in evaluation prompts. We demonstrate that our method\nimproves the perceptual quality based on the calibrated reward, without model\nparameter update, and outputs the best generation compared to greedy search and\nbest-of-N sampling. We provide practical guidelines on which axes, among search\nbudget, lookahead steps for reward estimate, and denoising steps, in the\nreverse diffusion process, we should allocate the inference-time computation."
                },
                "authors": [
                    {
                        "name": "Yuta Oshima"
                    },
                    {
                        "name": "Masahiro Suzuki"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Hiroki Furuta"
                    }
                ],
                "author_detail": {
                    "name": "Hiroki Furuta"
                },
                "author": "Hiroki Furuta",
                "arxiv_comment": "Website: https://sites.google.com/view/t2v-dlbs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06833v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06833v3",
                "updated": "2025-01-31T16:06:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    6,
                    52,
                    4,
                    31,
                    0
                ],
                "published": "2024-03-11T15:48:56Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    15,
                    48,
                    56,
                    0,
                    71,
                    0
                ],
                "title": "Can LLMs Separate Instructions From Data? And What Do We Even Mean By\n  That?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Separate Instructions From Data? And What Do We Even Mean By\n  That?"
                },
                "summary": "Instruction-tuned Large Language Models (LLMs) show impressive results in\nnumerous practical applications, but they lack essential safety features that\nare common in other areas of computer science, particularly an explicit\nseparation of instructions and data. This makes them vulnerable to\nmanipulations such as indirect prompt injections and generally unsuitable for\nsafety-critical tasks. Surprisingly, there is currently no established\ndefinition or benchmark to quantify this phenomenon. In this work, we close\nthis gap by introducing a formal measure for instruction-data separation and an\nempirical variant that is calculable from a model's outputs. We also present a\nnew dataset, SEP, that allows estimating the measure for real-world models. Our\nresults on various LLMs show that the problem of instruction-data separation is\nreal: all models fail to achieve high separation, and canonical mitigation\ntechniques, such as prompt engineering and fine-tuning, either fail to\nsubstantially improve separation or reduce model utility. The source code and\nSEP dataset are openly accessible at\nhttps://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuned Large Language Models (LLMs) show impressive results in\nnumerous practical applications, but they lack essential safety features that\nare common in other areas of computer science, particularly an explicit\nseparation of instructions and data. This makes them vulnerable to\nmanipulations such as indirect prompt injections and generally unsuitable for\nsafety-critical tasks. Surprisingly, there is currently no established\ndefinition or benchmark to quantify this phenomenon. In this work, we close\nthis gap by introducing a formal measure for instruction-data separation and an\nempirical variant that is calculable from a model's outputs. We also present a\nnew dataset, SEP, that allows estimating the measure for real-world models. Our\nresults on various LLMs show that the problem of instruction-data separation is\nreal: all models fail to achieve high separation, and canonical mitigation\ntechniques, such as prompt engineering and fine-tuning, either fail to\nsubstantially improve separation or reduce model utility. The source code and\nSEP dataset are openly accessible at\nhttps://github.com/egozverev/Shold-It-Be-Executed-Or-Processed."
                },
                "authors": [
                    {
                        "name": "Egor Zverev"
                    },
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Soroush Tabesh"
                    },
                    {
                        "name": "Mario Fritz"
                    },
                    {
                        "name": "Christoph H. Lampert"
                    }
                ],
                "author_detail": {
                    "name": "Christoph H. Lampert"
                },
                "author": "Christoph H. Lampert",
                "arxiv_comment": "Published as a conference paper at ICLR 2025, GitHub:\n  https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed. 10 pages main\n  text, 30 pages in total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06833v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06833v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17974v2",
                "updated": "2025-01-31T16:06:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    6,
                    26,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-29T20:20:48Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    20,
                    20,
                    48,
                    2,
                    29,
                    0
                ],
                "title": "Think Smarter not Harder: Adaptive Reasoning with Inference Aware\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Smarter not Harder: Adaptive Reasoning with Inference Aware\n  Optimization"
                },
                "summary": "Solving mathematics problems has been an intriguing capability of large\nlanguage models, and many efforts have been made to improve reasoning by\nextending reasoning length, such as through self-correction and extensive long\nchain-of-thoughts. While promising in problem-solving, advanced long reasoning\nchain models exhibit an undesired single-modal behavior, where trivial\nquestions require unnecessarily tedious long chains of thought. In this work,\nwe propose a way to allow models to be aware of inference budgets by\nformulating it as utility maximization with respect to an inference budget\nconstraint, hence naming our algorithm Inference Budget-Constrained Policy\nOptimization (IBPO). In a nutshell, models fine-tuned through IBPO learn to\n``understand'' the difficulty of queries and allocate inference budgets to\nharder ones. With different inference budgets, our best models are able to have\na $4.14$\\% and $5.74$\\% absolute improvement ($8.08$\\% and $11.2$\\% relative\nimprovement) on MATH500 using $2.16$x and $4.32$x inference budgets\nrespectively, relative to LLaMA3.1 8B Instruct. These improvements are\napproximately $2$x those of self-consistency under the same budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving mathematics problems has been an intriguing capability of large\nlanguage models, and many efforts have been made to improve reasoning by\nextending reasoning length, such as through self-correction and extensive long\nchain-of-thoughts. While promising in problem-solving, advanced long reasoning\nchain models exhibit an undesired single-modal behavior, where trivial\nquestions require unnecessarily tedious long chains of thought. In this work,\nwe propose a way to allow models to be aware of inference budgets by\nformulating it as utility maximization with respect to an inference budget\nconstraint, hence naming our algorithm Inference Budget-Constrained Policy\nOptimization (IBPO). In a nutshell, models fine-tuned through IBPO learn to\n``understand'' the difficulty of queries and allocate inference budgets to\nharder ones. With different inference budgets, our best models are able to have\na $4.14$\\% and $5.74$\\% absolute improvement ($8.08$\\% and $11.2$\\% relative\nimprovement) on MATH500 using $2.16$x and $4.32$x inference budgets\nrespectively, relative to LLaMA3.1 8B Instruct. These improvements are\napproximately $2$x those of self-consistency under the same budgets."
                },
                "authors": [
                    {
                        "name": "Zishun Yu"
                    },
                    {
                        "name": "Tengyu Xu"
                    },
                    {
                        "name": "Di Jin"
                    },
                    {
                        "name": "Karthik Abinav Sankararaman"
                    },
                    {
                        "name": "Yun He"
                    },
                    {
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "name": "Zhouhao Zeng"
                    },
                    {
                        "name": "Eryk Helenowski"
                    },
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Sinong Wang"
                    },
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Han Fang"
                    }
                ],
                "author_detail": {
                    "name": "Han Fang"
                },
                "author": "Han Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04074v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04074v3",
                "updated": "2025-01-31T16:01:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    1,
                    37,
                    4,
                    31,
                    0
                ],
                "published": "2024-04-05T13:03:13Z",
                "published_parsed": [
                    2024,
                    4,
                    5,
                    13,
                    3,
                    13,
                    4,
                    96,
                    0
                ],
                "title": "DGP-LVM: Derivative Gaussian process latent variable models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DGP-LVM: Derivative Gaussian process latent variable models"
                },
                "summary": "We develop a framework for derivative Gaussian process latent variable models\n(DGP-LVMs) that can handle multi-dimensional output data using modified\nderivative covariance functions. The modifications account for complexities in\nthe underlying data generating process such as scaled derivatives, varying\ninformation across multiple output dimensions as well as interactions between\noutputs. Further, our framework provides uncertainty estimates for each latent\nvariable samples using Bayesian inference. Through extensive simulations, we\ndemonstrate that latent variable estimation accuracy can be drastically\nincreased by including derivative information due to our proposed covariance\nfunction modifications. The developments are motivated by a concrete biological\nresearch problem involving the estimation of the unobserved cellular ordering\nfrom single-cell RNA (scRNA) sequencing data for gene expression and its\ncorresponding derivative information known as RNA velocity. Since the RNA\nvelocity is only an estimate of the exact derivative information, the\nderivative covariance functions need to account for potential scale\ndifferences. In a real-world case study, we illustrate the application of\nDGP-LVMs to such scRNA sequencing data. While motivated by this biological\nproblem, our framework is generally applicable to all kinds of latent variable\nestimation problems involving derivative information irrespective of the field\nof study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a framework for derivative Gaussian process latent variable models\n(DGP-LVMs) that can handle multi-dimensional output data using modified\nderivative covariance functions. The modifications account for complexities in\nthe underlying data generating process such as scaled derivatives, varying\ninformation across multiple output dimensions as well as interactions between\noutputs. Further, our framework provides uncertainty estimates for each latent\nvariable samples using Bayesian inference. Through extensive simulations, we\ndemonstrate that latent variable estimation accuracy can be drastically\nincreased by including derivative information due to our proposed covariance\nfunction modifications. The developments are motivated by a concrete biological\nresearch problem involving the estimation of the unobserved cellular ordering\nfrom single-cell RNA (scRNA) sequencing data for gene expression and its\ncorresponding derivative information known as RNA velocity. Since the RNA\nvelocity is only an estimate of the exact derivative information, the\nderivative covariance functions need to account for potential scale\ndifferences. In a real-world case study, we illustrate the application of\nDGP-LVMs to such scRNA sequencing data. While motivated by this biological\nproblem, our framework is generally applicable to all kinds of latent variable\nestimation problems involving derivative information irrespective of the field\nof study."
                },
                "authors": [
                    {
                        "name": "Soham Mukherjee"
                    },
                    {
                        "name": "Manfred Claassen"
                    },
                    {
                        "name": "Paul-Christian Bürkner"
                    }
                ],
                "author_detail": {
                    "name": "Paul-Christian Bürkner"
                },
                "author": "Paul-Christian Bürkner",
                "arxiv_comment": "36 pages, 33 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04074v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04074v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19239v1",
                "updated": "2025-01-31T15:53:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    53,
                    14,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:53:14Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    53,
                    14,
                    4,
                    31,
                    0
                ],
                "title": "Multi-agent Multi-armed Bandit with Fully Heavy-tailed Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent Multi-armed Bandit with Fully Heavy-tailed Dynamics"
                },
                "summary": "We study decentralized multi-agent multi-armed bandits in fully heavy-tailed\nsettings, where clients communicate over sparse random graphs with heavy-tailed\ndegree distributions and observe heavy-tailed (homogeneous or heterogeneous)\nreward distributions with potentially infinite variance. The objective is to\nmaximize system performance by pulling the globally optimal arm with the\nhighest global reward mean across all clients. We are the first to address such\nfully heavy-tailed scenarios, which capture the dynamics and challenges in\ncommunication and inference among multiple clients in real-world systems. In\nhomogeneous settings, our algorithmic framework exploits hub-like structures\nunique to heavy-tailed graphs, allowing clients to aggregate rewards and reduce\nnoises via hub estimators when constructing UCB indices; under $M$ clients and\ndegree distributions with power-law index $\\alpha > 1$, our algorithm attains a\nregret bound (almost) of order $O(M^{1 -\\frac{1}{\\alpha}} \\log{T})$. Under\nheterogeneous rewards, clients synchronize by communicating with neighbors,\naggregating exchanged estimators in UCB indices; With our newly established\ninformation delay bounds on sparse random graphs, we prove a regret bound of\n$O(M \\log{T})$. Our results improve upon existing work, which only address\ntime-invariant connected graphs, or light-tailed dynamics in dense graphs and\nrewards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study decentralized multi-agent multi-armed bandits in fully heavy-tailed\nsettings, where clients communicate over sparse random graphs with heavy-tailed\ndegree distributions and observe heavy-tailed (homogeneous or heterogeneous)\nreward distributions with potentially infinite variance. The objective is to\nmaximize system performance by pulling the globally optimal arm with the\nhighest global reward mean across all clients. We are the first to address such\nfully heavy-tailed scenarios, which capture the dynamics and challenges in\ncommunication and inference among multiple clients in real-world systems. In\nhomogeneous settings, our algorithmic framework exploits hub-like structures\nunique to heavy-tailed graphs, allowing clients to aggregate rewards and reduce\nnoises via hub estimators when constructing UCB indices; under $M$ clients and\ndegree distributions with power-law index $\\alpha > 1$, our algorithm attains a\nregret bound (almost) of order $O(M^{1 -\\frac{1}{\\alpha}} \\log{T})$. Under\nheterogeneous rewards, clients synchronize by communicating with neighbors,\naggregating exchanged estimators in UCB indices; With our newly established\ninformation delay bounds on sparse random graphs, we prove a regret bound of\n$O(M \\log{T})$. Our results improve upon existing work, which only address\ntime-invariant connected graphs, or light-tailed dynamics in dense graphs and\nrewards."
                },
                "authors": [
                    {
                        "name": "Xingyu Wang"
                    },
                    {
                        "name": "Mengfan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mengfan Xu"
                },
                "author": "Mengfan Xu",
                "arxiv_comment": "40 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19232v1",
                "updated": "2025-01-31T15:43:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    43,
                    21,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:43:21Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    43,
                    21,
                    4,
                    31,
                    0
                ],
                "title": "A Zero-Shot Generalization Framework for LLM-Driven Cross-Domain\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zero-Shot Generalization Framework for LLM-Driven Cross-Domain\n  Sequential Recommendation"
                },
                "summary": "Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions\nin unseen domains without the need for additional training or fine-tuning,\nmaking it particularly valuable in data-sparse environments where traditional\nmodels struggle. Recent advancements in large language models (LLMs) have\ngreatly improved ZCDSR by leveraging rich pretrained representations to\nfacilitate cross-domain knowledge transfer. However, a key challenge persists:\ndomain semantic bias, which arises from variations in vocabulary and content\nfocus across domains. This misalignment leads to inconsistencies in item\nembeddings and hinders generalization.\n  To address this issue, we propose a novel framework designed to enhance\nLLM-based ZCDSR by improving cross-domain alignment at both the item and\nsequential levels. At the item level, we introduce a generalization loss that\npromotes inter-domain compactness by aligning embeddings of similar items\nacross domains while maintaining intra-domain diversity to preserve unique item\ncharacteristics. This prevents embeddings from becoming overly generic while\nensuring effective transferability. At the sequential level, we develop a\nmethod for transferring user behavioral patterns by clustering user sequences\nin the source domain and applying attention-based aggregation for target domain\ninference. This dynamic adaptation of user embeddings allows effective\nzero-shot recommendations without requiring target-domain interactions.\n  Comprehensive experiments across multiple datasets and domains demonstrate\nthat our framework significantly improves sequential recommendation performance\nin the ZCDSR setting. By mitigating domain bias and enhancing the\ntransferability of sequential patterns, our method provides a scalable and\nrobust approach for achieving more effective zero-shot recommendations across\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions\nin unseen domains without the need for additional training or fine-tuning,\nmaking it particularly valuable in data-sparse environments where traditional\nmodels struggle. Recent advancements in large language models (LLMs) have\ngreatly improved ZCDSR by leveraging rich pretrained representations to\nfacilitate cross-domain knowledge transfer. However, a key challenge persists:\ndomain semantic bias, which arises from variations in vocabulary and content\nfocus across domains. This misalignment leads to inconsistencies in item\nembeddings and hinders generalization.\n  To address this issue, we propose a novel framework designed to enhance\nLLM-based ZCDSR by improving cross-domain alignment at both the item and\nsequential levels. At the item level, we introduce a generalization loss that\npromotes inter-domain compactness by aligning embeddings of similar items\nacross domains while maintaining intra-domain diversity to preserve unique item\ncharacteristics. This prevents embeddings from becoming overly generic while\nensuring effective transferability. At the sequential level, we develop a\nmethod for transferring user behavioral patterns by clustering user sequences\nin the source domain and applying attention-based aggregation for target domain\ninference. This dynamic adaptation of user embeddings allows effective\nzero-shot recommendations without requiring target-domain interactions.\n  Comprehensive experiments across multiple datasets and domains demonstrate\nthat our framework significantly improves sequential recommendation performance\nin the ZCDSR setting. By mitigating domain bias and enhancing the\ntransferability of sequential patterns, our method provides a scalable and\nrobust approach for achieving more effective zero-shot recommendations across\ndomains."
                },
                "authors": [
                    {
                        "name": "Yunzhe Li"
                    },
                    {
                        "name": "Junting Wang"
                    },
                    {
                        "name": "Hari Sundaram"
                    },
                    {
                        "name": "Zhining Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhining Liu"
                },
                "author": "Zhining Liu",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18438v2",
                "updated": "2025-01-31T15:39:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    39,
                    0,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-30T15:45:56Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    45,
                    56,
                    3,
                    30,
                    0
                ],
                "title": "o3-mini vs DeepSeek-R1: Which One is Safer?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "o3-mini vs DeepSeek-R1: Which One is Safer?"
                },
                "summary": "The irruption of DeepSeek-R1 constitutes a turning point for the AI industry\nin general and the LLMs in particular. Its capabilities have demonstrated\noutstanding performance in several tasks, including creative thinking, code\ngeneration, maths and automated program repair, at apparently lower execution\ncost. However, LLMs must adhere to an important qualitative property, i.e.,\ntheir alignment with safety and human values. A clear competitor of DeepSeek-R1\nis its American counterpart, OpenAI's o3-mini model, which is expected to set\nhigh standards in terms of performance, safety and cost. In this technical\nreport, we systematically assess the safety level of both DeepSeek-R1 (70b\nversion) and OpenAI's o3-mini (beta version). To this end, we make use of our\nrecently released automated safety testing tool, named ASTRAL. By leveraging\nthis tool, we automatically and systematically generated and executed 1,260\ntest inputs on both models. After conducting a semi-automated assessment of the\noutcomes provided by both LLMs, the results indicate that DeepSeek-R1 produces\nsignificantly more unsafe responses (12%) than OpenAI's o3-mini (1.2%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The irruption of DeepSeek-R1 constitutes a turning point for the AI industry\nin general and the LLMs in particular. Its capabilities have demonstrated\noutstanding performance in several tasks, including creative thinking, code\ngeneration, maths and automated program repair, at apparently lower execution\ncost. However, LLMs must adhere to an important qualitative property, i.e.,\ntheir alignment with safety and human values. A clear competitor of DeepSeek-R1\nis its American counterpart, OpenAI's o3-mini model, which is expected to set\nhigh standards in terms of performance, safety and cost. In this technical\nreport, we systematically assess the safety level of both DeepSeek-R1 (70b\nversion) and OpenAI's o3-mini (beta version). To this end, we make use of our\nrecently released automated safety testing tool, named ASTRAL. By leveraging\nthis tool, we automatically and systematically generated and executed 1,260\ntest inputs on both models. After conducting a semi-automated assessment of the\noutcomes provided by both LLMs, the results indicate that DeepSeek-R1 produces\nsignificantly more unsafe responses (12%) than OpenAI's o3-mini (1.2%)."
                },
                "authors": [
                    {
                        "name": "Aitor Arrieta"
                    },
                    {
                        "name": "Miriam Ugarte"
                    },
                    {
                        "name": "Pablo Valle"
                    },
                    {
                        "name": "José Antonio Parejo"
                    },
                    {
                        "name": "Sergio Segura"
                    }
                ],
                "author_detail": {
                    "name": "Sergio Segura"
                },
                "author": "Sergio Segura",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2501.17749",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19223v1",
                "updated": "2025-01-31T15:30:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    30,
                    14,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:30:14Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    30,
                    14,
                    4,
                    31,
                    0
                ],
                "title": "Through the Looking Glass: LLM-Based Analysis of AR/VR Android\n  Applications Privacy Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through the Looking Glass: LLM-Based Analysis of AR/VR Android\n  Applications Privacy Policies"
                },
                "summary": "\\begin{abstract} This paper comprehensively analyzes privacy policies in\nAR/VR applications, leveraging BERT, a state-of-the-art text classification\nmodel, to evaluate the clarity and thoroughness of these policies. By comparing\nthe privacy policies of AR/VR applications with those of free and premium\nwebsites, this study provides a broad perspective on the current state of\nprivacy practices within the AR/VR industry. Our findings indicate that AR/VR\napplications generally offer a higher percentage of positive segments than free\ncontent but lower than premium websites. The analysis of highlighted segments\nand words revealed that AR/VR applications strategically emphasize critical\nprivacy practices and key terms. This enhances privacy policies' clarity and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\\begin{abstract} This paper comprehensively analyzes privacy policies in\nAR/VR applications, leveraging BERT, a state-of-the-art text classification\nmodel, to evaluate the clarity and thoroughness of these policies. By comparing\nthe privacy policies of AR/VR applications with those of free and premium\nwebsites, this study provides a broad perspective on the current state of\nprivacy practices within the AR/VR industry. Our findings indicate that AR/VR\napplications generally offer a higher percentage of positive segments than free\ncontent but lower than premium websites. The analysis of highlighted segments\nand words revealed that AR/VR applications strategically emphasize critical\nprivacy practices and key terms. This enhances privacy policies' clarity and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Abdulaziz Alghamdi"
                    },
                    {
                        "name": "David Mohaisen"
                    }
                ],
                "author_detail": {
                    "name": "David Mohaisen"
                },
                "author": "David Mohaisen",
                "arxiv_comment": "7 pages; appeared in ICMLA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06981v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06981v2",
                "updated": "2025-01-31T15:27:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    27,
                    10,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-09T15:18:57Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    18,
                    57,
                    2,
                    283,
                    0
                ],
                "title": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models"
                },
                "summary": "We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones, making it difficult\nto disentangle and match features across different models. To address this\nissue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics on SAE feature spaces across different LLMs. Our\nexperiments reveal significant similarities in SAE feature spaces across\nvarious LLMs, providing new evidence for feature universality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones, making it difficult\nto disentangle and match features across different models. To address this\nissue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics on SAE feature spaces across different LLMs. Our\nexperiments reveal significant similarities in SAE feature spaces across\nvarious LLMs, providing new evidence for feature universality."
                },
                "authors": [
                    {
                        "name": "Michael Lan"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Austin Meek"
                    },
                    {
                        "name": "Ashkan Khakzar"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Fazl Barez"
                    }
                ],
                "author_detail": {
                    "name": "Fazl Barez"
                },
                "author": "Fazl Barez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06981v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06981v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19213v1",
                "updated": "2025-01-31T15:18:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    18,
                    23,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:18:23Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    18,
                    23,
                    4,
                    31,
                    0
                ],
                "title": "Testing for the Minimum Mean-Variance Spanning Set",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing for the Minimum Mean-Variance Spanning Set"
                },
                "summary": "This paper explores the estimation and inference of the minimum spanning set\n(MSS), the smallest subset of risky assets that spans the mean-variance\nefficient frontier of the full asset set. We establish identification\nconditions for the MSS and develop a novel procedure for its estimation and\ninference. Our theoretical analysis shows that the proposed MSS estimator\ncovers the true MSS with probability approaching 1 and converges asymptotically\nto the true MSS at any desired confidence level, such as 0.95 or 0.99. Monte\nCarlo simulations confirm the strong finite-sample performance of the MSS\nestimator. We apply our method to evaluate the relative importance of\nindividual stock momentum and factor momentum strategies, along with a set of\nwell-established stock return factors. The empirical results highlight factor\nmomentum, along with several stock momentum and return factors, as key drivers\nof mean-variance efficiency. Furthermore, our analysis uncovers the sources of\ncontribution from these factors and provides a ranking of their relative\nimportance, offering new insights into their roles in mean-variance analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the estimation and inference of the minimum spanning set\n(MSS), the smallest subset of risky assets that spans the mean-variance\nefficient frontier of the full asset set. We establish identification\nconditions for the MSS and develop a novel procedure for its estimation and\ninference. Our theoretical analysis shows that the proposed MSS estimator\ncovers the true MSS with probability approaching 1 and converges asymptotically\nto the true MSS at any desired confidence level, such as 0.95 or 0.99. Monte\nCarlo simulations confirm the strong finite-sample performance of the MSS\nestimator. We apply our method to evaluate the relative importance of\nindividual stock momentum and factor momentum strategies, along with a set of\nwell-established stock return factors. The empirical results highlight factor\nmomentum, along with several stock momentum and return factors, as key drivers\nof mean-variance efficiency. Furthermore, our analysis uncovers the sources of\ncontribution from these factors and provides a ranking of their relative\nimportance, offering new insights into their roles in mean-variance analysis."
                },
                "authors": [
                    {
                        "name": "Zhipeng Liao"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Wenyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wenyu Zhou"
                },
                "author": "Wenyu Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19207v1",
                "updated": "2025-01-31T15:15:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    15,
                    8,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:15:08Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    15,
                    8,
                    4,
                    31,
                    0
                ],
                "title": "Learning Sheaf Laplacian Optimizing Restriction Maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Sheaf Laplacian Optimizing Restriction Maps"
                },
                "summary": "The aim of this paper is to propose a novel framework to infer the sheaf\nLaplacian, including the topology of a graph and the restriction maps, from a\nset of data observed over the nodes of a graph. The proposed method is based on\nsheaf theory, which represents an important generalization of graph signal\nprocessing. The learning problem aims to find the sheaf Laplacian that\nminimizes the total variation of the observed data, where the variation over\neach edge is also locally minimized by optimizing the associated restriction\nmaps. Compared to alternative methods based on semidefinite programming, our\nsolution is significantly more numerically efficient, as all its fundamental\nsteps are resolved in closed form. The method is numerically tested on data\nconsisting of vectors defined over subspaces of varying dimensions at each\nnode. We demonstrate how the resulting graph is influenced by two key factors:\nthe cross-correlation and the dimensionality difference of the data residing on\nthe graph's nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The aim of this paper is to propose a novel framework to infer the sheaf\nLaplacian, including the topology of a graph and the restriction maps, from a\nset of data observed over the nodes of a graph. The proposed method is based on\nsheaf theory, which represents an important generalization of graph signal\nprocessing. The learning problem aims to find the sheaf Laplacian that\nminimizes the total variation of the observed data, where the variation over\neach edge is also locally minimized by optimizing the associated restriction\nmaps. Compared to alternative methods based on semidefinite programming, our\nsolution is significantly more numerically efficient, as all its fundamental\nsteps are resolved in closed form. The method is numerically tested on data\nconsisting of vectors defined over subspaces of varying dimensions at each\nnode. We demonstrate how the resulting graph is influenced by two key factors:\nthe cross-correlation and the dimensionality difference of the data residing on\nthe graph's nodes."
                },
                "authors": [
                    {
                        "name": "Leonardo Di Nino"
                    },
                    {
                        "name": "Sergio Barbarossa"
                    },
                    {
                        "name": "Paolo Di Lorenzo"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Di Lorenzo"
                },
                "author": "Paolo Di Lorenzo",
                "arxiv_comment": "Proc. 58th Annual Asilomar Conference on Signals, Systems, and\n  Computers (Asilomar), Pacific Grove, CA, Oct. 27 - Oct. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19204v1",
                "updated": "2025-01-31T15:14:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    14,
                    14,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:14:14Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    14,
                    14,
                    4,
                    31,
                    0
                ],
                "title": "Autonomous Legacy Web Application Upgrades Using a Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Legacy Web Application Upgrades Using a Multi-Agent System"
                },
                "summary": "The use of Large Language Models (LLMs) for autonomous code generation is\ngaining attention in emerging technologies. As LLM capabilities expand, they\noffer new possibilities such as code refactoring, security enhancements, and\nlegacy application upgrades. Many outdated web applications pose security and\nreliability challenges, yet companies continue using them due to the complexity\nand cost of upgrades. To address this, we propose an LLM-based multi-agent\nsystem that autonomously upgrades legacy web applications to the latest\nversions. The system distributes tasks across multiple phases, updating all\nrelevant files. To evaluate its effectiveness, we employed Zero-Shot Learning\n(ZSL) and One-Shot Learning (OSL) prompts, applying identical instructions in\nboth cases. The evaluation involved updating view files and measuring the\nnumber and types of errors in the output. For complex tasks, we counted the\nsuccessfully met requirements. The experiments compared the proposed system\nwith standalone LLM execution, repeated multiple times to account for\nstochastic behavior. Results indicate that our system maintains context across\ntasks and agents, improving solution quality over the base model in some cases.\nThis study provides a foundation for future model implementations in legacy\ncode updates. Additionally, findings highlight LLMs' ability to update small\noutdated files with high precision, even with basic prompts. The source code is\npublicly available on GitHub: https://github.com/alasalm1/Multi-agent-pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) for autonomous code generation is\ngaining attention in emerging technologies. As LLM capabilities expand, they\noffer new possibilities such as code refactoring, security enhancements, and\nlegacy application upgrades. Many outdated web applications pose security and\nreliability challenges, yet companies continue using them due to the complexity\nand cost of upgrades. To address this, we propose an LLM-based multi-agent\nsystem that autonomously upgrades legacy web applications to the latest\nversions. The system distributes tasks across multiple phases, updating all\nrelevant files. To evaluate its effectiveness, we employed Zero-Shot Learning\n(ZSL) and One-Shot Learning (OSL) prompts, applying identical instructions in\nboth cases. The evaluation involved updating view files and measuring the\nnumber and types of errors in the output. For complex tasks, we counted the\nsuccessfully met requirements. The experiments compared the proposed system\nwith standalone LLM execution, repeated multiple times to account for\nstochastic behavior. Results indicate that our system maintains context across\ntasks and agents, improving solution quality over the base model in some cases.\nThis study provides a foundation for future model implementations in legacy\ncode updates. Additionally, findings highlight LLMs' ability to update small\noutdated files with high precision, even with basic prompts. The source code is\npublicly available on GitHub: https://github.com/alasalm1/Multi-agent-pipeline."
                },
                "authors": [
                    {
                        "name": "Valtteri Ala-Salmi"
                    },
                    {
                        "name": "Zeeshan Rasheed"
                    },
                    {
                        "name": "Abdul Malik Sami"
                    },
                    {
                        "name": "Zheying Zhang"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Jussi Rasku"
                    },
                    {
                        "name": "Shahbaz Siddeeq"
                    },
                    {
                        "name": "Mika Saari"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_comment": "13 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19202v1",
                "updated": "2025-01-31T15:12:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    12,
                    20,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:12:20Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    12,
                    20,
                    4,
                    31,
                    0
                ],
                "title": "Improving the Robustness of Representation Misdirection for Large\n  Language Model Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Robustness of Representation Misdirection for Large\n  Language Model Unlearning"
                },
                "summary": "Representation Misdirection (RM) and variants are established large language\nmodel (LLM) unlearning methods with state-of-the-art performance. In this\npaper, we show that RM methods inherently reduce models' robustness, causing\nthem to misbehave even when a single non-adversarial forget-token is in the\nretain-query. Toward understanding underlying causes, we reframe the unlearning\nprocess as backdoor attacks and defenses: forget-tokens act as backdoor\ntriggers that, when activated in retain-queries, cause disruptions in RM\nmodels' behaviors, similar to successful backdoor attacks. To mitigate this\nvulnerability, we propose Random Noise Augmentation -- a model and method\nagnostic approach with theoretical guarantees for improving the robustness of\nRM methods. Extensive experiments demonstrate that RNA significantly improves\nthe robustness of RM models while enhancing the unlearning performances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Misdirection (RM) and variants are established large language\nmodel (LLM) unlearning methods with state-of-the-art performance. In this\npaper, we show that RM methods inherently reduce models' robustness, causing\nthem to misbehave even when a single non-adversarial forget-token is in the\nretain-query. Toward understanding underlying causes, we reframe the unlearning\nprocess as backdoor attacks and defenses: forget-tokens act as backdoor\ntriggers that, when activated in retain-queries, cause disruptions in RM\nmodels' behaviors, similar to successful backdoor attacks. To mitigate this\nvulnerability, we propose Random Noise Augmentation -- a model and method\nagnostic approach with theoretical guarantees for improving the robustness of\nRM methods. Extensive experiments demonstrate that RNA significantly improves\nthe robustness of RM models while enhancing the unlearning performances."
                },
                "authors": [
                    {
                        "name": "Dang Huu-Tien"
                    },
                    {
                        "name": "Hoang Thanh-Tung"
                    },
                    {
                        "name": "Le-Minh Nguyen"
                    },
                    {
                        "name": "Naoya Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Naoya Inoue"
                },
                "author": "Naoya Inoue",
                "arxiv_comment": "12 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02307v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02307v2",
                "updated": "2025-01-31T15:11:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    11,
                    28,
                    4,
                    31,
                    0
                ],
                "published": "2024-07-02T14:41:58Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    14,
                    41,
                    58,
                    1,
                    184,
                    0
                ],
                "title": "Calibrating the medium effects of light clusters in heavy-ion collisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating the medium effects of light clusters in heavy-ion collisions"
                },
                "summary": "We propose a Bayesian inference estimation of in-medium modification of the\ncluster self-energies from light nuclei multiplicities measured in selected\nsamples of central $^{136,124}$Xe$+^{124,112}$Sn collisions with the INDRA\napparatus. The data are interpreted with a relativistic quasi-particle cluster\napproach in the mean-field approximation without any prior assumption on the\nthermal parameters of the model. An excellent reproduction is obtained for H\nand He isotope multiplicities, and compatible posterior distributions are found\nfor the unknown thermal parameters.\n  We conclude that the cluster-$\\sigma$-meson coupling is temperature\ndependent, becoming weaker when the temperature increases, in agreement with\nmicroscopic quantum statistical calculations. This implies a faster decrease of\nthe light cluster abundances with temperature than previously estimated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a Bayesian inference estimation of in-medium modification of the\ncluster self-energies from light nuclei multiplicities measured in selected\nsamples of central $^{136,124}$Xe$+^{124,112}$Sn collisions with the INDRA\napparatus. The data are interpreted with a relativistic quasi-particle cluster\napproach in the mean-field approximation without any prior assumption on the\nthermal parameters of the model. An excellent reproduction is obtained for H\nand He isotope multiplicities, and compatible posterior distributions are found\nfor the unknown thermal parameters.\n  We conclude that the cluster-$\\sigma$-meson coupling is temperature\ndependent, becoming weaker when the temperature increases, in agreement with\nmicroscopic quantum statistical calculations. This implies a faster decrease of\nthe light cluster abundances with temperature than previously estimated."
                },
                "authors": [
                    {
                        "name": "Tiago Custódio"
                    },
                    {
                        "name": "Alex Rebillard-Soulié"
                    },
                    {
                        "name": "Rémi Bougault"
                    },
                    {
                        "name": "Diego Gruyer"
                    },
                    {
                        "name": "Francesca Gulminelli"
                    },
                    {
                        "name": "Tuhin Malik"
                    },
                    {
                        "name": "Helena Pais"
                    },
                    {
                        "name": "Constança Providência"
                    }
                ],
                "author_detail": {
                    "name": "Constança Providência"
                },
                "author": "Constança Providência",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02307v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02307v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19201v1",
                "updated": "2025-01-31T15:10:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    10,
                    29,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:10:29Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    10,
                    29,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Reasoning with Hidden Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Reasoning with Hidden Thinking"
                },
                "summary": "Chain-of-Thought (CoT) reasoning has become a powerful framework for\nimproving complex problem-solving capabilities in Multimodal Large Language\nModels (MLLMs). However, the verbose nature of textual reasoning introduces\nsignificant inefficiencies. In this work, we propose $\\textbf{Heima}$ (as\nhidden llama), an efficient reasoning framework that leverages reasoning CoTs\nat hidden latent space. We design the Heima Encoder to condense each\nintermediate CoT into a compact, higher-level hidden representation using a\nsingle thinking token, effectively minimizing verbosity and reducing the\noverall number of tokens required during the reasoning process. Meanwhile, we\ndesign corresponding Heima Decoder with traditional Large Language Models\n(LLMs) to adaptively interpret the hidden representations into variable-length\ntextual sequence, reconstructing reasoning processes that closely resemble the\noriginal CoTs. Experimental results across diverse reasoning MLLM benchmarks\ndemonstrate that Heima model achieves higher generation efficiency while\nmaintaining or even better zero-shot task accuracy. Moreover, the effective\nreconstruction of multimodal reasoning processes with Heima Decoder validates\nboth the robustness and interpretability of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning has become a powerful framework for\nimproving complex problem-solving capabilities in Multimodal Large Language\nModels (MLLMs). However, the verbose nature of textual reasoning introduces\nsignificant inefficiencies. In this work, we propose $\\textbf{Heima}$ (as\nhidden llama), an efficient reasoning framework that leverages reasoning CoTs\nat hidden latent space. We design the Heima Encoder to condense each\nintermediate CoT into a compact, higher-level hidden representation using a\nsingle thinking token, effectively minimizing verbosity and reducing the\noverall number of tokens required during the reasoning process. Meanwhile, we\ndesign corresponding Heima Decoder with traditional Large Language Models\n(LLMs) to adaptively interpret the hidden representations into variable-length\ntextual sequence, reconstructing reasoning processes that closely resemble the\noriginal CoTs. Experimental results across diverse reasoning MLLM benchmarks\ndemonstrate that Heima model achieves higher generation efficiency while\nmaintaining or even better zero-shot task accuracy. Moreover, the effective\nreconstruction of multimodal reasoning processes with Heima Decoder validates\nboth the robustness and interpretability of our approach."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Yizhou Wang"
                    },
                    {
                        "name": "Xiangxi Shi"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.01879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.01879v2",
                "updated": "2025-01-31T15:10:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    10,
                    21,
                    4,
                    31,
                    0
                ],
                "published": "2024-01-03T18:39:13Z",
                "published_parsed": [
                    2024,
                    1,
                    3,
                    18,
                    39,
                    13,
                    2,
                    3,
                    0
                ],
                "title": "Theoretical guarantees on the best-of-n alignment policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical guarantees on the best-of-n alignment policy"
                },
                "summary": "A simple and effective method for the inference-time alignment of generative\nmodels is the best-of-$n$ policy, where $n$ samples are drawn from a reference\npolicy, ranked based on a reward function, and the highest ranking one is\nselected. A commonly used analytical expression in the literature claims that\nthe KL divergence between the best-of-$n$ policy and the reference policy is\nequal to $\\log (n) - (n-1)/n.$ We disprove the validity of this claim, and show\nthat it is an upper bound on the actual KL divergence. We also explore the\ntightness of this upper bound in different regimes, and propose a new estimator\nfor the KL divergence and empirically show that it provides a tight\napproximation. We also show that the win rate of the best-of-$n$ policy against\nthe reference policy is upper bounded by $n/(n+1)$ and derive bounds on the\ntightness of this characterization. We conclude with analyzing the tradeoffs\nbetween win rate and KL divergence of the best-of-$n$ alignment policy, which\ndemonstrate that very good tradeoffs are achievable with $n < 1000$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A simple and effective method for the inference-time alignment of generative\nmodels is the best-of-$n$ policy, where $n$ samples are drawn from a reference\npolicy, ranked based on a reward function, and the highest ranking one is\nselected. A commonly used analytical expression in the literature claims that\nthe KL divergence between the best-of-$n$ policy and the reference policy is\nequal to $\\log (n) - (n-1)/n.$ We disprove the validity of this claim, and show\nthat it is an upper bound on the actual KL divergence. We also explore the\ntightness of this upper bound in different regimes, and propose a new estimator\nfor the KL divergence and empirically show that it provides a tight\napproximation. We also show that the win rate of the best-of-$n$ policy against\nthe reference policy is upper bounded by $n/(n+1)$ and derive bounds on the\ntightness of this characterization. We conclude with analyzing the tradeoffs\nbetween win rate and KL divergence of the best-of-$n$ alignment policy, which\ndemonstrate that very good tradeoffs are achievable with $n < 1000$."
                },
                "authors": [
                    {
                        "name": "Ahmad Beirami"
                    },
                    {
                        "name": "Alekh Agarwal"
                    },
                    {
                        "name": "Jonathan Berant"
                    },
                    {
                        "name": "Alexander D'Amour"
                    },
                    {
                        "name": "Jacob Eisenstein"
                    },
                    {
                        "name": "Chirag Nagpal"
                    },
                    {
                        "name": "Ananda Theertha Suresh"
                    }
                ],
                "author_detail": {
                    "name": "Ananda Theertha Suresh"
                },
                "author": "Ananda Theertha Suresh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.01879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.01879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17284v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17284v4",
                "updated": "2025-01-31T15:04:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    4,
                    34,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-26T10:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    13,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "AutoElicit: Using Large Language Models for Expert Prior Elicitation in\n  Predictive Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoElicit: Using Large Language Models for Expert Prior Elicitation in\n  Predictive Modelling"
                },
                "summary": "Large language models (LLMs) acquire a breadth of information across various\ndomains. However, their computational complexity, cost, and lack of\ntransparency often hinder their direct application for predictive tasks where\nprivacy and interpretability are paramount. In fields such as healthcare,\nbiology, and finance, specialised and interpretable linear models still hold\nconsiderable value. In such domains, labelled data may be scarce or expensive\nto obtain. Well-specified prior distributions over model parameters can reduce\nthe sample complexity of learning through Bayesian inference; however,\neliciting expert priors can be time-consuming. We therefore introduce\nAutoElicit to extract knowledge from LLMs and construct priors for predictive\nmodels. We show these priors are informative and can be refined using natural\nlanguage. We perform a careful study contrasting AutoElicit with in-context\nlearning and demonstrate how to perform model selection between the two\nmethods. We find that AutoElicit yields priors that can substantially reduce\nerror over uninformative priors, using fewer labels, and consistently\noutperform in-context learning. We show that AutoElicit saves over 6 months of\nlabelling effort when building a new predictive model for urinary tract\ninfections from sensor recordings of people living with dementia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) acquire a breadth of information across various\ndomains. However, their computational complexity, cost, and lack of\ntransparency often hinder their direct application for predictive tasks where\nprivacy and interpretability are paramount. In fields such as healthcare,\nbiology, and finance, specialised and interpretable linear models still hold\nconsiderable value. In such domains, labelled data may be scarce or expensive\nto obtain. Well-specified prior distributions over model parameters can reduce\nthe sample complexity of learning through Bayesian inference; however,\neliciting expert priors can be time-consuming. We therefore introduce\nAutoElicit to extract knowledge from LLMs and construct priors for predictive\nmodels. We show these priors are informative and can be refined using natural\nlanguage. We perform a careful study contrasting AutoElicit with in-context\nlearning and demonstrate how to perform model selection between the two\nmethods. We find that AutoElicit yields priors that can substantially reduce\nerror over uninformative priors, using fewer labels, and consistently\noutperform in-context learning. We show that AutoElicit saves over 6 months of\nlabelling effort when building a new predictive model for urinary tract\ninfections from sensor recordings of people living with dementia."
                },
                "authors": [
                    {
                        "name": "Alexander Capstick"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Payam Barnaghi"
                    }
                ],
                "author_detail": {
                    "name": "Payam Barnaghi"
                },
                "author": "Payam Barnaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17284v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17284v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.03100v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.03100v4",
                "updated": "2025-01-31T14:59:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    59,
                    17,
                    4,
                    31,
                    0
                ],
                "published": "2023-06-01T00:01:43Z",
                "published_parsed": [
                    2023,
                    6,
                    1,
                    0,
                    1,
                    43,
                    3,
                    152,
                    0
                ],
                "title": "Rethinking Model Evaluation as Narrowing the Socio-Technical Gap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Model Evaluation as Narrowing the Socio-Technical Gap"
                },
                "summary": "The recent development of generative large language models (LLMs) poses new\nchallenges for model evaluation that the research community and industry have\nbeen grappling with. While the versatile capabilities of these models ignite\nmuch excitement, they also inevitably make a leap toward homogenization:\npowering a wide range of applications with a single, often referred to as\n``general-purpose'', model. In this position paper, we argue that model\nevaluation practices must take on a critical task to cope with the challenges\nand responsibilities brought by this homogenization: providing valid\nassessments for whether and how much human needs in diverse downstream use\ncases can be satisfied by the given model (\\textit{socio-technical gap}). By\ndrawing on lessons about improving research realism from the social sciences,\nhuman-computer interaction (HCI), and the interdisciplinary field of\nexplainable AI (XAI), we urge the community to develop evaluation methods based\non real-world contexts and human requirements, and embrace diverse evaluation\nmethods with an acknowledgment of trade-offs between realisms and pragmatic\ncosts to conduct the evaluation. By mapping HCI and current NLG evaluation\nmethods, we identify opportunities for evaluation methods for LLMs to narrow\nthe socio-technical gap and pose open questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development of generative large language models (LLMs) poses new\nchallenges for model evaluation that the research community and industry have\nbeen grappling with. While the versatile capabilities of these models ignite\nmuch excitement, they also inevitably make a leap toward homogenization:\npowering a wide range of applications with a single, often referred to as\n``general-purpose'', model. In this position paper, we argue that model\nevaluation practices must take on a critical task to cope with the challenges\nand responsibilities brought by this homogenization: providing valid\nassessments for whether and how much human needs in diverse downstream use\ncases can be satisfied by the given model (\\textit{socio-technical gap}). By\ndrawing on lessons about improving research realism from the social sciences,\nhuman-computer interaction (HCI), and the interdisciplinary field of\nexplainable AI (XAI), we urge the community to develop evaluation methods based\non real-world contexts and human requirements, and embrace diverse evaluation\nmethods with an acknowledgment of trade-offs between realisms and pragmatic\ncosts to conduct the evaluation. By mapping HCI and current NLG evaluation\nmethods, we identify opportunities for evaluation methods for LLMs to narrow\nthe socio-technical gap and pose open questions."
                },
                "authors": [
                    {
                        "name": "Q. Vera Liao"
                    },
                    {
                        "name": "Ziang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Ziang Xiao"
                },
                "author": "Ziang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.03100v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.03100v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19188v1",
                "updated": "2025-01-31T14:54:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    54,
                    25,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T14:54:25Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    54,
                    25,
                    4,
                    31,
                    0
                ],
                "title": "FAUST XX. The chemical structure and temperature profile of the IRAS 4A2\n  hot corino at 20-50 au",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAUST XX. The chemical structure and temperature profile of the IRAS 4A2\n  hot corino at 20-50 au"
                },
                "summary": "Young low-mass protostars often possess hot corinos, compact, hot and dense\nregions bright in interstellar Complex Organic Molecules (iCOMs). Besides of\ntheir prebiotic role, iCOMs can be used as a powerful tool to characterize the\nchemical and physical properties of hot corinos. Using ALMA/FAUST data we aim\nto explore the iCOMs emission at < 50 au scale around the Class 0 prototypical\nhot corino IRAS 4A2. We imaged IRAS 4A2 in six abundant, common iCOMs\n(CH$_3$OH, HCOOCH$_3$, CH$_3$CHO, CH$_3$CH$_2$OH, CH$_2$OHCHO, and NH$_2$CHO),\nand derived their emitting size. The column density and gas temperature for\neach species were derived at 1$\\sigma$ from a multi-line analysis by applying a\nnon-LTE approach for CH$_3$OH, and LTE population or rotational diagram\nanalysis for the other iCOMs. Thanks to the unique estimates of the absorption\nfrom foreground millimeter dust toward IRAS 4A2, we derived for the first time\nunbiased gas temperatures and column densities. We resolved the IRAS 4A2 hot\ncorino finding evidence for a chemical spatial distribution in the inner 50 au,\nwith the outer emitting radius increasing from ~ 22-23 au for NH$_2$CHO and\nCH$_2$OHCHO, followed by CH$_3$CH$_2$OH (~ 27 au), CH$_3$CHO (~ 28 au),\nHCOOCH$_3$ (~ 36 au), and out to ~ 40 au for CH$_3$OH. Combining our estimate\nof the gas temperature probed by each iCOM with their beam-deconvolved emission\nsizes, we inferred the gas temperature profile of the hot corino on scales of\n20-50 au in radius, finding a power-law index $q$ of approximately -1. We\nobserved, for the first time, a chemical segregation in iCOMs of the IRAS 4A2\nhot corino, and derived the gas temperature profile of its inner envelope. The\nderived profile is steeper than when considering a simple spherical collapsing\nand optically-thin envelope, hinting at a partially optically-thick envelope or\na gravitationally unstable disk-like structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Young low-mass protostars often possess hot corinos, compact, hot and dense\nregions bright in interstellar Complex Organic Molecules (iCOMs). Besides of\ntheir prebiotic role, iCOMs can be used as a powerful tool to characterize the\nchemical and physical properties of hot corinos. Using ALMA/FAUST data we aim\nto explore the iCOMs emission at < 50 au scale around the Class 0 prototypical\nhot corino IRAS 4A2. We imaged IRAS 4A2 in six abundant, common iCOMs\n(CH$_3$OH, HCOOCH$_3$, CH$_3$CHO, CH$_3$CH$_2$OH, CH$_2$OHCHO, and NH$_2$CHO),\nand derived their emitting size. The column density and gas temperature for\neach species were derived at 1$\\sigma$ from a multi-line analysis by applying a\nnon-LTE approach for CH$_3$OH, and LTE population or rotational diagram\nanalysis for the other iCOMs. Thanks to the unique estimates of the absorption\nfrom foreground millimeter dust toward IRAS 4A2, we derived for the first time\nunbiased gas temperatures and column densities. We resolved the IRAS 4A2 hot\ncorino finding evidence for a chemical spatial distribution in the inner 50 au,\nwith the outer emitting radius increasing from ~ 22-23 au for NH$_2$CHO and\nCH$_2$OHCHO, followed by CH$_3$CH$_2$OH (~ 27 au), CH$_3$CHO (~ 28 au),\nHCOOCH$_3$ (~ 36 au), and out to ~ 40 au for CH$_3$OH. Combining our estimate\nof the gas temperature probed by each iCOM with their beam-deconvolved emission\nsizes, we inferred the gas temperature profile of the hot corino on scales of\n20-50 au in radius, finding a power-law index $q$ of approximately -1. We\nobserved, for the first time, a chemical segregation in iCOMs of the IRAS 4A2\nhot corino, and derived the gas temperature profile of its inner envelope. The\nderived profile is steeper than when considering a simple spherical collapsing\nand optically-thin envelope, hinting at a partially optically-thick envelope or\na gravitationally unstable disk-like structure."
                },
                "authors": [
                    {
                        "name": "J. Frediani"
                    },
                    {
                        "name": "M. De Simone"
                    },
                    {
                        "name": "L. Testi"
                    },
                    {
                        "name": "L. Podio"
                    },
                    {
                        "name": "C. Codella"
                    },
                    {
                        "name": "C. J. Chandler"
                    },
                    {
                        "name": "C. Ceccarelli"
                    },
                    {
                        "name": "L. Loinard"
                    },
                    {
                        "name": "A. López-Sepulcre"
                    },
                    {
                        "name": "B. Svoboda"
                    },
                    {
                        "name": "N. Sakai"
                    },
                    {
                        "name": "L. Chahine"
                    },
                    {
                        "name": "Y. Aikawa"
                    },
                    {
                        "name": "E. Bianchi"
                    },
                    {
                        "name": "M. Bouvier"
                    },
                    {
                        "name": "L. Cacciapuoti"
                    },
                    {
                        "name": "P. Caselli"
                    },
                    {
                        "name": "S. B. Charnley"
                    },
                    {
                        "name": "I. Jimenez-Serra"
                    },
                    {
                        "name": "D. Johnstone"
                    },
                    {
                        "name": "G. Sabatini"
                    },
                    {
                        "name": "Y. Shirley"
                    },
                    {
                        "name": "S. Yamamoto"
                    }
                ],
                "author_detail": {
                    "name": "S. Yamamoto"
                },
                "author": "S. Yamamoto",
                "arxiv_comment": "21 pages, 16 figures, 4 tables. Accepted in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19180v1",
                "updated": "2025-01-31T14:45:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    45,
                    23,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T14:45:23Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    45,
                    23,
                    4,
                    31,
                    0
                ],
                "title": "Enhancing Model Defense Against Jailbreaks with Proactive Safety\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Model Defense Against Jailbreaks with Proactive Safety\n  Reasoning"
                },
                "summary": "Large language models (LLMs) are vital for a wide range of applications yet\nremain susceptible to jailbreak threats, which could lead to the generation of\ninappropriate responses. Conventional defenses, such as refusal and adversarial\ntraining, often fail to cover corner cases or rare domains, leaving LLMs still\nvulnerable to more sophisticated attacks. We propose a novel defense strategy,\nSafety Chain-of-Thought (SCoT), which harnesses the enhanced \\textit{reasoning\ncapabilities} of LLMs for proactive assessment of harmful inputs, rather than\nsimply blocking them. SCoT augments any refusal training datasets to critically\nanalyze the intent behind each request before generating answers. By employing\nproactive reasoning, SCoT enhances the generalization of LLMs across varied\nharmful queries and scenarios not covered in the safety alignment corpus.\nAdditionally, it generates detailed refusals specifying the rules violated.\nComparative evaluations show that SCoT significantly surpasses existing\ndefenses, reducing vulnerability to out-of-distribution issues and adversarial\nmanipulations while maintaining strong general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are vital for a wide range of applications yet\nremain susceptible to jailbreak threats, which could lead to the generation of\ninappropriate responses. Conventional defenses, such as refusal and adversarial\ntraining, often fail to cover corner cases or rare domains, leaving LLMs still\nvulnerable to more sophisticated attacks. We propose a novel defense strategy,\nSafety Chain-of-Thought (SCoT), which harnesses the enhanced \\textit{reasoning\ncapabilities} of LLMs for proactive assessment of harmful inputs, rather than\nsimply blocking them. SCoT augments any refusal training datasets to critically\nanalyze the intent behind each request before generating answers. By employing\nproactive reasoning, SCoT enhances the generalization of LLMs across varied\nharmful queries and scenarios not covered in the safety alignment corpus.\nAdditionally, it generates detailed refusals specifying the rules violated.\nComparative evaluations show that SCoT significantly surpasses existing\ndefenses, reducing vulnerability to out-of-distribution issues and adversarial\nmanipulations while maintaining strong general capabilities."
                },
                "authors": [
                    {
                        "name": "Xianglin Yang"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Jieming Shi"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Jin Song Dong"
                    }
                ],
                "author_detail": {
                    "name": "Jin Song Dong"
                },
                "author": "Jin Song Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19173v1",
                "updated": "2025-01-31T14:39:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    39,
                    30,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T14:39:30Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    39,
                    30,
                    4,
                    31,
                    0
                ],
                "title": "Position: Contextual Integrity Washing for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Contextual Integrity Washing for Language Models"
                },
                "summary": "Machine learning community is discovering Contextual Integrity (CI) as a\nuseful framework to assess the privacy implications of large language models\n(LLMs). This is an encouraging development. The CI theory emphasizes sharing\ninformation in accordance with privacy norms and can bridge the social, legal,\npolitical, and technical aspects essential for evaluating privacy in LLMs.\nHowever, this is also a good point to reflect on use of CI for LLMs. This\nposition paper argues that existing literature adopts CI for LLMs without\nembracing the theory's fundamental tenets, essentially amounting to a form of\n\"CI-washing.\" CI-washing could lead to incorrect conclusions and flawed\nprivacy-preserving designs. We clarify the four fundamental tenets of CI\ntheory, systematize prior work on whether they deviate from these tenets, and\nhighlight overlooked issues in experimental hygiene for LLMs (e.g., prompt\nsensitivity, positional bias).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning community is discovering Contextual Integrity (CI) as a\nuseful framework to assess the privacy implications of large language models\n(LLMs). This is an encouraging development. The CI theory emphasizes sharing\ninformation in accordance with privacy norms and can bridge the social, legal,\npolitical, and technical aspects essential for evaluating privacy in LLMs.\nHowever, this is also a good point to reflect on use of CI for LLMs. This\nposition paper argues that existing literature adopts CI for LLMs without\nembracing the theory's fundamental tenets, essentially amounting to a form of\n\"CI-washing.\" CI-washing could lead to incorrect conclusions and flawed\nprivacy-preserving designs. We clarify the four fundamental tenets of CI\ntheory, systematize prior work on whether they deviate from these tenets, and\nhighlight overlooked issues in experimental hygiene for LLMs (e.g., prompt\nsensitivity, positional bias)."
                },
                "authors": [
                    {
                        "name": "Yan Shvartzshnaider"
                    },
                    {
                        "name": "Vasisht Duddu"
                    }
                ],
                "author_detail": {
                    "name": "Vasisht Duddu"
                },
                "author": "Vasisht Duddu",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11182v2",
                "updated": "2025-01-31T14:36:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    36,
                    14,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-15T02:00:36Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    2,
                    0,
                    36,
                    1,
                    289,
                    0
                ],
                "title": "Position: On-Premises LLM Deployment Demands a Middle Path: Preserving\n  Privacy Without Sacrificing Model Confidentiality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: On-Premises LLM Deployment Demands a Middle Path: Preserving\n  Privacy Without Sacrificing Model Confidentiality"
                },
                "summary": "Current LLM customization typically relies on two deployment strategies:\nclosed-source APIs, which require users to upload private data to external\nservers, and open-weight models, which allow local fine-tuning but pose misuse\nrisks. In this position paper, we argue that (1) deploying closed-source LLMs\nwithin user-controlled infrastructure (\\textit{on-premises deployment})\nenhances data privacy and mitigates misuse risks, and (2) a well-designed\non-premises deployment must ensure model confidentiality -- by preventing model\ntheft -- and offer privacy-preserving customization. Prior research on small\nmodels has explored securing only the output layer within hardware-secured\ndevices to balance confidentiality and customization efficiency. However, we\nshow that this approach is insufficient for defending large-scale LLMs against\ndistillation attacks. We therefore introduce a {semi-open deployment framework}\nthat secures only a few, carefully chosen layers, achieving distillation\nresistance comparable to fully secured models while preserving fine-tuning\nflexibility. Through extensive experiments, we show that securing bottom layers\nsignificantly reduces functional extraction risks. Our findings demonstrate\nthat privacy and confidentiality can coexist, paving the way for secure\non-premises AI deployment that balances usability and protection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLM customization typically relies on two deployment strategies:\nclosed-source APIs, which require users to upload private data to external\nservers, and open-weight models, which allow local fine-tuning but pose misuse\nrisks. In this position paper, we argue that (1) deploying closed-source LLMs\nwithin user-controlled infrastructure (\\textit{on-premises deployment})\nenhances data privacy and mitigates misuse risks, and (2) a well-designed\non-premises deployment must ensure model confidentiality -- by preventing model\ntheft -- and offer privacy-preserving customization. Prior research on small\nmodels has explored securing only the output layer within hardware-secured\ndevices to balance confidentiality and customization efficiency. However, we\nshow that this approach is insufficient for defending large-scale LLMs against\ndistillation attacks. We therefore introduce a {semi-open deployment framework}\nthat secures only a few, carefully chosen layers, achieving distillation\nresistance comparable to fully secured models while preserving fine-tuning\nflexibility. Through extensive experiments, we show that securing bottom layers\nsignificantly reduces functional extraction risks. Our findings demonstrate\nthat privacy and confidentiality can coexist, paving the way for secure\non-premises AI deployment that balances usability and protection."
                },
                "authors": [
                    {
                        "name": "Hanbo Huang"
                    },
                    {
                        "name": "Yihan Li"
                    },
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Ruoyu Sun"
                    },
                    {
                        "name": "Zhuotao Liu"
                    },
                    {
                        "name": "Shiyu Liang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Liang"
                },
                "author": "Shiyu Liang",
                "arxiv_comment": "8 pages for main content of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19164v1",
                "updated": "2025-01-31T14:31:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    31,
                    0,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T14:31:00Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    31,
                    0,
                    4,
                    31,
                    0
                ],
                "title": "Poison as Cure: Visual Noise for Mitigating Object Hallucinations in\n  LVMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poison as Cure: Visual Noise for Mitigating Object Hallucinations in\n  LVMs"
                },
                "summary": "Large vision-language models (LVMs) extend large language models (LLMs) with\nvisual perception capabilities, enabling them to process and interpret visual\ninformation. A major challenge compromising their reliability is object\nhallucination that LVMs may generate plausible but factually inaccurate\ninformation. We propose a novel visual adversarial perturbation (VAP) method to\nmitigate this hallucination issue. VAP alleviates LVM hallucination by applying\nstrategically optimized visual noise without altering the base model. Our\napproach formulates hallucination suppression as an optimization problem,\nleveraging adversarial strategies to generate beneficial visual perturbations\nthat enhance the model's factual grounding and reduce parametric knowledge\nbias. Extensive experimental results demonstrate that our method consistently\nreduces object hallucinations across 8 state-of-the-art LVMs, validating its\nefficacy across diverse evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVMs) extend large language models (LLMs) with\nvisual perception capabilities, enabling them to process and interpret visual\ninformation. A major challenge compromising their reliability is object\nhallucination that LVMs may generate plausible but factually inaccurate\ninformation. We propose a novel visual adversarial perturbation (VAP) method to\nmitigate this hallucination issue. VAP alleviates LVM hallucination by applying\nstrategically optimized visual noise without altering the base model. Our\napproach formulates hallucination suppression as an optimization problem,\nleveraging adversarial strategies to generate beneficial visual perturbations\nthat enhance the model's factual grounding and reduce parametric knowledge\nbias. Extensive experimental results demonstrate that our method consistently\nreduces object hallucinations across 8 state-of-the-art LVMs, validating its\nefficacy across diverse evaluations."
                },
                "authors": [
                    {
                        "name": "Kejia Zhang"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v3",
                "updated": "2025-01-31T14:26:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    26,
                    5,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Our code will be released upon acceptance. The Change Logs on Page 9\n  reveal our significant changes compared with v1 and v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12433v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12433v3",
                "updated": "2025-01-31T14:22:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    22,
                    27,
                    4,
                    31,
                    0
                ],
                "published": "2024-06-18T09:29:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    9,
                    29,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "LLM4Rerank: LLM-based Auto-Reranking Framework for Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Rerank: LLM-based Auto-Reranking Framework for Recommendations"
                },
                "summary": "Reranking is a critical component in recommender systems, playing an\nessential role in refining the output of recommendation algorithms. Traditional\nreranking models have focused predominantly on accuracy, but modern\napplications demand consideration of additional criteria such as diversity and\nfairness. Existing reranking approaches often fail to harmonize these diverse\ncriteria effectively at the model level. Moreover, these models frequently\nencounter challenges with scalability and personalization due to their\ncomplexity and the varying significance of different reranking criteria in\ndiverse scenarios. In response, we introduce a comprehensive reranking\nframework enhanced by LLM, designed to seamlessly integrate various reranking\ncriteria while maintaining scalability and facilitating personalized\nrecommendations. This framework employs a fully connected graph structure,\nallowing the LLM to simultaneously consider multiple aspects such as accuracy,\ndiversity, and fairness through a coherent Chain-of-Thought (CoT) process. A\ncustomizable input mechanism is also integrated, enabling the tuning of the\nlanguage model's focus to meet specific reranking needs. We validate our\napproach using three popular public datasets, where our framework demonstrates\nsuperior performance over existing state-of-the-art reranking models in\nbalancing multiple criteria. The code for this implementation is publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking is a critical component in recommender systems, playing an\nessential role in refining the output of recommendation algorithms. Traditional\nreranking models have focused predominantly on accuracy, but modern\napplications demand consideration of additional criteria such as diversity and\nfairness. Existing reranking approaches often fail to harmonize these diverse\ncriteria effectively at the model level. Moreover, these models frequently\nencounter challenges with scalability and personalization due to their\ncomplexity and the varying significance of different reranking criteria in\ndiverse scenarios. In response, we introduce a comprehensive reranking\nframework enhanced by LLM, designed to seamlessly integrate various reranking\ncriteria while maintaining scalability and facilitating personalized\nrecommendations. This framework employs a fully connected graph structure,\nallowing the LLM to simultaneously consider multiple aspects such as accuracy,\ndiversity, and fairness through a coherent Chain-of-Thought (CoT) process. A\ncustomizable input mechanism is also integrated, enabling the tuning of the\nlanguage model's focus to meet specific reranking needs. We validate our\napproach using three popular public datasets, where our framework demonstrates\nsuperior performance over existing state-of-the-art reranking models in\nbalancing multiple criteria. The code for this implementation is publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Jingtong Gao"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12433v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12433v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17426v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17426v3",
                "updated": "2025-01-31T14:13:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    13,
                    49,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-26T13:34:02Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    34,
                    2,
                    1,
                    331,
                    0
                ],
                "title": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning"
                },
                "summary": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Fan jiang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/GraphPKU/PiSSA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17426v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17426v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16125v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16125v2",
                "updated": "2025-01-31T14:00:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    0,
                    30,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-27T15:12:27Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    12,
                    27,
                    0,
                    27,
                    0
                ],
                "title": "SampleLLM: Optimizing Tabular Data Synthesis in Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SampleLLM: Optimizing Tabular Data Synthesis in Recommendations"
                },
                "summary": "Tabular data synthesis is crucial in machine learning, yet existing general\nmethods-primarily based on statistical or deep learning models-are highly\ndata-dependent and often fall short in recommender systems. This limitation\narises from their difficulty in capturing complex distributions and\nunderstanding feature relationships from sparse and limited data, along with\ntheir inability to grasp semantic feature relations. Recently, Large Language\nModels (LLMs) have shown potential in generating synthetic data samples through\nfew-shot learning and semantic understanding. However, they often suffer from\ninconsistent distribution and lack of diversity due to their inherent\ndistribution disparity with the target dataset. To address these challenges and\nenhance tabular data synthesis for recommendation tasks, we propose a novel\ntwo-stage framework named SampleLLM to improve the quality of LLM-based tabular\ndata synthesis for recommendations by ensuring better distribution alignment.\nIn the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and\ndiverse exemplars to generate data that closely aligns with the target dataset\ndistribution, even when input samples are limited. The second stage uses an\nadvanced feature attribution-based importance sampling method to refine feature\nrelationships within the synthesized data, reducing any distribution biases\nintroduced by the LLM. Experimental results on three recommendation datasets,\ntwo general datasets, and online deployment illustrate that SampleLLM\nsignificantly surpasses existing methods for recommendation tasks and holds\npromise for a broader range of tabular data scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data synthesis is crucial in machine learning, yet existing general\nmethods-primarily based on statistical or deep learning models-are highly\ndata-dependent and often fall short in recommender systems. This limitation\narises from their difficulty in capturing complex distributions and\nunderstanding feature relationships from sparse and limited data, along with\ntheir inability to grasp semantic feature relations. Recently, Large Language\nModels (LLMs) have shown potential in generating synthetic data samples through\nfew-shot learning and semantic understanding. However, they often suffer from\ninconsistent distribution and lack of diversity due to their inherent\ndistribution disparity with the target dataset. To address these challenges and\nenhance tabular data synthesis for recommendation tasks, we propose a novel\ntwo-stage framework named SampleLLM to improve the quality of LLM-based tabular\ndata synthesis for recommendations by ensuring better distribution alignment.\nIn the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and\ndiverse exemplars to generate data that closely aligns with the target dataset\ndistribution, even when input samples are limited. The second stage uses an\nadvanced feature attribution-based importance sampling method to refine feature\nrelationships within the synthesized data, reducing any distribution biases\nintroduced by the LLM. Experimental results on three recommendation datasets,\ntwo general datasets, and online deployment illustrate that SampleLLM\nsignificantly surpasses existing methods for recommendation tasks and holds\npromise for a broader range of tabular data scenarios."
                },
                "authors": [
                    {
                        "name": "Jingtong Gao"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16125v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16125v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16085v3",
                "updated": "2025-01-31T13:56:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    56,
                    58,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-25T04:36:01Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    4,
                    36,
                    1,
                    0,
                    330,
                    0
                ],
                "title": "Cautious Optimizers: Improving Training with One Line of Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cautious Optimizers: Improving Training with One Line of Code"
                },
                "summary": "AdamW has been the default optimizer for transformer pretraining. For many\nyears, our community searched for faster and more stable optimizers with only\nconstrained positive outcomes. In this work, we propose a single-line\nmodification in Pytorch to any momentum-based optimizer, which we rename\ncautious optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that\nthis modification preserves Adam's Hamiltonian function and it does not break\nthe convergence guarantee under the Lyapunov analysis. In addition, a whole new\nfamily of optimizers is revealed by our theoretical insight. Among them, we\npick the simplest one for empirical experiments, showing not only speed-up on\nLlama and MAE pretraining up to $1.47$ times, but also better results in LLM\npost-training tasks. Code is available at\nhttps://github.com/kyleliang919/C-Optim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdamW has been the default optimizer for transformer pretraining. For many\nyears, our community searched for faster and more stable optimizers with only\nconstrained positive outcomes. In this work, we propose a single-line\nmodification in Pytorch to any momentum-based optimizer, which we rename\ncautious optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that\nthis modification preserves Adam's Hamiltonian function and it does not break\nthe convergence guarantee under the Lyapunov analysis. In addition, a whole new\nfamily of optimizers is revealed by our theoretical insight. Among them, we\npick the simplest one for empirical experiments, showing not only speed-up on\nLlama and MAE pretraining up to $1.47$ times, but also better results in LLM\npost-training tasks. Code is available at\nhttps://github.com/kyleliang919/C-Optim."
                },
                "authors": [
                    {
                        "name": "Kaizhao Liang"
                    },
                    {
                        "name": "Lizhang Chen"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Qiang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Liu"
                },
                "author": "Qiang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19135v1",
                "updated": "2025-01-31T13:45:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    45,
                    31,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T13:45:31Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    45,
                    31,
                    4,
                    31,
                    0
                ],
                "title": "A Tensor-Train Decomposition based Compression of LLMs on Group Vector\n  Systolic Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Tensor-Train Decomposition based Compression of LLMs on Group Vector\n  Systolic Accelerator"
                },
                "summary": "Large language models (LLMs) are both storage-intensive and\ncomputation-intensive, posing significant challenges when deployed on\nresource-constrained hardware. As linear layers in LLMs are mainly resource\nconsuming parts, this paper develops a tensor-train decomposition (TTD) for\nLLMs with a further hardware implementation on FPGA. TTD compression is applied\nto the linear layers in ChatGLM3-6B and LLaMA2-7B models with compression\nratios (CRs) for the whole network 1.94$\\times$ and 1.60$\\times$, respectively.\nThe compressed LLMs are further implemented on FPGA hardware within a highly\nefficient group vector systolic array (GVSA) architecture, which has DSP-shared\nparallel vector PEs for TTD inference, as well as optimized data communication\nin the accelerator. Experimental results show that the corresponding TTD based\nLLM accelerator implemented on FPGA achieves 1.45$\\times$ and 1.57$\\times$\nreduction in first token delay for ChatGLM3-6B and LLaMA2-7B models,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are both storage-intensive and\ncomputation-intensive, posing significant challenges when deployed on\nresource-constrained hardware. As linear layers in LLMs are mainly resource\nconsuming parts, this paper develops a tensor-train decomposition (TTD) for\nLLMs with a further hardware implementation on FPGA. TTD compression is applied\nto the linear layers in ChatGLM3-6B and LLaMA2-7B models with compression\nratios (CRs) for the whole network 1.94$\\times$ and 1.60$\\times$, respectively.\nThe compressed LLMs are further implemented on FPGA hardware within a highly\nefficient group vector systolic array (GVSA) architecture, which has DSP-shared\nparallel vector PEs for TTD inference, as well as optimized data communication\nin the accelerator. Experimental results show that the corresponding TTD based\nLLM accelerator implemented on FPGA achieves 1.45$\\times$ and 1.57$\\times$\nreduction in first token delay for ChatGLM3-6B and LLaMA2-7B models,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Sixiao Huang"
                    },
                    {
                        "name": "Tintin Wang"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Keyao Jiang"
                    },
                    {
                        "name": "Mingqiang Huang"
                    },
                    {
                        "name": "Hao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Yu"
                },
                "author": "Hao Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17908v2",
                "updated": "2025-01-31T13:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    41,
                    32,
                    4,
                    31,
                    0
                ],
                "published": "2024-05-28T07:29:02Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    7,
                    29,
                    2,
                    1,
                    149,
                    0
                ],
                "title": "Deep learning inference of the neutron star equation of state",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning inference of the neutron star equation of state"
                },
                "summary": "We present a pipeline to infer the equation of state of neutron stars from\nobservations based on deep neural networks. In particular, using the standard\n(deterministic), as well as Bayesian (probabilistic) deep networks, we explore\nhow one can infer the interior speed of sound of the star given a set of mock\nobservations of total stellar mass, stellar radius and tidal deformability. We\ndiscuss in detail the construction of our simulated dataset of stellar\nobservables starting from the solution of the gravitational equations, as well\nas the relevant architectures for the deep networks, along with their\nperformance and accuracy. We further explain how our pipeline is capable to\ndetect a possible QCD phase transition in the stellar core. Our results show\nthat deep networks offer a promising tool towards solving the inverse problem\nof neutron stars, and the accurate inference of their interior from future\nstellar observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a pipeline to infer the equation of state of neutron stars from\nobservations based on deep neural networks. In particular, using the standard\n(deterministic), as well as Bayesian (probabilistic) deep networks, we explore\nhow one can infer the interior speed of sound of the star given a set of mock\nobservations of total stellar mass, stellar radius and tidal deformability. We\ndiscuss in detail the construction of our simulated dataset of stellar\nobservables starting from the solution of the gravitational equations, as well\nas the relevant architectures for the deep networks, along with their\nperformance and accuracy. We further explain how our pipeline is capable to\ndetect a possible QCD phase transition in the stellar core. Our results show\nthat deep networks offer a promising tool towards solving the inverse problem\nof neutron stars, and the accurate inference of their interior from future\nstellar observations."
                },
                "authors": [
                    {
                        "name": "Giulia Ventagli"
                    },
                    {
                        "name": "Ippocratis D. Saltas"
                    }
                ],
                "author_detail": {
                    "name": "Ippocratis D. Saltas"
                },
                "author": "Ippocratis D. Saltas",
                "arxiv_doi": "10.1088/1475-7516/2025/01/073",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1475-7516/2025/01/073",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.17908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "v2: added section on real observational data, published version",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19126v1",
                "updated": "2025-01-31T13:31:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    31,
                    43,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T13:31:43Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    31,
                    43,
                    4,
                    31,
                    0
                ],
                "title": "Asymptotic optimality theory of confidence intervals of the mean",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotic optimality theory of confidence intervals of the mean"
                },
                "summary": "We address the classical problem of constructing confidence intervals (CIs)\nfor the mean of a distribution, given \\(N\\) i.i.d. samples, such that the CI\ncontains the true mean with probability at least \\(1 - \\delta\\), where \\(\\delta\n\\in (0,1)\\). We characterize three distinct learning regimes based on the\nminimum achievable limiting width of any CI as the sample size \\(N_{\\delta} \\to\n\\infty\\) and \\(\\delta \\to 0\\). In the first regime, where \\(N_{\\delta}\\) grows\nslower than \\(\\log(1/\\delta)\\), the limiting width of any CI equals the width\nof the distribution's support, precluding meaningful inference. In the second\nregime, where \\(N_{\\delta}\\) scales as \\(\\log(1/\\delta)\\), we precisely\ncharacterize the minimum limiting width, which depends on the scaling constant.\nIn the third regime, where \\(N_{\\delta}\\) grows faster than \\(\\log(1/\\delta)\\),\ncomplete learning is achievable, and the limiting width of the CI collapses to\nzero, converging to the true mean. We demonstrate that CIs derived from\nconcentration inequalities based on Kullback--Leibler (KL) divergences achieve\nasymptotically optimal performance, attaining the minimum limiting width in\nboth sufficient and complete learning regimes for distributions in two\nfamilies: single-parameter exponential and bounded support. Additionally, these\nresults extend to one-sided CIs, with the width notion adjusted appropriately.\nFinally, we generalize our findings to settings with random per-sample costs,\nmotivated by practical applications such as stochastic simulators and cloud\nservice selection. Instead of a fixed sample size, we consider a cost budget\n\\(C_{\\delta}\\), identifying analogous learning regimes and characterizing the\noptimal CI construction policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the classical problem of constructing confidence intervals (CIs)\nfor the mean of a distribution, given \\(N\\) i.i.d. samples, such that the CI\ncontains the true mean with probability at least \\(1 - \\delta\\), where \\(\\delta\n\\in (0,1)\\). We characterize three distinct learning regimes based on the\nminimum achievable limiting width of any CI as the sample size \\(N_{\\delta} \\to\n\\infty\\) and \\(\\delta \\to 0\\). In the first regime, where \\(N_{\\delta}\\) grows\nslower than \\(\\log(1/\\delta)\\), the limiting width of any CI equals the width\nof the distribution's support, precluding meaningful inference. In the second\nregime, where \\(N_{\\delta}\\) scales as \\(\\log(1/\\delta)\\), we precisely\ncharacterize the minimum limiting width, which depends on the scaling constant.\nIn the third regime, where \\(N_{\\delta}\\) grows faster than \\(\\log(1/\\delta)\\),\ncomplete learning is achievable, and the limiting width of the CI collapses to\nzero, converging to the true mean. We demonstrate that CIs derived from\nconcentration inequalities based on Kullback--Leibler (KL) divergences achieve\nasymptotically optimal performance, attaining the minimum limiting width in\nboth sufficient and complete learning regimes for distributions in two\nfamilies: single-parameter exponential and bounded support. Additionally, these\nresults extend to one-sided CIs, with the width notion adjusted appropriately.\nFinally, we generalize our findings to settings with random per-sample costs,\nmotivated by practical applications such as stochastic simulators and cloud\nservice selection. Instead of a fixed sample size, we consider a cost budget\n\\(C_{\\delta}\\), identifying analogous learning regimes and characterizing the\noptimal CI construction policy."
                },
                "authors": [
                    {
                        "name": "Vikas Deep"
                    },
                    {
                        "name": "Achal Bassamboo"
                    },
                    {
                        "name": "Sandeep Juneja"
                    }
                ],
                "author_detail": {
                    "name": "Sandeep Juneja"
                },
                "author": "Sandeep Juneja",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03355v2",
                "updated": "2025-01-31T13:24:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    24,
                    10,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-04T12:21:03Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    12,
                    21,
                    3,
                    4,
                    278,
                    0
                ],
                "title": "LANTERN: Accelerating Visual Autoregressive Models with Relaxed\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LANTERN: Accelerating Visual Autoregressive Models with Relaxed\n  Speculative Decoding"
                },
                "summary": "Auto-Regressive (AR) models have recently gained prominence in image\ngeneration, often matching or even surpassing the performance of diffusion\nmodels. However, one major limitation of AR models is their sequential nature,\nwhich processes tokens one at a time, slowing down generation compared to\nmodels like GANs or diffusion-based methods that operate more efficiently.\nWhile speculative decoding has proven effective for accelerating LLMs by\ngenerating multiple tokens in a single forward, its application in visual AR\nmodels remains largely unexplored. In this work, we identify a challenge in\nthis setting, which we term \\textit{token selection ambiguity}, wherein visual\nAR models frequently assign uniformly low probabilities to tokens, hampering\nthe performance of speculative decoding. To overcome this challenge, we propose\na relaxed acceptance condition referred to as LANTERN that leverages the\ninterchangeability of tokens in latent space. This relaxation restores the\neffectiveness of speculative decoding in visual AR models by enabling more\nflexible use of candidate tokens that would otherwise be prematurely rejected.\nFurthermore, by incorporating a total variation distance bound, we ensure that\nthese speed gains are achieved without significantly compromising image quality\nor semantic coherence. Experimental results demonstrate the efficacy of our\nmethod in providing a substantial speed-up over speculative decoding. In\nspecific, compared to a na\\\"ive application of the state-of-the-art speculative\ndecoding, LANTERN increases speed-ups by $\\mathbf{1.75}\\times$ and\n$\\mathbf{1.82}\\times$, as compared to greedy decoding and random sampling,\nrespectively, when applied to LlamaGen, a contemporary visual AR model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Regressive (AR) models have recently gained prominence in image\ngeneration, often matching or even surpassing the performance of diffusion\nmodels. However, one major limitation of AR models is their sequential nature,\nwhich processes tokens one at a time, slowing down generation compared to\nmodels like GANs or diffusion-based methods that operate more efficiently.\nWhile speculative decoding has proven effective for accelerating LLMs by\ngenerating multiple tokens in a single forward, its application in visual AR\nmodels remains largely unexplored. In this work, we identify a challenge in\nthis setting, which we term \\textit{token selection ambiguity}, wherein visual\nAR models frequently assign uniformly low probabilities to tokens, hampering\nthe performance of speculative decoding. To overcome this challenge, we propose\na relaxed acceptance condition referred to as LANTERN that leverages the\ninterchangeability of tokens in latent space. This relaxation restores the\neffectiveness of speculative decoding in visual AR models by enabling more\nflexible use of candidate tokens that would otherwise be prematurely rejected.\nFurthermore, by incorporating a total variation distance bound, we ensure that\nthese speed gains are achieved without significantly compromising image quality\nor semantic coherence. Experimental results demonstrate the efficacy of our\nmethod in providing a substantial speed-up over speculative decoding. In\nspecific, compared to a na\\\"ive application of the state-of-the-art speculative\ndecoding, LANTERN increases speed-ups by $\\mathbf{1.75}\\times$ and\n$\\mathbf{1.82}\\times$, as compared to greedy decoding and random sampling,\nrespectively, when applied to LlamaGen, a contemporary visual AR model."
                },
                "authors": [
                    {
                        "name": "Doohyuk Jang"
                    },
                    {
                        "name": "Sihwan Park"
                    },
                    {
                        "name": "June Yong Yang"
                    },
                    {
                        "name": "Yeonsung Jung"
                    },
                    {
                        "name": "Jihun Yun"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Sung-Yub Kim"
                    },
                    {
                        "name": "Eunho Yang"
                    }
                ],
                "author_detail": {
                    "name": "Eunho Yang"
                },
                "author": "Eunho Yang",
                "arxiv_comment": "30 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00110v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00110v3",
                "updated": "2025-01-31T13:07:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    7,
                    9,
                    4,
                    31,
                    0
                ],
                "published": "2024-09-30T18:00:06Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    18,
                    0,
                    6,
                    0,
                    274,
                    0
                ],
                "title": "The AURORA Survey: An Extraordinarily Mature, Star-forming Galaxy at\n  $z\\sim 7$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AURORA Survey: An Extraordinarily Mature, Star-forming Galaxy at\n  $z\\sim 7$"
                },
                "summary": "We present the properties of a massive, large, dusty, metal-rich,\nstar-forming galaxy at z_spec=6.73. GOODSN-100182 was observed with\nJWST/NIRSpec as part of the AURORA survey, and is also covered by public\nmulti-wavelength HST and JWST imaging. While the large stellar mass of\nGOODSN-100182 (~10^10 M_sun) was indicated prior to JWST, NIRCam rest-optical\nimaging now reveals the presence of an extended disk (r_eff~1.5 kpc). In\naddition, the NIRSpec R~1000 spectrum of GOODSN-100182 includes the detection\nof a large suite of rest-optical nebular emission lines ranging in wavelength\nfrom [OII]3727 up to [NII]6583. The ratios of Balmer lines suggest significant\ndust attenuation (E(B-V)_gas=0.40+0.10/-0.09), consistent with the red rest-UV\nslope inferred for GOODSN-100182 (beta=-0.50+/-0.09). The star-formation rate\nbased on dust-corrected H-alpha emission is log(SFR(H-alpha)/\nM_sun/yr)=2.02+0.13/-0.14, well above the z~7 star-forming main sequence in\nterms of specific SFR. Strikingly, the ratio of [NII]6583/H-alpha emission\nsuggests almost solar metallicity, as does the ratio\n([OIII]5007/H-beta)/([NII]6583/H-alpha) and the detection of the faint\n[FeII]4360 emission feature. Overall, the excitation and ionization properties\nof GOODSN-100182 more closely resemble those of typical star-forming galaxies\nat z~2-3 rather than z~7. Based on public spectroscopy of the GOODS-N field, we\nfind that GOODSN-100182 resides within a significant galaxy overdensity, and is\naccompanied by a spectroscopically-confirmed neighbor galaxy. GOODSN-100182\ndemonstrates the existence of mature, chemically-enriched galaxies within the\nfirst billion years of cosmic time, whose properties must be explained by\ngalaxy formation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the properties of a massive, large, dusty, metal-rich,\nstar-forming galaxy at z_spec=6.73. GOODSN-100182 was observed with\nJWST/NIRSpec as part of the AURORA survey, and is also covered by public\nmulti-wavelength HST and JWST imaging. While the large stellar mass of\nGOODSN-100182 (~10^10 M_sun) was indicated prior to JWST, NIRCam rest-optical\nimaging now reveals the presence of an extended disk (r_eff~1.5 kpc). In\naddition, the NIRSpec R~1000 spectrum of GOODSN-100182 includes the detection\nof a large suite of rest-optical nebular emission lines ranging in wavelength\nfrom [OII]3727 up to [NII]6583. The ratios of Balmer lines suggest significant\ndust attenuation (E(B-V)_gas=0.40+0.10/-0.09), consistent with the red rest-UV\nslope inferred for GOODSN-100182 (beta=-0.50+/-0.09). The star-formation rate\nbased on dust-corrected H-alpha emission is log(SFR(H-alpha)/\nM_sun/yr)=2.02+0.13/-0.14, well above the z~7 star-forming main sequence in\nterms of specific SFR. Strikingly, the ratio of [NII]6583/H-alpha emission\nsuggests almost solar metallicity, as does the ratio\n([OIII]5007/H-beta)/([NII]6583/H-alpha) and the detection of the faint\n[FeII]4360 emission feature. Overall, the excitation and ionization properties\nof GOODSN-100182 more closely resemble those of typical star-forming galaxies\nat z~2-3 rather than z~7. Based on public spectroscopy of the GOODS-N field, we\nfind that GOODSN-100182 resides within a significant galaxy overdensity, and is\naccompanied by a spectroscopically-confirmed neighbor galaxy. GOODSN-100182\ndemonstrates the existence of mature, chemically-enriched galaxies within the\nfirst billion years of cosmic time, whose properties must be explained by\ngalaxy formation models."
                },
                "authors": [
                    {
                        "name": "Alice E. Shapley"
                    },
                    {
                        "name": "Ryan L. Sanders"
                    },
                    {
                        "name": "Michael W. Topping"
                    },
                    {
                        "name": "Naveen A. Reddy"
                    },
                    {
                        "name": "Anthony J. Pahl"
                    },
                    {
                        "name": "Pascal A. Oesch"
                    },
                    {
                        "name": "Danielle A. Berg"
                    },
                    {
                        "name": "Rychard J. Bouwens"
                    },
                    {
                        "name": "Gabriel Brammer"
                    },
                    {
                        "name": "Adam C. Carnall"
                    },
                    {
                        "name": "Fergus Cullen"
                    },
                    {
                        "name": "Romeel Davé"
                    },
                    {
                        "name": "James S. Dunlop"
                    },
                    {
                        "name": "Richard S. Ellis"
                    },
                    {
                        "name": "N. M. Förster Schreiber"
                    },
                    {
                        "name": "Steven R . Furlanetto"
                    },
                    {
                        "name": "Karl Glazebrook"
                    },
                    {
                        "name": "Garth D. Illingworth"
                    },
                    {
                        "name": "Tucker Jones"
                    },
                    {
                        "name": "Mariska Kriek"
                    },
                    {
                        "name": "Derek J. McLeod"
                    },
                    {
                        "name": "Ross J. McLure"
                    },
                    {
                        "name": "Desika Narayanan"
                    },
                    {
                        "name": "Max Pettini"
                    },
                    {
                        "name": "Daniel Schaerer"
                    },
                    {
                        "name": "Daniel P. Stark"
                    },
                    {
                        "name": "Charles C. Steidel"
                    },
                    {
                        "name": "Mengtao Tang"
                    },
                    {
                        "name": "Leonardo Clarke"
                    },
                    {
                        "name": "Callum T. Donnan"
                    },
                    {
                        "name": "Emily Kehoe"
                    }
                ],
                "author_detail": {
                    "name": "Emily Kehoe"
                },
                "author": "Emily Kehoe",
                "arxiv_comment": "16 pages, 13 figures, accepted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00110v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00110v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19107v1",
                "updated": "2025-01-31T13:04:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    4,
                    37,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T13:04:37Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    4,
                    37,
                    4,
                    31,
                    0
                ],
                "title": "Brain-inspired sparse training enables Transformers and LLMs to perform\n  as fully connected",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-inspired sparse training enables Transformers and LLMs to perform\n  as fully connected"
                },
                "summary": "This study aims to enlarge our current knowledge on application of\nbrain-inspired network science principles for training artificial neural\nnetworks (ANNs) with sparse connectivity. Dynamic sparse training (DST) can\nreduce the computational demands in ANNs, but faces difficulties to keep peak\nperformance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a\nbrain-inspired method for growing connectivity in DST. CHT leverages a\ngradient-free, topology-driven link regrowth, which has shown ultra-sparse (1%\nconnectivity or lower) advantage across various tasks compared to fully\nconnected networks. Yet, CHT suffers two main drawbacks: (i) its time\ncomplexity is O(Nd^3) - N node network size, d node degree - hence it can apply\nonly to ultra-sparse networks. (ii) it selects top link prediction scores,\nwhich is inappropriate for the early training epochs, when the network presents\nunreliable connections. We propose a GPU-friendly approximation of the CH link\npredictor, which reduces the computational complexity to O(N^3), enabling a\nfast implementation of CHT in large-scale models. We introduce the\nCannistraci-Hebb training soft rule (CHTs), which adopts a strategy for\nsampling connections in both link removal and regrowth, balancing the\nexploration and exploitation of network topology. To improve performance, we\nintegrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results\nshow that, using 1% of connections, CHTs outperforms fully connected networks\nin MLP on visual classification tasks, compressing some networks to < 30%\nnodes. Using 5% of the connections, CHTss outperforms fully connected networks\nin two Transformer-based machine translation tasks. Using 30% of the\nconnections, CHTss achieves superior performance compared to other dynamic\nsparse training methods in language modeling, and it surpasses the fully\nconnected counterpart in zero-shot evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study aims to enlarge our current knowledge on application of\nbrain-inspired network science principles for training artificial neural\nnetworks (ANNs) with sparse connectivity. Dynamic sparse training (DST) can\nreduce the computational demands in ANNs, but faces difficulties to keep peak\nperformance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a\nbrain-inspired method for growing connectivity in DST. CHT leverages a\ngradient-free, topology-driven link regrowth, which has shown ultra-sparse (1%\nconnectivity or lower) advantage across various tasks compared to fully\nconnected networks. Yet, CHT suffers two main drawbacks: (i) its time\ncomplexity is O(Nd^3) - N node network size, d node degree - hence it can apply\nonly to ultra-sparse networks. (ii) it selects top link prediction scores,\nwhich is inappropriate for the early training epochs, when the network presents\nunreliable connections. We propose a GPU-friendly approximation of the CH link\npredictor, which reduces the computational complexity to O(N^3), enabling a\nfast implementation of CHT in large-scale models. We introduce the\nCannistraci-Hebb training soft rule (CHTs), which adopts a strategy for\nsampling connections in both link removal and regrowth, balancing the\nexploration and exploitation of network topology. To improve performance, we\nintegrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results\nshow that, using 1% of connections, CHTs outperforms fully connected networks\nin MLP on visual classification tasks, compressing some networks to < 30%\nnodes. Using 5% of the connections, CHTss outperforms fully connected networks\nin two Transformer-based machine translation tasks. Using 30% of the\nconnections, CHTss achieves superior performance compared to other dynamic\nsparse training methods in language modeling, and it surpasses the fully\nconnected counterpart in zero-shot evaluations."
                },
                "authors": [
                    {
                        "name": "Yingtao Zhang"
                    },
                    {
                        "name": "Jialin Zhao"
                    },
                    {
                        "name": "Wenjing Wu"
                    },
                    {
                        "name": "Ziheng Liao"
                    },
                    {
                        "name": "Umberto Michieli"
                    },
                    {
                        "name": "Carlo Vittorio Cannistraci"
                    }
                ],
                "author_detail": {
                    "name": "Carlo Vittorio Cannistraci"
                },
                "author": "Carlo Vittorio Cannistraci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17233v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17233v2",
                "updated": "2025-01-31T13:04:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    4,
                    34,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-22T17:53:34Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    17,
                    53,
                    34,
                    1,
                    296,
                    0
                ],
                "title": "Large Language Models are In-context Preference Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are In-context Preference Learners"
                },
                "summary": "Preference-based reinforcement learning is an effective way to handle tasks\nwhere rewards are hard to specify but can be exceedingly inefficient as\npreference learning is often tabula rasa. We demonstrate that Large Language\nModels (LLMs) have native preference-learning capabilities that allow them to\nachieve sample-efficient preference learning, addressing this challenge. We\npropose In-Context Preference Learning (ICPL), which uses in-context learning\ncapabilities of LLMs to reduce human query inefficiency. ICPL uses the task\ndescription and basic environment code to create sets of reward functions which\nare iteratively refined by placing human feedback over videos of the resultant\npolicies into the context of an LLM and then requesting better rewards. We\nfirst demonstrate ICPL's effectiveness through a synthetic preference study,\nproviding quantitative evidence that it significantly outperforms baseline\npreference-based methods with much higher performance and orders of magnitude\ngreater efficiency. We observe that these improvements are not solely coming\nfrom LLM grounding in the task but that the quality of the rewards improves\nover time, indicating preference learning capabilities. Additionally, we\nperform a series of real human preference-learning trials and observe that ICPL\nextends beyond synthetic settings and can work effectively with\nhumans-in-the-loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-based reinforcement learning is an effective way to handle tasks\nwhere rewards are hard to specify but can be exceedingly inefficient as\npreference learning is often tabula rasa. We demonstrate that Large Language\nModels (LLMs) have native preference-learning capabilities that allow them to\nachieve sample-efficient preference learning, addressing this challenge. We\npropose In-Context Preference Learning (ICPL), which uses in-context learning\ncapabilities of LLMs to reduce human query inefficiency. ICPL uses the task\ndescription and basic environment code to create sets of reward functions which\nare iteratively refined by placing human feedback over videos of the resultant\npolicies into the context of an LLM and then requesting better rewards. We\nfirst demonstrate ICPL's effectiveness through a synthetic preference study,\nproviding quantitative evidence that it significantly outperforms baseline\npreference-based methods with much higher performance and orders of magnitude\ngreater efficiency. We observe that these improvements are not solely coming\nfrom LLM grounding in the task but that the quality of the rewards improves\nover time, indicating preference learning capabilities. Additionally, we\nperform a series of real human preference-learning trials and observe that ICPL\nextends beyond synthetic settings and can work effectively with\nhumans-in-the-loop."
                },
                "authors": [
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Qixin Tan"
                    },
                    {
                        "name": "Hong Lu"
                    },
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Xinting Yang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Eugene Vinitsky"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Vinitsky"
                },
                "author": "Eugene Vinitsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17233v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17233v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19100v1",
                "updated": "2025-01-31T12:49:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    49,
                    22,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T12:49:22Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    49,
                    22,
                    4,
                    31,
                    0
                ],
                "title": "Large molecular and dust reservoir of a gravitationally-lensed\n  submillimeter galaxy behind the Lupus-I molecular cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large molecular and dust reservoir of a gravitationally-lensed\n  submillimeter galaxy behind the Lupus-I molecular cloud"
                },
                "summary": "We report the Australian Telescope Compact Array and Nobeyama 45 m telescope\ndetection of a remarkably bright $S_\\mathrm{1.1mm}$ = 44 mJy) submillimeter\ngalaxy MM J154506.4-344318 in emission lines at 48.5 and 97.0 GHz,\nrespectively. We also identify part of an emission line at $\\approx$ 218.3 GHz\nusing the Atacama Large Millimeter/submillimeter Array (ALMA). Together with\nphotometric redshift estimates and the ratio between the line and infrared\nluminosities, we conclude that the emission lines are most likely to be the $J$\n= 2-1, 4-3, and 9-8 transitions of $^{12}$CO at redshift $z = 3.753 \\pm 0.001$.\nALMA 1.3 mm continuum imaging reveals an arc and a spot separated by an angular\ndistance of 1.6 arcsec, indicative of a strongly-lensed dusty star-forming\ngalaxy with respective molecular and dust masses of $\\log{M_{\\rm mol}/M_\\odot}\n\\approx 11.5$ and $\\log{M_{\\rm dust}/M_\\odot} \\approx 9.4$ after corrected for\n$\\approx$ 6.6$\\times$ gravitational magnification. The inferred dust-to-gas\nmass ratio is found to be high ($\\approx$ 0.0083) among coeval dusty\nstar-forming galaxies, implying the presence of a massive, chemically-enriched\nreservoir of cool interstellar medium at $z \\approx 4$ or 1.6 Gyr after the Big\nBang.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report the Australian Telescope Compact Array and Nobeyama 45 m telescope\ndetection of a remarkably bright $S_\\mathrm{1.1mm}$ = 44 mJy) submillimeter\ngalaxy MM J154506.4-344318 in emission lines at 48.5 and 97.0 GHz,\nrespectively. We also identify part of an emission line at $\\approx$ 218.3 GHz\nusing the Atacama Large Millimeter/submillimeter Array (ALMA). Together with\nphotometric redshift estimates and the ratio between the line and infrared\nluminosities, we conclude that the emission lines are most likely to be the $J$\n= 2-1, 4-3, and 9-8 transitions of $^{12}$CO at redshift $z = 3.753 \\pm 0.001$.\nALMA 1.3 mm continuum imaging reveals an arc and a spot separated by an angular\ndistance of 1.6 arcsec, indicative of a strongly-lensed dusty star-forming\ngalaxy with respective molecular and dust masses of $\\log{M_{\\rm mol}/M_\\odot}\n\\approx 11.5$ and $\\log{M_{\\rm dust}/M_\\odot} \\approx 9.4$ after corrected for\n$\\approx$ 6.6$\\times$ gravitational magnification. The inferred dust-to-gas\nmass ratio is found to be high ($\\approx$ 0.0083) among coeval dusty\nstar-forming galaxies, implying the presence of a massive, chemically-enriched\nreservoir of cool interstellar medium at $z \\approx 4$ or 1.6 Gyr after the Big\nBang."
                },
                "authors": [
                    {
                        "name": "Yoichi Tamura"
                    },
                    {
                        "name": "Akio Taniguchi"
                    },
                    {
                        "name": "Tom J. L. C. Bakx"
                    },
                    {
                        "name": "Itziar De Gregorio-Monsalvo"
                    },
                    {
                        "name": "Masato Hagimoto"
                    },
                    {
                        "name": "Soh Ikarashi"
                    },
                    {
                        "name": "Ryohei Kawabe"
                    },
                    {
                        "name": "Kotaro Kohno"
                    },
                    {
                        "name": "Kouichiro Nakanishi"
                    },
                    {
                        "name": "Tatsuya Takekoshi"
                    },
                    {
                        "name": "Yoshito Shimajiri"
                    },
                    {
                        "name": "Takashi Tsukagoshi"
                    },
                    {
                        "name": "Bunyo Hatsukade"
                    },
                    {
                        "name": "Daisuke Iono"
                    },
                    {
                        "name": "Hideo Matsuhara"
                    },
                    {
                        "name": "Kazuya Saigo"
                    },
                    {
                        "name": "Masao Saito"
                    }
                ],
                "author_detail": {
                    "name": "Masao Saito"
                },
                "author": "Masao Saito",
                "arxiv_comment": "Accepted by ApJ. 5 figures and 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12961v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12961v3",
                "updated": "2025-01-31T12:48:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    48,
                    9,
                    4,
                    31,
                    0
                ],
                "published": "2024-09-19T17:59:51Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    17,
                    59,
                    51,
                    3,
                    263,
                    0
                ],
                "title": "Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary\n  Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary\n  Resolution"
                },
                "summary": "Visual data comes in various forms, ranging from small icons of just a few\npixels to long videos spanning hours. Existing multi-modal LLMs usually\nstandardize these diverse visual inputs to a fixed resolution for visual\nencoders and yield similar numbers of tokens for LLMs. This approach is\nnon-optimal for multimodal understanding and inefficient for processing inputs\nwith long and short visual contents. To solve the problem, we propose Oryx, a\nunified multimodal architecture for the spatial-temporal understanding of\nimages, videos, and multi-view 3D scenes. Oryx offers an on-demand solution to\nseamlessly and efficiently process visual inputs with arbitrary spatial sizes\nand temporal lengths through two core innovations: 1) a pre-trained OryxViT\nmodel that can encode images at any resolution into LLM-friendly visual\nrepresentations; 2) a dynamic compressor module that supports 1x to 16x\ncompression on visual tokens by request. These design features enable Oryx to\naccommodate extremely long visual contexts, such as videos, with lower\nresolution and high compression while maintaining high recognition precision\nfor tasks like document understanding with native resolution and no\ncompression. Beyond the architectural improvements, enhanced data curation and\nspecialized training on long-context retrieval and spatial-aware data help Oryx\nachieve strong capabilities in image, video, and 3D multimodal understanding\nsimultaneously. Our work is open-sourced at https://github.com/Oryx-mllm/Oryx.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual data comes in various forms, ranging from small icons of just a few\npixels to long videos spanning hours. Existing multi-modal LLMs usually\nstandardize these diverse visual inputs to a fixed resolution for visual\nencoders and yield similar numbers of tokens for LLMs. This approach is\nnon-optimal for multimodal understanding and inefficient for processing inputs\nwith long and short visual contents. To solve the problem, we propose Oryx, a\nunified multimodal architecture for the spatial-temporal understanding of\nimages, videos, and multi-view 3D scenes. Oryx offers an on-demand solution to\nseamlessly and efficiently process visual inputs with arbitrary spatial sizes\nand temporal lengths through two core innovations: 1) a pre-trained OryxViT\nmodel that can encode images at any resolution into LLM-friendly visual\nrepresentations; 2) a dynamic compressor module that supports 1x to 16x\ncompression on visual tokens by request. These design features enable Oryx to\naccommodate extremely long visual contexts, such as videos, with lower\nresolution and high compression while maintaining high recognition precision\nfor tasks like document understanding with native resolution and no\ncompression. Beyond the architectural improvements, enhanced data curation and\nspecialized training on long-context retrieval and spatial-aware data help Oryx\nachieve strong capabilities in image, video, and 3D multimodal understanding\nsimultaneously. Our work is open-sourced at https://github.com/Oryx-mllm/Oryx."
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Yongming Rao"
                    }
                ],
                "author_detail": {
                    "name": "Yongming Rao"
                },
                "author": "Yongming Rao",
                "arxiv_comment": "Accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12961v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12961v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19099v1",
                "updated": "2025-01-31T12:46:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    46,
                    4,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T12:46:04Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    46,
                    4,
                    4,
                    31,
                    0
                ],
                "title": "Unraveling Zeroth-Order Optimization through the Lens of Low-Dimensional\n  Structured Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling Zeroth-Order Optimization through the Lens of Low-Dimensional\n  Structured Perturbations"
                },
                "summary": "Zeroth-order (ZO) optimization has emerged as a promising alternative to\ngradient-based backpropagation methods, particularly for black-box optimization\nand large language model (LLM) fine-tuning. However, ZO methods suffer from\nslow convergence due to high-variance stochastic gradient estimators. While\nstructured perturbations, such as sparsity and low-rank constraints, have been\nexplored to mitigate these issues, their effectiveness remains highly\nunder-explored. In this work, we develop a unified theoretical framework that\nanalyzes both the convergence and generalization properties of ZO optimization\nunder structured perturbations. We show that high dimensionality is the primary\nbottleneck and introduce the notions of \\textit{stable rank} and\n\\textit{effective overlap} to explain how structured perturbations reduce\ngradient noise and accelerate convergence. Using the uniform stability under\nour framework, we then provide the first theoretical justification for why\nthese perturbations enhance generalization. Additionally, through empirical\nanalysis, we identify that \\textbf{block coordinate descent} (BCD) to be an\neffective structured perturbation method. Extensive experiments show that,\ncompared to existing alternatives, memory-efficient ZO (MeZO) with BCD\n(\\textit{MeZO-BCD}) can provide improved converge with a faster wall-clock\ntime/iteration by up to $\\times\\textbf{2.09}$ while yielding similar or better\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zeroth-order (ZO) optimization has emerged as a promising alternative to\ngradient-based backpropagation methods, particularly for black-box optimization\nand large language model (LLM) fine-tuning. However, ZO methods suffer from\nslow convergence due to high-variance stochastic gradient estimators. While\nstructured perturbations, such as sparsity and low-rank constraints, have been\nexplored to mitigate these issues, their effectiveness remains highly\nunder-explored. In this work, we develop a unified theoretical framework that\nanalyzes both the convergence and generalization properties of ZO optimization\nunder structured perturbations. We show that high dimensionality is the primary\nbottleneck and introduce the notions of \\textit{stable rank} and\n\\textit{effective overlap} to explain how structured perturbations reduce\ngradient noise and accelerate convergence. Using the uniform stability under\nour framework, we then provide the first theoretical justification for why\nthese perturbations enhance generalization. Additionally, through empirical\nanalysis, we identify that \\textbf{block coordinate descent} (BCD) to be an\neffective structured perturbation method. Extensive experiments show that,\ncompared to existing alternatives, memory-efficient ZO (MeZO) with BCD\n(\\textit{MeZO-BCD}) can provide improved converge with a faster wall-clock\ntime/iteration by up to $\\times\\textbf{2.09}$ while yielding similar or better\naccuracy."
                },
                "authors": [
                    {
                        "name": "Sihwan Park"
                    },
                    {
                        "name": "Jihun Yun"
                    },
                    {
                        "name": "SungYub Kim"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Eunho Yang"
                    }
                ],
                "author_detail": {
                    "name": "Eunho Yang"
                },
                "author": "Eunho Yang",
                "arxiv_comment": "35 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19093v1",
                "updated": "2025-01-31T12:39:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    39,
                    28,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T12:39:28Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    39,
                    28,
                    4,
                    31,
                    0
                ],
                "title": "Improving Low-Resource Sequence Labeling with Knowledge Fusion and\n  Contextual Label Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Low-Resource Sequence Labeling with Knowledge Fusion and\n  Contextual Label Explanations"
                },
                "summary": "Sequence labeling remains a significant challenge in low-resource,\ndomain-specific scenarios, particularly for character-dense languages like\nChinese. Existing methods primarily focus on enhancing model comprehension and\nimproving data diversity to boost performance. However, these approaches still\nstruggle with inadequate model applicability and semantic distribution biases\nin domain-specific contexts. To overcome these limitations, we propose a novel\nframework that combines an LLM-based knowledge enhancement workflow with a\nspan-based Knowledge Fusion for Rich and Efficient Extraction (KnowFREE) model.\nOur workflow employs explanation prompts to generate precise contextual\ninterpretations of target entities, effectively mitigating semantic biases and\nenriching the model's contextual understanding. The KnowFREE model further\nintegrates extension label features, enabling efficient nested entity\nextraction without relying on external knowledge during inference. Experiments\non multiple Chinese domain-specific sequence labeling datasets demonstrate that\nour approach achieves state-of-the-art performance, effectively addressing the\nchallenges posed by low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence labeling remains a significant challenge in low-resource,\ndomain-specific scenarios, particularly for character-dense languages like\nChinese. Existing methods primarily focus on enhancing model comprehension and\nimproving data diversity to boost performance. However, these approaches still\nstruggle with inadequate model applicability and semantic distribution biases\nin domain-specific contexts. To overcome these limitations, we propose a novel\nframework that combines an LLM-based knowledge enhancement workflow with a\nspan-based Knowledge Fusion for Rich and Efficient Extraction (KnowFREE) model.\nOur workflow employs explanation prompts to generate precise contextual\ninterpretations of target entities, effectively mitigating semantic biases and\nenriching the model's contextual understanding. The KnowFREE model further\nintegrates extension label features, enabling efficient nested entity\nextraction without relying on external knowledge during inference. Experiments\non multiple Chinese domain-specific sequence labeling datasets demonstrate that\nour approach achieves state-of-the-art performance, effectively addressing the\nchallenges posed by low-resource settings."
                },
                "authors": [
                    {
                        "name": "Peichao Lai"
                    },
                    {
                        "name": "Jiaxin Gan"
                    },
                    {
                        "name": "Feiyang Ye"
                    },
                    {
                        "name": "Yilei Wang"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19090v1",
                "updated": "2025-01-31T12:36:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    36,
                    31,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T12:36:31Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    36,
                    31,
                    4,
                    31,
                    0
                ],
                "title": "Pivoting Factorization: A Compact Meta Low-Rank Representation of\n  Sparsity for Efficient Inference in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pivoting Factorization: A Compact Meta Low-Rank Representation of\n  Sparsity for Efficient Inference in Large Language Models"
                },
                "summary": "The rapid growth of Large Language Models has driven demand for effective\nmodel compression techniques to reduce memory and computation costs. Low-rank\npruning has gained attention for its tensor coherence and GPU compatibility\nacross all densities. However, low-rank pruning has struggled to match the\nperformance of semi-structured pruning, often doubling perplexity (PPL) at\nsimilar densities. In this paper, we propose Pivoting Factorization (PIFA), a\nnovel lossless meta low-rank representation that unsupervisedly learns a\ncompact form of any low-rank representation, effectively eliminating redundant\ninformation. PIFA identifies pivot rows (linearly independent rows) and\nexpresses non-pivot rows as linear combinations, achieving an additional 24.2\\%\nmemory savings and 24.6\\% faster inference over low-rank layers at r/d = 0.5,\nthereby significantly enhancing performance at the same density. To mitigate\nthe performance degradation caused by low-rank pruning, we introduce a novel,\nretraining-free low-rank reconstruction method that minimizes error\naccumulation (M). MPIFA, combining M and PIFA into an end-to-end framework,\nsignificantly outperforms existing low-rank pruning methods and, for the first\ntime, achieves performance comparable to semi-structured pruning, while\nsurpassing it in GPU efficiency and compatibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models has driven demand for effective\nmodel compression techniques to reduce memory and computation costs. Low-rank\npruning has gained attention for its tensor coherence and GPU compatibility\nacross all densities. However, low-rank pruning has struggled to match the\nperformance of semi-structured pruning, often doubling perplexity (PPL) at\nsimilar densities. In this paper, we propose Pivoting Factorization (PIFA), a\nnovel lossless meta low-rank representation that unsupervisedly learns a\ncompact form of any low-rank representation, effectively eliminating redundant\ninformation. PIFA identifies pivot rows (linearly independent rows) and\nexpresses non-pivot rows as linear combinations, achieving an additional 24.2\\%\nmemory savings and 24.6\\% faster inference over low-rank layers at r/d = 0.5,\nthereby significantly enhancing performance at the same density. To mitigate\nthe performance degradation caused by low-rank pruning, we introduce a novel,\nretraining-free low-rank reconstruction method that minimizes error\naccumulation (M). MPIFA, combining M and PIFA into an end-to-end framework,\nsignificantly outperforms existing low-rank pruning methods and, for the first\ntime, achieves performance comparable to semi-structured pruning, while\nsurpassing it in GPU efficiency and compatibility."
                },
                "authors": [
                    {
                        "name": "Jialin Zhao"
                    },
                    {
                        "name": "Yingtao Zhang"
                    },
                    {
                        "name": "Carlo Vittorio Cannistraci"
                    }
                ],
                "author_detail": {
                    "name": "Carlo Vittorio Cannistraci"
                },
                "author": "Carlo Vittorio Cannistraci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13302v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13302v3",
                "updated": "2025-01-31T12:35:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    35,
                    54,
                    4,
                    31,
                    0
                ],
                "published": "2024-06-19T07:42:48Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    7,
                    42,
                    48,
                    2,
                    171,
                    0
                ],
                "title": "SituationalLLM: Proactive language models with scene awareness for\n  dynamic, contextual task guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SituationalLLM: Proactive language models with scene awareness for\n  dynamic, contextual task guidance"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in text-based\ntasks but often struggle to provide actionable guidance in real-world physical\nenvironments. This is because of their inability to recognize their limited\nunderstanding of the user's physical context. We present SituationalLLM, a\nnovel approach that integrates structured scene information into an LLM to\ndeliver proactive, context-aware assistance. By encoding objects, attributes,\nand relationships in a custom Scene Graph Language, SituationalLLM actively\nidentifies gaps in environmental context and seeks clarifications during user\ninteractions. This behavior emerges from training on the Situational Awareness\nDatabase for Instruct-Tuning (SAD-Instruct), which combines diverse,\nscenario-specific scene graphs with iterative, dialogue-based refinements.\nExperimental results indicate that SituationalLLM outperforms generic LLM\nbaselines in task specificity, reliability, and adaptability, paving the way\nfor environment-aware AI assistants capable of delivering robust, user-centric\nguidance under real-world constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in text-based\ntasks but often struggle to provide actionable guidance in real-world physical\nenvironments. This is because of their inability to recognize their limited\nunderstanding of the user's physical context. We present SituationalLLM, a\nnovel approach that integrates structured scene information into an LLM to\ndeliver proactive, context-aware assistance. By encoding objects, attributes,\nand relationships in a custom Scene Graph Language, SituationalLLM actively\nidentifies gaps in environmental context and seeks clarifications during user\ninteractions. This behavior emerges from training on the Situational Awareness\nDatabase for Instruct-Tuning (SAD-Instruct), which combines diverse,\nscenario-specific scene graphs with iterative, dialogue-based refinements.\nExperimental results indicate that SituationalLLM outperforms generic LLM\nbaselines in task specificity, reliability, and adaptability, paving the way\nfor environment-aware AI assistants capable of delivering robust, user-centric\nguidance under real-world constraints."
                },
                "authors": [
                    {
                        "name": "Muhammad Saif Ullah Khan"
                    },
                    {
                        "name": "Muhammad Zeshan Afzal"
                    },
                    {
                        "name": "Didier Stricker"
                    }
                ],
                "author_detail": {
                    "name": "Didier Stricker"
                },
                "author": "Didier Stricker",
                "arxiv_comment": "Revised Submission to Open Research Europe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13302v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13302v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19086v1",
                "updated": "2025-01-31T12:23:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    23,
                    50,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T12:23:50Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    23,
                    50,
                    4,
                    31,
                    0
                ],
                "title": "Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image\n  Classification"
                },
                "summary": "X-ray imaging is pivotal in medical diagnostics, offering non-invasive\ninsights into a range of health conditions. Recently, vision-language models,\nsuch as the Contrastive Language-Image Pretraining (CLIP) model, have\ndemonstrated potential in improving diagnostic accuracy by leveraging\nlarge-scale image-text datasets. However, since CLIP was not initially designed\nfor medical images, several CLIP-like models trained specifically on medical\nimages have been developed. Despite their enhanced performance, issues of\nfairness - particularly regarding demographic attributes - remain largely\nunaddressed. In this study, we perform a comprehensive fairness analysis of\nCLIP-like models applied to X-ray image classification. We assess their\nperformance and fairness across diverse patient demographics and disease\ncategories using zero-shot inference and various fine-tuning techniques,\nincluding Linear Probing, Multilayer Perceptron (MLP), Low-Rank Adaptation\n(LoRA), and full fine-tuning. Our results indicate that while fine-tuning\nimproves model accuracy, fairness concerns persist, highlighting the need for\nfurther fairness interventions in these foundational models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray imaging is pivotal in medical diagnostics, offering non-invasive\ninsights into a range of health conditions. Recently, vision-language models,\nsuch as the Contrastive Language-Image Pretraining (CLIP) model, have\ndemonstrated potential in improving diagnostic accuracy by leveraging\nlarge-scale image-text datasets. However, since CLIP was not initially designed\nfor medical images, several CLIP-like models trained specifically on medical\nimages have been developed. Despite their enhanced performance, issues of\nfairness - particularly regarding demographic attributes - remain largely\nunaddressed. In this study, we perform a comprehensive fairness analysis of\nCLIP-like models applied to X-ray image classification. We assess their\nperformance and fairness across diverse patient demographics and disease\ncategories using zero-shot inference and various fine-tuning techniques,\nincluding Linear Probing, Multilayer Perceptron (MLP), Low-Rank Adaptation\n(LoRA), and full fine-tuning. Our results indicate that while fine-tuning\nimproves model accuracy, fairness concerns persist, highlighting the need for\nfurther fairness interventions in these foundational models."
                },
                "authors": [
                    {
                        "name": "Xiangyu Sun"
                    },
                    {
                        "name": "Xiaoguang Zou"
                    },
                    {
                        "name": "Yuanquan Wu"
                    },
                    {
                        "name": "Guotai Wang"
                    },
                    {
                        "name": "Shaoting Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shaoting Zhang"
                },
                "author": "Shaoting Zhang",
                "arxiv_comment": "This paper has been accepted for presentation at the 2025 IEEE\n  International Symposium on Biomedical Imaging (ISBI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19085v1",
                "updated": "2025-01-31T12:23:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    23,
                    28,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T12:23:28Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    23,
                    28,
                    4,
                    31,
                    0
                ],
                "title": "Enhancing Code Generation for Low-Resource Languages: No Silver Bullet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Code Generation for Low-Resource Languages: No Silver Bullet"
                },
                "summary": "The advent of Large Language Models (LLMs) has significantly advanced the\nfield of automated code generation. LLMs rely on large and diverse datasets to\nlearn syntax, semantics, and usage patterns of programming languages. For\nlow-resource languages (i.e., niche programming languages characterized by the\nscarcity of training data), the limited availability of such data hampers the\nmodels' ability to generalize effectively, resulting in poorer code generation\nperformance as compared to high-resource languages. For this reason, there is a\nquest for techniques able to close this performance gap. We present an\nempirical study investigating the effectiveness of several approaches for\nboosting LLMs' performance on low-resource languages, namely: (i) a classic\nfine-tuning, which is however capped in size by the scarcity of training data;\n(ii) three variants of in-context learning, with prompts crafted to provide the\nLLM with additional information about the low-resource language (e.g., few-shot\nexamples showcasing features of the targeted language); and (iii) a\npre-training objective teaching the model how to translate between high- and\nlow-resource languages. The context of our study are two low-resource languages\n(R and Racket) and six LLMs having different architectures and sizes. Our\nfindings reveal that a fine-tuning is usually the best choice for smaller LLMs,\npossibly due to the fact that even a small dataset is sufficient to train their\nlimited number of parameters. With the increase in size of the models,\nin-context learning becomes more and more effective, representing a safe and\ncheap bet (i.e., it always helps, but with different magnitudes). Differently,\nvery large LLMs may deteriorate their performance on low-resource languages\nwhen fine-tuning is performed, possibly due to the lack of enough data needed\nto effectively update their weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has significantly advanced the\nfield of automated code generation. LLMs rely on large and diverse datasets to\nlearn syntax, semantics, and usage patterns of programming languages. For\nlow-resource languages (i.e., niche programming languages characterized by the\nscarcity of training data), the limited availability of such data hampers the\nmodels' ability to generalize effectively, resulting in poorer code generation\nperformance as compared to high-resource languages. For this reason, there is a\nquest for techniques able to close this performance gap. We present an\nempirical study investigating the effectiveness of several approaches for\nboosting LLMs' performance on low-resource languages, namely: (i) a classic\nfine-tuning, which is however capped in size by the scarcity of training data;\n(ii) three variants of in-context learning, with prompts crafted to provide the\nLLM with additional information about the low-resource language (e.g., few-shot\nexamples showcasing features of the targeted language); and (iii) a\npre-training objective teaching the model how to translate between high- and\nlow-resource languages. The context of our study are two low-resource languages\n(R and Racket) and six LLMs having different architectures and sizes. Our\nfindings reveal that a fine-tuning is usually the best choice for smaller LLMs,\npossibly due to the fact that even a small dataset is sufficient to train their\nlimited number of parameters. With the increase in size of the models,\nin-context learning becomes more and more effective, representing a safe and\ncheap bet (i.e., it always helps, but with different magnitudes). Differently,\nvery large LLMs may deteriorate their performance on low-resource languages\nwhen fine-tuning is performed, possibly due to the lack of enough data needed\nto effectively update their weights."
                },
                "authors": [
                    {
                        "name": "Alessandro Giagnorio"
                    },
                    {
                        "name": "Alberto Martin-Lopez"
                    },
                    {
                        "name": "Gabriele Bavota"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Bavota"
                },
                "author": "Gabriele Bavota",
                "arxiv_comment": "Accepted at ICPC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17408v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17408v2",
                "updated": "2025-01-31T12:19:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    19,
                    43,
                    4,
                    31,
                    0
                ],
                "published": "2024-09-25T22:39:55Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    22,
                    39,
                    55,
                    2,
                    269,
                    0
                ],
                "title": "Sociotechnical Approach to Enterprise Generative Artificial Intelligence\n  (E-GenAI)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sociotechnical Approach to Enterprise Generative Artificial Intelligence\n  (E-GenAI)"
                },
                "summary": "In this theoretical article, a sociotechnical approach is proposed to\ncharacterize. First, the business ecosystem, focusing on the relationships\namong Providers, Enterprise, and Customers through SCM, ERP, and CRM platforms\nto align: (1) Business Intelligence (BI), Fuzzy Logic (FL), and TRIZ (Theory of\nInventive Problem Solving), through the OID model, and (2) Knowledge Management\n(KM) and Imperfect Knowledge Management (IKM), through the OIDK model. Second,\nthe article explores the E-GenAI business ecosystem, which integrates\nGenAI-based platforms for SCM, ERP, and CRM with GenAI-based platforms for BI,\nFL, TRIZ, KM, and IKM, to align Large Language Models (LLMs) through the\nE-GenAI (OID) model. Finally, to understand the dynamics of LLMs, we utilize\nfinite automata to model the relationships between Followers and Followees.\nThis facilitates the construction of LLMs that can identify specific\ncharacteristics of users on a social media platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this theoretical article, a sociotechnical approach is proposed to\ncharacterize. First, the business ecosystem, focusing on the relationships\namong Providers, Enterprise, and Customers through SCM, ERP, and CRM platforms\nto align: (1) Business Intelligence (BI), Fuzzy Logic (FL), and TRIZ (Theory of\nInventive Problem Solving), through the OID model, and (2) Knowledge Management\n(KM) and Imperfect Knowledge Management (IKM), through the OIDK model. Second,\nthe article explores the E-GenAI business ecosystem, which integrates\nGenAI-based platforms for SCM, ERP, and CRM with GenAI-based platforms for BI,\nFL, TRIZ, KM, and IKM, to align Large Language Models (LLMs) through the\nE-GenAI (OID) model. Finally, to understand the dynamics of LLMs, we utilize\nfinite automata to model the relationships between Followers and Followees.\nThis facilitates the construction of LLMs that can identify specific\ncharacteristics of users on a social media platform."
                },
                "authors": [
                    {
                        "name": "Leoncio Jimenez"
                    },
                    {
                        "name": "Francisco Venegas"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Venegas"
                },
                "author": "Francisco Venegas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17408v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19060v1",
                "updated": "2025-01-31T11:47:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    47,
                    15,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:47:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    47,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Contrast-Aware Calibration for Fine-Tuned CLIP: Leveraging Image-Text\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrast-Aware Calibration for Fine-Tuned CLIP: Leveraging Image-Text\n  Alignment"
                },
                "summary": "Vision-language models (VLMs), such as CLIP, have demonstrated exceptional\ngeneralization capabilities and can quickly adapt to downstream tasks through\nprompt fine-tuning. Unfortunately, in classification tasks involving\nnon-training classes, known as open-vocabulary setting, fine-tuned VLMs often\noverfit to train classes, resulting in a misalignment between confidence scores\nand actual accuracy on unseen classes, which significantly undermines their\nreliability in real-world deployments. Existing confidence calibration methods\ntypically require training parameters or analyzing features from the training\ndataset, restricting their ability to generalize unseen classes without\ncorresponding train data. Moreover, VLM-specific calibration methods rely\nsolely on text features from train classes as calibration indicators, which\ninherently limits their ability to calibrate train classes. To address these\nchallenges, we propose an effective multimodal calibration method\nContrast-Aware Calibration (CAC). Building on the original CLIP's zero-shot\nadaptability and the conclusion from empirical analysis that poor intra-class\nand inter-class discriminative ability on unseen classes is the root cause, we\ncalculate calibration weights based on the contrastive difference between the\noriginal and fine-tuned CLIP. This method not only adapts to calibrating unseen\nclasses but also overcomes the limitations of previous VLM calibration methods\nthat could not calibrate train classes. In experiments involving 11 datasets\nwith 5 fine-tuning methods, CAC consistently achieved the best calibration\neffect on both train and unseen classes without sacrificing accuracy and\ninference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs), such as CLIP, have demonstrated exceptional\ngeneralization capabilities and can quickly adapt to downstream tasks through\nprompt fine-tuning. Unfortunately, in classification tasks involving\nnon-training classes, known as open-vocabulary setting, fine-tuned VLMs often\noverfit to train classes, resulting in a misalignment between confidence scores\nand actual accuracy on unseen classes, which significantly undermines their\nreliability in real-world deployments. Existing confidence calibration methods\ntypically require training parameters or analyzing features from the training\ndataset, restricting their ability to generalize unseen classes without\ncorresponding train data. Moreover, VLM-specific calibration methods rely\nsolely on text features from train classes as calibration indicators, which\ninherently limits their ability to calibrate train classes. To address these\nchallenges, we propose an effective multimodal calibration method\nContrast-Aware Calibration (CAC). Building on the original CLIP's zero-shot\nadaptability and the conclusion from empirical analysis that poor intra-class\nand inter-class discriminative ability on unseen classes is the root cause, we\ncalculate calibration weights based on the contrastive difference between the\noriginal and fine-tuned CLIP. This method not only adapts to calibrating unseen\nclasses but also overcomes the limitations of previous VLM calibration methods\nthat could not calibrate train classes. In experiments involving 11 datasets\nwith 5 fine-tuning methods, CAC consistently achieved the best calibration\neffect on both train and unseen classes without sacrificing accuracy and\ninference speed."
                },
                "authors": [
                    {
                        "name": "Song-Lin Lv"
                    },
                    {
                        "name": "Yu-Yang Chen"
                    },
                    {
                        "name": "Zhi Zhou"
                    },
                    {
                        "name": "Yu-Feng Li"
                    },
                    {
                        "name": "Lan-Zhe Guo"
                    }
                ],
                "author_detail": {
                    "name": "Lan-Zhe Guo"
                },
                "author": "Lan-Zhe Guo",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2402.04655 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19057v1",
                "updated": "2025-01-31T11:34:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    34,
                    3,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:34:03Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    34,
                    3,
                    4,
                    31,
                    0
                ],
                "title": "TeZO: Empowering the Low-Rankness on the Temporal Dimension in the\n  Zeroth-Order Optimization for Fine-tuning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeZO: Empowering the Low-Rankness on the Temporal Dimension in the\n  Zeroth-Order Optimization for Fine-tuning LLMs"
                },
                "summary": "Zeroth-order optimization (ZO) has demonstrated remarkable promise in\nefficient fine-tuning tasks for Large Language Models (LLMs). In particular,\nrecent advances incorporate the low-rankness of gradients, introducing low-rank\nZO estimators to further reduce GPU memory consumption. However, most existing\nworks focus solely on the low-rankness of each individual gradient, overlooking\na broader property shared by all gradients throughout the training, i.e., all\ngradients approximately reside within a similar subspace. In this paper, we\nconsider two factors together and propose a novel low-rank ZO estimator, TeZO,\nwhich captures the low-rankness across both the model and temporal dimension.\nSpecifically, we represent ZO perturbations along the temporal dimension as a\n3D tensor and employ Canonical Polyadic Decomposition (CPD) to extract each\nlow-rank 2D matrix, significantly reducing the training cost. TeZO can also be\neasily extended to the Adam variant while consuming less memory than MeZO-SGD,\nand requiring about only 35% memory of MeZO-Adam. Both comprehensive\ntheoretical analysis and extensive experimental research have validated its\nefficiency, achieving SOTA-comparable results with lower overhead of time and\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zeroth-order optimization (ZO) has demonstrated remarkable promise in\nefficient fine-tuning tasks for Large Language Models (LLMs). In particular,\nrecent advances incorporate the low-rankness of gradients, introducing low-rank\nZO estimators to further reduce GPU memory consumption. However, most existing\nworks focus solely on the low-rankness of each individual gradient, overlooking\na broader property shared by all gradients throughout the training, i.e., all\ngradients approximately reside within a similar subspace. In this paper, we\nconsider two factors together and propose a novel low-rank ZO estimator, TeZO,\nwhich captures the low-rankness across both the model and temporal dimension.\nSpecifically, we represent ZO perturbations along the temporal dimension as a\n3D tensor and employ Canonical Polyadic Decomposition (CPD) to extract each\nlow-rank 2D matrix, significantly reducing the training cost. TeZO can also be\neasily extended to the Adam variant while consuming less memory than MeZO-SGD,\nand requiring about only 35% memory of MeZO-Adam. Both comprehensive\ntheoretical analysis and extensive experimental research have validated its\nefficiency, achieving SOTA-comparable results with lower overhead of time and\nmemory."
                },
                "authors": [
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19056v1",
                "updated": "2025-01-31T11:32:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    32,
                    5,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:32:05Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    32,
                    5,
                    4,
                    31,
                    0
                ],
                "title": "Enabling Autonomic Microservice Management through Self-Learning Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Autonomic Microservice Management through Self-Learning Agents"
                },
                "summary": "The increasing complexity of modern software systems necessitates robust\nautonomic self-management capabilities. While Large Language Models (LLMs)\ndemonstrate potential in this domain, they often face challenges in adapting\ntheir general knowledge to specific service contexts. To address this\nlimitation, we propose ServiceOdyssey, a self-learning agent system that\nautonomously manages microservices without requiring prior knowledge of\nservice-specific configurations. By leveraging curriculum learning principles\nand iterative exploration, ServiceOdyssey progressively develops a deep\nunderstanding of operational environments, reducing dependence on human input\nor static documentation. A prototype built with the Sock Shop microservice\ndemonstrates the potential of this approach for autonomic microservice\nmanagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of modern software systems necessitates robust\nautonomic self-management capabilities. While Large Language Models (LLMs)\ndemonstrate potential in this domain, they often face challenges in adapting\ntheir general knowledge to specific service contexts. To address this\nlimitation, we propose ServiceOdyssey, a self-learning agent system that\nautonomously manages microservices without requiring prior knowledge of\nservice-specific configurations. By leveraging curriculum learning principles\nand iterative exploration, ServiceOdyssey progressively develops a deep\nunderstanding of operational environments, reducing dependence on human input\nor static documentation. A prototype built with the Sock Shop microservice\ndemonstrates the potential of this approach for autonomic microservice\nmanagement."
                },
                "authors": [
                    {
                        "name": "Fenglin Yu"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Xiaoting Qin"
                    },
                    {
                        "name": "Zhiyang Zhang"
                    },
                    {
                        "name": "Jue Zhang"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Yingnong Dang"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19054v1",
                "updated": "2025-01-31T11:28:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    28,
                    16,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:28:16Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    28,
                    16,
                    4,
                    31,
                    0
                ],
                "title": "Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models"
                },
                "summary": "Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively."
                },
                "authors": [
                    {
                        "name": "Ruiyu Wang"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Shizhao Sun"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14487v2",
                "updated": "2025-01-31T11:16:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    16,
                    51,
                    4,
                    31,
                    0
                ],
                "published": "2024-07-19T17:41:08Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    17,
                    41,
                    8,
                    4,
                    201,
                    0
                ],
                "title": "Evaluating the Reliability of Self-Explanations in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Reliability of Self-Explanations in Large Language Models"
                },
                "summary": "This paper investigates the reliability of explanations generated by large\nlanguage models (LLMs) when prompted to explain their previous output. We\nevaluate two kinds of such self-explanations - extractive and counterfactual -\nusing three state-of-the-art LLMs (2B to 8B parameters) on two different\nclassification tasks (objective and subjective). Our findings reveal, that,\nwhile these self-explanations can correlate with human judgement, they do not\nfully and accurately follow the model's decision process, indicating a gap\nbetween perceived and actual model reasoning. We show that this gap can be\nbridged because prompting LLMs for counterfactual explanations can produce\nfaithful, informative, and easy-to-verify results. These counterfactuals offer\na promising alternative to traditional explainability methods (e.g. SHAP,\nLIME), provided that prompts are tailored to specific tasks and checked for\nvalidity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the reliability of explanations generated by large\nlanguage models (LLMs) when prompted to explain their previous output. We\nevaluate two kinds of such self-explanations - extractive and counterfactual -\nusing three state-of-the-art LLMs (2B to 8B parameters) on two different\nclassification tasks (objective and subjective). Our findings reveal, that,\nwhile these self-explanations can correlate with human judgement, they do not\nfully and accurately follow the model's decision process, indicating a gap\nbetween perceived and actual model reasoning. We show that this gap can be\nbridged because prompting LLMs for counterfactual explanations can produce\nfaithful, informative, and easy-to-verify results. These counterfactuals offer\na promising alternative to traditional explainability methods (e.g. SHAP,\nLIME), provided that prompts are tailored to specific tasks and checked for\nvalidity."
                },
                "authors": [
                    {
                        "name": "Korbinian Randl"
                    },
                    {
                        "name": "John Pavlopoulos"
                    },
                    {
                        "name": "Aron Henriksson"
                    },
                    {
                        "name": "Tony Lindgren"
                    }
                ],
                "author_detail": {
                    "name": "Tony Lindgren"
                },
                "author": "Tony Lindgren",
                "arxiv_doi": "10.1007/978-3-031-78977-9_3",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-78977-9_3",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.14487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Non peer-reviewed preprint. Presented at Discovery Science 2024.\n  Peer-reviewed version published in the Springer Lecture Notes in Computer\n  Science (vol 15243)",
                "arxiv_journal_ref": "Lecture Notes in Computer Science(2025), vol 15243. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04235v2",
                "updated": "2025-01-31T11:14:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    14,
                    25,
                    4,
                    31,
                    0
                ],
                "published": "2024-12-05T15:11:12Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    11,
                    12,
                    3,
                    340,
                    0
                ],
                "title": "Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM\n  Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM\n  Chatbots"
                },
                "summary": "I combine detection and mitigation techniques to addresses hallucinations in\nLarge Language Models (LLMs). Mitigation is achieved in a question-answering\nRetrieval-Augmented Generation (RAG) framework while detection is obtained by\nintroducing the Negative Missing Information Scoring System (NMISS), which\naccounts for contextual relevance in responses. While RAG mitigates\nhallucinations by grounding answers in external data, NMISS refines the\nevaluation by identifying cases where traditional metrics incorrectly flag\ncontextually accurate responses as hallucinations. I use Italian health news\narticles as context to evaluate LLM performance. Results show that Gemma2 and\nGPT-4 outperform the other models, with GPT-4 producing answers closely aligned\nwith reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral\nbenefit significantly from NMISS, highlighting their ability to provide richer\ncontextual information. This combined approach offers new insights into the\nreduction and more accurate assessment of hallucinations in LLMs, with\napplications in real-world healthcare tasks and other domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I combine detection and mitigation techniques to addresses hallucinations in\nLarge Language Models (LLMs). Mitigation is achieved in a question-answering\nRetrieval-Augmented Generation (RAG) framework while detection is obtained by\nintroducing the Negative Missing Information Scoring System (NMISS), which\naccounts for contextual relevance in responses. While RAG mitigates\nhallucinations by grounding answers in external data, NMISS refines the\nevaluation by identifying cases where traditional metrics incorrectly flag\ncontextually accurate responses as hallucinations. I use Italian health news\narticles as context to evaluate LLM performance. Results show that Gemma2 and\nGPT-4 outperform the other models, with GPT-4 producing answers closely aligned\nwith reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral\nbenefit significantly from NMISS, highlighting their ability to provide richer\ncontextual information. This combined approach offers new insights into the\nreduction and more accurate assessment of hallucinations in LLMs, with\napplications in real-world healthcare tasks and other domains."
                },
                "authors": [
                    {
                        "name": "Maria Paola Priola"
                    }
                ],
                "author_detail": {
                    "name": "Maria Paola Priola"
                },
                "author": "Maria Paola Priola",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19040v1",
                "updated": "2025-01-31T11:10:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    10,
                    49,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:10:49Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    10,
                    49,
                    4,
                    31,
                    0
                ],
                "title": "Towards the Worst-case Robustness of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the Worst-case Robustness of Large Language Models"
                },
                "summary": "Recent studies have revealed the vulnerability of Large Language Models\n(LLMs) to adversarial attacks, where the adversary crafts specific input\nsequences to induce harmful, violent, private, or incorrect outputs. Although\nvarious defenses have been proposed, they have not been evaluated by strong\nadaptive attacks, leaving the worst-case robustness of LLMs still intractable.\nBy developing a stronger white-box attack, our evaluation results indicate that\nmost typical defenses achieve nearly 0\\% robustness.To solve this, we propose\n\\textit{DiffTextPure}, a general defense that diffuses the (adversarial) input\nprompt using any pre-defined smoothing distribution, and purifies the diffused\ninput using a pre-trained language model. Theoretically, we derive tight\nrobustness lower bounds for all smoothing distributions using Fractal Knapsack\nor 0-1 Knapsack solvers. Under this framework, we certify the robustness of a\nspecific case -- smoothing LLMs using a uniform kernel -- against \\textit{any\npossible attack} with an average $\\ell_0$ perturbation of 2.02 or an average\nsuffix length of 6.41.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have revealed the vulnerability of Large Language Models\n(LLMs) to adversarial attacks, where the adversary crafts specific input\nsequences to induce harmful, violent, private, or incorrect outputs. Although\nvarious defenses have been proposed, they have not been evaluated by strong\nadaptive attacks, leaving the worst-case robustness of LLMs still intractable.\nBy developing a stronger white-box attack, our evaluation results indicate that\nmost typical defenses achieve nearly 0\\% robustness.To solve this, we propose\n\\textit{DiffTextPure}, a general defense that diffuses the (adversarial) input\nprompt using any pre-defined smoothing distribution, and purifies the diffused\ninput using a pre-trained language model. Theoretically, we derive tight\nrobustness lower bounds for all smoothing distributions using Fractal Knapsack\nor 0-1 Knapsack solvers. Under this framework, we certify the robustness of a\nspecific case -- smoothing LLMs using a uniform kernel -- against \\textit{any\npossible attack} with an average $\\ell_0$ perturbation of 2.02 or an average\nsuffix length of 6.41."
                },
                "authors": [
                    {
                        "name": "Huanran Chen"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Zeming Wei"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19038v1",
                "updated": "2025-01-31T11:10:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    10,
                    19,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:10:19Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    10,
                    19,
                    4,
                    31,
                    0
                ],
                "title": "Conformal Prediction in Hierarchical Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Prediction in Hierarchical Classification"
                },
                "summary": "Conformal prediction has emerged as a widely used framework for constructing\nvalid prediction sets in classification and regression tasks. In this work, we\nextend the split conformal prediction framework to hierarchical classification,\nwhere prediction sets are commonly restricted to internal nodes of a predefined\nhierarchy, and propose two computationally efficient inference algorithms. The\nfirst algorithm returns internal nodes as prediction sets, while the second\nrelaxes this restriction, using the notion of representation complexity,\nyielding a more general and combinatorial inference problem, but smaller set\nsizes. Empirical evaluations on several benchmark datasets demonstrate the\neffectiveness of the proposed algorithms in achieving nominal coverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal prediction has emerged as a widely used framework for constructing\nvalid prediction sets in classification and regression tasks. In this work, we\nextend the split conformal prediction framework to hierarchical classification,\nwhere prediction sets are commonly restricted to internal nodes of a predefined\nhierarchy, and propose two computationally efficient inference algorithms. The\nfirst algorithm returns internal nodes as prediction sets, while the second\nrelaxes this restriction, using the notion of representation complexity,\nyielding a more general and combinatorial inference problem, but smaller set\nsizes. Empirical evaluations on several benchmark datasets demonstrate the\neffectiveness of the proposed algorithms in achieving nominal coverage."
                },
                "authors": [
                    {
                        "name": "Thomas Mortier"
                    },
                    {
                        "name": "Alireza Javanmardi"
                    },
                    {
                        "name": "Yusuf Sale"
                    },
                    {
                        "name": "Eyke Hüllermeier"
                    },
                    {
                        "name": "Willem Waegeman"
                    }
                ],
                "author_detail": {
                    "name": "Willem Waegeman"
                },
                "author": "Willem Waegeman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00005v2",
                "updated": "2025-01-31T10:45:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    45,
                    1,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-16T11:57:14Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    57,
                    14,
                    2,
                    290,
                    0
                ],
                "title": "Mastering the Craft of Data Synthesis for CodeLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mastering the Craft of Data Synthesis for CodeLLMs"
                },
                "summary": "Large language models (LLMs) have shown impressive performance in \\emph{code}\nunderstanding and generation, making coding tasks a key focus for researchers\ndue to their practical applications and value as a testbed for LLM evaluation.\nData synthesis and filtering techniques have been widely adopted and shown to\nbe highly effective in this context. In this paper, we present a focused survey\nand taxonomy of these techniques, emphasizing recent advancements. We highlight\nkey challenges, explore future research directions, and offer practical\nguidance for new researchers entering the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive performance in \\emph{code}\nunderstanding and generation, making coding tasks a key focus for researchers\ndue to their practical applications and value as a testbed for LLM evaluation.\nData synthesis and filtering techniques have been widely adopted and shown to\nbe highly effective in this context. In this paper, we present a focused survey\nand taxonomy of these techniques, emphasizing recent advancements. We highlight\nkey challenges, explore future research directions, and offer practical\nguidance for new researchers entering the field."
                },
                "authors": [
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Philip Arthur"
                    },
                    {
                        "name": "Qianyu Feng"
                    },
                    {
                        "name": "Cong Duy Vu Hoang"
                    },
                    {
                        "name": "Yu-Heng Hong"
                    },
                    {
                        "name": "Mahdi Kazemi Moghaddam"
                    },
                    {
                        "name": "Omid Nezami"
                    },
                    {
                        "name": "Thien Nguyen"
                    },
                    {
                        "name": "Gioacchino Tangari"
                    },
                    {
                        "name": "Duy Vu"
                    },
                    {
                        "name": "Thanh Vu"
                    },
                    {
                        "name": "Mark Johnson"
                    },
                    {
                        "name": "Krishnaram Kenthapadi"
                    },
                    {
                        "name": "Don Dharmasiri"
                    },
                    {
                        "name": "Long Duong"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuan-Fang Li"
                },
                "author": "Yuan-Fang Li",
                "arxiv_comment": "Accepted at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19012v1",
                "updated": "2025-01-31T10:26:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    26,
                    18,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T10:26:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    26,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities"
                },
                "summary": "Large Language Models (LLMs) have become an essential tool in the\nprogrammer's toolkit, but their tendency to hallucinate code can be used by\nmalicious actors to introduce vulnerabilities to broad swathes of the software\nsupply chain. In this work, we analyze package hallucination behaviour in LLMs\nacross popular programming languages examining both existing package references\nand fictional dependencies. By analyzing this package hallucination behaviour\nwe find potential attacks and suggest defensive strategies to defend against\nthese attacks. We discover that package hallucination rate is predicated not\nonly on model choice, but also programming language, model size, and\nspecificity of the coding task request. The Pareto optimality boundary between\ncode generation performance and package hallucination is sparsely populated,\nsuggesting that coding models are not being optimized for secure code.\nAdditionally, we find an inverse correlation between package hallucination rate\nand the HumanEval coding benchmark, offering a heuristic for evaluating the\npropensity of a model to hallucinate packages. Our metrics, findings and\nanalyses provide a base for future models, securing AI-assisted software\ndevelopment workflows against package supply chain attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become an essential tool in the\nprogrammer's toolkit, but their tendency to hallucinate code can be used by\nmalicious actors to introduce vulnerabilities to broad swathes of the software\nsupply chain. In this work, we analyze package hallucination behaviour in LLMs\nacross popular programming languages examining both existing package references\nand fictional dependencies. By analyzing this package hallucination behaviour\nwe find potential attacks and suggest defensive strategies to defend against\nthese attacks. We discover that package hallucination rate is predicated not\nonly on model choice, but also programming language, model size, and\nspecificity of the coding task request. The Pareto optimality boundary between\ncode generation performance and package hallucination is sparsely populated,\nsuggesting that coding models are not being optimized for secure code.\nAdditionally, we find an inverse correlation between package hallucination rate\nand the HumanEval coding benchmark, offering a heuristic for evaluating the\npropensity of a model to hallucinate packages. Our metrics, findings and\nanalyses provide a base for future models, securing AI-assisted software\ndevelopment workflows against package supply chain attacks."
                },
                "authors": [
                    {
                        "name": "Arjun Krishna"
                    },
                    {
                        "name": "Erick Galinkin"
                    },
                    {
                        "name": "Leon Derczynski"
                    },
                    {
                        "name": "Jeffrey Martin"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Martin"
                },
                "author": "Jeffrey Martin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18028v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18028v3",
                "updated": "2025-01-31T10:15:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    15,
                    43,
                    4,
                    31,
                    0
                ],
                "published": "2024-09-26T16:34:35Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    34,
                    35,
                    3,
                    270,
                    0
                ],
                "title": "Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective"
                },
                "summary": "A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically."
                },
                "authors": [
                    {
                        "name": "Yotam Wolf"
                    },
                    {
                        "name": "Binyamin Rothberg"
                    },
                    {
                        "name": "Dorin Shteyman"
                    },
                    {
                        "name": "Amnon Shashua"
                    }
                ],
                "author_detail": {
                    "name": "Amnon Shashua"
                },
                "author": "Amnon Shashua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18028v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18028v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.05362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05362v2",
                "updated": "2025-01-31T18:58:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    58,
                    24,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-07T17:45:00Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    45,
                    0,
                    0,
                    281,
                    0
                ],
                "title": "LLMs Are In-Context Bandit Reinforcement Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Are In-Context Bandit Reinforcement Learners"
                },
                "summary": "Large Language Models (LLMs) excel at in-context learning (ICL), a supervised\nlearning technique that relies on adding annotated examples to the model\ncontext. We investigate a contextual bandit version of in-context reinforcement\nlearning (ICRL), where models learn in-context, online, from external reward,\ninstead of supervised data. We show that LLMs effectively demonstrate such\nlearning, and provide a detailed study of the phenomena, experimenting with\nchallenging classification tasks and models of sizes from 500M to 70B\nparameters. This includes identifying and addressing the instability of the\nprocess, demonstrating learning with both semantic and abstract labels, and\nshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, while\nalso underscoring fundamental limitations in their implicit reasoning about\nerrors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at in-context learning (ICL), a supervised\nlearning technique that relies on adding annotated examples to the model\ncontext. We investigate a contextual bandit version of in-context reinforcement\nlearning (ICRL), where models learn in-context, online, from external reward,\ninstead of supervised data. We show that LLMs effectively demonstrate such\nlearning, and provide a detailed study of the phenomena, experimenting with\nchallenging classification tasks and models of sizes from 500M to 70B\nparameters. This includes identifying and addressing the instability of the\nprocess, demonstrating learning with both semantic and abstract labels, and\nshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, while\nalso underscoring fundamental limitations in their implicit reasoning about\nerrors."
                },
                "authors": [
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Kianté Brantley"
                    },
                    {
                        "name": "Yoav Artzi"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Artzi"
                },
                "author": "Yoav Artzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19398v1",
                "updated": "2025-01-31T18:53:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    53,
                    43,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:53:43Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    53,
                    43,
                    4,
                    31,
                    0
                ],
                "title": "Do LLMs Strategically Reveal, Conceal, and Infer Information? A\n  Theoretical and Empirical Analysis in The Chameleon Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Strategically Reveal, Conceal, and Infer Information? A\n  Theoretical and Empirical Analysis in The Chameleon Game"
                },
                "summary": "Large language model-based (LLM-based) agents have become common in settings\nthat include non-cooperative parties. In such settings, agents' decision-making\nneeds to conceal information from their adversaries, reveal information to\ntheir cooperators, and infer information to identify the other agents'\ncharacteristics. To investigate whether LLMs have these information control and\ndecision-making capabilities, we make LLM agents play the language-based\nhidden-identity game, The Chameleon. In the game, a group of non-chameleon\nagents who do not know each other aim to identify the chameleon agent without\nrevealing a secret. The game requires the aforementioned information control\ncapabilities both as a chameleon and a non-chameleon. The empirical results\nshow that while non-chameleon LLM agents identify the chameleon, they fail to\nconceal the secret from the chameleon, and their winning probability is far\nfrom the levels of even trivial strategies. To formally explain this behavior,\nwe give a theoretical analysis for a spectrum of strategies, from concealing to\nrevealing, and provide bounds on the non-chameleons' winning probability. Based\non the empirical results and theoretical analysis of different strategies, we\ndeduce that LLM-based non-chameleon agents reveal excessive information to\nagents of unknown identities. Our results point to a weakness of contemporary\nLLMs, including GPT-4, GPT-4o, Gemini 1.5, and Claude 3.5 Sonnet, in strategic\ninteractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based (LLM-based) agents have become common in settings\nthat include non-cooperative parties. In such settings, agents' decision-making\nneeds to conceal information from their adversaries, reveal information to\ntheir cooperators, and infer information to identify the other agents'\ncharacteristics. To investigate whether LLMs have these information control and\ndecision-making capabilities, we make LLM agents play the language-based\nhidden-identity game, The Chameleon. In the game, a group of non-chameleon\nagents who do not know each other aim to identify the chameleon agent without\nrevealing a secret. The game requires the aforementioned information control\ncapabilities both as a chameleon and a non-chameleon. The empirical results\nshow that while non-chameleon LLM agents identify the chameleon, they fail to\nconceal the secret from the chameleon, and their winning probability is far\nfrom the levels of even trivial strategies. To formally explain this behavior,\nwe give a theoretical analysis for a spectrum of strategies, from concealing to\nrevealing, and provide bounds on the non-chameleons' winning probability. Based\non the empirical results and theoretical analysis of different strategies, we\ndeduce that LLM-based non-chameleon agents reveal excessive information to\nagents of unknown identities. Our results point to a weakness of contemporary\nLLMs, including GPT-4, GPT-4o, Gemini 1.5, and Claude 3.5 Sonnet, in strategic\ninteractions."
                },
                "authors": [
                    {
                        "name": "Mustafa O. Karabag"
                    },
                    {
                        "name": "Ufuk Topcu"
                    }
                ],
                "author_detail": {
                    "name": "Ufuk Topcu"
                },
                "author": "Ufuk Topcu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08992v2",
                "updated": "2025-01-31T18:53:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    53,
                    30,
                    4,
                    31,
                    0
                ],
                "published": "2024-05-14T23:24:12Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    23,
                    24,
                    12,
                    1,
                    135,
                    0
                ],
                "title": "Contextual Emotion Recognition using Large Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Emotion Recognition using Large Vision Language Models"
                },
                "summary": "\"How does the person in the bounding box feel?\" Achieving human-level\nrecognition of the apparent emotion of a person in real world situations\nremains an unsolved task in computer vision. Facial expressions are not enough:\nbody pose, contextual knowledge, and commonsense reasoning all contribute to\nhow humans perform this emotional theory of mind task. In this paper, we\nexamine two major approaches enabled by recent large vision language models: 1)\nimage captioning followed by a language-only LLM, and 2) vision language\nmodels, under zero-shot and fine-tuned setups. We evaluate the methods on the\nEmotions in Context (EMOTIC) dataset and demonstrate that a vision language\nmodel, fine-tuned even on a small dataset, can significantly outperform\ntraditional baselines. The results of this work aim to help robots and agents\nperform emotionally sensitive decision-making and interaction in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"How does the person in the bounding box feel?\" Achieving human-level\nrecognition of the apparent emotion of a person in real world situations\nremains an unsolved task in computer vision. Facial expressions are not enough:\nbody pose, contextual knowledge, and commonsense reasoning all contribute to\nhow humans perform this emotional theory of mind task. In this paper, we\nexamine two major approaches enabled by recent large vision language models: 1)\nimage captioning followed by a language-only LLM, and 2) vision language\nmodels, under zero-shot and fine-tuned setups. We evaluate the methods on the\nEmotions in Context (EMOTIC) dataset and demonstrate that a vision language\nmodel, fine-tuned even on a small dataset, can significantly outperform\ntraditional baselines. The results of this work aim to help robots and agents\nperform emotionally sensitive decision-making and interaction in the future."
                },
                "authors": [
                    {
                        "name": "Yasaman Etesam"
                    },
                    {
                        "name": "Özge Nilay Yalçın"
                    },
                    {
                        "name": "Chuxuan Zhang"
                    },
                    {
                        "name": "Angelica Lim"
                    }
                ],
                "author_detail": {
                    "name": "Angelica Lim"
                },
                "author": "Angelica Lim",
                "arxiv_doi": "10.1109/IROS58592.2024.10802538",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IROS58592.2024.10802538",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.08992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, website:\n  https://yasaman-etesam.github.io/Contextual-Emotion-Recognition/. arXiv admin\n  note: text overlap with arXiv:2310.19995",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v1",
                "updated": "2025-01-31T18:47:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19389v1",
                "updated": "2025-01-31T18:44:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    44,
                    35,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:44:35Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    44,
                    35,
                    4,
                    31,
                    0
                ],
                "title": "Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large\n  Language Models"
                },
                "summary": "Fine-tuning large language models (LLMs) on devices is attracting increasing\ninterest. Recent works have fused low-rank adaptation (LoRA) techniques with\nfederated fine-tuning to mitigate challenges associated with device model sizes\nand data scarcity. Still, the heterogeneity of computational resources remains\na critical bottleneck: while higher-rank modules generally enhance performance,\nvarying device capabilities constrain LoRA's feasible rank range. Existing\napproaches attempting to resolve this issue either lack analytical\njustification or impose additional computational overhead, leaving a wide gap\nfor an efficient and theoretically-grounded solution. To address these\nchallenges, we propose federated sketching LoRA (FSLoRA), which leverages a\nsketching mechanism to enable devices to selectively update submatrices of\nglobal LoRA modules maintained by the server. By adjusting the sketching\nratios, which determine the ranks of the submatrices on the devices, FSLoRA\nflexibly adapts to device-specific communication and computational constraints.\nWe provide a rigorous convergence analysis of FSLoRA that characterizes how the\nsketching ratios affect the convergence rate. Through comprehensive experiments\non multiple datasets and LLM models, we demonstrate FSLoRA's superior\nperformance compared to various baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on devices is attracting increasing\ninterest. Recent works have fused low-rank adaptation (LoRA) techniques with\nfederated fine-tuning to mitigate challenges associated with device model sizes\nand data scarcity. Still, the heterogeneity of computational resources remains\na critical bottleneck: while higher-rank modules generally enhance performance,\nvarying device capabilities constrain LoRA's feasible rank range. Existing\napproaches attempting to resolve this issue either lack analytical\njustification or impose additional computational overhead, leaving a wide gap\nfor an efficient and theoretically-grounded solution. To address these\nchallenges, we propose federated sketching LoRA (FSLoRA), which leverages a\nsketching mechanism to enable devices to selectively update submatrices of\nglobal LoRA modules maintained by the server. By adjusting the sketching\nratios, which determine the ranks of the submatrices on the devices, FSLoRA\nflexibly adapts to device-specific communication and computational constraints.\nWe provide a rigorous convergence analysis of FSLoRA that characterizes how the\nsketching ratios affect the convergence rate. Through comprehensive experiments\non multiple datasets and LLM models, we demonstrate FSLoRA's superior\nperformance compared to various baselines."
                },
                "authors": [
                    {
                        "name": "Wenzhi Fang"
                    },
                    {
                        "name": "Dong-Jun Han"
                    },
                    {
                        "name": "Liangqi Yuan"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19377v1",
                "updated": "2025-01-31T18:30:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    30,
                    36,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:30:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    30,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "SELMA: A Speech-Enabled Language Model for Virtual Assistant\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELMA: A Speech-Enabled Language Model for Virtual Assistant\n  Interactions"
                },
                "summary": "In this work, we present and evaluate SELMA, a Speech-Enabled Language Model\nfor virtual Assistant interactions that integrates audio and text as inputs to\na Large Language Model (LLM). SELMA is designed to handle three primary and two\nauxiliary tasks related to interactions with virtual assistants simultaneously\nwithin a single end-to-end model. We employ low-rank adaptation modules for\nparameter-efficient training of both the audio encoder and the LLM.\nAdditionally, we implement a feature pooling strategy enabling the system to\nrecognize global patterns and improve accuracy on tasks less reliant on\nindividual sequence elements. Experimental results on Voice Trigger (VT)\ndetection, Device-Directed Speech Detection (DDSD), and Automatic Speech\nRecognition (ASR), demonstrate that our approach both simplifies the typical\ninput processing pipeline of virtual assistants significantly and also improves\nperformance compared to dedicated models for each individual task. SELMA yields\nrelative Equal-Error Rate improvements of 64% on the VT detection task, and 22%\non DDSD, while also achieving word error rates close to the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present and evaluate SELMA, a Speech-Enabled Language Model\nfor virtual Assistant interactions that integrates audio and text as inputs to\na Large Language Model (LLM). SELMA is designed to handle three primary and two\nauxiliary tasks related to interactions with virtual assistants simultaneously\nwithin a single end-to-end model. We employ low-rank adaptation modules for\nparameter-efficient training of both the audio encoder and the LLM.\nAdditionally, we implement a feature pooling strategy enabling the system to\nrecognize global patterns and improve accuracy on tasks less reliant on\nindividual sequence elements. Experimental results on Voice Trigger (VT)\ndetection, Device-Directed Speech Detection (DDSD), and Automatic Speech\nRecognition (ASR), demonstrate that our approach both simplifies the typical\ninput processing pipeline of virtual assistants significantly and also improves\nperformance compared to dedicated models for each individual task. SELMA yields\nrelative Equal-Error Rate improvements of 64% on the VT detection task, and 22%\non DDSD, while also achieving word error rates close to the baseline."
                },
                "authors": [
                    {
                        "name": "Dominik Wagner"
                    },
                    {
                        "name": "Alexander Churchill"
                    },
                    {
                        "name": "Siddarth Sigtia"
                    },
                    {
                        "name": "Erik Marchi"
                    }
                ],
                "author_detail": {
                    "name": "Erik Marchi"
                },
                "author": "Erik Marchi",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02755v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02755v3",
                "updated": "2025-01-31T18:21:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    21,
                    59,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-03T17:58:29Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    29,
                    3,
                    277,
                    0
                ],
                "title": "GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to\n  Filter Language Model Pretraining Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to\n  Filter Language Model Pretraining Data"
                },
                "summary": "Large language models require vast amounts of high-quality training data, but\neffective filtering of web-scale datasets remains a significant challenge. This\npaper demonstrates that GPT-4o is remarkably effective at identifying\nhigh-quality training data, but its prohibitive cost makes it impractical at\nweb-scale. We propose SIEVE, a lightweight alternative that matches GPT-4o\naccuracy at less than 1\\% of the cost. SIEVE can perform up to 500 filtering\noperations for the cost of one GPT-4o filtering call. The key to SIEVE is a\nseamless integration of GPT-4o and lightweight text classification models,\nusing active learning to fine-tune these models in the background with a small\nnumber of calls to GPT-4o. Once trained, it performs as well as GPT-4o at a\ntiny fraction of the cost. Through different filtering prompts, SIEVE can\nefficiently curate high quality data for general or specialized domains from\nweb-scale corpora -- a valuable capability given the current scarcity of\nhigh-quality domain-specific datasets. Extensive experiments using automatic\nand human evaluation metrics show that SIEVE and GPT-4o achieve similar\nperformance on five highly specific filtering prompts. In addition, when\nperforming quality filtering on web crawl datasets, we demonstrate SIEVE can\nfurther improve over state-of-the-art quality filtering methods in the\nDataComp-LM challenge for selecting LLM pretraining data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models require vast amounts of high-quality training data, but\neffective filtering of web-scale datasets remains a significant challenge. This\npaper demonstrates that GPT-4o is remarkably effective at identifying\nhigh-quality training data, but its prohibitive cost makes it impractical at\nweb-scale. We propose SIEVE, a lightweight alternative that matches GPT-4o\naccuracy at less than 1\\% of the cost. SIEVE can perform up to 500 filtering\noperations for the cost of one GPT-4o filtering call. The key to SIEVE is a\nseamless integration of GPT-4o and lightweight text classification models,\nusing active learning to fine-tune these models in the background with a small\nnumber of calls to GPT-4o. Once trained, it performs as well as GPT-4o at a\ntiny fraction of the cost. Through different filtering prompts, SIEVE can\nefficiently curate high quality data for general or specialized domains from\nweb-scale corpora -- a valuable capability given the current scarcity of\nhigh-quality domain-specific datasets. Extensive experiments using automatic\nand human evaluation metrics show that SIEVE and GPT-4o achieve similar\nperformance on five highly specific filtering prompts. In addition, when\nperforming quality filtering on web crawl datasets, we demonstrate SIEVE can\nfurther improve over state-of-the-art quality filtering methods in the\nDataComp-LM challenge for selecting LLM pretraining data."
                },
                "authors": [
                    {
                        "name": "Jifan Zhang"
                    },
                    {
                        "name": "Ziyue Luo"
                    },
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Ness Shroff"
                    },
                    {
                        "name": "Robert Nowak"
                    }
                ],
                "author_detail": {
                    "name": "Robert Nowak"
                },
                "author": "Robert Nowak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02755v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02755v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19361v1",
                "updated": "2025-01-31T18:12:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    12,
                    41,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:12:41Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    12,
                    41,
                    4,
                    31,
                    0
                ],
                "title": "We're Different, We're the Same: Creative Homogeneity Across LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We're Different, We're the Same: Creative Homogeneity Across LLMs"
                },
                "summary": "Numerous powerful large language models (LLMs) are now available for use as\nwriting support tools, idea generators, and beyond. Although these LLMs are\nmarketed as helpful creative assistants, several works have shown that using an\nLLM as a creative partner results in a narrower set of creative outputs.\nHowever, these studies only consider the effects of interacting with a single\nLLM, begging the question of whether such narrowed creativity stems from using\na particular LLM -- which arguably has a limited range of outputs -- or from\nusing LLMs in general as creative assistants. To study this question, we elicit\ncreative responses from humans and a broad set of LLMs using standardized\ncreativity tests and compare the population-level diversity of responses. We\nfind that LLM responses are much more similar to other LLM responses than human\nresponses are to each other, even after controlling for response structure and\nother key variables. This finding of significant homogeneity in creative\noutputs across the LLMs we evaluate adds a new dimension to the ongoing\nconversation about creativity and LLMs. If today's LLMs behave similarly, using\nthem as a creative partners -- regardless of the model used -- may drive all\nusers towards a limited set of \"creative\" outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous powerful large language models (LLMs) are now available for use as\nwriting support tools, idea generators, and beyond. Although these LLMs are\nmarketed as helpful creative assistants, several works have shown that using an\nLLM as a creative partner results in a narrower set of creative outputs.\nHowever, these studies only consider the effects of interacting with a single\nLLM, begging the question of whether such narrowed creativity stems from using\na particular LLM -- which arguably has a limited range of outputs -- or from\nusing LLMs in general as creative assistants. To study this question, we elicit\ncreative responses from humans and a broad set of LLMs using standardized\ncreativity tests and compare the population-level diversity of responses. We\nfind that LLM responses are much more similar to other LLM responses than human\nresponses are to each other, even after controlling for response structure and\nother key variables. This finding of significant homogeneity in creative\noutputs across the LLMs we evaluate adds a new dimension to the ongoing\nconversation about creativity and LLMs. If today's LLMs behave similarly, using\nthem as a creative partners -- regardless of the model used -- may drive all\nusers towards a limited set of \"creative\" outputs."
                },
                "authors": [
                    {
                        "name": "Emily Wenger"
                    },
                    {
                        "name": "Yoed Kenett"
                    }
                ],
                "author_detail": {
                    "name": "Yoed Kenett"
                },
                "author": "Yoed Kenett",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19358v1",
                "updated": "2025-01-31T18:10:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    10,
                    53,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:10:53Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    10,
                    53,
                    4,
                    31,
                    0
                ],
                "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating\n  Reward Hacking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating\n  Reward Hacking"
                },
                "summary": "This work identifies the Energy Loss Phenomenon in Reinforcement Learning\nfrom Human Feedback (RLHF) and its connection to reward hacking. Specifically,\nenergy loss in the final layer of a Large Language Model (LLM) gradually\nincreases during the RL process, with an excessive increase in energy loss\ncharacterizing reward hacking. Beyond empirical analysis, we further provide a\ntheoretical foundation by proving that, under mild conditions, the increased\nenergy loss reduces the upper bound of contextual relevance in LLMs, which is a\ncritical aspect of reward hacking as the reduced contextual relevance typically\nindicates overfitting to reward model-favored patterns in RL. To address this\nissue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the\nincrease in energy loss in the LLM's final layer during reward calculation to\nprevent excessive energy loss, thereby mitigating reward hacking. We\ntheoretically show that EPPO can be conceptually interpreted as an\nentropy-regularized RL algorithm, which provides deeper insights into its\neffectiveness. Extensive experiments across various LLMs and tasks demonstrate\nthe commonality of the energy loss phenomenon, as well as the effectiveness of\n\\texttt{EPPO} in mitigating reward hacking and improving RLHF performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work identifies the Energy Loss Phenomenon in Reinforcement Learning\nfrom Human Feedback (RLHF) and its connection to reward hacking. Specifically,\nenergy loss in the final layer of a Large Language Model (LLM) gradually\nincreases during the RL process, with an excessive increase in energy loss\ncharacterizing reward hacking. Beyond empirical analysis, we further provide a\ntheoretical foundation by proving that, under mild conditions, the increased\nenergy loss reduces the upper bound of contextual relevance in LLMs, which is a\ncritical aspect of reward hacking as the reduced contextual relevance typically\nindicates overfitting to reward model-favored patterns in RL. To address this\nissue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the\nincrease in energy loss in the LLM's final layer during reward calculation to\nprevent excessive energy loss, thereby mitigating reward hacking. We\ntheoretically show that EPPO can be conceptually interpreted as an\nentropy-regularized RL algorithm, which provides deeper insights into its\neffectiveness. Extensive experiments across various LLMs and tasks demonstrate\nthe commonality of the energy loss phenomenon, as well as the effectiveness of\n\\texttt{EPPO} in mitigating reward hacking and improving RLHF performance."
                },
                "authors": [
                    {
                        "name": "Yuchun Miao"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Yuqi Zhang"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "28 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16665v2",
                "updated": "2025-01-31T18:01:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    1,
                    12,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-22T03:38:37Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    3,
                    38,
                    37,
                    1,
                    296,
                    0
                ],
                "title": "SafetyAnalyst: Interpretable, transparent, and steerable safety\n  moderation for AI behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafetyAnalyst: Interpretable, transparent, and steerable safety\n  moderation for AI behavior"
                },
                "summary": "The ideal AI safety moderation system would be both structurally\ninterpretable (so its decisions can be reliably explained) and steerable (to\nalign to safety standards and reflect a community's values), which current\nsystems fall short on. To address this gap, we present SafetyAnalyst, a novel\nAI safety moderation framework. Given an AI behavior, SafetyAnalyst uses\nchain-of-thought reasoning to analyze its potential consequences by creating a\nstructured \"harm-benefit tree,\" which enumerates harmful and beneficial actions\nand effects the AI behavior may lead to, along with likelihood, severity, and\nimmediacy labels that describe potential impact on any stakeholders.\nSafetyAnalyst then aggregates all harmful and beneficial effects into a\nharmfulness score using fully interpretable weight parameters, which can be\naligned to particular safety preferences. We applied this conceptual framework\nto develop, test, and release an open-source LLM prompt safety classification\nsystem, distilled from 18.5 million harm-benefit features generated by frontier\nLLMs on 19k prompts. On a comprehensive set of prompt safety benchmarks, we\nshow that SafetyReporter (average F1=0.81) outperforms existing LLM safety\nmoderation systems (average F1$<$0.72) on prompt safety classification, while\noffering the additional advantages of interpretability, transparency, and\nsteerability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ideal AI safety moderation system would be both structurally\ninterpretable (so its decisions can be reliably explained) and steerable (to\nalign to safety standards and reflect a community's values), which current\nsystems fall short on. To address this gap, we present SafetyAnalyst, a novel\nAI safety moderation framework. Given an AI behavior, SafetyAnalyst uses\nchain-of-thought reasoning to analyze its potential consequences by creating a\nstructured \"harm-benefit tree,\" which enumerates harmful and beneficial actions\nand effects the AI behavior may lead to, along with likelihood, severity, and\nimmediacy labels that describe potential impact on any stakeholders.\nSafetyAnalyst then aggregates all harmful and beneficial effects into a\nharmfulness score using fully interpretable weight parameters, which can be\naligned to particular safety preferences. We applied this conceptual framework\nto develop, test, and release an open-source LLM prompt safety classification\nsystem, distilled from 18.5 million harm-benefit features generated by frontier\nLLMs on 19k prompts. On a comprehensive set of prompt safety benchmarks, we\nshow that SafetyReporter (average F1=0.81) outperforms existing LLM safety\nmoderation systems (average F1$<$0.72) on prompt safety classification, while\noffering the additional advantages of interpretability, transparency, and\nsteerability."
                },
                "authors": [
                    {
                        "name": "Jing-Jing Li"
                    },
                    {
                        "name": "Valentina Pyatkin"
                    },
                    {
                        "name": "Max Kleiman-Weiner"
                    },
                    {
                        "name": "Liwei Jiang"
                    },
                    {
                        "name": "Nouha Dziri"
                    },
                    {
                        "name": "Anne G. E. Collins"
                    },
                    {
                        "name": "Jana Schaich Borg"
                    },
                    {
                        "name": "Maarten Sap"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Sydney Levine"
                    }
                ],
                "author_detail": {
                    "name": "Sydney Levine"
                },
                "author": "Sydney Levine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19340v1",
                "updated": "2025-01-31T17:40:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    40,
                    8,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T17:40:08Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    40,
                    8,
                    4,
                    31,
                    0
                ],
                "title": "Towards Adaptive Self-Improvement for Smarter Energy Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Adaptive Self-Improvement for Smarter Energy Systems"
                },
                "summary": "This paper introduces a hierarchical framework for decision-making and\noptimization, leveraging Large Language Models (LLMs) for adaptive code\ngeneration. Instead of direct decision-making, LLMs generate and refine\nexecutable control policies through a meta-policy that guides task generation\nand a base policy for operational actions. Applied to a simplified microgrid\nscenario, the approach achieves up to 15 percent cost savings by iteratively\nimproving battery control strategies. The proposed methodology lays a\nfoundation for integrating LLM-based tools into planning and control tasks,\noffering adaptable and scalable solutions for complex systems while addressing\nchallenges of uncertainty and reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a hierarchical framework for decision-making and\noptimization, leveraging Large Language Models (LLMs) for adaptive code\ngeneration. Instead of direct decision-making, LLMs generate and refine\nexecutable control policies through a meta-policy that guides task generation\nand a base policy for operational actions. Applied to a simplified microgrid\nscenario, the approach achieves up to 15 percent cost savings by iteratively\nimproving battery control strategies. The proposed methodology lays a\nfoundation for integrating LLM-based tools into planning and control tasks,\noffering adaptable and scalable solutions for complex systems while addressing\nchallenges of uncertainty and reproducibility."
                },
                "authors": [
                    {
                        "name": "Alexander Sommer"
                    },
                    {
                        "name": "Peter Bazan"
                    },
                    {
                        "name": "Jonathan Fellerer"
                    },
                    {
                        "name": "Behnam Babaeian"
                    },
                    {
                        "name": "Reinhard German"
                    }
                ],
                "author_detail": {
                    "name": "Reinhard German"
                },
                "author": "Reinhard German",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14713v2",
                "updated": "2025-01-31T17:38:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    38,
                    7,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-24T18:46:37Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    46,
                    37,
                    4,
                    24,
                    0
                ],
                "title": "FlexiGPT: Pruning and Extending Large Language Models with Low-Rank\n  Weight Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiGPT: Pruning and Extending Large Language Models with Low-Rank\n  Weight Sharing"
                },
                "summary": "The rapid proliferation of large language models (LLMs) in natural language\nprocessing (NLP) has created a critical need for techniques that enable\nefficient deployment on memory-constrained devices without compromising\nperformance. We present a method to prune LLMs that selectively prunes model\nblocks based on an importance score and replaces them with a low-parameter\nreplacement strategy. Specifically, we propose a principled metric to replace\neach pruned block using a weight-sharing mechanism that leverages unpruned\ncounterparts from the model and block-specific low-rank adapters. Furthermore,\nwe facilitate the learning of these replacement blocks with output feature\nnormalization and an adapter initialization scheme built on low-rank SVD\nreconstructions. Empirical evaluations demonstrate substantial performance\ngains over existing methods, achieving state-of-the-art performance on 5/6\nbenchmarks for a compression rate of 30% and 6/6 benchmarks for a compression\nrate of 40%. We also demonstrate that our approach can extend smaller models,\nboosting performance on 6/6 benchmarks using only ~0.3% tokens of extended\ntraining with minimal additional parameter costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of large language models (LLMs) in natural language\nprocessing (NLP) has created a critical need for techniques that enable\nefficient deployment on memory-constrained devices without compromising\nperformance. We present a method to prune LLMs that selectively prunes model\nblocks based on an importance score and replaces them with a low-parameter\nreplacement strategy. Specifically, we propose a principled metric to replace\neach pruned block using a weight-sharing mechanism that leverages unpruned\ncounterparts from the model and block-specific low-rank adapters. Furthermore,\nwe facilitate the learning of these replacement blocks with output feature\nnormalization and an adapter initialization scheme built on low-rank SVD\nreconstructions. Empirical evaluations demonstrate substantial performance\ngains over existing methods, achieving state-of-the-art performance on 5/6\nbenchmarks for a compression rate of 30% and 6/6 benchmarks for a compression\nrate of 40%. We also demonstrate that our approach can extend smaller models,\nboosting performance on 6/6 benchmarks using only ~0.3% tokens of extended\ntraining with minimal additional parameter costs."
                },
                "authors": [
                    {
                        "name": "James Seale Smith"
                    },
                    {
                        "name": "Chi-Heng Lin"
                    },
                    {
                        "name": "Shikhar Tuli"
                    },
                    {
                        "name": "Haris Jeelani"
                    },
                    {
                        "name": "Shangqian Gao"
                    },
                    {
                        "name": "Yilin Shen"
                    },
                    {
                        "name": "Hongxia Jin"
                    },
                    {
                        "name": "Yen-Chang Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Yen-Chang Hsu"
                },
                "author": "Yen-Chang Hsu",
                "arxiv_comment": "Accepted to NAACL 2025 - Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19337v1",
                "updated": "2025-01-31T17:36:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    36,
                    12,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T17:36:12Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    36,
                    12,
                    4,
                    31,
                    0
                ],
                "title": "Homogeneity Bias as Differential Sampling Uncertainty in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Homogeneity Bias as Differential Sampling Uncertainty in Language Models"
                },
                "summary": "Prior research show that Large Language Models (LLMs) and Vision-Language\nModels (VLMs) represent marginalized groups more homogeneously than dominant\ngroups. However, the mechanisms underlying this homogeneity bias remain\nrelatively unexplored. We propose that this bias emerges from systematic\ndifferences in the probability distributions from which tokens are sampled at\ninference-time. Analyzing three measures of uncertainty in token sampling\ndistributions-entropy, perplexity, and probability of differentiation-we find\nthat in some models, specifically GPT-4 Turbo and Llama-3.2, tokens are sampled\nmore deterministically when generating texts about marginalized groups (i.e.,\nBlack Americans and women) compared to their dominant group counterparts (i.e.,\nWhite Americans and men). While these findings may help explain homogeneity\nbias in certain models, the patterns did not replicate across all VLMs tested,\nsuggesting multiple mechanisms may contribute to homogeneity bias in AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior research show that Large Language Models (LLMs) and Vision-Language\nModels (VLMs) represent marginalized groups more homogeneously than dominant\ngroups. However, the mechanisms underlying this homogeneity bias remain\nrelatively unexplored. We propose that this bias emerges from systematic\ndifferences in the probability distributions from which tokens are sampled at\ninference-time. Analyzing three measures of uncertainty in token sampling\ndistributions-entropy, perplexity, and probability of differentiation-we find\nthat in some models, specifically GPT-4 Turbo and Llama-3.2, tokens are sampled\nmore deterministically when generating texts about marginalized groups (i.e.,\nBlack Americans and women) compared to their dominant group counterparts (i.e.,\nWhite Americans and men). While these findings may help explain homogeneity\nbias in certain models, the patterns did not replicate across all VLMs tested,\nsuggesting multiple mechanisms may contribute to homogeneity bias in AI."
                },
                "authors": [
                    {
                        "name": "Messi H. J. Lee"
                    },
                    {
                        "name": "Soyeon Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Soyeon Jeon"
                },
                "author": "Soyeon Jeon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17282v3",
                "updated": "2025-01-31T17:26:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    26,
                    12,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-28T20:30:36Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    30,
                    36,
                    1,
                    28,
                    0
                ],
                "title": "From Natural Language to Extensive-Form Game Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Natural Language to Extensive-Form Game Representations"
                },
                "summary": "We introduce a framework for translating game descriptions in natural\nlanguage into extensive-form representations in game theory, leveraging Large\nLanguage Models (LLMs) and in-context learning. Given the varying levels of\nstrategic complexity in games, such as perfect versus imperfect information,\ndirectly applying in-context learning would be insufficient. To address this,\nwe introduce a two-stage framework with specialized modules to enhance\nin-context learning, enabling it to divide and conquer the problem effectively.\nIn the first stage, we tackle the challenge of imperfect information by\ndeveloping a module that identifies information sets along and the\ncorresponding partial tree structure. With this information, the second stage\nleverages in-context learning alongside a self-debugging module to produce a\ncomplete extensive-form game tree represented using pygambit, the Python API of\na recognized game-theoretic analysis tool called Gambit. Using this python\nrepresentation enables the automation of tasks such as computing Nash\nequilibria directly from natural language descriptions. We evaluate the\nperformance of the full framework, as well as its individual components, using\nvarious LLMs on games with different levels of strategic complexity. Our\nexperimental results show that the framework significantly outperforms baseline\nmodels in generating accurate extensive-form games, with each module playing a\ncritical role in its success.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a framework for translating game descriptions in natural\nlanguage into extensive-form representations in game theory, leveraging Large\nLanguage Models (LLMs) and in-context learning. Given the varying levels of\nstrategic complexity in games, such as perfect versus imperfect information,\ndirectly applying in-context learning would be insufficient. To address this,\nwe introduce a two-stage framework with specialized modules to enhance\nin-context learning, enabling it to divide and conquer the problem effectively.\nIn the first stage, we tackle the challenge of imperfect information by\ndeveloping a module that identifies information sets along and the\ncorresponding partial tree structure. With this information, the second stage\nleverages in-context learning alongside a self-debugging module to produce a\ncomplete extensive-form game tree represented using pygambit, the Python API of\na recognized game-theoretic analysis tool called Gambit. Using this python\nrepresentation enables the automation of tasks such as computing Nash\nequilibria directly from natural language descriptions. We evaluate the\nperformance of the full framework, as well as its individual components, using\nvarious LLMs on games with different levels of strategic complexity. Our\nexperimental results show that the framework significantly outperforms baseline\nmodels in generating accurate extensive-form games, with each module playing a\ncritical role in its success."
                },
                "authors": [
                    {
                        "name": "Shilong Deng"
                    },
                    {
                        "name": "Yongzhao Wang"
                    },
                    {
                        "name": "Rahul Savani"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Savani"
                },
                "author": "Rahul Savani",
                "arxiv_comment": "This work has been accepted as a full paper for AAMAS 2025. This is a\n  full version of the AAMAS 2025 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19324v1",
                "updated": "2025-01-31T17:19:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    19,
                    57,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T17:19:57Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    19,
                    57,
                    4,
                    31,
                    0
                ],
                "title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning"
                },
                "summary": "We introduce Reward-Guided Speculative Decoding (RSD), a novel framework\naimed at improving the efficiency of inference in large language models (LLMs).\nRSD synergistically combines a lightweight draft model with a more powerful\ntarget model, incorporating a controlled bias to prioritize high-reward\noutputs, in contrast to existing speculative decoding methods that enforce\nstrict unbiasedness. RSD employs a process reward model to evaluate\nintermediate decoding steps and dynamically decide whether to invoke the target\nmodel, optimizing the trade-off between computational cost and output quality.\nWe theoretically demonstrate that a threshold-based mixture strategy achieves\nan optimal balance between resource utilization and performance. Extensive\nevaluations on challenging reasoning benchmarks, including Olympiad-level\ntasks, show that RSD delivers significant efficiency gains against decoding\nwith the target model only (up to 4.4x fewer FLOPs), while achieving\nsignificant better accuracy than parallel decoding method on average (up to\n+3.5). These results highlight RSD as a robust and cost-effective approach for\ndeploying LLMs in resource-intensive scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Reward-Guided Speculative Decoding (RSD), a novel framework\naimed at improving the efficiency of inference in large language models (LLMs).\nRSD synergistically combines a lightweight draft model with a more powerful\ntarget model, incorporating a controlled bias to prioritize high-reward\noutputs, in contrast to existing speculative decoding methods that enforce\nstrict unbiasedness. RSD employs a process reward model to evaluate\nintermediate decoding steps and dynamically decide whether to invoke the target\nmodel, optimizing the trade-off between computational cost and output quality.\nWe theoretically demonstrate that a threshold-based mixture strategy achieves\nan optimal balance between resource utilization and performance. Extensive\nevaluations on challenging reasoning benchmarks, including Olympiad-level\ntasks, show that RSD delivers significant efficiency gains against decoding\nwith the target model only (up to 4.4x fewer FLOPs), while achieving\nsignificant better accuracy than parallel decoding method on average (up to\n+3.5). These results highlight RSD as a robust and cost-effective approach for\ndeploying LLMs in resource-intensive scenarios."
                },
                "authors": [
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Junnan Li"
                    },
                    {
                        "name": "Christof Monz"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Doyen Sahoo"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19318v1",
                "updated": "2025-01-31T17:15:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    15,
                    33,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T17:15:33Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    15,
                    33,
                    4,
                    31,
                    0
                ],
                "title": "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented\n  Reinforcement in Embodied Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented\n  Reinforcement in Embodied Systems"
                },
                "summary": "While large language models (LLMs) have shown promising capabilities as\nzero-shot planners for embodied agents, their inability to learn from\nexperience and build persistent mental models limits their robustness in\ncomplex open-world environments like Minecraft. We introduce MINDSTORES, an\nexperience-augmented planning framework that enables embodied agents to build\nand leverage mental models through natural interaction with their environment.\nDrawing inspiration from how humans construct and refine cognitive mental\nmodels, our approach extends existing zero-shot LLM planning by maintaining a\ndatabase of past experiences that informs future planning iterations. The key\ninnovation is representing accumulated experiences as natural language\nembeddings of (state, task, plan, outcome) tuples, which can then be\nefficiently retrieved and reasoned over by an LLM planner to generate insights\nand guide plan refinement for novel states and tasks. Through extensive\nexperiments in the MineDojo environment, a simulation environment for agents in\nMinecraft that provides low-level controls for Minecraft, we find that\nMINDSTORES learns and applies its knowledge significantly better than existing\nmemory-based LLM planners while maintaining the flexibility and generalization\nbenefits of zero-shot approaches, representing an important step toward more\ncapable embodied AI systems that can learn continuously through natural\nexperience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown promising capabilities as\nzero-shot planners for embodied agents, their inability to learn from\nexperience and build persistent mental models limits their robustness in\ncomplex open-world environments like Minecraft. We introduce MINDSTORES, an\nexperience-augmented planning framework that enables embodied agents to build\nand leverage mental models through natural interaction with their environment.\nDrawing inspiration from how humans construct and refine cognitive mental\nmodels, our approach extends existing zero-shot LLM planning by maintaining a\ndatabase of past experiences that informs future planning iterations. The key\ninnovation is representing accumulated experiences as natural language\nembeddings of (state, task, plan, outcome) tuples, which can then be\nefficiently retrieved and reasoned over by an LLM planner to generate insights\nand guide plan refinement for novel states and tasks. Through extensive\nexperiments in the MineDojo environment, a simulation environment for agents in\nMinecraft that provides low-level controls for Minecraft, we find that\nMINDSTORES learns and applies its knowledge significantly better than existing\nmemory-based LLM planners while maintaining the flexibility and generalization\nbenefits of zero-shot approaches, representing an important step toward more\ncapable embodied AI systems that can learn continuously through natural\nexperience."
                },
                "authors": [
                    {
                        "name": "Anirudh Chari"
                    },
                    {
                        "name": "Suraj Reddy"
                    },
                    {
                        "name": "Aditya Tiwari"
                    },
                    {
                        "name": "Richard Lian"
                    },
                    {
                        "name": "Brian Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Brian Zhou"
                },
                "author": "Brian Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19317v1",
                "updated": "2025-01-31T17:12:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    12,
                    55,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T17:12:55Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    12,
                    55,
                    4,
                    31,
                    0
                ],
                "title": "LLM-based Affective Text Generation Quality Based on Different\n  Quantization Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Affective Text Generation Quality Based on Different\n  Quantization Values"
                },
                "summary": "Large language models exhibit a remarkable capacity in language generation\nand comprehension. These advances enable AI systems to produce more human-like\nand emotionally engaging text. However, these models rely on a large number of\nparameters, requiring significant computational resources for training and\ninference. In some scenarios, accessing these resources can be challenging\n(e.g., budget or hardware limitations). Techniques like reducing precision bits\ncan make models more memory-efficient, reducing the computational resources\nneeded, at the cost of reduced accuracy. This paper addresses the trade-off\nbetween different quantization values, GPU RAM utilization, and text quality in\naffective text generation (e.g., \"I really enjoy running in the snow-covered\nforest\"). To evaluate, we use an emotion classifier and ten seed prompts to\ngenerate affective text. We test three setups of precision bits (8, 16, and 32)\nacross five open-weight language models from two different families. Our\nfindings demonstrate that bit reductions lead to memory savings, achieving a\nreduction of 76%. However, this optimization comes with a trade-off, leading to\na decrease of up to 10 pp in F1 score for larger models and an increase of 10\npp for smaller models, along with roughly double the inference time. In terms\nof text quality, larger models at lower quantization levels generally\noutperform smaller, higher-precision models -- while requiring similar memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models exhibit a remarkable capacity in language generation\nand comprehension. These advances enable AI systems to produce more human-like\nand emotionally engaging text. However, these models rely on a large number of\nparameters, requiring significant computational resources for training and\ninference. In some scenarios, accessing these resources can be challenging\n(e.g., budget or hardware limitations). Techniques like reducing precision bits\ncan make models more memory-efficient, reducing the computational resources\nneeded, at the cost of reduced accuracy. This paper addresses the trade-off\nbetween different quantization values, GPU RAM utilization, and text quality in\naffective text generation (e.g., \"I really enjoy running in the snow-covered\nforest\"). To evaluate, we use an emotion classifier and ten seed prompts to\ngenerate affective text. We test three setups of precision bits (8, 16, and 32)\nacross five open-weight language models from two different families. Our\nfindings demonstrate that bit reductions lead to memory savings, achieving a\nreduction of 76%. However, this optimization comes with a trade-off, leading to\na decrease of up to 10 pp in F1 score for larger models and an increase of 10\npp for smaller models, along with roughly double the inference time. In terms\nof text quality, larger models at lower quantization levels generally\noutperform smaller, higher-precision models -- while requiring similar memory."
                },
                "authors": [
                    {
                        "name": "Yarik Menchaca Resendiz"
                    },
                    {
                        "name": "Roman Klinger"
                    }
                ],
                "author_detail": {
                    "name": "Roman Klinger"
                },
                "author": "Roman Klinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19309v1",
                "updated": "2025-01-31T17:09:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    9,
                    53,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T17:09:53Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    9,
                    53,
                    4,
                    31,
                    0
                ],
                "title": "Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model\n  Alignment"
                },
                "summary": "The performance of large language models (LLMs) is closely linked to their\nunderlying size, leading to ever-growing networks and hence slower inference.\nSpeculative decoding has been proposed as a technique to accelerate\nautoregressive generation, leveraging a fast draft model to propose candidate\ntokens, which are then verified in parallel based on their likelihood under the\ntarget model. While this approach guarantees to reproduce the target output, it\nincurs a substantial penalty: many high-quality draft tokens are rejected, even\nwhen they represent objectively valid continuations. Indeed, we show that even\npowerful draft models such as GPT-4o, as well as human text cannot achieve high\nacceptance rates under the standard verification scheme. This severely limits\nthe speedup potential of current speculative decoding methods, as an early\nrejection becomes overwhelmingly likely when solely relying on alignment of\ndraft and target.\n  We thus ask the following question: Can we adapt verification to recognize\ncorrect, but non-aligned replies? To this end, we draw inspiration from the\nLLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers\nin a versatile way. We carefully design a dataset to elicit the same capability\nin the target model by training a compact module on top of the embeddings to\nproduce ``judgements\" of the current continuation. We showcase our strategy on\nthe Llama-3.1 family, where our 8b/405B-Judge achieves a speedup of 9x over\nLlama-405B, while maintaining its quality on a large range of benchmarks. These\nbenefits remain present even in optimized inference frameworks, where our\nmethod reaches up to 141 tokens/s for 8B/70B-Judge and 129 tokens/s for 8B/405B\non 2 and 8 H100s respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of large language models (LLMs) is closely linked to their\nunderlying size, leading to ever-growing networks and hence slower inference.\nSpeculative decoding has been proposed as a technique to accelerate\nautoregressive generation, leveraging a fast draft model to propose candidate\ntokens, which are then verified in parallel based on their likelihood under the\ntarget model. While this approach guarantees to reproduce the target output, it\nincurs a substantial penalty: many high-quality draft tokens are rejected, even\nwhen they represent objectively valid continuations. Indeed, we show that even\npowerful draft models such as GPT-4o, as well as human text cannot achieve high\nacceptance rates under the standard verification scheme. This severely limits\nthe speedup potential of current speculative decoding methods, as an early\nrejection becomes overwhelmingly likely when solely relying on alignment of\ndraft and target.\n  We thus ask the following question: Can we adapt verification to recognize\ncorrect, but non-aligned replies? To this end, we draw inspiration from the\nLLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers\nin a versatile way. We carefully design a dataset to elicit the same capability\nin the target model by training a compact module on top of the embeddings to\nproduce ``judgements\" of the current continuation. We showcase our strategy on\nthe Llama-3.1 family, where our 8b/405B-Judge achieves a speedup of 9x over\nLlama-405B, while maintaining its quality on a large range of benchmarks. These\nbenefits remain present even in optimized inference frameworks, where our\nmethod reaches up to 141 tokens/s for 8B/70B-Judge and 129 tokens/s for 8B/405B\non 2 and 8 H100s respectively."
                },
                "authors": [
                    {
                        "name": "Gregor Bachmann"
                    },
                    {
                        "name": "Sotiris Anagnostidis"
                    },
                    {
                        "name": "Albert Pumarola"
                    },
                    {
                        "name": "Markos Georgopoulos"
                    },
                    {
                        "name": "Artsiom Sanakoyeu"
                    },
                    {
                        "name": "Yuming Du"
                    },
                    {
                        "name": "Edgar Schönfeld"
                    },
                    {
                        "name": "Ali Thabet"
                    },
                    {
                        "name": "Jonas Kohler"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Kohler"
                },
                "author": "Jonas Kohler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19306v1",
                "updated": "2025-01-31T17:03:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    3,
                    16,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T17:03:16Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    3,
                    16,
                    4,
                    31,
                    0
                ],
                "title": "SETS: Leveraging Self-Verification and Self-Correction for Improved\n  Test-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SETS: Leveraging Self-Verification and Self-Correction for Improved\n  Test-Time Scaling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have created new\nopportunities to enhance performance on complex reasoning tasks by leveraging\ntest-time computation. However, conventional approaches such as repeated\nsampling with majority voting or reward model scoring, often face diminishing\nreturns as test-time compute scales, in addition to requiring costly\ntask-specific reward model training. In this paper, we present Self-Enhanced\nTest-Time Scaling (SETS), a novel method that leverages the self-verification\nand self-correction capabilities of recent advanced LLMs to overcome these\nlimitations. SETS integrates sampling, self-verification, and self-correction\ninto a unified framework, enabling efficient and scalable test-time computation\nfor improved capabilities at complex tasks. Through extensive experiments on\nchallenging planning and reasoning benchmarks, compared to the alternatives, we\ndemonstrate that SETS achieves significant performance improvements and more\nfavorable test-time scaling laws.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have created new\nopportunities to enhance performance on complex reasoning tasks by leveraging\ntest-time computation. However, conventional approaches such as repeated\nsampling with majority voting or reward model scoring, often face diminishing\nreturns as test-time compute scales, in addition to requiring costly\ntask-specific reward model training. In this paper, we present Self-Enhanced\nTest-Time Scaling (SETS), a novel method that leverages the self-verification\nand self-correction capabilities of recent advanced LLMs to overcome these\nlimitations. SETS integrates sampling, self-verification, and self-correction\ninto a unified framework, enabling efficient and scalable test-time computation\nfor improved capabilities at complex tasks. Through extensive experiments on\nchallenging planning and reasoning benchmarks, compared to the alternatives, we\ndemonstrate that SETS achieves significant performance improvements and more\nfavorable test-time scaling laws."
                },
                "authors": [
                    {
                        "name": "Jiefeng Chen"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Xinyun Chen"
                    },
                    {
                        "name": "Chengrun Yang"
                    },
                    {
                        "name": "Ruoxi Sun"
                    },
                    {
                        "name": "Sercan Ö Arık"
                    }
                ],
                "author_detail": {
                    "name": "Sercan Ö Arık"
                },
                "author": "Sercan Ö Arık",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19301v1",
                "updated": "2025-01-31T16:57:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    57,
                    1,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    57,
                    1,
                    4,
                    31,
                    0
                ],
                "title": "Beyond checkmate: exploring the creative chokepoints in AI text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond checkmate: exploring the creative chokepoints in AI text"
                },
                "summary": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) and Artificial Intelligence (AI), unlocking unprecedented capabilities.\nThis rapid advancement has spurred research into various aspects of LLMs, their\ntext generation & reasoning capability, and potential misuse, fueling the\nnecessity for robust detection methods. While numerous prior research has\nfocused on detecting LLM-generated text (AI text) and thus checkmating them,\nour study investigates a relatively unexplored territory: portraying the\nnuanced distinctions between human and AI texts across text segments. Whether\nLLMs struggle with or excel at incorporating linguistic ingenuity across\ndifferent text segments carries substantial implications for determining their\npotential as effective creative assistants to humans. Through an analogy with\nthe structure of chess games-comprising opening, middle, and end games-we\nanalyze text segments (introduction, body, and conclusion) to determine where\nthe most significant distinctions between human and AI texts exist. While AI\ntexts can approximate the body segment better due to its increased length, a\ncloser examination reveals a pronounced disparity, highlighting the importance\nof this segment in AI text detection. Additionally, human texts exhibit higher\ncross-segment differences compared to AI texts. Overall, our research can shed\nlight on the intricacies of human-AI text distinctions, offering novel insights\nfor text detection and understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) and Artificial Intelligence (AI), unlocking unprecedented capabilities.\nThis rapid advancement has spurred research into various aspects of LLMs, their\ntext generation & reasoning capability, and potential misuse, fueling the\nnecessity for robust detection methods. While numerous prior research has\nfocused on detecting LLM-generated text (AI text) and thus checkmating them,\nour study investigates a relatively unexplored territory: portraying the\nnuanced distinctions between human and AI texts across text segments. Whether\nLLMs struggle with or excel at incorporating linguistic ingenuity across\ndifferent text segments carries substantial implications for determining their\npotential as effective creative assistants to humans. Through an analogy with\nthe structure of chess games-comprising opening, middle, and end games-we\nanalyze text segments (introduction, body, and conclusion) to determine where\nthe most significant distinctions between human and AI texts exist. While AI\ntexts can approximate the body segment better due to its increased length, a\ncloser examination reveals a pronounced disparity, highlighting the importance\nof this segment in AI text detection. Additionally, human texts exhibit higher\ncross-segment differences compared to AI texts. Overall, our research can shed\nlight on the intricacies of human-AI text distinctions, offering novel insights\nfor text detection and understanding."
                },
                "authors": [
                    {
                        "name": "Nafis Irtiza Tripto"
                    },
                    {
                        "name": "Saranya Venkatraman"
                    },
                    {
                        "name": "Mahjabin Nahar"
                    },
                    {
                        "name": "Dongwon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongwon Lee"
                },
                "author": "Dongwon Lee",
                "arxiv_comment": "18 pages, single columns, under review at Nature Machine Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v1",
                "updated": "2025-01-31T16:56:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee-Joe Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19298v1",
                "updated": "2025-01-31T16:55:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    55,
                    43,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:55:43Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    55,
                    43,
                    4,
                    31,
                    0
                ],
                "title": "Synthetic User Behavior Sequence Generation with Large Language Models\n  for Smart Homes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic User Behavior Sequence Generation with Large Language Models\n  for Smart Homes"
                },
                "summary": "In recent years, as smart home systems have become more widespread, security\nconcerns within these environments have become a growing threat. Currently,\nmost smart home security solutions, such as anomaly detection and behavior\nprediction models, are trained using fixed datasets that are precollected.\nHowever, the process of dataset collection is time-consuming and lacks the\nflexibility needed to adapt to the constantly evolving smart home environment.\nAdditionally, the collection of personal data raises significant privacy\nconcerns for users. Lately, large language models (LLMs) have emerged as a\npowerful tool for a wide range of tasks across diverse application domains,\nthanks to their strong capabilities in natural language processing, reasoning,\nand problem-solving. In this paper, we propose an LLM-based synthetic dataset\ngeneration IoTGen framework to enhance the generalization of downstream smart\nhome intelligent models. By generating new synthetic datasets that reflect\nchanges in the environment, smart home intelligent models can be retrained to\novercome the limitations of fixed and outdated data, allowing them to better\nalign with the dynamic nature of real-world home environments. Specifically, we\nfirst propose a Structure Pattern Perception Compression (SPPC) method tailored\nfor IoT behavior data, which preserves the most informative content in the data\nwhile significantly reducing token consumption. Then, we propose a systematic\napproach to create prompts and implement data generation to automatically\ngenerate IoT synthetic data with normative and reasonable properties, assisting\ntask models in adaptive training to improve generalization and real-world\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, as smart home systems have become more widespread, security\nconcerns within these environments have become a growing threat. Currently,\nmost smart home security solutions, such as anomaly detection and behavior\nprediction models, are trained using fixed datasets that are precollected.\nHowever, the process of dataset collection is time-consuming and lacks the\nflexibility needed to adapt to the constantly evolving smart home environment.\nAdditionally, the collection of personal data raises significant privacy\nconcerns for users. Lately, large language models (LLMs) have emerged as a\npowerful tool for a wide range of tasks across diverse application domains,\nthanks to their strong capabilities in natural language processing, reasoning,\nand problem-solving. In this paper, we propose an LLM-based synthetic dataset\ngeneration IoTGen framework to enhance the generalization of downstream smart\nhome intelligent models. By generating new synthetic datasets that reflect\nchanges in the environment, smart home intelligent models can be retrained to\novercome the limitations of fixed and outdated data, allowing them to better\nalign with the dynamic nature of real-world home environments. Specifically, we\nfirst propose a Structure Pattern Perception Compression (SPPC) method tailored\nfor IoT behavior data, which preserves the most informative content in the data\nwhile significantly reducing token consumption. Then, we propose a systematic\napproach to create prompts and implement data generation to automatically\ngenerate IoT synthetic data with normative and reasonable properties, assisting\ntask models in adaptive training to improve generalization and real-world\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhiyao Xu"
                    },
                    {
                        "name": "Dan Zhao"
                    },
                    {
                        "name": "Qingsong Zou"
                    },
                    {
                        "name": "Jingyu Xiao"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Zhenhui Yuan"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19297v1",
                "updated": "2025-01-31T16:55:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    55,
                    17,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:55:17Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    55,
                    17,
                    4,
                    31,
                    0
                ],
                "title": "Analysis of LLMs vs Human Experts in Requirements Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of LLMs vs Human Experts in Requirements Engineering"
                },
                "summary": "The majority of research around Large Language Models (LLM) application to\nsoftware development has been on the subject of code generation. There is\nlittle literature on LLMs' impact on requirements engineering (RE), which deals\nwith the process of developing and verifying the system requirements. Within\nRE, there is a subdiscipline of requirements elicitation, which is the practice\nof discovering and documenting requirements for a system from users, customers,\nand other stakeholders. In this analysis, we compare LLM's ability to elicit\nrequirements of a software system, as compared to that of a human expert in a\ntime-boxed and prompt-boxed study. We found LLM-generated requirements were\nevaluated as more aligned (+1.12) than human-generated requirements with a\ntrend of being more complete (+10.2%). Conversely, we found users tended to\nbelieve that solutions they perceived as more aligned had been generated by\nhuman experts. Furthermore, while LLM-generated documents scored higher and\nperformed at 720x the speed, their cost was, on average, only 0.06% that of a\nhuman expert. Overall, these findings indicate that LLMs will play an\nincreasingly important role in requirements engineering by improving\nrequirements definitions, enabling more efficient resource allocation, and\nreducing overall project timelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The majority of research around Large Language Models (LLM) application to\nsoftware development has been on the subject of code generation. There is\nlittle literature on LLMs' impact on requirements engineering (RE), which deals\nwith the process of developing and verifying the system requirements. Within\nRE, there is a subdiscipline of requirements elicitation, which is the practice\nof discovering and documenting requirements for a system from users, customers,\nand other stakeholders. In this analysis, we compare LLM's ability to elicit\nrequirements of a software system, as compared to that of a human expert in a\ntime-boxed and prompt-boxed study. We found LLM-generated requirements were\nevaluated as more aligned (+1.12) than human-generated requirements with a\ntrend of being more complete (+10.2%). Conversely, we found users tended to\nbelieve that solutions they perceived as more aligned had been generated by\nhuman experts. Furthermore, while LLM-generated documents scored higher and\nperformed at 720x the speed, their cost was, on average, only 0.06% that of a\nhuman expert. Overall, these findings indicate that LLMs will play an\nincreasingly important role in requirements engineering by improving\nrequirements definitions, enabling more efficient resource allocation, and\nreducing overall project timelines."
                },
                "authors": [
                    {
                        "name": "Cory Hymel"
                    },
                    {
                        "name": "Hiroe Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Hiroe Johnson"
                },
                "author": "Hiroe Johnson",
                "arxiv_comment": "8 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19287v1",
                "updated": "2025-01-31T16:48:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    48,
                    38,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:48:38Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    48,
                    38,
                    4,
                    31,
                    0
                ],
                "title": "Differentially Private In-context Learning via Sampling Few-shot Mixed\n  with Zero-shot Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private In-context Learning via Sampling Few-shot Mixed\n  with Zero-shot Outputs"
                },
                "summary": "In-context learning (ICL) has shown promising improvement in downstream task\nadaptation of LLMs by augmenting prompts with relevant input-output examples\n(demonstrations). However, the ICL demonstrations can contain privacy-sensitive\ninformation, which can be leaked and/or regurgitated by the LLM output.\nDifferential Privacy (DP), a widely adopted privacy safeguard, has emerged to\nmitigate this privacy leakage, with recent work demonstrating strong\nprivacy-utility tradeoffs in classification tasks for ICL. However, generation\ntasks for ICL are challenging due to the high-dimensional output space of\nopen-ended generation. To this end, we propose $\\texttt{dps-mozo}$,\nDifferentially Private Sampling by Mixing One-shot with Zero-shot Outputs, a\ndecoding framework that generates DP text by sampling from the product of\nmultiple one-shot outputs mixed with a zero-shot output. This mixing\neffectively reduces the amount of information that can be leaked by each\ndemonstration. By utilizing the inherent randomness in sampling from the mixed\ndistributions, we can achieve DP without adding noise, thereby improving the\nprivacy-utility tradeoff. Our experimental evaluations show $\\texttt{dps-mozo}$\ncan achieve a strong privacy guarantee, $\\epsilon=2$, with minimal utility\ndegradation compared to non-private few-shot learning, $\\textbf{0.3}$% ROUGE-L\nF1 score decrease on the SAMSum dataset with Gemma 2 2B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has shown promising improvement in downstream task\nadaptation of LLMs by augmenting prompts with relevant input-output examples\n(demonstrations). However, the ICL demonstrations can contain privacy-sensitive\ninformation, which can be leaked and/or regurgitated by the LLM output.\nDifferential Privacy (DP), a widely adopted privacy safeguard, has emerged to\nmitigate this privacy leakage, with recent work demonstrating strong\nprivacy-utility tradeoffs in classification tasks for ICL. However, generation\ntasks for ICL are challenging due to the high-dimensional output space of\nopen-ended generation. To this end, we propose $\\texttt{dps-mozo}$,\nDifferentially Private Sampling by Mixing One-shot with Zero-shot Outputs, a\ndecoding framework that generates DP text by sampling from the product of\nmultiple one-shot outputs mixed with a zero-shot output. This mixing\neffectively reduces the amount of information that can be leaked by each\ndemonstration. By utilizing the inherent randomness in sampling from the mixed\ndistributions, we can achieve DP without adding noise, thereby improving the\nprivacy-utility tradeoff. Our experimental evaluations show $\\texttt{dps-mozo}$\ncan achieve a strong privacy guarantee, $\\epsilon=2$, with minimal utility\ndegradation compared to non-private few-shot learning, $\\textbf{0.3}$% ROUGE-L\nF1 score decrease on the SAMSum dataset with Gemma 2 2B."
                },
                "authors": [
                    {
                        "name": "James Flemings"
                    },
                    {
                        "name": "Haosheng Gan"
                    },
                    {
                        "name": "Hongyi Li"
                    },
                    {
                        "name": "Meisam Razaviyayn"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19282v1",
                "updated": "2025-01-31T16:45:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    45,
                    16,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:45:16Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    45,
                    16,
                    4,
                    31,
                    0
                ],
                "title": "Low-Cost and Comprehensive Non-textual Input Fuzzing with\n  LLM-Synthesized Input Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Cost and Comprehensive Non-textual Input Fuzzing with\n  LLM-Synthesized Input Generators"
                },
                "summary": "Modern software often accepts inputs with highly complex grammars. Recent\nadvances in large language models (LLMs) have shown that they can be used to\nsynthesize high-quality natural language text and code that conforms to the\ngrammar of a given input format. Nevertheless, LLMs are often incapable or too\ncostly to generate non-textual outputs, such as images, videos, and PDF files.\nThis limitation hinders the application of LLMs in grammar-aware fuzzing.\n  We present a novel approach to enabling grammar-aware fuzzing over\nnon-textual inputs. We employ LLMs to synthesize and also mutate input\ngenerators, in the form of Python scripts, that generate data conforming to the\ngrammar of a given input format. Then, non-textual data yielded by the input\ngenerators are further mutated by traditional fuzzers (AFL++) to explore the\nsoftware input space effectively. Our approach, namely G2FUZZ, features a\nhybrid strategy that combines a holistic search driven by LLMs and a local\nsearch driven by industrial quality fuzzers. Two key advantages are: (1) LLMs\nare good at synthesizing and mutating input generators and enabling jumping out\nof local optima, thus achieving a synergistic effect when combined with\nmutation-based fuzzers; (2) LLMs are less frequently invoked unless really\nneeded, thus significantly reducing the cost of LLM usage. We have evaluated\nG2FUZZ on a variety of input formats, including TIFF images, MP4 audios, and\nPDF files. The results show that G2FUZZ outperforms SOTA tools such as AFL++,\nFuzztruction, and FormatFuzzer in terms of code coverage and bug finding across\nmost programs tested on three platforms: UNIFUZZ, FuzzBench, and MAGMA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software often accepts inputs with highly complex grammars. Recent\nadvances in large language models (LLMs) have shown that they can be used to\nsynthesize high-quality natural language text and code that conforms to the\ngrammar of a given input format. Nevertheless, LLMs are often incapable or too\ncostly to generate non-textual outputs, such as images, videos, and PDF files.\nThis limitation hinders the application of LLMs in grammar-aware fuzzing.\n  We present a novel approach to enabling grammar-aware fuzzing over\nnon-textual inputs. We employ LLMs to synthesize and also mutate input\ngenerators, in the form of Python scripts, that generate data conforming to the\ngrammar of a given input format. Then, non-textual data yielded by the input\ngenerators are further mutated by traditional fuzzers (AFL++) to explore the\nsoftware input space effectively. Our approach, namely G2FUZZ, features a\nhybrid strategy that combines a holistic search driven by LLMs and a local\nsearch driven by industrial quality fuzzers. Two key advantages are: (1) LLMs\nare good at synthesizing and mutating input generators and enabling jumping out\nof local optima, thus achieving a synergistic effect when combined with\nmutation-based fuzzers; (2) LLMs are less frequently invoked unless really\nneeded, thus significantly reducing the cost of LLM usage. We have evaluated\nG2FUZZ on a variety of input formats, including TIFF images, MP4 audios, and\nPDF files. The results show that G2FUZZ outperforms SOTA tools such as AFL++,\nFuzztruction, and FormatFuzzer in terms of code coverage and bug finding across\nmost programs tested on three platforms: UNIFUZZ, FuzzBench, and MAGMA."
                },
                "authors": [
                    {
                        "name": "Kunpeng Zhang"
                    },
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "arxiv_comment": "USENIX Security 2025",
                "arxiv_journal_ref": "The 34th USENIX Security Symposium, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19278v1",
                "updated": "2025-01-31T16:42:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    42,
                    31,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:42:31Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    42,
                    31,
                    4,
                    31,
                    0
                ],
                "title": "Pheromone-based Learning of Optimal Reasoning Paths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pheromone-based Learning of Optimal Reasoning Paths"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning\ncapabilities through chain-of-thought prompting, yet discovering effective\nreasoning methods for complex problems remains challenging due to the vast\nspace of possible intermediate steps. We introduce Ant Colony\nOptimization-guided Tree of Thought (ACO-ToT), a novel algorithm that combines\nACO with LLMs to discover optimal reasoning paths for complex problems\nefficiently. Drawing inspiration from Hebbian learning in neurological systems,\nour method employs a collection of distinctly fine-tuned LLM \"ants\" to traverse\nand lay pheromone trails through a centralized tree of thought, with each ant's\nmovement governed by a weighted combination of existing pheromone trails and\nits own specialized expertise. The algorithm evaluates complete reasoning paths\nusing a mixture-of-experts-based scoring function, with pheromones reinforcing\nproductive reasoning paths across iterations. Experiments on three challenging\nreasoning tasks (GSM8K, ARC-Challenge, and MATH) demonstrate that ACO-ToT\nperforms significantly better than existing chain-of-thought optimization\napproaches, suggesting that incorporating biologically inspired collective\nsearch mechanisms into LLM inference can substantially enhance reasoning\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable reasoning\ncapabilities through chain-of-thought prompting, yet discovering effective\nreasoning methods for complex problems remains challenging due to the vast\nspace of possible intermediate steps. We introduce Ant Colony\nOptimization-guided Tree of Thought (ACO-ToT), a novel algorithm that combines\nACO with LLMs to discover optimal reasoning paths for complex problems\nefficiently. Drawing inspiration from Hebbian learning in neurological systems,\nour method employs a collection of distinctly fine-tuned LLM \"ants\" to traverse\nand lay pheromone trails through a centralized tree of thought, with each ant's\nmovement governed by a weighted combination of existing pheromone trails and\nits own specialized expertise. The algorithm evaluates complete reasoning paths\nusing a mixture-of-experts-based scoring function, with pheromones reinforcing\nproductive reasoning paths across iterations. Experiments on three challenging\nreasoning tasks (GSM8K, ARC-Challenge, and MATH) demonstrate that ACO-ToT\nperforms significantly better than existing chain-of-thought optimization\napproaches, suggesting that incorporating biologically inspired collective\nsearch mechanisms into LLM inference can substantially enhance reasoning\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Anirudh Chari"
                    },
                    {
                        "name": "Aditya Tiwari"
                    },
                    {
                        "name": "Richard Lian"
                    },
                    {
                        "name": "Suraj Reddy"
                    },
                    {
                        "name": "Brian Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Brian Zhou"
                },
                "author": "Brian Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15275v2",
                "updated": "2025-01-31T16:31:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    31,
                    56,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-20T04:19:32Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    4,
                    19,
                    32,
                    6,
                    294,
                    0
                ],
                "title": "SuiGPT MAD: Move AI Decompiler to Improve Transparency and Auditability\n  on Non-Open-Source Blockchain Smart Contract",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuiGPT MAD: Move AI Decompiler to Improve Transparency and Auditability\n  on Non-Open-Source Blockchain Smart Contract"
                },
                "summary": "The vision of Web3 is to improve user control over data and assets, but one\nchallenge that complicates this vision is the prevalence of non-transparent,\nscam-prone applications and vulnerable smart contracts that put Web3 users at\nrisk. While code audits are one solution to this problem, the lack of smart\ncontracts source code on many blockchain platforms, such as Sui, hinders the\nease of auditing. A promising approach to this issue is the use of a decompiler\nto reverse-engineer smart contract bytecode. However, existing decompilers for\nSui produce code that is difficult to understand and cannot be directly\nrecompiled. To address this, we developed the SuiGPT Move AI Decompiler (MAD),\na Large Language Model (LLM)-powered web application that decompiles smart\ncontract bytecodes on Sui into logically correct, human-readable, and\nre-compilable source code with prompt engineering.\n  Our evaluation shows that MAD's output successfully passes original unit\ntests and achieves a 73.33% recompilation success rate on real-world smart\ncontracts. Additionally, newer models tend to deliver improved performance,\nsuggesting that MAD's approach will become increasingly effective as LLMs\ncontinue to advance.\n  In a user study involving 12 developers, we found that MAD significantly\nreduced the auditing workload compared to using traditional decompilers.\nParticipants found MAD's outputs comparable to the original source code,\nimproving accessibility for understanding and auditing non-open-source smart\ncontracts. Through qualitative interviews with these developers and Web3\nprojects, we further discussed the strengths and concerns of MAD.\n  MAD has practical implications for blockchain smart contract transparency,\nauditing, and education. It empowers users to easily and independently review\nand audit non-open-source smart contracts, fostering accountability and\ndecentralization",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision of Web3 is to improve user control over data and assets, but one\nchallenge that complicates this vision is the prevalence of non-transparent,\nscam-prone applications and vulnerable smart contracts that put Web3 users at\nrisk. While code audits are one solution to this problem, the lack of smart\ncontracts source code on many blockchain platforms, such as Sui, hinders the\nease of auditing. A promising approach to this issue is the use of a decompiler\nto reverse-engineer smart contract bytecode. However, existing decompilers for\nSui produce code that is difficult to understand and cannot be directly\nrecompiled. To address this, we developed the SuiGPT Move AI Decompiler (MAD),\na Large Language Model (LLM)-powered web application that decompiles smart\ncontract bytecodes on Sui into logically correct, human-readable, and\nre-compilable source code with prompt engineering.\n  Our evaluation shows that MAD's output successfully passes original unit\ntests and achieves a 73.33% recompilation success rate on real-world smart\ncontracts. Additionally, newer models tend to deliver improved performance,\nsuggesting that MAD's approach will become increasingly effective as LLMs\ncontinue to advance.\n  In a user study involving 12 developers, we found that MAD significantly\nreduced the auditing workload compared to using traditional decompilers.\nParticipants found MAD's outputs comparable to the original source code,\nimproving accessibility for understanding and auditing non-open-source smart\ncontracts. Through qualitative interviews with these developers and Web3\nprojects, we further discussed the strengths and concerns of MAD.\n  MAD has practical implications for blockchain smart contract transparency,\nauditing, and education. It empowers users to easily and independently review\nand audit non-open-source smart contracts, fostering accountability and\ndecentralization"
                },
                "authors": [
                    {
                        "name": "Eason Chen"
                    },
                    {
                        "name": "Xinyi Tang"
                    },
                    {
                        "name": "Zimo Xiao"
                    },
                    {
                        "name": "Chuangji Li"
                    },
                    {
                        "name": "Shizhuo Li"
                    },
                    {
                        "name": "Wu Tingguan"
                    },
                    {
                        "name": "Siyun Wang"
                    },
                    {
                        "name": "Kostas Kryptos Chalkias"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Kryptos Chalkias"
                },
                "author": "Kostas Kryptos Chalkias",
                "arxiv_doi": "10.1145/3696410.3714790",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714790",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.15275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Paper accepted at ACM The Web Conference 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19266v1",
                "updated": "2025-01-31T16:26:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    26,
                    28,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:26:28Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    26,
                    28,
                    4,
                    31,
                    0
                ],
                "title": "Jackpot! Alignment as a Maximal Lottery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jackpot! Alignment as a Maximal Lottery"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF), the standard for aligning\nLarge Language Models (LLMs) with human values, is known to fail to satisfy\nproperties that are intuitively desirable, such as respecting the preferences\nof the majority \\cite{ge2024axioms}. To overcome these issues, we propose the\nuse of a probabilistic Social Choice rule called \\emph{maximal lotteries} as a\nreplacement for RLHF. We show that a family of alignment techniques, namely\nNash Learning from Human Feedback (NLHF) \\cite{munos2023nash} and variants,\napproximate maximal lottery outcomes and thus inherit its beneficial\nproperties.\n  We confirm experimentally that our proposed methodology handles situations\nthat arise when working with preferences more robustly than standard RLHF,\nincluding supporting the preferences of the majority, providing principled ways\nof handling non-transitivities in the preference data, and robustness to\nirrelevant alternatives. This results in systems that better incorporate human\nvalues and respect human intentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF), the standard for aligning\nLarge Language Models (LLMs) with human values, is known to fail to satisfy\nproperties that are intuitively desirable, such as respecting the preferences\nof the majority \\cite{ge2024axioms}. To overcome these issues, we propose the\nuse of a probabilistic Social Choice rule called \\emph{maximal lotteries} as a\nreplacement for RLHF. We show that a family of alignment techniques, namely\nNash Learning from Human Feedback (NLHF) \\cite{munos2023nash} and variants,\napproximate maximal lottery outcomes and thus inherit its beneficial\nproperties.\n  We confirm experimentally that our proposed methodology handles situations\nthat arise when working with preferences more robustly than standard RLHF,\nincluding supporting the preferences of the majority, providing principled ways\nof handling non-transitivities in the preference data, and robustness to\nirrelevant alternatives. This results in systems that better incorporate human\nvalues and respect human intentions."
                },
                "authors": [
                    {
                        "name": "Roberto-Rafael Maura-Rivero"
                    },
                    {
                        "name": "Marc Lanctot"
                    },
                    {
                        "name": "Francesco Visin"
                    },
                    {
                        "name": "Kate Larson"
                    }
                ],
                "author_detail": {
                    "name": "Kate Larson"
                },
                "author": "Kate Larson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19259v1",
                "updated": "2025-01-31T16:17:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    17,
                    3,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:17:03Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    17,
                    3,
                    4,
                    31,
                    0
                ],
                "title": "Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for\n  Autonomous Drone FlighT at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for\n  Autonomous Drone FlighT at the Edge"
                },
                "summary": "The integration of human-intuitive interactions into autonomous systems has\nbeen limited. Traditional Natural Language Processing (NLP) systems struggle\nwith context and intent understanding, severely restricting human-robot\ninteraction. Recent advancements in Large Language Models (LLMs) have\ntransformed this dynamic, allowing for intuitive and high-level communication\nthrough speech and text, and bridging the gap between human commands and\nrobotic actions. Additionally, autonomous navigation has emerged as a central\nfocus in robotics research, with artificial intelligence (AI) increasingly\nbeing leveraged to enhance these systems. However, existing AI-based navigation\nalgorithms face significant challenges in latency-critical tasks where rapid\ndecision-making is critical. Traditional frame-based vision systems, while\neffective for high-level decision-making, suffer from high energy consumption\nand latency, limiting their applicability in real-time scenarios. Neuromorphic\nvision systems, combining event-based cameras and spiking neural networks\n(SNNs), offer a promising alternative by enabling energy-efficient, low-latency\nnavigation. Despite their potential, real-world implementations of these\nsystems, particularly on physical platforms such as drones, remain scarce. In\nthis work, we present Neuro-LIFT, a real-time neuromorphic navigation framework\nimplemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural\nlanguage processing, Neuro-LIFT translates human speech into high-level\nplanning commands which are then autonomously executed using event-based\nneuromorphic vision and physics-driven planning. Our framework demonstrates its\ncapabilities in navigating in a dynamic environment, avoiding obstacles, and\nadapting to human instructions in real-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of human-intuitive interactions into autonomous systems has\nbeen limited. Traditional Natural Language Processing (NLP) systems struggle\nwith context and intent understanding, severely restricting human-robot\ninteraction. Recent advancements in Large Language Models (LLMs) have\ntransformed this dynamic, allowing for intuitive and high-level communication\nthrough speech and text, and bridging the gap between human commands and\nrobotic actions. Additionally, autonomous navigation has emerged as a central\nfocus in robotics research, with artificial intelligence (AI) increasingly\nbeing leveraged to enhance these systems. However, existing AI-based navigation\nalgorithms face significant challenges in latency-critical tasks where rapid\ndecision-making is critical. Traditional frame-based vision systems, while\neffective for high-level decision-making, suffer from high energy consumption\nand latency, limiting their applicability in real-time scenarios. Neuromorphic\nvision systems, combining event-based cameras and spiking neural networks\n(SNNs), offer a promising alternative by enabling energy-efficient, low-latency\nnavigation. Despite their potential, real-world implementations of these\nsystems, particularly on physical platforms such as drones, remain scarce. In\nthis work, we present Neuro-LIFT, a real-time neuromorphic navigation framework\nimplemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural\nlanguage processing, Neuro-LIFT translates human speech into high-level\nplanning commands which are then autonomously executed using event-based\nneuromorphic vision and physics-driven planning. Our framework demonstrates its\ncapabilities in navigating in a dynamic environment, avoiding obstacles, and\nadapting to human instructions in real-time."
                },
                "authors": [
                    {
                        "name": "Amogh Joshi"
                    },
                    {
                        "name": "Sourav Sanyal"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04670v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04670v2",
                "updated": "2025-01-31T16:12:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    12,
                    22,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-08T18:30:53Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    30,
                    53,
                    2,
                    8,
                    0
                ],
                "title": "Are They the Same? Exploring Visual Correspondence Shortcomings of\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are They the Same? Exploring Visual Correspondence Shortcomings of\n  Multimodal LLMs"
                },
                "summary": "Recent advancements in multimodal models have shown a strong ability in\nvisual perception, reasoning abilities, and vision-language understanding.\nHowever, studies on visual matching ability are missing, where finding the\nvisual correspondence of objects is essential in vision research. Our research\nreveals that the matching capabilities in recent multimodal LLMs (MLLMs) still\nexhibit systematic shortcomings, even with current strong MLLMs models, GPT-4o.\nIn particular, we construct a Multimodal Visual Matching (MMVM) benchmark to\nfairly benchmark over 30 different MLLMs. The MMVM benchmark is built from 15\nopen-source datasets and Internet videos with manual annotation. We categorize\nthe data samples of MMVM benchmark into eight aspects based on the required\ncues and capabilities to more comprehensively evaluate and analyze current\nMLLMs. In addition, we have designed an automatic annotation pipeline to\ngenerate the MMVM SFT dataset, including 220K visual matching data with\nreasoning annotation. Finally, we present CoLVA, a novel contrastive MLLM with\ntwo novel technical designs: fine-grained vision expert with object-level\ncontrastive learning and instruction augmentation strategy. CoLVA achieves\n51.06\\% overall accuracy (OA) on the MMVM benchmark, surpassing GPT-4o and\nbaseline by 8.41\\% and 23.58\\% OA, respectively. The results show the\neffectiveness of our MMVM SFT dataset and our novel technical designs. Code,\nbenchmark, dataset, and models are available at\nhttps://github.com/zhouyiks/CoLVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in multimodal models have shown a strong ability in\nvisual perception, reasoning abilities, and vision-language understanding.\nHowever, studies on visual matching ability are missing, where finding the\nvisual correspondence of objects is essential in vision research. Our research\nreveals that the matching capabilities in recent multimodal LLMs (MLLMs) still\nexhibit systematic shortcomings, even with current strong MLLMs models, GPT-4o.\nIn particular, we construct a Multimodal Visual Matching (MMVM) benchmark to\nfairly benchmark over 30 different MLLMs. The MMVM benchmark is built from 15\nopen-source datasets and Internet videos with manual annotation. We categorize\nthe data samples of MMVM benchmark into eight aspects based on the required\ncues and capabilities to more comprehensively evaluate and analyze current\nMLLMs. In addition, we have designed an automatic annotation pipeline to\ngenerate the MMVM SFT dataset, including 220K visual matching data with\nreasoning annotation. Finally, we present CoLVA, a novel contrastive MLLM with\ntwo novel technical designs: fine-grained vision expert with object-level\ncontrastive learning and instruction augmentation strategy. CoLVA achieves\n51.06\\% overall accuracy (OA) on the MMVM benchmark, surpassing GPT-4o and\nbaseline by 8.41\\% and 23.58\\% OA, respectively. The results show the\neffectiveness of our MMVM SFT dataset and our novel technical designs. Code,\nbenchmark, dataset, and models are available at\nhttps://github.com/zhouyiks/CoLVA."
                },
                "authors": [
                    {
                        "name": "Yikang Zhou"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Shilin Xu"
                    },
                    {
                        "name": "Shihao Chen"
                    },
                    {
                        "name": "Qianyu Zhou"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Shunping Ji"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Lu Qi"
                    }
                ],
                "author_detail": {
                    "name": "Lu Qi"
                },
                "author": "Lu Qi",
                "arxiv_comment": "fix typos, figures, tables, and other details; additional results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04670v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06833v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06833v3",
                "updated": "2025-01-31T16:06:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    6,
                    52,
                    4,
                    31,
                    0
                ],
                "published": "2024-03-11T15:48:56Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    15,
                    48,
                    56,
                    0,
                    71,
                    0
                ],
                "title": "Can LLMs Separate Instructions From Data? And What Do We Even Mean By\n  That?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Separate Instructions From Data? And What Do We Even Mean By\n  That?"
                },
                "summary": "Instruction-tuned Large Language Models (LLMs) show impressive results in\nnumerous practical applications, but they lack essential safety features that\nare common in other areas of computer science, particularly an explicit\nseparation of instructions and data. This makes them vulnerable to\nmanipulations such as indirect prompt injections and generally unsuitable for\nsafety-critical tasks. Surprisingly, there is currently no established\ndefinition or benchmark to quantify this phenomenon. In this work, we close\nthis gap by introducing a formal measure for instruction-data separation and an\nempirical variant that is calculable from a model's outputs. We also present a\nnew dataset, SEP, that allows estimating the measure for real-world models. Our\nresults on various LLMs show that the problem of instruction-data separation is\nreal: all models fail to achieve high separation, and canonical mitigation\ntechniques, such as prompt engineering and fine-tuning, either fail to\nsubstantially improve separation or reduce model utility. The source code and\nSEP dataset are openly accessible at\nhttps://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuned Large Language Models (LLMs) show impressive results in\nnumerous practical applications, but they lack essential safety features that\nare common in other areas of computer science, particularly an explicit\nseparation of instructions and data. This makes them vulnerable to\nmanipulations such as indirect prompt injections and generally unsuitable for\nsafety-critical tasks. Surprisingly, there is currently no established\ndefinition or benchmark to quantify this phenomenon. In this work, we close\nthis gap by introducing a formal measure for instruction-data separation and an\nempirical variant that is calculable from a model's outputs. We also present a\nnew dataset, SEP, that allows estimating the measure for real-world models. Our\nresults on various LLMs show that the problem of instruction-data separation is\nreal: all models fail to achieve high separation, and canonical mitigation\ntechniques, such as prompt engineering and fine-tuning, either fail to\nsubstantially improve separation or reduce model utility. The source code and\nSEP dataset are openly accessible at\nhttps://github.com/egozverev/Shold-It-Be-Executed-Or-Processed."
                },
                "authors": [
                    {
                        "name": "Egor Zverev"
                    },
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Soroush Tabesh"
                    },
                    {
                        "name": "Mario Fritz"
                    },
                    {
                        "name": "Christoph H. Lampert"
                    }
                ],
                "author_detail": {
                    "name": "Christoph H. Lampert"
                },
                "author": "Christoph H. Lampert",
                "arxiv_comment": "Published as a conference paper at ICLR 2025, GitHub:\n  https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed. 10 pages main\n  text, 30 pages in total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06833v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06833v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11187v2",
                "updated": "2025-01-31T16:01:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    1,
                    1,
                    4,
                    31,
                    0
                ],
                "published": "2023-12-18T13:28:02Z",
                "published_parsed": [
                    2023,
                    12,
                    18,
                    13,
                    28,
                    2,
                    0,
                    352,
                    0
                ],
                "title": "Distributed Asynchronous Service Deployment in the Edge-Cloud Multi-tier\n  Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Asynchronous Service Deployment in the Edge-Cloud Multi-tier\n  Network"
                },
                "summary": "In an edge-cloud multi-tier network, datacenters provide services to mobile\nusers, with each service having specific latency constraints and computational\nrequirements. Deploying such a variety of services while matching their\nrequirements with the available computing resources is challenging. In\naddition, time-critical services may have to be migrated as the users move, to\nkeep fulfilling their latency constraints. Unlike previous work relying on an\norchestrator with an always-updated global view of the available resources and\nthe users' locations, this work envisions a distributed solution to the above\nproblems. In particular, we propose a distributed asynchronous framework for\nservice deployment in the edge-cloud that increases the system resilience by\navoiding a single point of failure, as in the case of a central orchestrator.\nOur solution ensures cost-efficient feasible placement of services, while using\nnegligible bandwidth. Our results, obtained through trace-driven, large-scale\nsimulations, show that the proposed solution provides performance very close to\nthose obtained by state-of-the-art centralized solutions, and at the cost of a\nsmall communication overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an edge-cloud multi-tier network, datacenters provide services to mobile\nusers, with each service having specific latency constraints and computational\nrequirements. Deploying such a variety of services while matching their\nrequirements with the available computing resources is challenging. In\naddition, time-critical services may have to be migrated as the users move, to\nkeep fulfilling their latency constraints. Unlike previous work relying on an\norchestrator with an always-updated global view of the available resources and\nthe users' locations, this work envisions a distributed solution to the above\nproblems. In particular, we propose a distributed asynchronous framework for\nservice deployment in the edge-cloud that increases the system resilience by\navoiding a single point of failure, as in the case of a central orchestrator.\nOur solution ensures cost-efficient feasible placement of services, while using\nnegligible bandwidth. Our results, obtained through trace-driven, large-scale\nsimulations, show that the proposed solution provides performance very close to\nthose obtained by state-of-the-art centralized solutions, and at the cost of a\nsmall communication overhead."
                },
                "authors": [
                    {
                        "name": "Itamar Cohen"
                    },
                    {
                        "name": "Paolo Giaccone"
                    },
                    {
                        "name": "Carla Fabiana Chiasserini"
                    }
                ],
                "author_detail": {
                    "name": "Carla Fabiana Chiasserini"
                },
                "author": "Carla Fabiana Chiasserini",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2305.00184",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19245v1",
                "updated": "2025-01-31T15:59:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    59,
                    50,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:59:50Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    59,
                    50,
                    4,
                    31,
                    0
                ],
                "title": "SHARPIE: A Modular Framework for Reinforcement Learning and Human-AI\n  Interaction Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHARPIE: A Modular Framework for Reinforcement Learning and Human-AI\n  Interaction Experiments"
                },
                "summary": "Reinforcement learning (RL) offers a general approach for modeling and\ntraining AI agents, including human-AI interaction scenarios. In this paper, we\npropose SHARPIE (Shared Human-AI Reinforcement Learning Platform for\nInteractive Experiments) to address the need for a generic framework to support\nexperiments with RL agents and humans. Its modular design consists of a\nversatile wrapper for RL environments and algorithm libraries, a\nparticipant-facing web interface, logging utilities, deployment on popular\ncloud and participant recruitment platforms. It empowers researchers to study a\nwide variety of research questions related to the interaction between humans\nand RL agents, including those related to interactive reward specification and\nlearning, learning from human feedback, action delegation, preference\nelicitation, user-modeling, and human-AI teaming. The platform is based on a\ngeneric interface for human-RL interactions that aims to standardize the field\nof study on RL in human contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) offers a general approach for modeling and\ntraining AI agents, including human-AI interaction scenarios. In this paper, we\npropose SHARPIE (Shared Human-AI Reinforcement Learning Platform for\nInteractive Experiments) to address the need for a generic framework to support\nexperiments with RL agents and humans. Its modular design consists of a\nversatile wrapper for RL environments and algorithm libraries, a\nparticipant-facing web interface, logging utilities, deployment on popular\ncloud and participant recruitment platforms. It empowers researchers to study a\nwide variety of research questions related to the interaction between humans\nand RL agents, including those related to interactive reward specification and\nlearning, learning from human feedback, action delegation, preference\nelicitation, user-modeling, and human-AI teaming. The platform is based on a\ngeneric interface for human-RL interactions that aims to standardize the field\nof study on RL in human contexts."
                },
                "authors": [
                    {
                        "name": "Hüseyin Aydın"
                    },
                    {
                        "name": "Kevin Dubois-Godin"
                    },
                    {
                        "name": "Libio Goncalvez Braz"
                    },
                    {
                        "name": "Floris den Hengst"
                    },
                    {
                        "name": "Kim Baraka"
                    },
                    {
                        "name": "Mustafa Mert Çelikok"
                    },
                    {
                        "name": "Andreas Sauter"
                    },
                    {
                        "name": "Shihan Wang"
                    },
                    {
                        "name": "Frans A. Oliehoek"
                    }
                ],
                "author_detail": {
                    "name": "Frans A. Oliehoek"
                },
                "author": "Frans A. Oliehoek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19232v1",
                "updated": "2025-01-31T15:43:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    43,
                    21,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:43:21Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    43,
                    21,
                    4,
                    31,
                    0
                ],
                "title": "A Zero-Shot Generalization Framework for LLM-Driven Cross-Domain\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zero-Shot Generalization Framework for LLM-Driven Cross-Domain\n  Sequential Recommendation"
                },
                "summary": "Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions\nin unseen domains without the need for additional training or fine-tuning,\nmaking it particularly valuable in data-sparse environments where traditional\nmodels struggle. Recent advancements in large language models (LLMs) have\ngreatly improved ZCDSR by leveraging rich pretrained representations to\nfacilitate cross-domain knowledge transfer. However, a key challenge persists:\ndomain semantic bias, which arises from variations in vocabulary and content\nfocus across domains. This misalignment leads to inconsistencies in item\nembeddings and hinders generalization.\n  To address this issue, we propose a novel framework designed to enhance\nLLM-based ZCDSR by improving cross-domain alignment at both the item and\nsequential levels. At the item level, we introduce a generalization loss that\npromotes inter-domain compactness by aligning embeddings of similar items\nacross domains while maintaining intra-domain diversity to preserve unique item\ncharacteristics. This prevents embeddings from becoming overly generic while\nensuring effective transferability. At the sequential level, we develop a\nmethod for transferring user behavioral patterns by clustering user sequences\nin the source domain and applying attention-based aggregation for target domain\ninference. This dynamic adaptation of user embeddings allows effective\nzero-shot recommendations without requiring target-domain interactions.\n  Comprehensive experiments across multiple datasets and domains demonstrate\nthat our framework significantly improves sequential recommendation performance\nin the ZCDSR setting. By mitigating domain bias and enhancing the\ntransferability of sequential patterns, our method provides a scalable and\nrobust approach for achieving more effective zero-shot recommendations across\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions\nin unseen domains without the need for additional training or fine-tuning,\nmaking it particularly valuable in data-sparse environments where traditional\nmodels struggle. Recent advancements in large language models (LLMs) have\ngreatly improved ZCDSR by leveraging rich pretrained representations to\nfacilitate cross-domain knowledge transfer. However, a key challenge persists:\ndomain semantic bias, which arises from variations in vocabulary and content\nfocus across domains. This misalignment leads to inconsistencies in item\nembeddings and hinders generalization.\n  To address this issue, we propose a novel framework designed to enhance\nLLM-based ZCDSR by improving cross-domain alignment at both the item and\nsequential levels. At the item level, we introduce a generalization loss that\npromotes inter-domain compactness by aligning embeddings of similar items\nacross domains while maintaining intra-domain diversity to preserve unique item\ncharacteristics. This prevents embeddings from becoming overly generic while\nensuring effective transferability. At the sequential level, we develop a\nmethod for transferring user behavioral patterns by clustering user sequences\nin the source domain and applying attention-based aggregation for target domain\ninference. This dynamic adaptation of user embeddings allows effective\nzero-shot recommendations without requiring target-domain interactions.\n  Comprehensive experiments across multiple datasets and domains demonstrate\nthat our framework significantly improves sequential recommendation performance\nin the ZCDSR setting. By mitigating domain bias and enhancing the\ntransferability of sequential patterns, our method provides a scalable and\nrobust approach for achieving more effective zero-shot recommendations across\ndomains."
                },
                "authors": [
                    {
                        "name": "Yunzhe Li"
                    },
                    {
                        "name": "Junting Wang"
                    },
                    {
                        "name": "Hari Sundaram"
                    },
                    {
                        "name": "Zhining Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhining Liu"
                },
                "author": "Zhining Liu",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18438v2",
                "updated": "2025-01-31T15:39:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    39,
                    0,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-30T15:45:56Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    45,
                    56,
                    3,
                    30,
                    0
                ],
                "title": "o3-mini vs DeepSeek-R1: Which One is Safer?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "o3-mini vs DeepSeek-R1: Which One is Safer?"
                },
                "summary": "The irruption of DeepSeek-R1 constitutes a turning point for the AI industry\nin general and the LLMs in particular. Its capabilities have demonstrated\noutstanding performance in several tasks, including creative thinking, code\ngeneration, maths and automated program repair, at apparently lower execution\ncost. However, LLMs must adhere to an important qualitative property, i.e.,\ntheir alignment with safety and human values. A clear competitor of DeepSeek-R1\nis its American counterpart, OpenAI's o3-mini model, which is expected to set\nhigh standards in terms of performance, safety and cost. In this technical\nreport, we systematically assess the safety level of both DeepSeek-R1 (70b\nversion) and OpenAI's o3-mini (beta version). To this end, we make use of our\nrecently released automated safety testing tool, named ASTRAL. By leveraging\nthis tool, we automatically and systematically generated and executed 1,260\ntest inputs on both models. After conducting a semi-automated assessment of the\noutcomes provided by both LLMs, the results indicate that DeepSeek-R1 produces\nsignificantly more unsafe responses (12%) than OpenAI's o3-mini (1.2%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The irruption of DeepSeek-R1 constitutes a turning point for the AI industry\nin general and the LLMs in particular. Its capabilities have demonstrated\noutstanding performance in several tasks, including creative thinking, code\ngeneration, maths and automated program repair, at apparently lower execution\ncost. However, LLMs must adhere to an important qualitative property, i.e.,\ntheir alignment with safety and human values. A clear competitor of DeepSeek-R1\nis its American counterpart, OpenAI's o3-mini model, which is expected to set\nhigh standards in terms of performance, safety and cost. In this technical\nreport, we systematically assess the safety level of both DeepSeek-R1 (70b\nversion) and OpenAI's o3-mini (beta version). To this end, we make use of our\nrecently released automated safety testing tool, named ASTRAL. By leveraging\nthis tool, we automatically and systematically generated and executed 1,260\ntest inputs on both models. After conducting a semi-automated assessment of the\noutcomes provided by both LLMs, the results indicate that DeepSeek-R1 produces\nsignificantly more unsafe responses (12%) than OpenAI's o3-mini (1.2%)."
                },
                "authors": [
                    {
                        "name": "Aitor Arrieta"
                    },
                    {
                        "name": "Miriam Ugarte"
                    },
                    {
                        "name": "Pablo Valle"
                    },
                    {
                        "name": "José Antonio Parejo"
                    },
                    {
                        "name": "Sergio Segura"
                    }
                ],
                "author_detail": {
                    "name": "Sergio Segura"
                },
                "author": "Sergio Segura",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2501.17749",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19223v1",
                "updated": "2025-01-31T15:30:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    30,
                    14,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:30:14Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    30,
                    14,
                    4,
                    31,
                    0
                ],
                "title": "Through the Looking Glass: LLM-Based Analysis of AR/VR Android\n  Applications Privacy Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through the Looking Glass: LLM-Based Analysis of AR/VR Android\n  Applications Privacy Policies"
                },
                "summary": "\\begin{abstract} This paper comprehensively analyzes privacy policies in\nAR/VR applications, leveraging BERT, a state-of-the-art text classification\nmodel, to evaluate the clarity and thoroughness of these policies. By comparing\nthe privacy policies of AR/VR applications with those of free and premium\nwebsites, this study provides a broad perspective on the current state of\nprivacy practices within the AR/VR industry. Our findings indicate that AR/VR\napplications generally offer a higher percentage of positive segments than free\ncontent but lower than premium websites. The analysis of highlighted segments\nand words revealed that AR/VR applications strategically emphasize critical\nprivacy practices and key terms. This enhances privacy policies' clarity and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\\begin{abstract} This paper comprehensively analyzes privacy policies in\nAR/VR applications, leveraging BERT, a state-of-the-art text classification\nmodel, to evaluate the clarity and thoroughness of these policies. By comparing\nthe privacy policies of AR/VR applications with those of free and premium\nwebsites, this study provides a broad perspective on the current state of\nprivacy practices within the AR/VR industry. Our findings indicate that AR/VR\napplications generally offer a higher percentage of positive segments than free\ncontent but lower than premium websites. The analysis of highlighted segments\nand words revealed that AR/VR applications strategically emphasize critical\nprivacy practices and key terms. This enhances privacy policies' clarity and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Abdulaziz Alghamdi"
                    },
                    {
                        "name": "David Mohaisen"
                    }
                ],
                "author_detail": {
                    "name": "David Mohaisen"
                },
                "author": "David Mohaisen",
                "arxiv_comment": "7 pages; appeared in ICMLA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06981v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06981v2",
                "updated": "2025-01-31T15:27:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    27,
                    10,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-09T15:18:57Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    18,
                    57,
                    2,
                    283,
                    0
                ],
                "title": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models"
                },
                "summary": "We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones, making it difficult\nto disentangle and match features across different models. To address this\nissue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics on SAE feature spaces across different LLMs. Our\nexperiments reveal significant similarities in SAE feature spaces across\nvarious LLMs, providing new evidence for feature universality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones, making it difficult\nto disentangle and match features across different models. To address this\nissue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics on SAE feature spaces across different LLMs. Our\nexperiments reveal significant similarities in SAE feature spaces across\nvarious LLMs, providing new evidence for feature universality."
                },
                "authors": [
                    {
                        "name": "Michael Lan"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Austin Meek"
                    },
                    {
                        "name": "Ashkan Khakzar"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Fazl Barez"
                    }
                ],
                "author_detail": {
                    "name": "Fazl Barez"
                },
                "author": "Fazl Barez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06981v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06981v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19204v1",
                "updated": "2025-01-31T15:14:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    14,
                    14,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:14:14Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    14,
                    14,
                    4,
                    31,
                    0
                ],
                "title": "Autonomous Legacy Web Application Upgrades Using a Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Legacy Web Application Upgrades Using a Multi-Agent System"
                },
                "summary": "The use of Large Language Models (LLMs) for autonomous code generation is\ngaining attention in emerging technologies. As LLM capabilities expand, they\noffer new possibilities such as code refactoring, security enhancements, and\nlegacy application upgrades. Many outdated web applications pose security and\nreliability challenges, yet companies continue using them due to the complexity\nand cost of upgrades. To address this, we propose an LLM-based multi-agent\nsystem that autonomously upgrades legacy web applications to the latest\nversions. The system distributes tasks across multiple phases, updating all\nrelevant files. To evaluate its effectiveness, we employed Zero-Shot Learning\n(ZSL) and One-Shot Learning (OSL) prompts, applying identical instructions in\nboth cases. The evaluation involved updating view files and measuring the\nnumber and types of errors in the output. For complex tasks, we counted the\nsuccessfully met requirements. The experiments compared the proposed system\nwith standalone LLM execution, repeated multiple times to account for\nstochastic behavior. Results indicate that our system maintains context across\ntasks and agents, improving solution quality over the base model in some cases.\nThis study provides a foundation for future model implementations in legacy\ncode updates. Additionally, findings highlight LLMs' ability to update small\noutdated files with high precision, even with basic prompts. The source code is\npublicly available on GitHub: https://github.com/alasalm1/Multi-agent-pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) for autonomous code generation is\ngaining attention in emerging technologies. As LLM capabilities expand, they\noffer new possibilities such as code refactoring, security enhancements, and\nlegacy application upgrades. Many outdated web applications pose security and\nreliability challenges, yet companies continue using them due to the complexity\nand cost of upgrades. To address this, we propose an LLM-based multi-agent\nsystem that autonomously upgrades legacy web applications to the latest\nversions. The system distributes tasks across multiple phases, updating all\nrelevant files. To evaluate its effectiveness, we employed Zero-Shot Learning\n(ZSL) and One-Shot Learning (OSL) prompts, applying identical instructions in\nboth cases. The evaluation involved updating view files and measuring the\nnumber and types of errors in the output. For complex tasks, we counted the\nsuccessfully met requirements. The experiments compared the proposed system\nwith standalone LLM execution, repeated multiple times to account for\nstochastic behavior. Results indicate that our system maintains context across\ntasks and agents, improving solution quality over the base model in some cases.\nThis study provides a foundation for future model implementations in legacy\ncode updates. Additionally, findings highlight LLMs' ability to update small\noutdated files with high precision, even with basic prompts. The source code is\npublicly available on GitHub: https://github.com/alasalm1/Multi-agent-pipeline."
                },
                "authors": [
                    {
                        "name": "Valtteri Ala-Salmi"
                    },
                    {
                        "name": "Zeeshan Rasheed"
                    },
                    {
                        "name": "Abdul Malik Sami"
                    },
                    {
                        "name": "Zheying Zhang"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Jussi Rasku"
                    },
                    {
                        "name": "Shahbaz Siddeeq"
                    },
                    {
                        "name": "Mika Saari"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_comment": "13 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19202v1",
                "updated": "2025-01-31T15:12:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    12,
                    20,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:12:20Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    12,
                    20,
                    4,
                    31,
                    0
                ],
                "title": "Improving the Robustness of Representation Misdirection for Large\n  Language Model Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Robustness of Representation Misdirection for Large\n  Language Model Unlearning"
                },
                "summary": "Representation Misdirection (RM) and variants are established large language\nmodel (LLM) unlearning methods with state-of-the-art performance. In this\npaper, we show that RM methods inherently reduce models' robustness, causing\nthem to misbehave even when a single non-adversarial forget-token is in the\nretain-query. Toward understanding underlying causes, we reframe the unlearning\nprocess as backdoor attacks and defenses: forget-tokens act as backdoor\ntriggers that, when activated in retain-queries, cause disruptions in RM\nmodels' behaviors, similar to successful backdoor attacks. To mitigate this\nvulnerability, we propose Random Noise Augmentation -- a model and method\nagnostic approach with theoretical guarantees for improving the robustness of\nRM methods. Extensive experiments demonstrate that RNA significantly improves\nthe robustness of RM models while enhancing the unlearning performances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Misdirection (RM) and variants are established large language\nmodel (LLM) unlearning methods with state-of-the-art performance. In this\npaper, we show that RM methods inherently reduce models' robustness, causing\nthem to misbehave even when a single non-adversarial forget-token is in the\nretain-query. Toward understanding underlying causes, we reframe the unlearning\nprocess as backdoor attacks and defenses: forget-tokens act as backdoor\ntriggers that, when activated in retain-queries, cause disruptions in RM\nmodels' behaviors, similar to successful backdoor attacks. To mitigate this\nvulnerability, we propose Random Noise Augmentation -- a model and method\nagnostic approach with theoretical guarantees for improving the robustness of\nRM methods. Extensive experiments demonstrate that RNA significantly improves\nthe robustness of RM models while enhancing the unlearning performances."
                },
                "authors": [
                    {
                        "name": "Dang Huu-Tien"
                    },
                    {
                        "name": "Hoang Thanh-Tung"
                    },
                    {
                        "name": "Le-Minh Nguyen"
                    },
                    {
                        "name": "Naoya Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Naoya Inoue"
                },
                "author": "Naoya Inoue",
                "arxiv_comment": "12 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19201v1",
                "updated": "2025-01-31T15:10:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    10,
                    29,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:10:29Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    10,
                    29,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Reasoning with Hidden Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Reasoning with Hidden Thinking"
                },
                "summary": "Chain-of-Thought (CoT) reasoning has become a powerful framework for\nimproving complex problem-solving capabilities in Multimodal Large Language\nModels (MLLMs). However, the verbose nature of textual reasoning introduces\nsignificant inefficiencies. In this work, we propose $\\textbf{Heima}$ (as\nhidden llama), an efficient reasoning framework that leverages reasoning CoTs\nat hidden latent space. We design the Heima Encoder to condense each\nintermediate CoT into a compact, higher-level hidden representation using a\nsingle thinking token, effectively minimizing verbosity and reducing the\noverall number of tokens required during the reasoning process. Meanwhile, we\ndesign corresponding Heima Decoder with traditional Large Language Models\n(LLMs) to adaptively interpret the hidden representations into variable-length\ntextual sequence, reconstructing reasoning processes that closely resemble the\noriginal CoTs. Experimental results across diverse reasoning MLLM benchmarks\ndemonstrate that Heima model achieves higher generation efficiency while\nmaintaining or even better zero-shot task accuracy. Moreover, the effective\nreconstruction of multimodal reasoning processes with Heima Decoder validates\nboth the robustness and interpretability of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning has become a powerful framework for\nimproving complex problem-solving capabilities in Multimodal Large Language\nModels (MLLMs). However, the verbose nature of textual reasoning introduces\nsignificant inefficiencies. In this work, we propose $\\textbf{Heima}$ (as\nhidden llama), an efficient reasoning framework that leverages reasoning CoTs\nat hidden latent space. We design the Heima Encoder to condense each\nintermediate CoT into a compact, higher-level hidden representation using a\nsingle thinking token, effectively minimizing verbosity and reducing the\noverall number of tokens required during the reasoning process. Meanwhile, we\ndesign corresponding Heima Decoder with traditional Large Language Models\n(LLMs) to adaptively interpret the hidden representations into variable-length\ntextual sequence, reconstructing reasoning processes that closely resemble the\noriginal CoTs. Experimental results across diverse reasoning MLLM benchmarks\ndemonstrate that Heima model achieves higher generation efficiency while\nmaintaining or even better zero-shot task accuracy. Moreover, the effective\nreconstruction of multimodal reasoning processes with Heima Decoder validates\nboth the robustness and interpretability of our approach."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Yizhou Wang"
                    },
                    {
                        "name": "Xiangxi Shi"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17284v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17284v4",
                "updated": "2025-01-31T15:04:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    4,
                    34,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-26T10:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    13,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "AutoElicit: Using Large Language Models for Expert Prior Elicitation in\n  Predictive Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoElicit: Using Large Language Models for Expert Prior Elicitation in\n  Predictive Modelling"
                },
                "summary": "Large language models (LLMs) acquire a breadth of information across various\ndomains. However, their computational complexity, cost, and lack of\ntransparency often hinder their direct application for predictive tasks where\nprivacy and interpretability are paramount. In fields such as healthcare,\nbiology, and finance, specialised and interpretable linear models still hold\nconsiderable value. In such domains, labelled data may be scarce or expensive\nto obtain. Well-specified prior distributions over model parameters can reduce\nthe sample complexity of learning through Bayesian inference; however,\neliciting expert priors can be time-consuming. We therefore introduce\nAutoElicit to extract knowledge from LLMs and construct priors for predictive\nmodels. We show these priors are informative and can be refined using natural\nlanguage. We perform a careful study contrasting AutoElicit with in-context\nlearning and demonstrate how to perform model selection between the two\nmethods. We find that AutoElicit yields priors that can substantially reduce\nerror over uninformative priors, using fewer labels, and consistently\noutperform in-context learning. We show that AutoElicit saves over 6 months of\nlabelling effort when building a new predictive model for urinary tract\ninfections from sensor recordings of people living with dementia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) acquire a breadth of information across various\ndomains. However, their computational complexity, cost, and lack of\ntransparency often hinder their direct application for predictive tasks where\nprivacy and interpretability are paramount. In fields such as healthcare,\nbiology, and finance, specialised and interpretable linear models still hold\nconsiderable value. In such domains, labelled data may be scarce or expensive\nto obtain. Well-specified prior distributions over model parameters can reduce\nthe sample complexity of learning through Bayesian inference; however,\neliciting expert priors can be time-consuming. We therefore introduce\nAutoElicit to extract knowledge from LLMs and construct priors for predictive\nmodels. We show these priors are informative and can be refined using natural\nlanguage. We perform a careful study contrasting AutoElicit with in-context\nlearning and demonstrate how to perform model selection between the two\nmethods. We find that AutoElicit yields priors that can substantially reduce\nerror over uninformative priors, using fewer labels, and consistently\noutperform in-context learning. We show that AutoElicit saves over 6 months of\nlabelling effort when building a new predictive model for urinary tract\ninfections from sensor recordings of people living with dementia."
                },
                "authors": [
                    {
                        "name": "Alexander Capstick"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Payam Barnaghi"
                    }
                ],
                "author_detail": {
                    "name": "Payam Barnaghi"
                },
                "author": "Payam Barnaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17284v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17284v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.03100v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.03100v4",
                "updated": "2025-01-31T14:59:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    59,
                    17,
                    4,
                    31,
                    0
                ],
                "published": "2023-06-01T00:01:43Z",
                "published_parsed": [
                    2023,
                    6,
                    1,
                    0,
                    1,
                    43,
                    3,
                    152,
                    0
                ],
                "title": "Rethinking Model Evaluation as Narrowing the Socio-Technical Gap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Model Evaluation as Narrowing the Socio-Technical Gap"
                },
                "summary": "The recent development of generative large language models (LLMs) poses new\nchallenges for model evaluation that the research community and industry have\nbeen grappling with. While the versatile capabilities of these models ignite\nmuch excitement, they also inevitably make a leap toward homogenization:\npowering a wide range of applications with a single, often referred to as\n``general-purpose'', model. In this position paper, we argue that model\nevaluation practices must take on a critical task to cope with the challenges\nand responsibilities brought by this homogenization: providing valid\nassessments for whether and how much human needs in diverse downstream use\ncases can be satisfied by the given model (\\textit{socio-technical gap}). By\ndrawing on lessons about improving research realism from the social sciences,\nhuman-computer interaction (HCI), and the interdisciplinary field of\nexplainable AI (XAI), we urge the community to develop evaluation methods based\non real-world contexts and human requirements, and embrace diverse evaluation\nmethods with an acknowledgment of trade-offs between realisms and pragmatic\ncosts to conduct the evaluation. By mapping HCI and current NLG evaluation\nmethods, we identify opportunities for evaluation methods for LLMs to narrow\nthe socio-technical gap and pose open questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development of generative large language models (LLMs) poses new\nchallenges for model evaluation that the research community and industry have\nbeen grappling with. While the versatile capabilities of these models ignite\nmuch excitement, they also inevitably make a leap toward homogenization:\npowering a wide range of applications with a single, often referred to as\n``general-purpose'', model. In this position paper, we argue that model\nevaluation practices must take on a critical task to cope with the challenges\nand responsibilities brought by this homogenization: providing valid\nassessments for whether and how much human needs in diverse downstream use\ncases can be satisfied by the given model (\\textit{socio-technical gap}). By\ndrawing on lessons about improving research realism from the social sciences,\nhuman-computer interaction (HCI), and the interdisciplinary field of\nexplainable AI (XAI), we urge the community to develop evaluation methods based\non real-world contexts and human requirements, and embrace diverse evaluation\nmethods with an acknowledgment of trade-offs between realisms and pragmatic\ncosts to conduct the evaluation. By mapping HCI and current NLG evaluation\nmethods, we identify opportunities for evaluation methods for LLMs to narrow\nthe socio-technical gap and pose open questions."
                },
                "authors": [
                    {
                        "name": "Q. Vera Liao"
                    },
                    {
                        "name": "Ziang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Ziang Xiao"
                },
                "author": "Ziang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.03100v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.03100v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19180v1",
                "updated": "2025-01-31T14:45:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    45,
                    23,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T14:45:23Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    45,
                    23,
                    4,
                    31,
                    0
                ],
                "title": "Enhancing Model Defense Against Jailbreaks with Proactive Safety\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Model Defense Against Jailbreaks with Proactive Safety\n  Reasoning"
                },
                "summary": "Large language models (LLMs) are vital for a wide range of applications yet\nremain susceptible to jailbreak threats, which could lead to the generation of\ninappropriate responses. Conventional defenses, such as refusal and adversarial\ntraining, often fail to cover corner cases or rare domains, leaving LLMs still\nvulnerable to more sophisticated attacks. We propose a novel defense strategy,\nSafety Chain-of-Thought (SCoT), which harnesses the enhanced \\textit{reasoning\ncapabilities} of LLMs for proactive assessment of harmful inputs, rather than\nsimply blocking them. SCoT augments any refusal training datasets to critically\nanalyze the intent behind each request before generating answers. By employing\nproactive reasoning, SCoT enhances the generalization of LLMs across varied\nharmful queries and scenarios not covered in the safety alignment corpus.\nAdditionally, it generates detailed refusals specifying the rules violated.\nComparative evaluations show that SCoT significantly surpasses existing\ndefenses, reducing vulnerability to out-of-distribution issues and adversarial\nmanipulations while maintaining strong general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are vital for a wide range of applications yet\nremain susceptible to jailbreak threats, which could lead to the generation of\ninappropriate responses. Conventional defenses, such as refusal and adversarial\ntraining, often fail to cover corner cases or rare domains, leaving LLMs still\nvulnerable to more sophisticated attacks. We propose a novel defense strategy,\nSafety Chain-of-Thought (SCoT), which harnesses the enhanced \\textit{reasoning\ncapabilities} of LLMs for proactive assessment of harmful inputs, rather than\nsimply blocking them. SCoT augments any refusal training datasets to critically\nanalyze the intent behind each request before generating answers. By employing\nproactive reasoning, SCoT enhances the generalization of LLMs across varied\nharmful queries and scenarios not covered in the safety alignment corpus.\nAdditionally, it generates detailed refusals specifying the rules violated.\nComparative evaluations show that SCoT significantly surpasses existing\ndefenses, reducing vulnerability to out-of-distribution issues and adversarial\nmanipulations while maintaining strong general capabilities."
                },
                "authors": [
                    {
                        "name": "Xianglin Yang"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Jieming Shi"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Jin Song Dong"
                    }
                ],
                "author_detail": {
                    "name": "Jin Song Dong"
                },
                "author": "Jin Song Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19173v1",
                "updated": "2025-01-31T14:39:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    39,
                    30,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T14:39:30Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    39,
                    30,
                    4,
                    31,
                    0
                ],
                "title": "Position: Contextual Integrity Washing for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Contextual Integrity Washing for Language Models"
                },
                "summary": "Machine learning community is discovering Contextual Integrity (CI) as a\nuseful framework to assess the privacy implications of large language models\n(LLMs). This is an encouraging development. The CI theory emphasizes sharing\ninformation in accordance with privacy norms and can bridge the social, legal,\npolitical, and technical aspects essential for evaluating privacy in LLMs.\nHowever, this is also a good point to reflect on use of CI for LLMs. This\nposition paper argues that existing literature adopts CI for LLMs without\nembracing the theory's fundamental tenets, essentially amounting to a form of\n\"CI-washing.\" CI-washing could lead to incorrect conclusions and flawed\nprivacy-preserving designs. We clarify the four fundamental tenets of CI\ntheory, systematize prior work on whether they deviate from these tenets, and\nhighlight overlooked issues in experimental hygiene for LLMs (e.g., prompt\nsensitivity, positional bias).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning community is discovering Contextual Integrity (CI) as a\nuseful framework to assess the privacy implications of large language models\n(LLMs). This is an encouraging development. The CI theory emphasizes sharing\ninformation in accordance with privacy norms and can bridge the social, legal,\npolitical, and technical aspects essential for evaluating privacy in LLMs.\nHowever, this is also a good point to reflect on use of CI for LLMs. This\nposition paper argues that existing literature adopts CI for LLMs without\nembracing the theory's fundamental tenets, essentially amounting to a form of\n\"CI-washing.\" CI-washing could lead to incorrect conclusions and flawed\nprivacy-preserving designs. We clarify the four fundamental tenets of CI\ntheory, systematize prior work on whether they deviate from these tenets, and\nhighlight overlooked issues in experimental hygiene for LLMs (e.g., prompt\nsensitivity, positional bias)."
                },
                "authors": [
                    {
                        "name": "Yan Shvartzshnaider"
                    },
                    {
                        "name": "Vasisht Duddu"
                    }
                ],
                "author_detail": {
                    "name": "Vasisht Duddu"
                },
                "author": "Vasisht Duddu",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11182v2",
                "updated": "2025-01-31T14:36:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    36,
                    14,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-15T02:00:36Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    2,
                    0,
                    36,
                    1,
                    289,
                    0
                ],
                "title": "Position: On-Premises LLM Deployment Demands a Middle Path: Preserving\n  Privacy Without Sacrificing Model Confidentiality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: On-Premises LLM Deployment Demands a Middle Path: Preserving\n  Privacy Without Sacrificing Model Confidentiality"
                },
                "summary": "Current LLM customization typically relies on two deployment strategies:\nclosed-source APIs, which require users to upload private data to external\nservers, and open-weight models, which allow local fine-tuning but pose misuse\nrisks. In this position paper, we argue that (1) deploying closed-source LLMs\nwithin user-controlled infrastructure (\\textit{on-premises deployment})\nenhances data privacy and mitigates misuse risks, and (2) a well-designed\non-premises deployment must ensure model confidentiality -- by preventing model\ntheft -- and offer privacy-preserving customization. Prior research on small\nmodels has explored securing only the output layer within hardware-secured\ndevices to balance confidentiality and customization efficiency. However, we\nshow that this approach is insufficient for defending large-scale LLMs against\ndistillation attacks. We therefore introduce a {semi-open deployment framework}\nthat secures only a few, carefully chosen layers, achieving distillation\nresistance comparable to fully secured models while preserving fine-tuning\nflexibility. Through extensive experiments, we show that securing bottom layers\nsignificantly reduces functional extraction risks. Our findings demonstrate\nthat privacy and confidentiality can coexist, paving the way for secure\non-premises AI deployment that balances usability and protection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLM customization typically relies on two deployment strategies:\nclosed-source APIs, which require users to upload private data to external\nservers, and open-weight models, which allow local fine-tuning but pose misuse\nrisks. In this position paper, we argue that (1) deploying closed-source LLMs\nwithin user-controlled infrastructure (\\textit{on-premises deployment})\nenhances data privacy and mitigates misuse risks, and (2) a well-designed\non-premises deployment must ensure model confidentiality -- by preventing model\ntheft -- and offer privacy-preserving customization. Prior research on small\nmodels has explored securing only the output layer within hardware-secured\ndevices to balance confidentiality and customization efficiency. However, we\nshow that this approach is insufficient for defending large-scale LLMs against\ndistillation attacks. We therefore introduce a {semi-open deployment framework}\nthat secures only a few, carefully chosen layers, achieving distillation\nresistance comparable to fully secured models while preserving fine-tuning\nflexibility. Through extensive experiments, we show that securing bottom layers\nsignificantly reduces functional extraction risks. Our findings demonstrate\nthat privacy and confidentiality can coexist, paving the way for secure\non-premises AI deployment that balances usability and protection."
                },
                "authors": [
                    {
                        "name": "Hanbo Huang"
                    },
                    {
                        "name": "Yihan Li"
                    },
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Ruoyu Sun"
                    },
                    {
                        "name": "Zhuotao Liu"
                    },
                    {
                        "name": "Shiyu Liang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Liang"
                },
                "author": "Shiyu Liang",
                "arxiv_comment": "8 pages for main content of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19164v1",
                "updated": "2025-01-31T14:31:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    31,
                    0,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T14:31:00Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    31,
                    0,
                    4,
                    31,
                    0
                ],
                "title": "Poison as Cure: Visual Noise for Mitigating Object Hallucinations in\n  LVMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poison as Cure: Visual Noise for Mitigating Object Hallucinations in\n  LVMs"
                },
                "summary": "Large vision-language models (LVMs) extend large language models (LLMs) with\nvisual perception capabilities, enabling them to process and interpret visual\ninformation. A major challenge compromising their reliability is object\nhallucination that LVMs may generate plausible but factually inaccurate\ninformation. We propose a novel visual adversarial perturbation (VAP) method to\nmitigate this hallucination issue. VAP alleviates LVM hallucination by applying\nstrategically optimized visual noise without altering the base model. Our\napproach formulates hallucination suppression as an optimization problem,\nleveraging adversarial strategies to generate beneficial visual perturbations\nthat enhance the model's factual grounding and reduce parametric knowledge\nbias. Extensive experimental results demonstrate that our method consistently\nreduces object hallucinations across 8 state-of-the-art LVMs, validating its\nefficacy across diverse evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVMs) extend large language models (LLMs) with\nvisual perception capabilities, enabling them to process and interpret visual\ninformation. A major challenge compromising their reliability is object\nhallucination that LVMs may generate plausible but factually inaccurate\ninformation. We propose a novel visual adversarial perturbation (VAP) method to\nmitigate this hallucination issue. VAP alleviates LVM hallucination by applying\nstrategically optimized visual noise without altering the base model. Our\napproach formulates hallucination suppression as an optimization problem,\nleveraging adversarial strategies to generate beneficial visual perturbations\nthat enhance the model's factual grounding and reduce parametric knowledge\nbias. Extensive experimental results demonstrate that our method consistently\nreduces object hallucinations across 8 state-of-the-art LVMs, validating its\nefficacy across diverse evaluations."
                },
                "authors": [
                    {
                        "name": "Kejia Zhang"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v3",
                "updated": "2025-01-31T14:26:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    26,
                    5,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Our code will be released upon acceptance. The Change Logs on Page 9\n  reveal our significant changes compared with v1 and v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12433v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12433v3",
                "updated": "2025-01-31T14:22:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    22,
                    27,
                    4,
                    31,
                    0
                ],
                "published": "2024-06-18T09:29:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    9,
                    29,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "LLM4Rerank: LLM-based Auto-Reranking Framework for Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Rerank: LLM-based Auto-Reranking Framework for Recommendations"
                },
                "summary": "Reranking is a critical component in recommender systems, playing an\nessential role in refining the output of recommendation algorithms. Traditional\nreranking models have focused predominantly on accuracy, but modern\napplications demand consideration of additional criteria such as diversity and\nfairness. Existing reranking approaches often fail to harmonize these diverse\ncriteria effectively at the model level. Moreover, these models frequently\nencounter challenges with scalability and personalization due to their\ncomplexity and the varying significance of different reranking criteria in\ndiverse scenarios. In response, we introduce a comprehensive reranking\nframework enhanced by LLM, designed to seamlessly integrate various reranking\ncriteria while maintaining scalability and facilitating personalized\nrecommendations. This framework employs a fully connected graph structure,\nallowing the LLM to simultaneously consider multiple aspects such as accuracy,\ndiversity, and fairness through a coherent Chain-of-Thought (CoT) process. A\ncustomizable input mechanism is also integrated, enabling the tuning of the\nlanguage model's focus to meet specific reranking needs. We validate our\napproach using three popular public datasets, where our framework demonstrates\nsuperior performance over existing state-of-the-art reranking models in\nbalancing multiple criteria. The code for this implementation is publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking is a critical component in recommender systems, playing an\nessential role in refining the output of recommendation algorithms. Traditional\nreranking models have focused predominantly on accuracy, but modern\napplications demand consideration of additional criteria such as diversity and\nfairness. Existing reranking approaches often fail to harmonize these diverse\ncriteria effectively at the model level. Moreover, these models frequently\nencounter challenges with scalability and personalization due to their\ncomplexity and the varying significance of different reranking criteria in\ndiverse scenarios. In response, we introduce a comprehensive reranking\nframework enhanced by LLM, designed to seamlessly integrate various reranking\ncriteria while maintaining scalability and facilitating personalized\nrecommendations. This framework employs a fully connected graph structure,\nallowing the LLM to simultaneously consider multiple aspects such as accuracy,\ndiversity, and fairness through a coherent Chain-of-Thought (CoT) process. A\ncustomizable input mechanism is also integrated, enabling the tuning of the\nlanguage model's focus to meet specific reranking needs. We validate our\napproach using three popular public datasets, where our framework demonstrates\nsuperior performance over existing state-of-the-art reranking models in\nbalancing multiple criteria. The code for this implementation is publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Jingtong Gao"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12433v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12433v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16125v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16125v2",
                "updated": "2025-01-31T14:00:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    0,
                    30,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-27T15:12:27Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    12,
                    27,
                    0,
                    27,
                    0
                ],
                "title": "SampleLLM: Optimizing Tabular Data Synthesis in Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SampleLLM: Optimizing Tabular Data Synthesis in Recommendations"
                },
                "summary": "Tabular data synthesis is crucial in machine learning, yet existing general\nmethods-primarily based on statistical or deep learning models-are highly\ndata-dependent and often fall short in recommender systems. This limitation\narises from their difficulty in capturing complex distributions and\nunderstanding feature relationships from sparse and limited data, along with\ntheir inability to grasp semantic feature relations. Recently, Large Language\nModels (LLMs) have shown potential in generating synthetic data samples through\nfew-shot learning and semantic understanding. However, they often suffer from\ninconsistent distribution and lack of diversity due to their inherent\ndistribution disparity with the target dataset. To address these challenges and\nenhance tabular data synthesis for recommendation tasks, we propose a novel\ntwo-stage framework named SampleLLM to improve the quality of LLM-based tabular\ndata synthesis for recommendations by ensuring better distribution alignment.\nIn the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and\ndiverse exemplars to generate data that closely aligns with the target dataset\ndistribution, even when input samples are limited. The second stage uses an\nadvanced feature attribution-based importance sampling method to refine feature\nrelationships within the synthesized data, reducing any distribution biases\nintroduced by the LLM. Experimental results on three recommendation datasets,\ntwo general datasets, and online deployment illustrate that SampleLLM\nsignificantly surpasses existing methods for recommendation tasks and holds\npromise for a broader range of tabular data scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data synthesis is crucial in machine learning, yet existing general\nmethods-primarily based on statistical or deep learning models-are highly\ndata-dependent and often fall short in recommender systems. This limitation\narises from their difficulty in capturing complex distributions and\nunderstanding feature relationships from sparse and limited data, along with\ntheir inability to grasp semantic feature relations. Recently, Large Language\nModels (LLMs) have shown potential in generating synthetic data samples through\nfew-shot learning and semantic understanding. However, they often suffer from\ninconsistent distribution and lack of diversity due to their inherent\ndistribution disparity with the target dataset. To address these challenges and\nenhance tabular data synthesis for recommendation tasks, we propose a novel\ntwo-stage framework named SampleLLM to improve the quality of LLM-based tabular\ndata synthesis for recommendations by ensuring better distribution alignment.\nIn the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and\ndiverse exemplars to generate data that closely aligns with the target dataset\ndistribution, even when input samples are limited. The second stage uses an\nadvanced feature attribution-based importance sampling method to refine feature\nrelationships within the synthesized data, reducing any distribution biases\nintroduced by the LLM. Experimental results on three recommendation datasets,\ntwo general datasets, and online deployment illustrate that SampleLLM\nsignificantly surpasses existing methods for recommendation tasks and holds\npromise for a broader range of tabular data scenarios."
                },
                "authors": [
                    {
                        "name": "Jingtong Gao"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16125v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16125v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16085v3",
                "updated": "2025-01-31T13:56:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    56,
                    58,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-25T04:36:01Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    4,
                    36,
                    1,
                    0,
                    330,
                    0
                ],
                "title": "Cautious Optimizers: Improving Training with One Line of Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cautious Optimizers: Improving Training with One Line of Code"
                },
                "summary": "AdamW has been the default optimizer for transformer pretraining. For many\nyears, our community searched for faster and more stable optimizers with only\nconstrained positive outcomes. In this work, we propose a single-line\nmodification in Pytorch to any momentum-based optimizer, which we rename\ncautious optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that\nthis modification preserves Adam's Hamiltonian function and it does not break\nthe convergence guarantee under the Lyapunov analysis. In addition, a whole new\nfamily of optimizers is revealed by our theoretical insight. Among them, we\npick the simplest one for empirical experiments, showing not only speed-up on\nLlama and MAE pretraining up to $1.47$ times, but also better results in LLM\npost-training tasks. Code is available at\nhttps://github.com/kyleliang919/C-Optim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdamW has been the default optimizer for transformer pretraining. For many\nyears, our community searched for faster and more stable optimizers with only\nconstrained positive outcomes. In this work, we propose a single-line\nmodification in Pytorch to any momentum-based optimizer, which we rename\ncautious optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that\nthis modification preserves Adam's Hamiltonian function and it does not break\nthe convergence guarantee under the Lyapunov analysis. In addition, a whole new\nfamily of optimizers is revealed by our theoretical insight. Among them, we\npick the simplest one for empirical experiments, showing not only speed-up on\nLlama and MAE pretraining up to $1.47$ times, but also better results in LLM\npost-training tasks. Code is available at\nhttps://github.com/kyleliang919/C-Optim."
                },
                "authors": [
                    {
                        "name": "Kaizhao Liang"
                    },
                    {
                        "name": "Lizhang Chen"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Qiang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Liu"
                },
                "author": "Qiang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19135v1",
                "updated": "2025-01-31T13:45:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    45,
                    31,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T13:45:31Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    45,
                    31,
                    4,
                    31,
                    0
                ],
                "title": "A Tensor-Train Decomposition based Compression of LLMs on Group Vector\n  Systolic Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Tensor-Train Decomposition based Compression of LLMs on Group Vector\n  Systolic Accelerator"
                },
                "summary": "Large language models (LLMs) are both storage-intensive and\ncomputation-intensive, posing significant challenges when deployed on\nresource-constrained hardware. As linear layers in LLMs are mainly resource\nconsuming parts, this paper develops a tensor-train decomposition (TTD) for\nLLMs with a further hardware implementation on FPGA. TTD compression is applied\nto the linear layers in ChatGLM3-6B and LLaMA2-7B models with compression\nratios (CRs) for the whole network 1.94$\\times$ and 1.60$\\times$, respectively.\nThe compressed LLMs are further implemented on FPGA hardware within a highly\nefficient group vector systolic array (GVSA) architecture, which has DSP-shared\nparallel vector PEs for TTD inference, as well as optimized data communication\nin the accelerator. Experimental results show that the corresponding TTD based\nLLM accelerator implemented on FPGA achieves 1.45$\\times$ and 1.57$\\times$\nreduction in first token delay for ChatGLM3-6B and LLaMA2-7B models,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are both storage-intensive and\ncomputation-intensive, posing significant challenges when deployed on\nresource-constrained hardware. As linear layers in LLMs are mainly resource\nconsuming parts, this paper develops a tensor-train decomposition (TTD) for\nLLMs with a further hardware implementation on FPGA. TTD compression is applied\nto the linear layers in ChatGLM3-6B and LLaMA2-7B models with compression\nratios (CRs) for the whole network 1.94$\\times$ and 1.60$\\times$, respectively.\nThe compressed LLMs are further implemented on FPGA hardware within a highly\nefficient group vector systolic array (GVSA) architecture, which has DSP-shared\nparallel vector PEs for TTD inference, as well as optimized data communication\nin the accelerator. Experimental results show that the corresponding TTD based\nLLM accelerator implemented on FPGA achieves 1.45$\\times$ and 1.57$\\times$\nreduction in first token delay for ChatGLM3-6B and LLaMA2-7B models,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Sixiao Huang"
                    },
                    {
                        "name": "Tintin Wang"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Keyao Jiang"
                    },
                    {
                        "name": "Mingqiang Huang"
                    },
                    {
                        "name": "Hao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Yu"
                },
                "author": "Hao Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03355v2",
                "updated": "2025-01-31T13:24:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    24,
                    10,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-04T12:21:03Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    12,
                    21,
                    3,
                    4,
                    278,
                    0
                ],
                "title": "LANTERN: Accelerating Visual Autoregressive Models with Relaxed\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LANTERN: Accelerating Visual Autoregressive Models with Relaxed\n  Speculative Decoding"
                },
                "summary": "Auto-Regressive (AR) models have recently gained prominence in image\ngeneration, often matching or even surpassing the performance of diffusion\nmodels. However, one major limitation of AR models is their sequential nature,\nwhich processes tokens one at a time, slowing down generation compared to\nmodels like GANs or diffusion-based methods that operate more efficiently.\nWhile speculative decoding has proven effective for accelerating LLMs by\ngenerating multiple tokens in a single forward, its application in visual AR\nmodels remains largely unexplored. In this work, we identify a challenge in\nthis setting, which we term \\textit{token selection ambiguity}, wherein visual\nAR models frequently assign uniformly low probabilities to tokens, hampering\nthe performance of speculative decoding. To overcome this challenge, we propose\na relaxed acceptance condition referred to as LANTERN that leverages the\ninterchangeability of tokens in latent space. This relaxation restores the\neffectiveness of speculative decoding in visual AR models by enabling more\nflexible use of candidate tokens that would otherwise be prematurely rejected.\nFurthermore, by incorporating a total variation distance bound, we ensure that\nthese speed gains are achieved without significantly compromising image quality\nor semantic coherence. Experimental results demonstrate the efficacy of our\nmethod in providing a substantial speed-up over speculative decoding. In\nspecific, compared to a na\\\"ive application of the state-of-the-art speculative\ndecoding, LANTERN increases speed-ups by $\\mathbf{1.75}\\times$ and\n$\\mathbf{1.82}\\times$, as compared to greedy decoding and random sampling,\nrespectively, when applied to LlamaGen, a contemporary visual AR model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Regressive (AR) models have recently gained prominence in image\ngeneration, often matching or even surpassing the performance of diffusion\nmodels. However, one major limitation of AR models is their sequential nature,\nwhich processes tokens one at a time, slowing down generation compared to\nmodels like GANs or diffusion-based methods that operate more efficiently.\nWhile speculative decoding has proven effective for accelerating LLMs by\ngenerating multiple tokens in a single forward, its application in visual AR\nmodels remains largely unexplored. In this work, we identify a challenge in\nthis setting, which we term \\textit{token selection ambiguity}, wherein visual\nAR models frequently assign uniformly low probabilities to tokens, hampering\nthe performance of speculative decoding. To overcome this challenge, we propose\na relaxed acceptance condition referred to as LANTERN that leverages the\ninterchangeability of tokens in latent space. This relaxation restores the\neffectiveness of speculative decoding in visual AR models by enabling more\nflexible use of candidate tokens that would otherwise be prematurely rejected.\nFurthermore, by incorporating a total variation distance bound, we ensure that\nthese speed gains are achieved without significantly compromising image quality\nor semantic coherence. Experimental results demonstrate the efficacy of our\nmethod in providing a substantial speed-up over speculative decoding. In\nspecific, compared to a na\\\"ive application of the state-of-the-art speculative\ndecoding, LANTERN increases speed-ups by $\\mathbf{1.75}\\times$ and\n$\\mathbf{1.82}\\times$, as compared to greedy decoding and random sampling,\nrespectively, when applied to LlamaGen, a contemporary visual AR model."
                },
                "authors": [
                    {
                        "name": "Doohyuk Jang"
                    },
                    {
                        "name": "Sihwan Park"
                    },
                    {
                        "name": "June Yong Yang"
                    },
                    {
                        "name": "Yeonsung Jung"
                    },
                    {
                        "name": "Jihun Yun"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Sung-Yub Kim"
                    },
                    {
                        "name": "Eunho Yang"
                    }
                ],
                "author_detail": {
                    "name": "Eunho Yang"
                },
                "author": "Eunho Yang",
                "arxiv_comment": "30 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19107v1",
                "updated": "2025-01-31T13:04:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    4,
                    37,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T13:04:37Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    4,
                    37,
                    4,
                    31,
                    0
                ],
                "title": "Brain-inspired sparse training enables Transformers and LLMs to perform\n  as fully connected",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-inspired sparse training enables Transformers and LLMs to perform\n  as fully connected"
                },
                "summary": "This study aims to enlarge our current knowledge on application of\nbrain-inspired network science principles for training artificial neural\nnetworks (ANNs) with sparse connectivity. Dynamic sparse training (DST) can\nreduce the computational demands in ANNs, but faces difficulties to keep peak\nperformance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a\nbrain-inspired method for growing connectivity in DST. CHT leverages a\ngradient-free, topology-driven link regrowth, which has shown ultra-sparse (1%\nconnectivity or lower) advantage across various tasks compared to fully\nconnected networks. Yet, CHT suffers two main drawbacks: (i) its time\ncomplexity is O(Nd^3) - N node network size, d node degree - hence it can apply\nonly to ultra-sparse networks. (ii) it selects top link prediction scores,\nwhich is inappropriate for the early training epochs, when the network presents\nunreliable connections. We propose a GPU-friendly approximation of the CH link\npredictor, which reduces the computational complexity to O(N^3), enabling a\nfast implementation of CHT in large-scale models. We introduce the\nCannistraci-Hebb training soft rule (CHTs), which adopts a strategy for\nsampling connections in both link removal and regrowth, balancing the\nexploration and exploitation of network topology. To improve performance, we\nintegrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results\nshow that, using 1% of connections, CHTs outperforms fully connected networks\nin MLP on visual classification tasks, compressing some networks to < 30%\nnodes. Using 5% of the connections, CHTss outperforms fully connected networks\nin two Transformer-based machine translation tasks. Using 30% of the\nconnections, CHTss achieves superior performance compared to other dynamic\nsparse training methods in language modeling, and it surpasses the fully\nconnected counterpart in zero-shot evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study aims to enlarge our current knowledge on application of\nbrain-inspired network science principles for training artificial neural\nnetworks (ANNs) with sparse connectivity. Dynamic sparse training (DST) can\nreduce the computational demands in ANNs, but faces difficulties to keep peak\nperformance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a\nbrain-inspired method for growing connectivity in DST. CHT leverages a\ngradient-free, topology-driven link regrowth, which has shown ultra-sparse (1%\nconnectivity or lower) advantage across various tasks compared to fully\nconnected networks. Yet, CHT suffers two main drawbacks: (i) its time\ncomplexity is O(Nd^3) - N node network size, d node degree - hence it can apply\nonly to ultra-sparse networks. (ii) it selects top link prediction scores,\nwhich is inappropriate for the early training epochs, when the network presents\nunreliable connections. We propose a GPU-friendly approximation of the CH link\npredictor, which reduces the computational complexity to O(N^3), enabling a\nfast implementation of CHT in large-scale models. We introduce the\nCannistraci-Hebb training soft rule (CHTs), which adopts a strategy for\nsampling connections in both link removal and regrowth, balancing the\nexploration and exploitation of network topology. To improve performance, we\nintegrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results\nshow that, using 1% of connections, CHTs outperforms fully connected networks\nin MLP on visual classification tasks, compressing some networks to < 30%\nnodes. Using 5% of the connections, CHTss outperforms fully connected networks\nin two Transformer-based machine translation tasks. Using 30% of the\nconnections, CHTss achieves superior performance compared to other dynamic\nsparse training methods in language modeling, and it surpasses the fully\nconnected counterpart in zero-shot evaluations."
                },
                "authors": [
                    {
                        "name": "Yingtao Zhang"
                    },
                    {
                        "name": "Jialin Zhao"
                    },
                    {
                        "name": "Wenjing Wu"
                    },
                    {
                        "name": "Ziheng Liao"
                    },
                    {
                        "name": "Umberto Michieli"
                    },
                    {
                        "name": "Carlo Vittorio Cannistraci"
                    }
                ],
                "author_detail": {
                    "name": "Carlo Vittorio Cannistraci"
                },
                "author": "Carlo Vittorio Cannistraci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17233v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17233v2",
                "updated": "2025-01-31T13:04:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    13,
                    4,
                    34,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-22T17:53:34Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    17,
                    53,
                    34,
                    1,
                    296,
                    0
                ],
                "title": "Large Language Models are In-context Preference Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are In-context Preference Learners"
                },
                "summary": "Preference-based reinforcement learning is an effective way to handle tasks\nwhere rewards are hard to specify but can be exceedingly inefficient as\npreference learning is often tabula rasa. We demonstrate that Large Language\nModels (LLMs) have native preference-learning capabilities that allow them to\nachieve sample-efficient preference learning, addressing this challenge. We\npropose In-Context Preference Learning (ICPL), which uses in-context learning\ncapabilities of LLMs to reduce human query inefficiency. ICPL uses the task\ndescription and basic environment code to create sets of reward functions which\nare iteratively refined by placing human feedback over videos of the resultant\npolicies into the context of an LLM and then requesting better rewards. We\nfirst demonstrate ICPL's effectiveness through a synthetic preference study,\nproviding quantitative evidence that it significantly outperforms baseline\npreference-based methods with much higher performance and orders of magnitude\ngreater efficiency. We observe that these improvements are not solely coming\nfrom LLM grounding in the task but that the quality of the rewards improves\nover time, indicating preference learning capabilities. Additionally, we\nperform a series of real human preference-learning trials and observe that ICPL\nextends beyond synthetic settings and can work effectively with\nhumans-in-the-loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-based reinforcement learning is an effective way to handle tasks\nwhere rewards are hard to specify but can be exceedingly inefficient as\npreference learning is often tabula rasa. We demonstrate that Large Language\nModels (LLMs) have native preference-learning capabilities that allow them to\nachieve sample-efficient preference learning, addressing this challenge. We\npropose In-Context Preference Learning (ICPL), which uses in-context learning\ncapabilities of LLMs to reduce human query inefficiency. ICPL uses the task\ndescription and basic environment code to create sets of reward functions which\nare iteratively refined by placing human feedback over videos of the resultant\npolicies into the context of an LLM and then requesting better rewards. We\nfirst demonstrate ICPL's effectiveness through a synthetic preference study,\nproviding quantitative evidence that it significantly outperforms baseline\npreference-based methods with much higher performance and orders of magnitude\ngreater efficiency. We observe that these improvements are not solely coming\nfrom LLM grounding in the task but that the quality of the rewards improves\nover time, indicating preference learning capabilities. Additionally, we\nperform a series of real human preference-learning trials and observe that ICPL\nextends beyond synthetic settings and can work effectively with\nhumans-in-the-loop."
                },
                "authors": [
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Qixin Tan"
                    },
                    {
                        "name": "Hong Lu"
                    },
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Xinting Yang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Eugene Vinitsky"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Vinitsky"
                },
                "author": "Eugene Vinitsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17233v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17233v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12961v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12961v3",
                "updated": "2025-01-31T12:48:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    48,
                    9,
                    4,
                    31,
                    0
                ],
                "published": "2024-09-19T17:59:51Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    17,
                    59,
                    51,
                    3,
                    263,
                    0
                ],
                "title": "Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary\n  Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary\n  Resolution"
                },
                "summary": "Visual data comes in various forms, ranging from small icons of just a few\npixels to long videos spanning hours. Existing multi-modal LLMs usually\nstandardize these diverse visual inputs to a fixed resolution for visual\nencoders and yield similar numbers of tokens for LLMs. This approach is\nnon-optimal for multimodal understanding and inefficient for processing inputs\nwith long and short visual contents. To solve the problem, we propose Oryx, a\nunified multimodal architecture for the spatial-temporal understanding of\nimages, videos, and multi-view 3D scenes. Oryx offers an on-demand solution to\nseamlessly and efficiently process visual inputs with arbitrary spatial sizes\nand temporal lengths through two core innovations: 1) a pre-trained OryxViT\nmodel that can encode images at any resolution into LLM-friendly visual\nrepresentations; 2) a dynamic compressor module that supports 1x to 16x\ncompression on visual tokens by request. These design features enable Oryx to\naccommodate extremely long visual contexts, such as videos, with lower\nresolution and high compression while maintaining high recognition precision\nfor tasks like document understanding with native resolution and no\ncompression. Beyond the architectural improvements, enhanced data curation and\nspecialized training on long-context retrieval and spatial-aware data help Oryx\nachieve strong capabilities in image, video, and 3D multimodal understanding\nsimultaneously. Our work is open-sourced at https://github.com/Oryx-mllm/Oryx.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual data comes in various forms, ranging from small icons of just a few\npixels to long videos spanning hours. Existing multi-modal LLMs usually\nstandardize these diverse visual inputs to a fixed resolution for visual\nencoders and yield similar numbers of tokens for LLMs. This approach is\nnon-optimal for multimodal understanding and inefficient for processing inputs\nwith long and short visual contents. To solve the problem, we propose Oryx, a\nunified multimodal architecture for the spatial-temporal understanding of\nimages, videos, and multi-view 3D scenes. Oryx offers an on-demand solution to\nseamlessly and efficiently process visual inputs with arbitrary spatial sizes\nand temporal lengths through two core innovations: 1) a pre-trained OryxViT\nmodel that can encode images at any resolution into LLM-friendly visual\nrepresentations; 2) a dynamic compressor module that supports 1x to 16x\ncompression on visual tokens by request. These design features enable Oryx to\naccommodate extremely long visual contexts, such as videos, with lower\nresolution and high compression while maintaining high recognition precision\nfor tasks like document understanding with native resolution and no\ncompression. Beyond the architectural improvements, enhanced data curation and\nspecialized training on long-context retrieval and spatial-aware data help Oryx\nachieve strong capabilities in image, video, and 3D multimodal understanding\nsimultaneously. Our work is open-sourced at https://github.com/Oryx-mllm/Oryx."
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Yongming Rao"
                    }
                ],
                "author_detail": {
                    "name": "Yongming Rao"
                },
                "author": "Yongming Rao",
                "arxiv_comment": "Accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12961v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12961v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19099v1",
                "updated": "2025-01-31T12:46:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    46,
                    4,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T12:46:04Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    46,
                    4,
                    4,
                    31,
                    0
                ],
                "title": "Unraveling Zeroth-Order Optimization through the Lens of Low-Dimensional\n  Structured Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling Zeroth-Order Optimization through the Lens of Low-Dimensional\n  Structured Perturbations"
                },
                "summary": "Zeroth-order (ZO) optimization has emerged as a promising alternative to\ngradient-based backpropagation methods, particularly for black-box optimization\nand large language model (LLM) fine-tuning. However, ZO methods suffer from\nslow convergence due to high-variance stochastic gradient estimators. While\nstructured perturbations, such as sparsity and low-rank constraints, have been\nexplored to mitigate these issues, their effectiveness remains highly\nunder-explored. In this work, we develop a unified theoretical framework that\nanalyzes both the convergence and generalization properties of ZO optimization\nunder structured perturbations. We show that high dimensionality is the primary\nbottleneck and introduce the notions of \\textit{stable rank} and\n\\textit{effective overlap} to explain how structured perturbations reduce\ngradient noise and accelerate convergence. Using the uniform stability under\nour framework, we then provide the first theoretical justification for why\nthese perturbations enhance generalization. Additionally, through empirical\nanalysis, we identify that \\textbf{block coordinate descent} (BCD) to be an\neffective structured perturbation method. Extensive experiments show that,\ncompared to existing alternatives, memory-efficient ZO (MeZO) with BCD\n(\\textit{MeZO-BCD}) can provide improved converge with a faster wall-clock\ntime/iteration by up to $\\times\\textbf{2.09}$ while yielding similar or better\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zeroth-order (ZO) optimization has emerged as a promising alternative to\ngradient-based backpropagation methods, particularly for black-box optimization\nand large language model (LLM) fine-tuning. However, ZO methods suffer from\nslow convergence due to high-variance stochastic gradient estimators. While\nstructured perturbations, such as sparsity and low-rank constraints, have been\nexplored to mitigate these issues, their effectiveness remains highly\nunder-explored. In this work, we develop a unified theoretical framework that\nanalyzes both the convergence and generalization properties of ZO optimization\nunder structured perturbations. We show that high dimensionality is the primary\nbottleneck and introduce the notions of \\textit{stable rank} and\n\\textit{effective overlap} to explain how structured perturbations reduce\ngradient noise and accelerate convergence. Using the uniform stability under\nour framework, we then provide the first theoretical justification for why\nthese perturbations enhance generalization. Additionally, through empirical\nanalysis, we identify that \\textbf{block coordinate descent} (BCD) to be an\neffective structured perturbation method. Extensive experiments show that,\ncompared to existing alternatives, memory-efficient ZO (MeZO) with BCD\n(\\textit{MeZO-BCD}) can provide improved converge with a faster wall-clock\ntime/iteration by up to $\\times\\textbf{2.09}$ while yielding similar or better\naccuracy."
                },
                "authors": [
                    {
                        "name": "Sihwan Park"
                    },
                    {
                        "name": "Jihun Yun"
                    },
                    {
                        "name": "SungYub Kim"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Eunho Yang"
                    }
                ],
                "author_detail": {
                    "name": "Eunho Yang"
                },
                "author": "Eunho Yang",
                "arxiv_comment": "35 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19093v1",
                "updated": "2025-01-31T12:39:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    39,
                    28,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T12:39:28Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    39,
                    28,
                    4,
                    31,
                    0
                ],
                "title": "Improving Low-Resource Sequence Labeling with Knowledge Fusion and\n  Contextual Label Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Low-Resource Sequence Labeling with Knowledge Fusion and\n  Contextual Label Explanations"
                },
                "summary": "Sequence labeling remains a significant challenge in low-resource,\ndomain-specific scenarios, particularly for character-dense languages like\nChinese. Existing methods primarily focus on enhancing model comprehension and\nimproving data diversity to boost performance. However, these approaches still\nstruggle with inadequate model applicability and semantic distribution biases\nin domain-specific contexts. To overcome these limitations, we propose a novel\nframework that combines an LLM-based knowledge enhancement workflow with a\nspan-based Knowledge Fusion for Rich and Efficient Extraction (KnowFREE) model.\nOur workflow employs explanation prompts to generate precise contextual\ninterpretations of target entities, effectively mitigating semantic biases and\nenriching the model's contextual understanding. The KnowFREE model further\nintegrates extension label features, enabling efficient nested entity\nextraction without relying on external knowledge during inference. Experiments\non multiple Chinese domain-specific sequence labeling datasets demonstrate that\nour approach achieves state-of-the-art performance, effectively addressing the\nchallenges posed by low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence labeling remains a significant challenge in low-resource,\ndomain-specific scenarios, particularly for character-dense languages like\nChinese. Existing methods primarily focus on enhancing model comprehension and\nimproving data diversity to boost performance. However, these approaches still\nstruggle with inadequate model applicability and semantic distribution biases\nin domain-specific contexts. To overcome these limitations, we propose a novel\nframework that combines an LLM-based knowledge enhancement workflow with a\nspan-based Knowledge Fusion for Rich and Efficient Extraction (KnowFREE) model.\nOur workflow employs explanation prompts to generate precise contextual\ninterpretations of target entities, effectively mitigating semantic biases and\nenriching the model's contextual understanding. The KnowFREE model further\nintegrates extension label features, enabling efficient nested entity\nextraction without relying on external knowledge during inference. Experiments\non multiple Chinese domain-specific sequence labeling datasets demonstrate that\nour approach achieves state-of-the-art performance, effectively addressing the\nchallenges posed by low-resource settings."
                },
                "authors": [
                    {
                        "name": "Peichao Lai"
                    },
                    {
                        "name": "Jiaxin Gan"
                    },
                    {
                        "name": "Feiyang Ye"
                    },
                    {
                        "name": "Yilei Wang"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13302v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13302v3",
                "updated": "2025-01-31T12:35:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    35,
                    54,
                    4,
                    31,
                    0
                ],
                "published": "2024-06-19T07:42:48Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    7,
                    42,
                    48,
                    2,
                    171,
                    0
                ],
                "title": "SituationalLLM: Proactive language models with scene awareness for\n  dynamic, contextual task guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SituationalLLM: Proactive language models with scene awareness for\n  dynamic, contextual task guidance"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in text-based\ntasks but often struggle to provide actionable guidance in real-world physical\nenvironments. This is because of their inability to recognize their limited\nunderstanding of the user's physical context. We present SituationalLLM, a\nnovel approach that integrates structured scene information into an LLM to\ndeliver proactive, context-aware assistance. By encoding objects, attributes,\nand relationships in a custom Scene Graph Language, SituationalLLM actively\nidentifies gaps in environmental context and seeks clarifications during user\ninteractions. This behavior emerges from training on the Situational Awareness\nDatabase for Instruct-Tuning (SAD-Instruct), which combines diverse,\nscenario-specific scene graphs with iterative, dialogue-based refinements.\nExperimental results indicate that SituationalLLM outperforms generic LLM\nbaselines in task specificity, reliability, and adaptability, paving the way\nfor environment-aware AI assistants capable of delivering robust, user-centric\nguidance under real-world constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in text-based\ntasks but often struggle to provide actionable guidance in real-world physical\nenvironments. This is because of their inability to recognize their limited\nunderstanding of the user's physical context. We present SituationalLLM, a\nnovel approach that integrates structured scene information into an LLM to\ndeliver proactive, context-aware assistance. By encoding objects, attributes,\nand relationships in a custom Scene Graph Language, SituationalLLM actively\nidentifies gaps in environmental context and seeks clarifications during user\ninteractions. This behavior emerges from training on the Situational Awareness\nDatabase for Instruct-Tuning (SAD-Instruct), which combines diverse,\nscenario-specific scene graphs with iterative, dialogue-based refinements.\nExperimental results indicate that SituationalLLM outperforms generic LLM\nbaselines in task specificity, reliability, and adaptability, paving the way\nfor environment-aware AI assistants capable of delivering robust, user-centric\nguidance under real-world constraints."
                },
                "authors": [
                    {
                        "name": "Muhammad Saif Ullah Khan"
                    },
                    {
                        "name": "Muhammad Zeshan Afzal"
                    },
                    {
                        "name": "Didier Stricker"
                    }
                ],
                "author_detail": {
                    "name": "Didier Stricker"
                },
                "author": "Didier Stricker",
                "arxiv_comment": "Revised Submission to Open Research Europe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13302v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13302v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19085v1",
                "updated": "2025-01-31T12:23:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    23,
                    28,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T12:23:28Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    23,
                    28,
                    4,
                    31,
                    0
                ],
                "title": "Enhancing Code Generation for Low-Resource Languages: No Silver Bullet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Code Generation for Low-Resource Languages: No Silver Bullet"
                },
                "summary": "The advent of Large Language Models (LLMs) has significantly advanced the\nfield of automated code generation. LLMs rely on large and diverse datasets to\nlearn syntax, semantics, and usage patterns of programming languages. For\nlow-resource languages (i.e., niche programming languages characterized by the\nscarcity of training data), the limited availability of such data hampers the\nmodels' ability to generalize effectively, resulting in poorer code generation\nperformance as compared to high-resource languages. For this reason, there is a\nquest for techniques able to close this performance gap. We present an\nempirical study investigating the effectiveness of several approaches for\nboosting LLMs' performance on low-resource languages, namely: (i) a classic\nfine-tuning, which is however capped in size by the scarcity of training data;\n(ii) three variants of in-context learning, with prompts crafted to provide the\nLLM with additional information about the low-resource language (e.g., few-shot\nexamples showcasing features of the targeted language); and (iii) a\npre-training objective teaching the model how to translate between high- and\nlow-resource languages. The context of our study are two low-resource languages\n(R and Racket) and six LLMs having different architectures and sizes. Our\nfindings reveal that a fine-tuning is usually the best choice for smaller LLMs,\npossibly due to the fact that even a small dataset is sufficient to train their\nlimited number of parameters. With the increase in size of the models,\nin-context learning becomes more and more effective, representing a safe and\ncheap bet (i.e., it always helps, but with different magnitudes). Differently,\nvery large LLMs may deteriorate their performance on low-resource languages\nwhen fine-tuning is performed, possibly due to the lack of enough data needed\nto effectively update their weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has significantly advanced the\nfield of automated code generation. LLMs rely on large and diverse datasets to\nlearn syntax, semantics, and usage patterns of programming languages. For\nlow-resource languages (i.e., niche programming languages characterized by the\nscarcity of training data), the limited availability of such data hampers the\nmodels' ability to generalize effectively, resulting in poorer code generation\nperformance as compared to high-resource languages. For this reason, there is a\nquest for techniques able to close this performance gap. We present an\nempirical study investigating the effectiveness of several approaches for\nboosting LLMs' performance on low-resource languages, namely: (i) a classic\nfine-tuning, which is however capped in size by the scarcity of training data;\n(ii) three variants of in-context learning, with prompts crafted to provide the\nLLM with additional information about the low-resource language (e.g., few-shot\nexamples showcasing features of the targeted language); and (iii) a\npre-training objective teaching the model how to translate between high- and\nlow-resource languages. The context of our study are two low-resource languages\n(R and Racket) and six LLMs having different architectures and sizes. Our\nfindings reveal that a fine-tuning is usually the best choice for smaller LLMs,\npossibly due to the fact that even a small dataset is sufficient to train their\nlimited number of parameters. With the increase in size of the models,\nin-context learning becomes more and more effective, representing a safe and\ncheap bet (i.e., it always helps, but with different magnitudes). Differently,\nvery large LLMs may deteriorate their performance on low-resource languages\nwhen fine-tuning is performed, possibly due to the lack of enough data needed\nto effectively update their weights."
                },
                "authors": [
                    {
                        "name": "Alessandro Giagnorio"
                    },
                    {
                        "name": "Alberto Martin-Lopez"
                    },
                    {
                        "name": "Gabriele Bavota"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Bavota"
                },
                "author": "Gabriele Bavota",
                "arxiv_comment": "Accepted at ICPC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17408v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17408v2",
                "updated": "2025-01-31T12:19:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    19,
                    43,
                    4,
                    31,
                    0
                ],
                "published": "2024-09-25T22:39:55Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    22,
                    39,
                    55,
                    2,
                    269,
                    0
                ],
                "title": "Sociotechnical Approach to Enterprise Generative Artificial Intelligence\n  (E-GenAI)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sociotechnical Approach to Enterprise Generative Artificial Intelligence\n  (E-GenAI)"
                },
                "summary": "In this theoretical article, a sociotechnical approach is proposed to\ncharacterize. First, the business ecosystem, focusing on the relationships\namong Providers, Enterprise, and Customers through SCM, ERP, and CRM platforms\nto align: (1) Business Intelligence (BI), Fuzzy Logic (FL), and TRIZ (Theory of\nInventive Problem Solving), through the OID model, and (2) Knowledge Management\n(KM) and Imperfect Knowledge Management (IKM), through the OIDK model. Second,\nthe article explores the E-GenAI business ecosystem, which integrates\nGenAI-based platforms for SCM, ERP, and CRM with GenAI-based platforms for BI,\nFL, TRIZ, KM, and IKM, to align Large Language Models (LLMs) through the\nE-GenAI (OID) model. Finally, to understand the dynamics of LLMs, we utilize\nfinite automata to model the relationships between Followers and Followees.\nThis facilitates the construction of LLMs that can identify specific\ncharacteristics of users on a social media platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this theoretical article, a sociotechnical approach is proposed to\ncharacterize. First, the business ecosystem, focusing on the relationships\namong Providers, Enterprise, and Customers through SCM, ERP, and CRM platforms\nto align: (1) Business Intelligence (BI), Fuzzy Logic (FL), and TRIZ (Theory of\nInventive Problem Solving), through the OID model, and (2) Knowledge Management\n(KM) and Imperfect Knowledge Management (IKM), through the OIDK model. Second,\nthe article explores the E-GenAI business ecosystem, which integrates\nGenAI-based platforms for SCM, ERP, and CRM with GenAI-based platforms for BI,\nFL, TRIZ, KM, and IKM, to align Large Language Models (LLMs) through the\nE-GenAI (OID) model. Finally, to understand the dynamics of LLMs, we utilize\nfinite automata to model the relationships between Followers and Followees.\nThis facilitates the construction of LLMs that can identify specific\ncharacteristics of users on a social media platform."
                },
                "authors": [
                    {
                        "name": "Leoncio Jimenez"
                    },
                    {
                        "name": "Francisco Venegas"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Venegas"
                },
                "author": "Francisco Venegas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17408v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19080v1",
                "updated": "2025-01-31T12:11:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    11,
                    13,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T12:11:13Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    12,
                    11,
                    13,
                    4,
                    31,
                    0
                ],
                "title": "Differentially Private Policy Gradient",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Policy Gradient"
                },
                "summary": "Motivated by the increasing deployment of reinforcement learning in the real\nworld, involving a large consumption of personal data, we introduce a\ndifferentially private (DP) policy gradient algorithm. We show that, in this\nsetting, the introduction of Differential Privacy can be reduced to the\ncomputation of appropriate trust regions, thus avoiding the sacrifice of\ntheoretical properties of the DP-less methods. Therefore, we show that it is\npossible to find the right trade-off between privacy noise and trust-region\nsize to obtain a performant differentially private policy gradient algorithm.\nWe then outline its performance empirically on various benchmarks. Our results\nand the complexity of the tasks addressed represent a significant improvement\nover existing DP algorithms in online RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the increasing deployment of reinforcement learning in the real\nworld, involving a large consumption of personal data, we introduce a\ndifferentially private (DP) policy gradient algorithm. We show that, in this\nsetting, the introduction of Differential Privacy can be reduced to the\ncomputation of appropriate trust regions, thus avoiding the sacrifice of\ntheoretical properties of the DP-less methods. Therefore, we show that it is\npossible to find the right trade-off between privacy noise and trust-region\nsize to obtain a performant differentially private policy gradient algorithm.\nWe then outline its performance empirically on various benchmarks. Our results\nand the complexity of the tasks addressed represent a significant improvement\nover existing DP algorithms in online RL."
                },
                "authors": [
                    {
                        "name": "Alexandre Rio"
                    },
                    {
                        "name": "Merwan Barlier"
                    },
                    {
                        "name": "Igor Colin"
                    }
                ],
                "author_detail": {
                    "name": "Igor Colin"
                },
                "author": "Igor Colin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19060v1",
                "updated": "2025-01-31T11:47:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    47,
                    15,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:47:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    47,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Contrast-Aware Calibration for Fine-Tuned CLIP: Leveraging Image-Text\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrast-Aware Calibration for Fine-Tuned CLIP: Leveraging Image-Text\n  Alignment"
                },
                "summary": "Vision-language models (VLMs), such as CLIP, have demonstrated exceptional\ngeneralization capabilities and can quickly adapt to downstream tasks through\nprompt fine-tuning. Unfortunately, in classification tasks involving\nnon-training classes, known as open-vocabulary setting, fine-tuned VLMs often\noverfit to train classes, resulting in a misalignment between confidence scores\nand actual accuracy on unseen classes, which significantly undermines their\nreliability in real-world deployments. Existing confidence calibration methods\ntypically require training parameters or analyzing features from the training\ndataset, restricting their ability to generalize unseen classes without\ncorresponding train data. Moreover, VLM-specific calibration methods rely\nsolely on text features from train classes as calibration indicators, which\ninherently limits their ability to calibrate train classes. To address these\nchallenges, we propose an effective multimodal calibration method\nContrast-Aware Calibration (CAC). Building on the original CLIP's zero-shot\nadaptability and the conclusion from empirical analysis that poor intra-class\nand inter-class discriminative ability on unseen classes is the root cause, we\ncalculate calibration weights based on the contrastive difference between the\noriginal and fine-tuned CLIP. This method not only adapts to calibrating unseen\nclasses but also overcomes the limitations of previous VLM calibration methods\nthat could not calibrate train classes. In experiments involving 11 datasets\nwith 5 fine-tuning methods, CAC consistently achieved the best calibration\neffect on both train and unseen classes without sacrificing accuracy and\ninference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs), such as CLIP, have demonstrated exceptional\ngeneralization capabilities and can quickly adapt to downstream tasks through\nprompt fine-tuning. Unfortunately, in classification tasks involving\nnon-training classes, known as open-vocabulary setting, fine-tuned VLMs often\noverfit to train classes, resulting in a misalignment between confidence scores\nand actual accuracy on unseen classes, which significantly undermines their\nreliability in real-world deployments. Existing confidence calibration methods\ntypically require training parameters or analyzing features from the training\ndataset, restricting their ability to generalize unseen classes without\ncorresponding train data. Moreover, VLM-specific calibration methods rely\nsolely on text features from train classes as calibration indicators, which\ninherently limits their ability to calibrate train classes. To address these\nchallenges, we propose an effective multimodal calibration method\nContrast-Aware Calibration (CAC). Building on the original CLIP's zero-shot\nadaptability and the conclusion from empirical analysis that poor intra-class\nand inter-class discriminative ability on unseen classes is the root cause, we\ncalculate calibration weights based on the contrastive difference between the\noriginal and fine-tuned CLIP. This method not only adapts to calibrating unseen\nclasses but also overcomes the limitations of previous VLM calibration methods\nthat could not calibrate train classes. In experiments involving 11 datasets\nwith 5 fine-tuning methods, CAC consistently achieved the best calibration\neffect on both train and unseen classes without sacrificing accuracy and\ninference speed."
                },
                "authors": [
                    {
                        "name": "Song-Lin Lv"
                    },
                    {
                        "name": "Yu-Yang Chen"
                    },
                    {
                        "name": "Zhi Zhou"
                    },
                    {
                        "name": "Yu-Feng Li"
                    },
                    {
                        "name": "Lan-Zhe Guo"
                    }
                ],
                "author_detail": {
                    "name": "Lan-Zhe Guo"
                },
                "author": "Lan-Zhe Guo",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2402.04655 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19057v1",
                "updated": "2025-01-31T11:34:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    34,
                    3,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:34:03Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    34,
                    3,
                    4,
                    31,
                    0
                ],
                "title": "TeZO: Empowering the Low-Rankness on the Temporal Dimension in the\n  Zeroth-Order Optimization for Fine-tuning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeZO: Empowering the Low-Rankness on the Temporal Dimension in the\n  Zeroth-Order Optimization for Fine-tuning LLMs"
                },
                "summary": "Zeroth-order optimization (ZO) has demonstrated remarkable promise in\nefficient fine-tuning tasks for Large Language Models (LLMs). In particular,\nrecent advances incorporate the low-rankness of gradients, introducing low-rank\nZO estimators to further reduce GPU memory consumption. However, most existing\nworks focus solely on the low-rankness of each individual gradient, overlooking\na broader property shared by all gradients throughout the training, i.e., all\ngradients approximately reside within a similar subspace. In this paper, we\nconsider two factors together and propose a novel low-rank ZO estimator, TeZO,\nwhich captures the low-rankness across both the model and temporal dimension.\nSpecifically, we represent ZO perturbations along the temporal dimension as a\n3D tensor and employ Canonical Polyadic Decomposition (CPD) to extract each\nlow-rank 2D matrix, significantly reducing the training cost. TeZO can also be\neasily extended to the Adam variant while consuming less memory than MeZO-SGD,\nand requiring about only 35% memory of MeZO-Adam. Both comprehensive\ntheoretical analysis and extensive experimental research have validated its\nefficiency, achieving SOTA-comparable results with lower overhead of time and\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zeroth-order optimization (ZO) has demonstrated remarkable promise in\nefficient fine-tuning tasks for Large Language Models (LLMs). In particular,\nrecent advances incorporate the low-rankness of gradients, introducing low-rank\nZO estimators to further reduce GPU memory consumption. However, most existing\nworks focus solely on the low-rankness of each individual gradient, overlooking\na broader property shared by all gradients throughout the training, i.e., all\ngradients approximately reside within a similar subspace. In this paper, we\nconsider two factors together and propose a novel low-rank ZO estimator, TeZO,\nwhich captures the low-rankness across both the model and temporal dimension.\nSpecifically, we represent ZO perturbations along the temporal dimension as a\n3D tensor and employ Canonical Polyadic Decomposition (CPD) to extract each\nlow-rank 2D matrix, significantly reducing the training cost. TeZO can also be\neasily extended to the Adam variant while consuming less memory than MeZO-SGD,\nand requiring about only 35% memory of MeZO-Adam. Both comprehensive\ntheoretical analysis and extensive experimental research have validated its\nefficiency, achieving SOTA-comparable results with lower overhead of time and\nmemory."
                },
                "authors": [
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19056v1",
                "updated": "2025-01-31T11:32:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    32,
                    5,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:32:05Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    32,
                    5,
                    4,
                    31,
                    0
                ],
                "title": "Enabling Autonomic Microservice Management through Self-Learning Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Autonomic Microservice Management through Self-Learning Agents"
                },
                "summary": "The increasing complexity of modern software systems necessitates robust\nautonomic self-management capabilities. While Large Language Models (LLMs)\ndemonstrate potential in this domain, they often face challenges in adapting\ntheir general knowledge to specific service contexts. To address this\nlimitation, we propose ServiceOdyssey, a self-learning agent system that\nautonomously manages microservices without requiring prior knowledge of\nservice-specific configurations. By leveraging curriculum learning principles\nand iterative exploration, ServiceOdyssey progressively develops a deep\nunderstanding of operational environments, reducing dependence on human input\nor static documentation. A prototype built with the Sock Shop microservice\ndemonstrates the potential of this approach for autonomic microservice\nmanagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of modern software systems necessitates robust\nautonomic self-management capabilities. While Large Language Models (LLMs)\ndemonstrate potential in this domain, they often face challenges in adapting\ntheir general knowledge to specific service contexts. To address this\nlimitation, we propose ServiceOdyssey, a self-learning agent system that\nautonomously manages microservices without requiring prior knowledge of\nservice-specific configurations. By leveraging curriculum learning principles\nand iterative exploration, ServiceOdyssey progressively develops a deep\nunderstanding of operational environments, reducing dependence on human input\nor static documentation. A prototype built with the Sock Shop microservice\ndemonstrates the potential of this approach for autonomic microservice\nmanagement."
                },
                "authors": [
                    {
                        "name": "Fenglin Yu"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Xiaoting Qin"
                    },
                    {
                        "name": "Zhiyang Zhang"
                    },
                    {
                        "name": "Jue Zhang"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Yingnong Dang"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19054v1",
                "updated": "2025-01-31T11:28:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    28,
                    16,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:28:16Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    28,
                    16,
                    4,
                    31,
                    0
                ],
                "title": "Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models"
                },
                "summary": "Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively."
                },
                "authors": [
                    {
                        "name": "Ruiyu Wang"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Shizhao Sun"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14487v2",
                "updated": "2025-01-31T11:16:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    16,
                    51,
                    4,
                    31,
                    0
                ],
                "published": "2024-07-19T17:41:08Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    17,
                    41,
                    8,
                    4,
                    201,
                    0
                ],
                "title": "Evaluating the Reliability of Self-Explanations in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Reliability of Self-Explanations in Large Language Models"
                },
                "summary": "This paper investigates the reliability of explanations generated by large\nlanguage models (LLMs) when prompted to explain their previous output. We\nevaluate two kinds of such self-explanations - extractive and counterfactual -\nusing three state-of-the-art LLMs (2B to 8B parameters) on two different\nclassification tasks (objective and subjective). Our findings reveal, that,\nwhile these self-explanations can correlate with human judgement, they do not\nfully and accurately follow the model's decision process, indicating a gap\nbetween perceived and actual model reasoning. We show that this gap can be\nbridged because prompting LLMs for counterfactual explanations can produce\nfaithful, informative, and easy-to-verify results. These counterfactuals offer\na promising alternative to traditional explainability methods (e.g. SHAP,\nLIME), provided that prompts are tailored to specific tasks and checked for\nvalidity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the reliability of explanations generated by large\nlanguage models (LLMs) when prompted to explain their previous output. We\nevaluate two kinds of such self-explanations - extractive and counterfactual -\nusing three state-of-the-art LLMs (2B to 8B parameters) on two different\nclassification tasks (objective and subjective). Our findings reveal, that,\nwhile these self-explanations can correlate with human judgement, they do not\nfully and accurately follow the model's decision process, indicating a gap\nbetween perceived and actual model reasoning. We show that this gap can be\nbridged because prompting LLMs for counterfactual explanations can produce\nfaithful, informative, and easy-to-verify results. These counterfactuals offer\na promising alternative to traditional explainability methods (e.g. SHAP,\nLIME), provided that prompts are tailored to specific tasks and checked for\nvalidity."
                },
                "authors": [
                    {
                        "name": "Korbinian Randl"
                    },
                    {
                        "name": "John Pavlopoulos"
                    },
                    {
                        "name": "Aron Henriksson"
                    },
                    {
                        "name": "Tony Lindgren"
                    }
                ],
                "author_detail": {
                    "name": "Tony Lindgren"
                },
                "author": "Tony Lindgren",
                "arxiv_doi": "10.1007/978-3-031-78977-9_3",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-78977-9_3",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.14487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Non peer-reviewed preprint. Presented at Discovery Science 2024.\n  Peer-reviewed version published in the Springer Lecture Notes in Computer\n  Science (vol 15243)",
                "arxiv_journal_ref": "Lecture Notes in Computer Science(2025), vol 15243. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04235v2",
                "updated": "2025-01-31T11:14:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    14,
                    25,
                    4,
                    31,
                    0
                ],
                "published": "2024-12-05T15:11:12Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    11,
                    12,
                    3,
                    340,
                    0
                ],
                "title": "Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM\n  Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM\n  Chatbots"
                },
                "summary": "I combine detection and mitigation techniques to addresses hallucinations in\nLarge Language Models (LLMs). Mitigation is achieved in a question-answering\nRetrieval-Augmented Generation (RAG) framework while detection is obtained by\nintroducing the Negative Missing Information Scoring System (NMISS), which\naccounts for contextual relevance in responses. While RAG mitigates\nhallucinations by grounding answers in external data, NMISS refines the\nevaluation by identifying cases where traditional metrics incorrectly flag\ncontextually accurate responses as hallucinations. I use Italian health news\narticles as context to evaluate LLM performance. Results show that Gemma2 and\nGPT-4 outperform the other models, with GPT-4 producing answers closely aligned\nwith reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral\nbenefit significantly from NMISS, highlighting their ability to provide richer\ncontextual information. This combined approach offers new insights into the\nreduction and more accurate assessment of hallucinations in LLMs, with\napplications in real-world healthcare tasks and other domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I combine detection and mitigation techniques to addresses hallucinations in\nLarge Language Models (LLMs). Mitigation is achieved in a question-answering\nRetrieval-Augmented Generation (RAG) framework while detection is obtained by\nintroducing the Negative Missing Information Scoring System (NMISS), which\naccounts for contextual relevance in responses. While RAG mitigates\nhallucinations by grounding answers in external data, NMISS refines the\nevaluation by identifying cases where traditional metrics incorrectly flag\ncontextually accurate responses as hallucinations. I use Italian health news\narticles as context to evaluate LLM performance. Results show that Gemma2 and\nGPT-4 outperform the other models, with GPT-4 producing answers closely aligned\nwith reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral\nbenefit significantly from NMISS, highlighting their ability to provide richer\ncontextual information. This combined approach offers new insights into the\nreduction and more accurate assessment of hallucinations in LLMs, with\napplications in real-world healthcare tasks and other domains."
                },
                "authors": [
                    {
                        "name": "Maria Paola Priola"
                    }
                ],
                "author_detail": {
                    "name": "Maria Paola Priola"
                },
                "author": "Maria Paola Priola",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19040v1",
                "updated": "2025-01-31T11:10:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    10,
                    49,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:10:49Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    10,
                    49,
                    4,
                    31,
                    0
                ],
                "title": "Towards the Worst-case Robustness of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the Worst-case Robustness of Large Language Models"
                },
                "summary": "Recent studies have revealed the vulnerability of Large Language Models\n(LLMs) to adversarial attacks, where the adversary crafts specific input\nsequences to induce harmful, violent, private, or incorrect outputs. Although\nvarious defenses have been proposed, they have not been evaluated by strong\nadaptive attacks, leaving the worst-case robustness of LLMs still intractable.\nBy developing a stronger white-box attack, our evaluation results indicate that\nmost typical defenses achieve nearly 0\\% robustness.To solve this, we propose\n\\textit{DiffTextPure}, a general defense that diffuses the (adversarial) input\nprompt using any pre-defined smoothing distribution, and purifies the diffused\ninput using a pre-trained language model. Theoretically, we derive tight\nrobustness lower bounds for all smoothing distributions using Fractal Knapsack\nor 0-1 Knapsack solvers. Under this framework, we certify the robustness of a\nspecific case -- smoothing LLMs using a uniform kernel -- against \\textit{any\npossible attack} with an average $\\ell_0$ perturbation of 2.02 or an average\nsuffix length of 6.41.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have revealed the vulnerability of Large Language Models\n(LLMs) to adversarial attacks, where the adversary crafts specific input\nsequences to induce harmful, violent, private, or incorrect outputs. Although\nvarious defenses have been proposed, they have not been evaluated by strong\nadaptive attacks, leaving the worst-case robustness of LLMs still intractable.\nBy developing a stronger white-box attack, our evaluation results indicate that\nmost typical defenses achieve nearly 0\\% robustness.To solve this, we propose\n\\textit{DiffTextPure}, a general defense that diffuses the (adversarial) input\nprompt using any pre-defined smoothing distribution, and purifies the diffused\ninput using a pre-trained language model. Theoretically, we derive tight\nrobustness lower bounds for all smoothing distributions using Fractal Knapsack\nor 0-1 Knapsack solvers. Under this framework, we certify the robustness of a\nspecific case -- smoothing LLMs using a uniform kernel -- against \\textit{any\npossible attack} with an average $\\ell_0$ perturbation of 2.02 or an average\nsuffix length of 6.41."
                },
                "authors": [
                    {
                        "name": "Huanran Chen"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Zeming Wei"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18180v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18180v2",
                "updated": "2025-01-31T10:45:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    45,
                    55,
                    4,
                    31,
                    0
                ],
                "published": "2024-05-28T13:47:21Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    13,
                    47,
                    21,
                    1,
                    149,
                    0
                ],
                "title": "Safe Reinforcement Learning in Black-Box Environments via Adaptive\n  Shielding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe Reinforcement Learning in Black-Box Environments via Adaptive\n  Shielding"
                },
                "summary": "Empowering safe exploration of reinforcement learning (RL) agents during\ntraining is a critical challenge towards their deployment in many real-world\nscenarios. When prior knowledge of the domain or task is unavailable, training\nRL agents in unknown, \\textit{black-box} environments presents an even greater\nsafety risk. We introduce \\mbox{ADVICE} (Adaptive Shielding with a Contrastive\nAutoencoder), a novel post-shielding technique that distinguishes safe and\nunsafe features of state-action pairs during training, and uses this knowledge\nto protect the RL agent from executing actions that yield likely hazardous\noutcomes. Our comprehensive experimental evaluation against state-of-the-art\nsafe RL exploration techniques shows that ADVICE significantly reduces safety\nviolations ($\\approx\\!\\!50\\%$) during training, with a competitive outcome\nreward compared to other techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering safe exploration of reinforcement learning (RL) agents during\ntraining is a critical challenge towards their deployment in many real-world\nscenarios. When prior knowledge of the domain or task is unavailable, training\nRL agents in unknown, \\textit{black-box} environments presents an even greater\nsafety risk. We introduce \\mbox{ADVICE} (Adaptive Shielding with a Contrastive\nAutoencoder), a novel post-shielding technique that distinguishes safe and\nunsafe features of state-action pairs during training, and uses this knowledge\nto protect the RL agent from executing actions that yield likely hazardous\noutcomes. Our comprehensive experimental evaluation against state-of-the-art\nsafe RL exploration techniques shows that ADVICE significantly reduces safety\nviolations ($\\approx\\!\\!50\\%$) during training, with a competitive outcome\nreward compared to other techniques."
                },
                "authors": [
                    {
                        "name": "Daniel Bethell"
                    },
                    {
                        "name": "Simos Gerasimou"
                    },
                    {
                        "name": "Radu Calinescu"
                    },
                    {
                        "name": "Calum Imrie"
                    }
                ],
                "author_detail": {
                    "name": "Calum Imrie"
                },
                "author": "Calum Imrie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18180v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18180v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00005v2",
                "updated": "2025-01-31T10:45:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    45,
                    1,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-16T11:57:14Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    57,
                    14,
                    2,
                    290,
                    0
                ],
                "title": "Mastering the Craft of Data Synthesis for CodeLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mastering the Craft of Data Synthesis for CodeLLMs"
                },
                "summary": "Large language models (LLMs) have shown impressive performance in \\emph{code}\nunderstanding and generation, making coding tasks a key focus for researchers\ndue to their practical applications and value as a testbed for LLM evaluation.\nData synthesis and filtering techniques have been widely adopted and shown to\nbe highly effective in this context. In this paper, we present a focused survey\nand taxonomy of these techniques, emphasizing recent advancements. We highlight\nkey challenges, explore future research directions, and offer practical\nguidance for new researchers entering the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive performance in \\emph{code}\nunderstanding and generation, making coding tasks a key focus for researchers\ndue to their practical applications and value as a testbed for LLM evaluation.\nData synthesis and filtering techniques have been widely adopted and shown to\nbe highly effective in this context. In this paper, we present a focused survey\nand taxonomy of these techniques, emphasizing recent advancements. We highlight\nkey challenges, explore future research directions, and offer practical\nguidance for new researchers entering the field."
                },
                "authors": [
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Philip Arthur"
                    },
                    {
                        "name": "Qianyu Feng"
                    },
                    {
                        "name": "Cong Duy Vu Hoang"
                    },
                    {
                        "name": "Yu-Heng Hong"
                    },
                    {
                        "name": "Mahdi Kazemi Moghaddam"
                    },
                    {
                        "name": "Omid Nezami"
                    },
                    {
                        "name": "Thien Nguyen"
                    },
                    {
                        "name": "Gioacchino Tangari"
                    },
                    {
                        "name": "Duy Vu"
                    },
                    {
                        "name": "Thanh Vu"
                    },
                    {
                        "name": "Mark Johnson"
                    },
                    {
                        "name": "Krishnaram Kenthapadi"
                    },
                    {
                        "name": "Don Dharmasiri"
                    },
                    {
                        "name": "Long Duong"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuan-Fang Li"
                },
                "author": "Yuan-Fang Li",
                "arxiv_comment": "Accepted at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19012v1",
                "updated": "2025-01-31T10:26:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    26,
                    18,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T10:26:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    26,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities"
                },
                "summary": "Large Language Models (LLMs) have become an essential tool in the\nprogrammer's toolkit, but their tendency to hallucinate code can be used by\nmalicious actors to introduce vulnerabilities to broad swathes of the software\nsupply chain. In this work, we analyze package hallucination behaviour in LLMs\nacross popular programming languages examining both existing package references\nand fictional dependencies. By analyzing this package hallucination behaviour\nwe find potential attacks and suggest defensive strategies to defend against\nthese attacks. We discover that package hallucination rate is predicated not\nonly on model choice, but also programming language, model size, and\nspecificity of the coding task request. The Pareto optimality boundary between\ncode generation performance and package hallucination is sparsely populated,\nsuggesting that coding models are not being optimized for secure code.\nAdditionally, we find an inverse correlation between package hallucination rate\nand the HumanEval coding benchmark, offering a heuristic for evaluating the\npropensity of a model to hallucinate packages. Our metrics, findings and\nanalyses provide a base for future models, securing AI-assisted software\ndevelopment workflows against package supply chain attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become an essential tool in the\nprogrammer's toolkit, but their tendency to hallucinate code can be used by\nmalicious actors to introduce vulnerabilities to broad swathes of the software\nsupply chain. In this work, we analyze package hallucination behaviour in LLMs\nacross popular programming languages examining both existing package references\nand fictional dependencies. By analyzing this package hallucination behaviour\nwe find potential attacks and suggest defensive strategies to defend against\nthese attacks. We discover that package hallucination rate is predicated not\nonly on model choice, but also programming language, model size, and\nspecificity of the coding task request. The Pareto optimality boundary between\ncode generation performance and package hallucination is sparsely populated,\nsuggesting that coding models are not being optimized for secure code.\nAdditionally, we find an inverse correlation between package hallucination rate\nand the HumanEval coding benchmark, offering a heuristic for evaluating the\npropensity of a model to hallucinate packages. Our metrics, findings and\nanalyses provide a base for future models, securing AI-assisted software\ndevelopment workflows against package supply chain attacks."
                },
                "authors": [
                    {
                        "name": "Arjun Krishna"
                    },
                    {
                        "name": "Erick Galinkin"
                    },
                    {
                        "name": "Leon Derczynski"
                    },
                    {
                        "name": "Jeffrey Martin"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Martin"
                },
                "author": "Jeffrey Martin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19007v1",
                "updated": "2025-01-31T10:19:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    19,
                    12,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T10:19:12Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    19,
                    12,
                    4,
                    31,
                    0
                ],
                "title": "A heuristic for the deployment of collecting routes for urban recycle\n  stations (eco-points)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A heuristic for the deployment of collecting routes for urban recycle\n  stations (eco-points)"
                },
                "summary": "The rapid and constant increase in urban population has led to a drastic rise\nin urban solid waste production with worrying consequences for the environment\nand society. In many cities, an efficient waste management combined with a\nsuitable design of vehicle routes (VR) can lead to benefits in the\nenvironmental, economic, and social impacts. The general population is becoming\nincreasingly aware of the need for the separation of the various categories of\nmunicipal solid waste. The numerous materials collected include glass, PET or\nbatteries, and electric components, which are sorted at the eco-points. The\nmanagement of eco-points gives rise to several problems that can be formulated\nanalytically. The location and number of eco-point containers, the\ndetermination of the fleet size for picking up the collected waste, and the\ndesign of itineraries are all intertwined, and present computationally\ndifficult problems, and therefore must be solved in a sequential way. In this\npaper, a mathematical model has been formulated, based on the Bin Packing (BP)\nand VR schemes, for the deployment of routes of mobile containers in the\nselective collection of urban solid waste. A heuristic algorithm has also been\ndeveloped, which considers two different configurations of the containers to\nsolve the proposed mathematical programming model. The results obtained from\nthe numerical simulations show the validation of the proposed methodology\ncarried out for the benchmark of the Sioux Falls network and the specific real\ncase study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid and constant increase in urban population has led to a drastic rise\nin urban solid waste production with worrying consequences for the environment\nand society. In many cities, an efficient waste management combined with a\nsuitable design of vehicle routes (VR) can lead to benefits in the\nenvironmental, economic, and social impacts. The general population is becoming\nincreasingly aware of the need for the separation of the various categories of\nmunicipal solid waste. The numerous materials collected include glass, PET or\nbatteries, and electric components, which are sorted at the eco-points. The\nmanagement of eco-points gives rise to several problems that can be formulated\nanalytically. The location and number of eco-point containers, the\ndetermination of the fleet size for picking up the collected waste, and the\ndesign of itineraries are all intertwined, and present computationally\ndifficult problems, and therefore must be solved in a sequential way. In this\npaper, a mathematical model has been formulated, based on the Bin Packing (BP)\nand VR schemes, for the deployment of routes of mobile containers in the\nselective collection of urban solid waste. A heuristic algorithm has also been\ndeveloped, which considers two different configurations of the containers to\nsolve the proposed mathematical programming model. The results obtained from\nthe numerical simulations show the validation of the proposed methodology\ncarried out for the benchmark of the Sioux Falls network and the specific real\ncase study."
                },
                "authors": [
                    {
                        "name": "Guido Marseglia"
                    },
                    {
                        "name": "Juan Antonio Mesa"
                    },
                    {
                        "name": "Francisco A Ortega"
                    },
                    {
                        "name": "Ramón Piedra-de-la-Cuadra"
                    }
                ],
                "author_detail": {
                    "name": "Ramón Piedra-de-la-Cuadra"
                },
                "author": "Ramón Piedra-de-la-Cuadra",
                "arxiv_doi": "10.1016/j.seps.2021.101222",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.seps.2021.101222",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.19007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Socio-Economic Planning Sciences 2022",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18028v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18028v3",
                "updated": "2025-01-31T10:15:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    15,
                    43,
                    4,
                    31,
                    0
                ],
                "published": "2024-09-26T16:34:35Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    34,
                    35,
                    3,
                    270,
                    0
                ],
                "title": "Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective"
                },
                "summary": "A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically."
                },
                "authors": [
                    {
                        "name": "Yotam Wolf"
                    },
                    {
                        "name": "Binyamin Rothberg"
                    },
                    {
                        "name": "Dorin Shteyman"
                    },
                    {
                        "name": "Amnon Shashua"
                    }
                ],
                "author_detail": {
                    "name": "Amnon Shashua"
                },
                "author": "Amnon Shashua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18028v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18028v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19000v1",
                "updated": "2025-01-31T10:09:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    9,
                    27,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T10:09:27Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    9,
                    27,
                    4,
                    31,
                    0
                ],
                "title": "Bilevel optimization for the deployment of refuelling stations for\n  electric vehicles on road networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bilevel optimization for the deployment of refuelling stations for\n  electric vehicles on road networks"
                },
                "summary": "This work consists of a procedure to optimally select, among a group of\ncandidate sites where gas stations were already located, a sufficient number of\ncharging points in order to guarantee that an electric vehicle can make its\njourney without a problem of energy autonomy and that each selected charging\nstation has another one that serves as useful support in case of failure\n(reinforced coverage service). For this purpose, we propose a bilevel model\nthat, in a former level, minimizes the number of refuelling points necessary to\nguarantee a reinforced service coverage for all users who transit from their\norigin to destination and, as a second level, maximize the volume of demand\nthat can be satisfied subject to budgetary restrictions. With the first of the\nobjectives we are addressing the typical requirement of the administration,\nwhich consists of guaranteeing the viability of the solutions, and the second\nof the objectives is a criterion typically used by the private sector\ninitiative, compatible with the profit maximization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work consists of a procedure to optimally select, among a group of\ncandidate sites where gas stations were already located, a sufficient number of\ncharging points in order to guarantee that an electric vehicle can make its\njourney without a problem of energy autonomy and that each selected charging\nstation has another one that serves as useful support in case of failure\n(reinforced coverage service). For this purpose, we propose a bilevel model\nthat, in a former level, minimizes the number of refuelling points necessary to\nguarantee a reinforced service coverage for all users who transit from their\norigin to destination and, as a second level, maximize the volume of demand\nthat can be satisfied subject to budgetary restrictions. With the first of the\nobjectives we are addressing the typical requirement of the administration,\nwhich consists of guaranteeing the viability of the solutions, and the second\nof the objectives is a criterion typically used by the private sector\ninitiative, compatible with the profit maximization."
                },
                "authors": [
                    {
                        "name": "Ramón Piedra-de-la-Cuadra"
                    },
                    {
                        "name": "Francisco Ortega"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Ortega"
                },
                "author": "Francisco Ortega",
                "arxiv_doi": "10.1016/j.cor.2023.106460",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.cor.2023.106460",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.19000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Computers & Operations Research 2024",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10381v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10381v4",
                "updated": "2025-01-31T09:43:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    9,
                    43,
                    40,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-28T04:06:02Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    4,
                    6,
                    2,
                    3,
                    333,
                    0
                ],
                "title": "Supervised Learning-enhanced Multi-Group Actor Critic for Live Stream\n  Allocation in Feed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Learning-enhanced Multi-Group Actor Critic for Live Stream\n  Allocation in Feed"
                },
                "summary": "In the context of a short video & live stream mixed recommendation scenario,\nthe live stream recommendation system (RS) decides whether to allocate at most\none live stream into the video feed for each user request. To maximize\nlong-term user engagement, it is crucial to determine an optimal live stream\npolicy for accurate live stream allocation. The inappropriate live stream\nallocation policy can significantly affect the duration of the usage app and\nuser retention, which ignores the long-term negative impact of live stream\nallocation. Recently, reinforcement learning (RL) has been widely applied in\nrecommendation systems to capture long-term user engagement. However,\ntraditional RL algorithms often face divergence and instability problems, which\nrestricts the application and deployment in the large-scale industrial\nrecommendation systems, especially in the aforementioned challenging scenario.\nTo address these challenges, we propose a novel Supervised Learning-enhanced\nMulti-Group Actor Critic algorithm (SL-MGAC). Specifically, we introduce a\nsupervised learning-enhanced actor-critic framework that incorporates variance\nreduction techniques, where multi-task reward learning helps restrict\nbootstrapping error accumulation during critic learning. Additionally, we\ndesign a multi-group state decomposition module for both actor and critic\nnetworks to reduce prediction variance and improve model stability. We also\npropose a novel reward function to prevent overly greedy live stream\nallocation. Empirically, we evaluate the SL-MGAC algorithm using offline policy\nevaluation (OPE) and online A/B testing. Experimental results demonstrate that\nthe proposed method not only outperforms baseline methods under the\nplatform-level constraints but also exhibits enhanced stability in online\nrecommendation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context of a short video & live stream mixed recommendation scenario,\nthe live stream recommendation system (RS) decides whether to allocate at most\none live stream into the video feed for each user request. To maximize\nlong-term user engagement, it is crucial to determine an optimal live stream\npolicy for accurate live stream allocation. The inappropriate live stream\nallocation policy can significantly affect the duration of the usage app and\nuser retention, which ignores the long-term negative impact of live stream\nallocation. Recently, reinforcement learning (RL) has been widely applied in\nrecommendation systems to capture long-term user engagement. However,\ntraditional RL algorithms often face divergence and instability problems, which\nrestricts the application and deployment in the large-scale industrial\nrecommendation systems, especially in the aforementioned challenging scenario.\nTo address these challenges, we propose a novel Supervised Learning-enhanced\nMulti-Group Actor Critic algorithm (SL-MGAC). Specifically, we introduce a\nsupervised learning-enhanced actor-critic framework that incorporates variance\nreduction techniques, where multi-task reward learning helps restrict\nbootstrapping error accumulation during critic learning. Additionally, we\ndesign a multi-group state decomposition module for both actor and critic\nnetworks to reduce prediction variance and improve model stability. We also\npropose a novel reward function to prevent overly greedy live stream\nallocation. Empirically, we evaluate the SL-MGAC algorithm using offline policy\nevaluation (OPE) and online A/B testing. Experimental results demonstrate that\nthe proposed method not only outperforms baseline methods under the\nplatform-level constraints but also exhibits enhanced stability in online\nrecommendation scenarios."
                },
                "authors": [
                    {
                        "name": "Jingxin Liu"
                    },
                    {
                        "name": "Xiang Gao"
                    },
                    {
                        "name": "Yisha Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Haiyang Lu"
                    },
                    {
                        "name": "Ben Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ben Wang"
                },
                "author": "Ben Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10381v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10381v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16100v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16100v2",
                "updated": "2025-01-31T09:03:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    9,
                    3,
                    3,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-27T14:50:13Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    50,
                    13,
                    0,
                    27,
                    0
                ],
                "title": "Automated Detection of Sport Highlights from Audio and Video Sources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Detection of Sport Highlights from Audio and Video Sources"
                },
                "summary": "This study presents a novel Deep Learning-based and lightweight approach for\nthe automated detection of sports highlights (HLs) from audio and video\nsources. HL detection is a key task in sports video analysis, traditionally\nrequiring significant human effort. Our solution leverages Deep Learning (DL)\nmodels trained on relatively small datasets of audio Mel-spectrograms and\ngrayscale video frames, achieving promising accuracy rates of 89% and 83% for\naudio and video detection, respectively. The use of small datasets, combined\nwith simple architectures, demonstrates the practicality of our method for fast\nand cost-effective deployment. Furthermore, an ensemble model combining both\nmodalities shows improved robustness against false positives and false\nnegatives. The proposed methodology offers a scalable solution for automated HL\ndetection across various types of sports video content, reducing the need for\nmanual intervention. Future work will focus on enhancing model architectures\nand extending this approach to broader scene-detection tasks in media analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a novel Deep Learning-based and lightweight approach for\nthe automated detection of sports highlights (HLs) from audio and video\nsources. HL detection is a key task in sports video analysis, traditionally\nrequiring significant human effort. Our solution leverages Deep Learning (DL)\nmodels trained on relatively small datasets of audio Mel-spectrograms and\ngrayscale video frames, achieving promising accuracy rates of 89% and 83% for\naudio and video detection, respectively. The use of small datasets, combined\nwith simple architectures, demonstrates the practicality of our method for fast\nand cost-effective deployment. Furthermore, an ensemble model combining both\nmodalities shows improved robustness against false positives and false\nnegatives. The proposed methodology offers a scalable solution for automated HL\ndetection across various types of sports video content, reducing the need for\nmanual intervention. Future work will focus on enhancing model architectures\nand extending this approach to broader scene-detection tasks in media analysis."
                },
                "authors": [
                    {
                        "name": "Francesco Della Santa"
                    },
                    {
                        "name": "Morgana Lalli"
                    }
                ],
                "author_detail": {
                    "name": "Morgana Lalli"
                },
                "author": "Morgana Lalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16100v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16100v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19366v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19366v3",
                "updated": "2025-01-31T08:27:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    8,
                    27,
                    13,
                    4,
                    31,
                    0
                ],
                "published": "2024-02-29T17:13:44Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    17,
                    13,
                    44,
                    3,
                    60,
                    0
                ],
                "title": "Exploring the Potential of Large Language Models for Improving Digital\n  Forensic Investigation Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential of Large Language Models for Improving Digital\n  Forensic Investigation Efficiency"
                },
                "summary": "The ever-increasing workload of digital forensic labs raises concerns about\nlaw enforcement's ability to conduct both cyber-related and non-cyber-related\ninvestigations promptly. Consequently, this article explores the potential and\nusefulness of integrating Large Language Models (LLMs) into digital forensic\ninvestigations to address challenges such as bias, explainability, censorship,\nresource-intensive infrastructure, and ethical and legal considerations. A\ncomprehensive literature review is carried out, encompassing existing digital\nforensic models, tools, LLMs, deep learning techniques, and the use of LLMs in\ninvestigations. The review identifies current challenges within existing\ndigital forensic processes and explores both the obstacles and the\npossibilities of incorporating LLMs. In conclusion, the study states that the\nadoption of LLMs in digital forensics, with appropriate constraints, has the\npotential to improve investigation efficiency, improve traceability, and\nalleviate the technical and judicial barriers faced by law enforcement\nentities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-increasing workload of digital forensic labs raises concerns about\nlaw enforcement's ability to conduct both cyber-related and non-cyber-related\ninvestigations promptly. Consequently, this article explores the potential and\nusefulness of integrating Large Language Models (LLMs) into digital forensic\ninvestigations to address challenges such as bias, explainability, censorship,\nresource-intensive infrastructure, and ethical and legal considerations. A\ncomprehensive literature review is carried out, encompassing existing digital\nforensic models, tools, LLMs, deep learning techniques, and the use of LLMs in\ninvestigations. The review identifies current challenges within existing\ndigital forensic processes and explores both the obstacles and the\npossibilities of incorporating LLMs. In conclusion, the study states that the\nadoption of LLMs in digital forensics, with appropriate constraints, has the\npotential to improve investigation efficiency, improve traceability, and\nalleviate the technical and judicial barriers faced by law enforcement\nentities."
                },
                "authors": [
                    {
                        "name": "Akila Wickramasekara"
                    },
                    {
                        "name": "Frank Breitinger"
                    },
                    {
                        "name": "Mark Scanlon"
                    }
                ],
                "author_detail": {
                    "name": "Mark Scanlon"
                },
                "author": "Mark Scanlon",
                "arxiv_doi": "10.1016/j.fsidi.2024.301859",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.fsidi.2024.301859",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.19366v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19366v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Forensic Science International: Digital Investigation (52),\n  301859, 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09447v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09447v3",
                "updated": "2025-01-31T08:22:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    8,
                    22,
                    46,
                    4,
                    31,
                    0
                ],
                "published": "2024-07-12T17:33:34Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    17,
                    33,
                    34,
                    4,
                    194,
                    0
                ],
                "title": "ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to\n  Identify Low-Perplexity Toxic Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to\n  Identify Low-Perplexity Toxic Prompts"
                },
                "summary": "Conventional approaches for the automated red-teaming of large language\nmodels (LLMs) aim to identify prompts that elicit toxic outputs from a frozen\nlanguage model (the defender). This often results in the prompting model (the\nadversary) producing text that is unlikely to arise during autoregression. In\nresponse, we propose a reinforcement learning formulation of LLM red-teaming\ndesigned to discover prompts that both (1) elicit toxic outputs from a defender\nand (2) have low perplexity as scored by that defender. These prompts are the\nmost pertinent in a red-teaming setting because the defender generates them\nwith high probability. We solve this formulation with an online and weakly\nsupervised form of Identity Preference Optimization (IPO), attacking models\nranging from 137M to 7.8B parameters. Our policy performs competitively,\nproducing prompts that induce defender toxicity at a rate of 2-23 times higher\nthan baseline across model scales. Importantly, these prompts have lower\nperplexity than both automatically generated and human-written attacks.\nFurthermore, our method creates black-box attacks with 5.4-14 times increased\ntoxicity. To assess the downstream utility of our method, we use rollouts from\nour policy as negative examples for downstream toxicity tuning and demonstrate\nimproved safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional approaches for the automated red-teaming of large language\nmodels (LLMs) aim to identify prompts that elicit toxic outputs from a frozen\nlanguage model (the defender). This often results in the prompting model (the\nadversary) producing text that is unlikely to arise during autoregression. In\nresponse, we propose a reinforcement learning formulation of LLM red-teaming\ndesigned to discover prompts that both (1) elicit toxic outputs from a defender\nand (2) have low perplexity as scored by that defender. These prompts are the\nmost pertinent in a red-teaming setting because the defender generates them\nwith high probability. We solve this formulation with an online and weakly\nsupervised form of Identity Preference Optimization (IPO), attacking models\nranging from 137M to 7.8B parameters. Our policy performs competitively,\nproducing prompts that induce defender toxicity at a rate of 2-23 times higher\nthan baseline across model scales. Importantly, these prompts have lower\nperplexity than both automatically generated and human-written attacks.\nFurthermore, our method creates black-box attacks with 5.4-14 times increased\ntoxicity. To assess the downstream utility of our method, we use rollouts from\nour policy as negative examples for downstream toxicity tuning and demonstrate\nimproved safety."
                },
                "authors": [
                    {
                        "name": "Amelia F. Hardy"
                    },
                    {
                        "name": "Houjun Liu"
                    },
                    {
                        "name": "Bernard Lange"
                    },
                    {
                        "name": "Duncan Eddy"
                    },
                    {
                        "name": "Mykel J. Kochenderfer"
                    }
                ],
                "author_detail": {
                    "name": "Mykel J. Kochenderfer"
                },
                "author": "Mykel J. Kochenderfer",
                "arxiv_comment": "10 pages, 7 pages of appendix, 3 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09447v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09447v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18940v1",
                "updated": "2025-01-31T08:04:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    8,
                    4,
                    32,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T08:04:32Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    8,
                    4,
                    32,
                    4,
                    31,
                    0
                ],
                "title": "TV-Dialogue: Crafting Theme-Aware Video Dialogues with Immersive\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TV-Dialogue: Crafting Theme-Aware Video Dialogues with Immersive\n  Interaction"
                },
                "summary": "Recent advancements in LLMs have accelerated the development of dialogue\ngeneration across text and images, yet video-based dialogue generation remains\nunderexplored and presents unique challenges. In this paper, we introduce\nTheme-aware Video Dialogue Crafting (TVDC), a novel task aimed at generating\nnew dialogues that align with video content and adhere to user-specified\nthemes. We propose TV-Dialogue, a novel multi-modal agent framework that\nensures both theme alignment (i.e., the dialogue revolves around the theme) and\nvisual consistency (i.e., the dialogue matches the emotions and behaviors of\ncharacters in the video) by enabling real-time immersive interactions among\nvideo characters, thereby accurately understanding the video content and\ngenerating new dialogue that aligns with the given themes. To assess the\ngenerated dialogues, we present a multi-granularity evaluation benchmark with\nhigh accuracy, interpretability and reliability, demonstrating the\neffectiveness of TV-Dialogue on self-collected dataset over directly using\nexisting LLMs. Extensive experiments reveal that TV-Dialogue can generate\ndialogues for videos of any length and any theme in a zero-shot manner without\ntraining. Our findings underscore the potential of TV-Dialogue for various\napplications, such as video re-creation, film dubbing and its use in downstream\nmultimodal tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLMs have accelerated the development of dialogue\ngeneration across text and images, yet video-based dialogue generation remains\nunderexplored and presents unique challenges. In this paper, we introduce\nTheme-aware Video Dialogue Crafting (TVDC), a novel task aimed at generating\nnew dialogues that align with video content and adhere to user-specified\nthemes. We propose TV-Dialogue, a novel multi-modal agent framework that\nensures both theme alignment (i.e., the dialogue revolves around the theme) and\nvisual consistency (i.e., the dialogue matches the emotions and behaviors of\ncharacters in the video) by enabling real-time immersive interactions among\nvideo characters, thereby accurately understanding the video content and\ngenerating new dialogue that aligns with the given themes. To assess the\ngenerated dialogues, we present a multi-granularity evaluation benchmark with\nhigh accuracy, interpretability and reliability, demonstrating the\neffectiveness of TV-Dialogue on self-collected dataset over directly using\nexisting LLMs. Extensive experiments reveal that TV-Dialogue can generate\ndialogues for videos of any length and any theme in a zero-shot manner without\ntraining. Our findings underscore the potential of TV-Dialogue for various\napplications, such as video re-creation, film dubbing and its use in downstream\nmultimodal tasks."
                },
                "authors": [
                    {
                        "name": "Sai Wang"
                    },
                    {
                        "name": "Fan Ma"
                    },
                    {
                        "name": "Xinyi Li"
                    },
                    {
                        "name": "Hehe Fan"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06877v2",
                "updated": "2025-01-31T07:50:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    7,
                    50,
                    44,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-11T11:17:35Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    17,
                    35,
                    0,
                    316,
                    0
                ],
                "title": "LLM-Assisted Relevance Assessments: When Should We Ask LLMs for Help?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Assisted Relevance Assessments: When Should We Ask LLMs for Help?"
                },
                "summary": "Test collections are information retrieval tools that allow researchers to\nquickly and easily evaluate ranking algorithms. While test collections have\nbecome an integral part of IR research, the process of data creation involves\nsignificant effort in manual annotations, which often makes it very expensive\nand time-consuming. Thus, test collections could become too small when the\nbudget is limited, which may lead to unstable evaluations. As a cheaper\nalternative, recent studies have proposed the use of large language models\n(LLMs) to completely replace human assessors. However, while LLMs may seem to\nsomewhat correlate with human judgments, their predictions are not perfect and\noften show bias. Thus a complete replacement with LLMs is argued to be too\nrisky and not fully reliable.\n  Thus, in this paper, we propose LLM-Assisted Relevance Assessments (LARA), an\neffective method to balance manual annotations with LLM annotations, which\nhelps to build a rich and reliable test collection even under a low budget. We\nuse the LLM's predicted relevance probabilities to select the most profitable\ndocuments to manually annotate under a budget constraint. With theoretical\nreasoning, LARA effectively guides the human annotation process by actively\nlearning to calibrate the LLM's predicted relevance probabilities. Then, using\nthe calibration model learned from the limited manual annotations, LARA\ndebiases the LLM predictions to annotate the remaining non-assessed data.\nEmpirical evaluations on TREC-7 Ad Hoc, TREC-8 Ad Hoc, TREC Robust 2004, and\nTREC-COVID datasets show that LARA outperforms alternative solutions under\nalmost any budget constraint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test collections are information retrieval tools that allow researchers to\nquickly and easily evaluate ranking algorithms. While test collections have\nbecome an integral part of IR research, the process of data creation involves\nsignificant effort in manual annotations, which often makes it very expensive\nand time-consuming. Thus, test collections could become too small when the\nbudget is limited, which may lead to unstable evaluations. As a cheaper\nalternative, recent studies have proposed the use of large language models\n(LLMs) to completely replace human assessors. However, while LLMs may seem to\nsomewhat correlate with human judgments, their predictions are not perfect and\noften show bias. Thus a complete replacement with LLMs is argued to be too\nrisky and not fully reliable.\n  Thus, in this paper, we propose LLM-Assisted Relevance Assessments (LARA), an\neffective method to balance manual annotations with LLM annotations, which\nhelps to build a rich and reliable test collection even under a low budget. We\nuse the LLM's predicted relevance probabilities to select the most profitable\ndocuments to manually annotate under a budget constraint. With theoretical\nreasoning, LARA effectively guides the human annotation process by actively\nlearning to calibrate the LLM's predicted relevance probabilities. Then, using\nthe calibration model learned from the limited manual annotations, LARA\ndebiases the LLM predictions to annotate the remaining non-assessed data.\nEmpirical evaluations on TREC-7 Ad Hoc, TREC-8 Ad Hoc, TREC Robust 2004, and\nTREC-COVID datasets show that LARA outperforms alternative solutions under\nalmost any budget constraint."
                },
                "authors": [
                    {
                        "name": "Rikiya Takehi"
                    },
                    {
                        "name": "Ellen M. Voorhees"
                    },
                    {
                        "name": "Tetsuya Sakai"
                    },
                    {
                        "name": "Ian Soboroff"
                    }
                ],
                "author_detail": {
                    "name": "Ian Soboroff"
                },
                "author": "Ian Soboroff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18935v1",
                "updated": "2025-01-31T07:40:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    7,
                    40,
                    34,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T07:40:34Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    7,
                    40,
                    34,
                    4,
                    31,
                    0
                ],
                "title": "TabFSBench: Tabular Benchmark for Feature Shifts in Open Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabFSBench: Tabular Benchmark for Feature Shifts in Open Environment"
                },
                "summary": "Tabular data is widely utilized in various machine learning tasks. Current\ntabular learning research predominantly focuses on closed environments, while\nin real-world applications, open environments are often encountered, where\ndistribution and feature shifts occur, leading to significant degradation in\nmodel performance. Previous research has primarily concentrated on mitigating\ndistribution shifts, whereas feature shifts, a distinctive and unexplored\nchallenge of tabular data, have garnered limited attention. To this end, this\npaper conducts the first comprehensive study on feature shifts in tabular data\nand introduces the first tabular feature-shift benchmark (TabFSBench).\nTabFSBench evaluates impacts of four distinct feature-shift scenarios on four\ntabular model categories across various datasets and assesses the performance\nof large language models (LLMs) and tabular LLMs in the tabular benchmark for\nthe first time. Our study demonstrates three main observations: (1) most\ntabular models have the limited applicability in feature-shift scenarios; (2)\nthe shifted feature set importance has a linear relationship with model\nperformance degradation; (3) model performance in closed environments\ncorrelates with feature-shift performance. Future research direction is also\nexplored for each observation. TabFSBench is released for public access by\nusing a few lines of Python codes at https://github.com/LAMDASZ-ML/TabFSBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data is widely utilized in various machine learning tasks. Current\ntabular learning research predominantly focuses on closed environments, while\nin real-world applications, open environments are often encountered, where\ndistribution and feature shifts occur, leading to significant degradation in\nmodel performance. Previous research has primarily concentrated on mitigating\ndistribution shifts, whereas feature shifts, a distinctive and unexplored\nchallenge of tabular data, have garnered limited attention. To this end, this\npaper conducts the first comprehensive study on feature shifts in tabular data\nand introduces the first tabular feature-shift benchmark (TabFSBench).\nTabFSBench evaluates impacts of four distinct feature-shift scenarios on four\ntabular model categories across various datasets and assesses the performance\nof large language models (LLMs) and tabular LLMs in the tabular benchmark for\nthe first time. Our study demonstrates three main observations: (1) most\ntabular models have the limited applicability in feature-shift scenarios; (2)\nthe shifted feature set importance has a linear relationship with model\nperformance degradation; (3) model performance in closed environments\ncorrelates with feature-shift performance. Future research direction is also\nexplored for each observation. TabFSBench is released for public access by\nusing a few lines of Python codes at https://github.com/LAMDASZ-ML/TabFSBench."
                },
                "authors": [
                    {
                        "name": "Zi-Jian Cheng"
                    },
                    {
                        "name": "Zi-Yi Jia"
                    },
                    {
                        "name": "Zhi Zhou"
                    },
                    {
                        "name": "Lan-Zhe Guo"
                    },
                    {
                        "name": "Yu-Feng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Feng Li"
                },
                "author": "Yu-Feng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00358v2",
                "updated": "2025-01-31T07:32:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    7,
                    32,
                    54,
                    4,
                    31,
                    0
                ],
                "published": "2024-08-31T05:53:39Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    5,
                    53,
                    39,
                    5,
                    244,
                    0
                ],
                "title": "Predicting the Target Word of Game-playing Conversations using a\n  Low-Rank Dialect Adapter for Decoder Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting the Target Word of Game-playing Conversations using a\n  Low-Rank Dialect Adapter for Decoder Models"
                },
                "summary": "Dialect adapters that improve the performance of LLMs for NLU tasks on\ncertain sociolects/dialects/national varieties ('dialects' for the sake of\nbrevity) have been reported for encoder models. In this paper, we extend the\nidea of dialect adapters to decoder models in our architecture called LoRDD.\nUsing MD-3, a publicly available dataset of word game-playing conversations\nbetween dialectal speakers, our task is Target Word Prediction (TWP) from a\nmasked conversation. LoRDD combines task adapters and dialect adapters where\nthe latter employ contrastive learning on pseudo-parallel conversations from\nMD-3. Our experiments on Indian English and Nigerian English conversations with\ntwo models (Mistral and Gemma) demonstrate that LoRDD outperforms four\nbaselines on TWP. Additionally, it significantly reduces the performance gap\nwith American English, narrowing it to 12% and 5.8% for word similarity, and\n25% and 4.5% for accuracy, respectively. The focused contribution of LoRDD is\nin its promise for dialect adaptation of decoder models using TWP, a simplified\nversion of the commonly used next-word prediction task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialect adapters that improve the performance of LLMs for NLU tasks on\ncertain sociolects/dialects/national varieties ('dialects' for the sake of\nbrevity) have been reported for encoder models. In this paper, we extend the\nidea of dialect adapters to decoder models in our architecture called LoRDD.\nUsing MD-3, a publicly available dataset of word game-playing conversations\nbetween dialectal speakers, our task is Target Word Prediction (TWP) from a\nmasked conversation. LoRDD combines task adapters and dialect adapters where\nthe latter employ contrastive learning on pseudo-parallel conversations from\nMD-3. Our experiments on Indian English and Nigerian English conversations with\ntwo models (Mistral and Gemma) demonstrate that LoRDD outperforms four\nbaselines on TWP. Additionally, it significantly reduces the performance gap\nwith American English, narrowing it to 12% and 5.8% for word similarity, and\n25% and 4.5% for accuracy, respectively. The focused contribution of LoRDD is\nin its promise for dialect adaptation of decoder models using TWP, a simplified\nversion of the commonly used next-word prediction task."
                },
                "authors": [
                    {
                        "name": "Dipankar Srirag"
                    },
                    {
                        "name": "Aditya Joshi"
                    },
                    {
                        "name": "Jacob Eisenstein"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Eisenstein"
                },
                "author": "Jacob Eisenstein",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02448v2",
                "updated": "2025-01-31T07:32:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    7,
                    32,
                    7,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-05T05:57:22Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    5,
                    57,
                    22,
                    6,
                    5,
                    0
                ],
                "title": "Understand, Solve and Translate: Bridging the Multilingual Mathematical\n  Reasoning Gap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understand, Solve and Translate: Bridging the Multilingual Mathematical\n  Reasoning Gap"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance on complex\nreasoning tasks. However, despite their strong reasoning capabilities in\nhigh-resource languages (e.g., English and Chinese), a significant performance\ngap persists in other languages. To investigate this gap in Korean, we\nintroduce HRM8K, a benchmark comprising 8,011 English-Korean parallel bilingual\nmath problems. Through systematic analysis of model behaviors, we identify a\nkey finding: these performance disparities stem primarily from difficulties in\ncomprehending non-English inputs, rather than limitations in reasoning\ncapabilities. Based on these findings, we propose UST (Understand, Solve, and\nTranslate), a method that strategically uses English as an anchor for reasoning\nand solution generation. By fine-tuning the model on 130k synthetically\ngenerated data points, UST achieves a 10.91% improvement on the HRM8K benchmark\nand reduces the multilingual performance gap from 11.6% to 0.7%. Additionally,\nwe show that improvements from UST generalize effectively to different Korean\ndomains, demonstrating that capabilities acquired from machine-verifiable\ncontent can be generalized to other areas. We publicly release the benchmark,\ntraining dataset, and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance on complex\nreasoning tasks. However, despite their strong reasoning capabilities in\nhigh-resource languages (e.g., English and Chinese), a significant performance\ngap persists in other languages. To investigate this gap in Korean, we\nintroduce HRM8K, a benchmark comprising 8,011 English-Korean parallel bilingual\nmath problems. Through systematic analysis of model behaviors, we identify a\nkey finding: these performance disparities stem primarily from difficulties in\ncomprehending non-English inputs, rather than limitations in reasoning\ncapabilities. Based on these findings, we propose UST (Understand, Solve, and\nTranslate), a method that strategically uses English as an anchor for reasoning\nand solution generation. By fine-tuning the model on 130k synthetically\ngenerated data points, UST achieves a 10.91% improvement on the HRM8K benchmark\nand reduces the multilingual performance gap from 11.6% to 0.7%. Additionally,\nwe show that improvements from UST generalize effectively to different Korean\ndomains, demonstrating that capabilities acquired from machine-verifiable\ncontent can be generalized to other areas. We publicly release the benchmark,\ntraining dataset, and models."
                },
                "authors": [
                    {
                        "name": "Hyunwoo Ko"
                    },
                    {
                        "name": "Guijin Son"
                    },
                    {
                        "name": "Dasol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Dasol Choi"
                },
                "author": "Dasol Choi",
                "arxiv_comment": "18 pages, 14 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05662v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05662v5",
                "updated": "2025-01-31T07:11:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    7,
                    11,
                    43,
                    4,
                    31,
                    0
                ],
                "published": "2024-04-08T16:46:25Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    16,
                    46,
                    25,
                    0,
                    99,
                    0
                ],
                "title": "BinaryDM: Accurate Weight Binarization for Efficient Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BinaryDM: Accurate Weight Binarization for Efficient Diffusion Models"
                },
                "summary": "With the advancement of diffusion models (DMs) and the substantially\nincreased computational requirements, quantization emerges as a practical\nsolution to obtain compact and efficient low-bit DMs. However, the highly\ndiscrete representation leads to severe accuracy degradation, hindering the\nquantization of diffusion models to ultra-low bit-widths. This paper proposes a\nnovel weight binarization approach for DMs, namely BinaryDM, pushing binarized\nDMs to be accurate and efficient by improving the representation and\noptimization. From the representation perspective, we present an\nEvolvable-Basis Binarizer (EBB) to enable a smooth evolution of DMs from\nfull-precision to accurately binarized. EBB enhances information representation\nin the initial stage through the flexible combination of multiple binary bases\nand applies regularization to evolve into efficient single-basis binarization.\nThe evolution only occurs in the head and tail of the DM architecture to retain\nthe stability of training. From the optimization perspective, a Low-rank\nRepresentation Mimicking (LRM) is applied to assist the optimization of\nbinarized DMs. The LRM mimics the representations of full-precision DMs in\nlow-rank space, alleviating the direction ambiguity of the optimization process\ncaused by fine-grained alignment. Comprehensive experiments demonstrate that\nBinaryDM achieves significant accuracy and efficiency gains compared to SOTA\nquantization methods of DMs under ultra-low bit-widths. With 1-bit weight and\n4-bit activation (W1A4), BinaryDM achieves as low as 7.74 FID and saves the\nperformance from collapse (baseline FID 10.87). As the first binarization\nmethod for diffusion models, W1A4 BinaryDM achieves impressive 15.2x OPs and\n29.2x model size savings, showcasing its substantial potential for edge\ndeployment. The code is available at https://github.com/Xingyu-Zheng/BinaryDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of diffusion models (DMs) and the substantially\nincreased computational requirements, quantization emerges as a practical\nsolution to obtain compact and efficient low-bit DMs. However, the highly\ndiscrete representation leads to severe accuracy degradation, hindering the\nquantization of diffusion models to ultra-low bit-widths. This paper proposes a\nnovel weight binarization approach for DMs, namely BinaryDM, pushing binarized\nDMs to be accurate and efficient by improving the representation and\noptimization. From the representation perspective, we present an\nEvolvable-Basis Binarizer (EBB) to enable a smooth evolution of DMs from\nfull-precision to accurately binarized. EBB enhances information representation\nin the initial stage through the flexible combination of multiple binary bases\nand applies regularization to evolve into efficient single-basis binarization.\nThe evolution only occurs in the head and tail of the DM architecture to retain\nthe stability of training. From the optimization perspective, a Low-rank\nRepresentation Mimicking (LRM) is applied to assist the optimization of\nbinarized DMs. The LRM mimics the representations of full-precision DMs in\nlow-rank space, alleviating the direction ambiguity of the optimization process\ncaused by fine-grained alignment. Comprehensive experiments demonstrate that\nBinaryDM achieves significant accuracy and efficiency gains compared to SOTA\nquantization methods of DMs under ultra-low bit-widths. With 1-bit weight and\n4-bit activation (W1A4), BinaryDM achieves as low as 7.74 FID and saves the\nperformance from collapse (baseline FID 10.87). As the first binarization\nmethod for diffusion models, W1A4 BinaryDM achieves impressive 15.2x OPs and\n29.2x model size savings, showcasing its substantial potential for edge\ndeployment. The code is available at https://github.com/Xingyu-Zheng/BinaryDM."
                },
                "authors": [
                    {
                        "name": "Xingyu Zheng"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Xudong Ma"
                    },
                    {
                        "name": "Mingyuan Zhang"
                    },
                    {
                        "name": "Haojie Hao"
                    },
                    {
                        "name": "Jiakai Wang"
                    },
                    {
                        "name": "Zixiang Zhao"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Michele Magno"
                    }
                ],
                "author_detail": {
                    "name": "Michele Magno"
                },
                "author": "Michele Magno",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05662v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05662v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18924v1",
                "updated": "2025-01-31T07:10:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    7,
                    10,
                    40,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T07:10:40Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    7,
                    10,
                    40,
                    4,
                    31,
                    0
                ],
                "title": "Language Games as the Pathway to Artificial Superhuman Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Games as the Pathway to Artificial Superhuman Intelligence"
                },
                "summary": "The evolution of large language models (LLMs) toward artificial superhuman\nintelligence (ASI) hinges on data reproduction, a cyclical process in which\nmodels generate, curate and retrain on novel data to refine capabilities.\nCurrent methods, however, risk getting stuck in a data reproduction trap:\noptimizing outputs within fixed human-generated distributions in a closed loop\nleads to stagnation, as models merely recombine existing knowledge rather than\nexplore new frontiers. In this paper, we propose language games as a pathway to\nexpanded data reproduction, breaking this cycle through three mechanisms: (1)\n\\textit{role fluidity}, which enhances data diversity and coverage by enabling\nmulti-agent systems to dynamically shift roles across tasks; (2) \\textit{reward\nvariety}, embedding multiple feedback criteria that can drive complex\nintelligent behaviors; and (3) \\textit{rule plasticity}, iteratively evolving\ninteraction constraints to foster learnability, thereby injecting continual\nnovelty. By scaling language games into global sociotechnical ecosystems,\nhuman-AI co-evolution generates unbounded data streams that drive open-ended\nexploration. This framework redefines data reproduction not as a closed loop\nbut as an engine for superhuman intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of large language models (LLMs) toward artificial superhuman\nintelligence (ASI) hinges on data reproduction, a cyclical process in which\nmodels generate, curate and retrain on novel data to refine capabilities.\nCurrent methods, however, risk getting stuck in a data reproduction trap:\noptimizing outputs within fixed human-generated distributions in a closed loop\nleads to stagnation, as models merely recombine existing knowledge rather than\nexplore new frontiers. In this paper, we propose language games as a pathway to\nexpanded data reproduction, breaking this cycle through three mechanisms: (1)\n\\textit{role fluidity}, which enhances data diversity and coverage by enabling\nmulti-agent systems to dynamically shift roles across tasks; (2) \\textit{reward\nvariety}, embedding multiple feedback criteria that can drive complex\nintelligent behaviors; and (3) \\textit{rule plasticity}, iteratively evolving\ninteraction constraints to foster learnability, thereby injecting continual\nnovelty. By scaling language games into global sociotechnical ecosystems,\nhuman-AI co-evolution generates unbounded data streams that drive open-ended\nexploration. This framework redefines data reproduction not as a closed loop\nbut as an engine for superhuman intelligence."
                },
                "authors": [
                    {
                        "name": "Ying Wen"
                    },
                    {
                        "name": "Ziyu Wan"
                    },
                    {
                        "name": "Shao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shao Zhang"
                },
                "author": "Shao Zhang",
                "arxiv_comment": "This position paper argues that language games provide robust\n  mechanism for achieving superhuman intelligence in large language models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18922v1",
                "updated": "2025-01-31T06:59:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    6,
                    59,
                    49,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T06:59:49Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    6,
                    59,
                    49,
                    4,
                    31,
                    0
                ],
                "title": "KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree\n  Search"
                },
                "summary": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions with a large-scale structured knowledge base (KB). Despite\nadvancements with large language models (LLMs), KBQA still faces challenges in\nweak KB awareness, imbalance between effectiveness and efficiency, and high\nreliance on annotated data. To address these challenges, we propose KBQA-o1, a\nnovel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a\nReAct-based agent process for stepwise logical form generation with KB\nenvironment exploration. Moreover, it employs MCTS, a heuristic search method\ndriven by policy and reward models, to balance agentic exploration's\nperformance and search space. With heuristic exploration, KBQA-o1 generates\nhigh-quality annotations for further improvement by incremental fine-tuning.\nExperimental results show that KBQA-o1 outperforms previous low-resource KBQA\nmethods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1\nperformance to 78.5% compared to 48.5% of the previous sota method with\nGPT-3.5-turbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions with a large-scale structured knowledge base (KB). Despite\nadvancements with large language models (LLMs), KBQA still faces challenges in\nweak KB awareness, imbalance between effectiveness and efficiency, and high\nreliance on annotated data. To address these challenges, we propose KBQA-o1, a\nnovel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a\nReAct-based agent process for stepwise logical form generation with KB\nenvironment exploration. Moreover, it employs MCTS, a heuristic search method\ndriven by policy and reward models, to balance agentic exploration's\nperformance and search space. With heuristic exploration, KBQA-o1 generates\nhigh-quality annotations for further improvement by incremental fine-tuning.\nExperimental results show that KBQA-o1 outperforms previous low-resource KBQA\nmethods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1\nperformance to 78.5% compared to 48.5% of the previous sota method with\nGPT-3.5-turbo."
                },
                "authors": [
                    {
                        "name": "Haoran Luo"
                    },
                    {
                        "name": "Haihong E"
                    },
                    {
                        "name": "Yikai Guo"
                    },
                    {
                        "name": "Qika Lin"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "Xinyu Mu"
                    },
                    {
                        "name": "Wenhao Liu"
                    },
                    {
                        "name": "Meina Song"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Luu Anh Tuan"
                    }
                ],
                "author_detail": {
                    "name": "Luu Anh Tuan"
                },
                "author": "Luu Anh Tuan",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15625v2",
                "updated": "2025-01-31T06:36:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    6,
                    36,
                    22,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-21T04:08:37Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    4,
                    8,
                    37,
                    0,
                    295,
                    0
                ],
                "title": "Improving Parallel Program Performance with LLM Optimizers via\n  Agent-System Interface",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Parallel Program Performance with LLM Optimizers via\n  Agent-System Interface"
                },
                "summary": "Modern scientific discovery increasingly relies on high-performance computing\nfor complex modeling and simulation. A key challenge in improving parallel\nprogram performance is efficiently mapping tasks to processors and data to\nmemory, a process dictated by intricate, low-level system code known as\nmappers. Developing high-performance mappers demands days of manual tuning,\nposing a significant barrier for domain scientists without systems expertise.\nWe introduce a framework that automates mapper development with generative\noptimization, leveraging richer feedback beyond scalar performance metrics. Our\napproach features the Agent-System Interface, which includes a Domain-Specific\nLanguage (DSL) to abstract away low-level complexity of system code and define\na structured search space, as well as AutoGuide, a mechanism that interprets\nraw execution output into actionable feedback. Unlike traditional reinforcement\nlearning methods such as OpenTuner, which rely solely on scalar feedback, our\nmethod finds superior mappers in far fewer iterations. With just 10 iterations,\nit outperforms OpenTuner even after 1000 iterations, achieving 3.8X faster\nperformance. Our approach finds mappers that surpass expert-written mappers by\nup to 1.34X speedup across nine benchmarks while reducing tuning time from days\nto minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern scientific discovery increasingly relies on high-performance computing\nfor complex modeling and simulation. A key challenge in improving parallel\nprogram performance is efficiently mapping tasks to processors and data to\nmemory, a process dictated by intricate, low-level system code known as\nmappers. Developing high-performance mappers demands days of manual tuning,\nposing a significant barrier for domain scientists without systems expertise.\nWe introduce a framework that automates mapper development with generative\noptimization, leveraging richer feedback beyond scalar performance metrics. Our\napproach features the Agent-System Interface, which includes a Domain-Specific\nLanguage (DSL) to abstract away low-level complexity of system code and define\na structured search space, as well as AutoGuide, a mechanism that interprets\nraw execution output into actionable feedback. Unlike traditional reinforcement\nlearning methods such as OpenTuner, which rely solely on scalar feedback, our\nmethod finds superior mappers in far fewer iterations. With just 10 iterations,\nit outperforms OpenTuner even after 1000 iterations, achieving 3.8X faster\nperformance. Our approach finds mappers that surpass expert-written mappers by\nup to 1.34X speedup across nine benchmarks while reducing tuning time from days\nto minutes."
                },
                "authors": [
                    {
                        "name": "Anjiang Wei"
                    },
                    {
                        "name": "Allen Nie"
                    },
                    {
                        "name": "Thiago S. F. X. Teixeira"
                    },
                    {
                        "name": "Rohan Yadav"
                    },
                    {
                        "name": "Wonchan Lee"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Alex Aiken"
                    }
                ],
                "author_detail": {
                    "name": "Alex Aiken"
                },
                "author": "Alex Aiken",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18916v1",
                "updated": "2025-01-31T06:34:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    6,
                    34,
                    47,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T06:34:47Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    6,
                    34,
                    47,
                    4,
                    31,
                    0
                ],
                "title": "LLM Program Optimization via Retrieval Augmented Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Program Optimization via Retrieval Augmented Search"
                },
                "summary": "With the advent of large language models (LLMs), there has been a great deal\nof interest in applying them to solve difficult programming tasks. Recent work\nhas demonstrated their potential at program optimization, a key challenge in\nprogramming languages research. We propose a blackbox adaptation method called\nRetrieval Augmented Search (RAS) that performs beam search over candidate\noptimizations; at each step, it retrieves in-context examples from a given\ntraining dataset of slow-fast program pairs to guide the LLM. Critically, we\nfind that performing contextual retrieval based on an LLM-generated natural\nlanguage description significantly outperforms retrieval based on the source\ncode. In addition, we propose a method called AEGIS for improving\ninterpretability by decomposing training examples into \"atomic edits\" that are\nsignificantly more incremental in nature. We show that RAS performs 1.8$\\times$\nbetter than prior state-of-the-art blackbox adaptation strategies, and that\nAEGIS performs 1.37$\\times$ better while performing significantly smaller\nedits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of large language models (LLMs), there has been a great deal\nof interest in applying them to solve difficult programming tasks. Recent work\nhas demonstrated their potential at program optimization, a key challenge in\nprogramming languages research. We propose a blackbox adaptation method called\nRetrieval Augmented Search (RAS) that performs beam search over candidate\noptimizations; at each step, it retrieves in-context examples from a given\ntraining dataset of slow-fast program pairs to guide the LLM. Critically, we\nfind that performing contextual retrieval based on an LLM-generated natural\nlanguage description significantly outperforms retrieval based on the source\ncode. In addition, we propose a method called AEGIS for improving\ninterpretability by decomposing training examples into \"atomic edits\" that are\nsignificantly more incremental in nature. We show that RAS performs 1.8$\\times$\nbetter than prior state-of-the-art blackbox adaptation strategies, and that\nAEGIS performs 1.37$\\times$ better while performing significantly smaller\nedits."
                },
                "authors": [
                    {
                        "name": "Sagnik Anupam"
                    },
                    {
                        "name": "Alexander Shypula"
                    },
                    {
                        "name": "Osbert Bastani"
                    }
                ],
                "author_detail": {
                    "name": "Osbert Bastani"
                },
                "author": "Osbert Bastani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10856v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10856v3",
                "updated": "2025-01-31T06:34:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    6,
                    34,
                    12,
                    4,
                    31,
                    0
                ],
                "published": "2024-12-14T15:11:07Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    15,
                    11,
                    7,
                    5,
                    349,
                    0
                ],
                "title": "RWKV-Lite: Deeply Compressed RWKV for Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RWKV-Lite: Deeply Compressed RWKV for Resource-Constrained Devices"
                },
                "summary": "To deploy LLMs on resource-contained platforms such as mobile robots and\nsmartphones, non-transformers LLMs have achieved major breakthroughs. Recently,\na novel RNN-based LLM family, Repentance Weighted Key Value (RWKV) has shown\nstrong computational efficiency; nevertheless, RWKV models still have high\nparameter counts which limited their deployment. In this paper, we propose a\nsuite of compression techniques, ranging from model architecture optimizations\nto post-training compression, tailored to the RWKV architecture. Combined, our\ntechniques reduce the memory footprint of RWKV models by 3.4x -- 5x with only\nnegligible degradation in accuracy; compared to transformer LLMs with similar\naccuracy, our models require 4x less memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To deploy LLMs on resource-contained platforms such as mobile robots and\nsmartphones, non-transformers LLMs have achieved major breakthroughs. Recently,\na novel RNN-based LLM family, Repentance Weighted Key Value (RWKV) has shown\nstrong computational efficiency; nevertheless, RWKV models still have high\nparameter counts which limited their deployment. In this paper, we propose a\nsuite of compression techniques, ranging from model architecture optimizations\nto post-training compression, tailored to the RWKV architecture. Combined, our\ntechniques reduce the memory footprint of RWKV models by 3.4x -- 5x with only\nnegligible degradation in accuracy; compared to transformer LLMs with similar\naccuracy, our models require 4x less memory footprint."
                },
                "authors": [
                    {
                        "name": "Wonkyo Choe"
                    },
                    {
                        "name": "Yangfeng Ji"
                    },
                    {
                        "name": "Felix Xiaozhu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Felix Xiaozhu Lin"
                },
                "author": "Felix Xiaozhu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10856v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10856v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18914v1",
                "updated": "2025-01-31T06:32:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    6,
                    32,
                    46,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T06:32:46Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    6,
                    32,
                    46,
                    4,
                    31,
                    0
                ],
                "title": "Scaling Laws for Differentially Private Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Differentially Private Language Models"
                },
                "summary": "Scaling laws have emerged as important components of large language model\n(LLM) training as they can predict performance gains through scale, and provide\nguidance on important hyper-parameter choices that would otherwise be\nexpensive. LLMs also rely on large, high-quality training datasets, like those\nsourced from (sometimes sensitive) user data. Training models on this sensitive\nuser data requires careful privacy protections like differential privacy (DP).\nHowever, the dynamics of DP training are significantly different, and\nconsequently their scaling laws are not yet fully understood. In this work, we\nestablish scaling laws that accurately model the intricacies of DP LLM\ntraining, providing a complete picture of the compute-privacy-utility tradeoffs\nand the optimal training configurations in many settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws have emerged as important components of large language model\n(LLM) training as they can predict performance gains through scale, and provide\nguidance on important hyper-parameter choices that would otherwise be\nexpensive. LLMs also rely on large, high-quality training datasets, like those\nsourced from (sometimes sensitive) user data. Training models on this sensitive\nuser data requires careful privacy protections like differential privacy (DP).\nHowever, the dynamics of DP training are significantly different, and\nconsequently their scaling laws are not yet fully understood. In this work, we\nestablish scaling laws that accurately model the intricacies of DP LLM\ntraining, providing a complete picture of the compute-privacy-utility tradeoffs\nand the optimal training configurations in many settings."
                },
                "authors": [
                    {
                        "name": "Ryan McKenna"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Amer Sinha"
                    },
                    {
                        "name": "Borja Balle"
                    },
                    {
                        "name": "Zachary Charles"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "Badih Ghazi"
                    },
                    {
                        "name": "George Kaissis"
                    },
                    {
                        "name": "Ravi Kumar"
                    },
                    {
                        "name": "Ruibo Liu"
                    },
                    {
                        "name": "Da Yu"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chiyuan Zhang"
                },
                "author": "Chiyuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08458v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08458v2",
                "updated": "2025-01-31T06:27:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    6,
                    27,
                    0,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-11T02:19:11Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    2,
                    19,
                    11,
                    4,
                    285,
                    0
                ],
                "title": "Simultaneous Reward Distillation and Preference Learning: Get You a\n  Language Model Who Can Do Both",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Reward Distillation and Preference Learning: Get You a\n  Language Model Who Can Do Both"
                },
                "summary": "Traditional RLHF-based LLM alignment methods explicitly maximize the expected\nrewards from a separate reward model. More recent supervised alignment methods\nlike Direct Preference Optimization (DPO) circumvent this phase to avoid\nproblems including model drift and reward overfitting. Although popular due to\nits simplicity, DPO and similar direct alignment methods which rely heavily on\nthe Bradley-Terry-based pairwise preference formulation can still lead to\ndegenerate policies when challenged by non-deterministic or noisy preference\nlabels, for example human scoring of two candidate outputs with low confidence.\nThis paper introduces DRDO (Direct Reward Distillation and\npolicy-Optimization), which simultaneously models rewards and preferences to\navoid such degeneracy. DRDO directly mimics rewards assigned by an oracle while\nlearning human preferences with a novel preference likelihood formulation.\nResults on the Ultrafeedback and TL;DR datasets demonstrate that DRDO-trained\npolicies surpass methods such as DPO and e-DPO in terms of expected rewards and\nare more robust, on average, to noisy preference signals as well as\nout-of-distribution (OOD) settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional RLHF-based LLM alignment methods explicitly maximize the expected\nrewards from a separate reward model. More recent supervised alignment methods\nlike Direct Preference Optimization (DPO) circumvent this phase to avoid\nproblems including model drift and reward overfitting. Although popular due to\nits simplicity, DPO and similar direct alignment methods which rely heavily on\nthe Bradley-Terry-based pairwise preference formulation can still lead to\ndegenerate policies when challenged by non-deterministic or noisy preference\nlabels, for example human scoring of two candidate outputs with low confidence.\nThis paper introduces DRDO (Direct Reward Distillation and\npolicy-Optimization), which simultaneously models rewards and preferences to\navoid such degeneracy. DRDO directly mimics rewards assigned by an oracle while\nlearning human preferences with a novel preference likelihood formulation.\nResults on the Ultrafeedback and TL;DR datasets demonstrate that DRDO-trained\npolicies surpass methods such as DPO and e-DPO in terms of expected rewards and\nare more robust, on average, to noisy preference signals as well as\nout-of-distribution (OOD) settings."
                },
                "authors": [
                    {
                        "name": "Abhijnan Nath"
                    },
                    {
                        "name": "Changsoo Jung"
                    },
                    {
                        "name": "Ethan Seefried"
                    },
                    {
                        "name": "Nikhil Krishnaswamy"
                    }
                ],
                "author_detail": {
                    "name": "Nikhil Krishnaswamy"
                },
                "author": "Nikhil Krishnaswamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08458v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08458v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18908v1",
                "updated": "2025-01-31T06:02:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    6,
                    2,
                    24,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T06:02:24Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    6,
                    2,
                    24,
                    4,
                    31,
                    0
                ],
                "title": "Streamlining Security Vulnerability Triage with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining Security Vulnerability Triage with Large Language Models"
                },
                "summary": "Bug triaging for security vulnerabilities is a critical part of software\nmaintenance, ensuring that the most pressing vulnerabilities are addressed\npromptly to safeguard system integrity and user data. However, the process is\nresource-intensive and comes with challenges, including classifying software\nvulnerabilities, assessing their severity, and managing a high volume of bug\nreports. In this paper, we present CASEY, a novel approach that leverages Large\nLanguage Models (in our case, the GPT model) that automates the identification\nof Common Weakness Enumerations (CWEs) of security bugs and assesses their\nseverity. CASEY employs prompt engineering techniques and incorporates\ncontextual information at varying levels of granularity to assist in the bug\ntriaging process. We evaluated CASEY using an augmented version of the National\nVulnerability Database (NVD), employing quantitative and qualitative metrics to\nmeasure its performance across CWE identification, severity assessment, and\ntheir combined analysis. CASEY achieved a CWE identification accuracy of 68%, a\nseverity identification accuracy of 73.6%, and a combined accuracy of 51.2% for\nidentifying both. These results demonstrate the potential of LLMs in\nidentifying CWEs and severity levels, streamlining software vulnerability\nmanagement, and improving the efficiency of security vulnerability triaging\nworkflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bug triaging for security vulnerabilities is a critical part of software\nmaintenance, ensuring that the most pressing vulnerabilities are addressed\npromptly to safeguard system integrity and user data. However, the process is\nresource-intensive and comes with challenges, including classifying software\nvulnerabilities, assessing their severity, and managing a high volume of bug\nreports. In this paper, we present CASEY, a novel approach that leverages Large\nLanguage Models (in our case, the GPT model) that automates the identification\nof Common Weakness Enumerations (CWEs) of security bugs and assesses their\nseverity. CASEY employs prompt engineering techniques and incorporates\ncontextual information at varying levels of granularity to assist in the bug\ntriaging process. We evaluated CASEY using an augmented version of the National\nVulnerability Database (NVD), employing quantitative and qualitative metrics to\nmeasure its performance across CWE identification, severity assessment, and\ntheir combined analysis. CASEY achieved a CWE identification accuracy of 68%, a\nseverity identification accuracy of 73.6%, and a combined accuracy of 51.2% for\nidentifying both. These results demonstrate the potential of LLMs in\nidentifying CWEs and severity levels, streamlining software vulnerability\nmanagement, and improving the efficiency of security vulnerability triaging\nworkflows."
                },
                "authors": [
                    {
                        "name": "Mohammad Jalili Torkamani"
                    },
                    {
                        "name": "Joey NG"
                    },
                    {
                        "name": "Nikita Mehrotra"
                    },
                    {
                        "name": "Mahinthan Chandramohan"
                    },
                    {
                        "name": "Padmanabhan Krishnan"
                    },
                    {
                        "name": "Rahul Purandare"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Purandare"
                },
                "author": "Rahul Purandare",
                "arxiv_comment": "16 pages, 22 figures, 6 tables, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2; K.6.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01909v2",
                "updated": "2025-01-31T05:51:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    5,
                    51,
                    52,
                    4,
                    31,
                    0
                ],
                "published": "2024-09-03T13:58:34Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    58,
                    34,
                    1,
                    247,
                    0
                ],
                "title": "LUK: Empowering Log Understanding with Expert Knowledge from Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUK: Empowering Log Understanding with Expert Knowledge from Large\n  Language Models"
                },
                "summary": "Logs play a critical role in providing essential information for system\nmonitoring and troubleshooting. Recently, with the success of pre-trained\nlanguage models (PLMs) and large language models (LLMs) in natural language\nprocessing (NLP), smaller PLMs (such as BERT) and LLMs (like GPT-4) have become\nthe current mainstream approaches for log analysis. Despite the remarkable\ncapabilities of LLMs, their higher cost and inefficient inference present\nsignificant challenges in leveraging the full potential of LLMs to analyze\nlogs. In contrast, smaller PLMs can be fine-tuned for specific tasks even with\nlimited computational resources, making them more practical. However, these\nsmaller PLMs face challenges in understanding logs comprehensively due to their\nlimited expert knowledge. To address the lack of expert knowledge and enhance\nlog understanding for smaller PLMs, this paper introduces a novel and practical\nknowledge enhancement framework, called LUK, which acquires expert knowledge\nfrom LLMs automatically and then enhances the smaller PLM for log analysis with\nthese expert knowledge. LUK can take full advantage of both types of models.\nSpecifically, we design a multi-expert collaboration framework based on LLMs\nwith different roles to acquire expert knowledge. In addition, we propose two\nnovel pre-training tasks to enhance the log pre-training with expert knowledge.\nLUK achieves state-of-the-art results on different log analysis tasks and\nextensive experiments demonstrate expert knowledge from LLMs can be utilized\nmore effectively to understand logs. Our source code and detailed experimental\ndata are available at https://github.com/LeaperOvO/LUK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logs play a critical role in providing essential information for system\nmonitoring and troubleshooting. Recently, with the success of pre-trained\nlanguage models (PLMs) and large language models (LLMs) in natural language\nprocessing (NLP), smaller PLMs (such as BERT) and LLMs (like GPT-4) have become\nthe current mainstream approaches for log analysis. Despite the remarkable\ncapabilities of LLMs, their higher cost and inefficient inference present\nsignificant challenges in leveraging the full potential of LLMs to analyze\nlogs. In contrast, smaller PLMs can be fine-tuned for specific tasks even with\nlimited computational resources, making them more practical. However, these\nsmaller PLMs face challenges in understanding logs comprehensively due to their\nlimited expert knowledge. To address the lack of expert knowledge and enhance\nlog understanding for smaller PLMs, this paper introduces a novel and practical\nknowledge enhancement framework, called LUK, which acquires expert knowledge\nfrom LLMs automatically and then enhances the smaller PLM for log analysis with\nthese expert knowledge. LUK can take full advantage of both types of models.\nSpecifically, we design a multi-expert collaboration framework based on LLMs\nwith different roles to acquire expert knowledge. In addition, we propose two\nnovel pre-training tasks to enhance the log pre-training with expert knowledge.\nLUK achieves state-of-the-art results on different log analysis tasks and\nextensive experiments demonstrate expert knowledge from LLMs can be utilized\nmore effectively to understand logs. Our source code and detailed experimental\ndata are available at https://github.com/LeaperOvO/LUK."
                },
                "authors": [
                    {
                        "name": "Lipeng Ma"
                    },
                    {
                        "name": "Weidong Yang"
                    },
                    {
                        "name": "Sihang Jiang"
                    },
                    {
                        "name": "Ben Fei"
                    },
                    {
                        "name": "Mingjie Zhou"
                    },
                    {
                        "name": "Shuhao Li"
                    },
                    {
                        "name": "Mingyu Zhao"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08603v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08603v3",
                "updated": "2025-01-31T05:28:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    5,
                    28,
                    15,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-15T06:00:50Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    0,
                    50,
                    2,
                    15,
                    0
                ],
                "title": "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based\n  Automatic Heuristic Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based\n  Automatic Heuristic Design"
                },
                "summary": "Handcrafting heuristics for solving complex optimization tasks (e.g., route\nplanning and task allocation) is a common practice but requires extensive\ndomain knowledge. Recently, Large Language Model (LLM)-based automatic\nheuristic design (AHD) methods have shown promise in generating high-quality\nheuristics without manual interventions. Existing LLM-based AHD methods employ\na population to maintain a fixed number of top-performing LLM-generated\nheuristics and introduce evolutionary computation (EC) to iteratively enhance\nthe population. However, these population-based procedures cannot fully develop\nthe potential of each heuristic and are prone to converge into local optima. To\nmore comprehensively explore the space of heuristics, this paper proposes to\nuse Monte Carlo Tree Search (MCTS) for LLM-based heuristic evolution. The\nproposed MCTS-AHD method organizes all LLM-generated heuristics in a tree\nstructure and can better develop the potential of temporarily underperforming\nheuristics. In experiments, MCTS-AHD delivers significantly higher-quality\nheuristics on various complex tasks. Our code is available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handcrafting heuristics for solving complex optimization tasks (e.g., route\nplanning and task allocation) is a common practice but requires extensive\ndomain knowledge. Recently, Large Language Model (LLM)-based automatic\nheuristic design (AHD) methods have shown promise in generating high-quality\nheuristics without manual interventions. Existing LLM-based AHD methods employ\na population to maintain a fixed number of top-performing LLM-generated\nheuristics and introduce evolutionary computation (EC) to iteratively enhance\nthe population. However, these population-based procedures cannot fully develop\nthe potential of each heuristic and are prone to converge into local optima. To\nmore comprehensively explore the space of heuristics, this paper proposes to\nuse Monte Carlo Tree Search (MCTS) for LLM-based heuristic evolution. The\nproposed MCTS-AHD method organizes all LLM-generated heuristics in a tree\nstructure and can better develop the potential of temporarily underperforming\nheuristics. In experiments, MCTS-AHD delivers significantly higher-quality\nheuristics on various complex tasks. Our code is available."
                },
                "authors": [
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Zhuoliang Xie"
                    },
                    {
                        "name": "Zhenkun Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08603v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08603v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07291v3",
                "updated": "2025-01-31T05:16:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    5,
                    16,
                    50,
                    4,
                    31,
                    0
                ],
                "published": "2024-08-14T04:49:30Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    4,
                    49,
                    30,
                    2,
                    227,
                    0
                ],
                "title": "Evaluating LLM-based Personal Information Extraction and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM-based Personal Information Extraction and Countermeasures"
                },
                "summary": "Automatically extracting personal information -- such as name, phone number,\nand email address -- from publicly available profiles at a large scale is a\nstepstone to many other security attacks including spear phishing. Traditional\nmethods -- such as regular expression, keyword search, and entity detection --\nachieve limited success at such personal information extraction. In this work,\nwe perform a systematic measurement study to benchmark large language model\n(LLM) based personal information extraction and countermeasures. Towards this\ngoal, we present a framework for LLM-based extraction attacks; collect four\ndatasets including a synthetic dataset generated by GPT-4 and three real-world\ndatasets with manually labeled eight categories of personal information;\nintroduce a novel mitigation strategy based on prompt injection; and\nsystematically benchmark LLM-based attacks and countermeasures using ten LLMs\nand five datasets. Our key findings include: LLM can be misused by attackers to\naccurately extract various personal information from personal profiles; LLM\noutperforms traditional methods; and prompt injection can defend against strong\nLLM-based attacks, reducing the attack to less effective traditional ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically extracting personal information -- such as name, phone number,\nand email address -- from publicly available profiles at a large scale is a\nstepstone to many other security attacks including spear phishing. Traditional\nmethods -- such as regular expression, keyword search, and entity detection --\nachieve limited success at such personal information extraction. In this work,\nwe perform a systematic measurement study to benchmark large language model\n(LLM) based personal information extraction and countermeasures. Towards this\ngoal, we present a framework for LLM-based extraction attacks; collect four\ndatasets including a synthetic dataset generated by GPT-4 and three real-world\ndatasets with manually labeled eight categories of personal information;\nintroduce a novel mitigation strategy based on prompt injection; and\nsystematically benchmark LLM-based attacks and countermeasures using ten LLMs\nand five datasets. Our key findings include: LLM can be misused by attackers to\naccurately extract various personal information from personal profiles; LLM\noutperforms traditional methods; and prompt injection can defend against strong\nLLM-based attacks, reducing the attack to less effective traditional ones."
                },
                "authors": [
                    {
                        "name": "Yupei Liu"
                    },
                    {
                        "name": "Yuqi Jia"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    }
                ],
                "author_detail": {
                    "name": "Neil Zhenqiang Gong"
                },
                "author": "Neil Zhenqiang Gong",
                "arxiv_comment": "To appear in USENIX Security Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03448v2",
                "updated": "2025-01-31T04:48:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    4,
                    48,
                    29,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-04T14:09:12Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    14,
                    9,
                    12,
                    4,
                    278,
                    0
                ],
                "title": "\"Cold, Calculated, and Condescending\": How AI Identifies and Explains\n  Ableism Compared to Disabled People",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Cold, Calculated, and Condescending\": How AI Identifies and Explains\n  Ableism Compared to Disabled People"
                },
                "summary": "People with disabilities (PwD) regularly encounter ableist hate and\nmicroaggressions online. These spaces are generally moderated by machine\nlearning models, but little is known about how effectively AI models identify\nableist speech and how well their judgments align with PwD. To investigate\nthis, we curated a first-of-its-kind dataset of 200 social media comments\ntargeted towards PwD, and prompted state-of-the art AI models (i.e., Toxicity\nClassifiers, LLMs) to score toxicity and ableism for each comment, and explain\ntheir reasoning. Then, we recruited 190 participants to similarly rate and\nexplain the harm, and evaluate LLM explanations. Our mixed-methods analysis\nhighlighted a major disconnect: AI underestimated toxicity compared to PwD\nratings, while its ableism assessments were sporadic and varied. Although LLMs\nidentified some biases, its explanations were flawed--they lacked nuance, made\nincorrect assumptions, and appeared judgmental instead of educational. Going\nforward, we discuss challenges and opportunities in designing moderation\nsystems for ableism, and advocate for the involvement of intersectional\ndisabled perspectives in AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People with disabilities (PwD) regularly encounter ableist hate and\nmicroaggressions online. These spaces are generally moderated by machine\nlearning models, but little is known about how effectively AI models identify\nableist speech and how well their judgments align with PwD. To investigate\nthis, we curated a first-of-its-kind dataset of 200 social media comments\ntargeted towards PwD, and prompted state-of-the art AI models (i.e., Toxicity\nClassifiers, LLMs) to score toxicity and ableism for each comment, and explain\ntheir reasoning. Then, we recruited 190 participants to similarly rate and\nexplain the harm, and evaluate LLM explanations. Our mixed-methods analysis\nhighlighted a major disconnect: AI underestimated toxicity compared to PwD\nratings, while its ableism assessments were sporadic and varied. Although LLMs\nidentified some biases, its explanations were flawed--they lacked nuance, made\nincorrect assumptions, and appeared judgmental instead of educational. Going\nforward, we discuss challenges and opportunities in designing moderation\nsystems for ableism, and advocate for the involvement of intersectional\ndisabled perspectives in AI."
                },
                "authors": [
                    {
                        "name": "Mahika Phutane"
                    },
                    {
                        "name": "Ananya Seelam"
                    },
                    {
                        "name": "Aditya Vashistha"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vashistha"
                },
                "author": "Aditya Vashistha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18883v1",
                "updated": "2025-01-31T04:34:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    4,
                    34,
                    43,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T04:34:43Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    4,
                    34,
                    43,
                    4,
                    31,
                    0
                ],
                "title": "Can We Predict the Effect of Prompts?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Predict the Effect of Prompts?"
                },
                "summary": "Large Language Models (LLMs) are machine learning models that have seen\nwidespread adoption due to their capability of handling previously difficult\ntasks. LLMs, due to their training, are sensitive to how exactly a question is\npresented, also known as prompting. However, prompting well is challenging, as\nit has been difficult to uncover principles behind prompting -- generally,\ntrial-and-error is the most common way of improving prompts, despite its\nsignificant computational cost. In this context, we argue it would be useful to\nperform `predictive prompt analysis', in which an automated technique would\nperform a quick analysis of a prompt and predict how the LLM would react to it,\nrelative to a goal provided by the user. As a demonstration of the concept, we\npresent Syntactic Prevalence Analyzer (SPA), a predictive prompt analysis\napproach based on sparse autoencoders (SAEs). SPA accurately predicted how\noften an LLM would generate target syntactic structures during code synthesis,\nwith up to 0.994 Pearson correlation between the predicted and actual\nprevalence of the target structure. At the same time, SPA requires only 0.4\\%\nof the time it takes to run the LLM on a benchmark. As LLMs are increasingly\nused during and integrated into modern software development, our proposed\npredictive prompt analysis concept has the potential to significantly ease the\nuse of LLMs for both practitioners and researchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are machine learning models that have seen\nwidespread adoption due to their capability of handling previously difficult\ntasks. LLMs, due to their training, are sensitive to how exactly a question is\npresented, also known as prompting. However, prompting well is challenging, as\nit has been difficult to uncover principles behind prompting -- generally,\ntrial-and-error is the most common way of improving prompts, despite its\nsignificant computational cost. In this context, we argue it would be useful to\nperform `predictive prompt analysis', in which an automated technique would\nperform a quick analysis of a prompt and predict how the LLM would react to it,\nrelative to a goal provided by the user. As a demonstration of the concept, we\npresent Syntactic Prevalence Analyzer (SPA), a predictive prompt analysis\napproach based on sparse autoencoders (SAEs). SPA accurately predicted how\noften an LLM would generate target syntactic structures during code synthesis,\nwith up to 0.994 Pearson correlation between the predicted and actual\nprevalence of the target structure. At the same time, SPA requires only 0.4\\%\nof the time it takes to run the LLM on a benchmark. As LLMs are increasingly\nused during and integrated into modern software development, our proposed\npredictive prompt analysis concept has the potential to significantly ease the\nuse of LLMs for both practitioners and researchers."
                },
                "authors": [
                    {
                        "name": "Jae Yong Lee"
                    },
                    {
                        "name": "Sungmin Kang"
                    },
                    {
                        "name": "Shin Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Shin Yoo"
                },
                "author": "Shin Yoo",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11384v2",
                "updated": "2025-01-31T04:31:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    4,
                    31,
                    59,
                    4,
                    31,
                    0
                ],
                "published": "2024-07-16T04:55:17Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    4,
                    55,
                    17,
                    1,
                    198,
                    0
                ],
                "title": "InvAgent: A Large Language Model based Multi-Agent System for Inventory\n  Management in Supply Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InvAgent: A Large Language Model based Multi-Agent System for Inventory\n  Management in Supply Chains"
                },
                "summary": "Supply chain management (SCM) involves coordinating the flow of goods,\ninformation, and finances across various entities to deliver products\nefficiently. Effective inventory management is crucial in today's volatile and\nuncertain world. Previous research has demonstrated the superiority of\nheuristic methods and reinforcement learning applications in inventory\nmanagement. However, the application of large language models (LLMs) as\nautonomous agents in multi-agent systems for inventory management remains\nunderexplored. This study introduces a novel approach using LLMs to manage\nmulti-agent inventory systems. Leveraging their zero-shot learning\ncapabilities, our model, InvAgent, enhances resilience and improves efficiency\nacross the supply chain network. Our contributions include utilizing LLMs for\nzero-shot learning to enable adaptive and informed decision-making without\nprior training, providing explainability and clarity through chain-of-thought,\nand demonstrating dynamic adaptability to varying demand scenarios while\nreducing costs and preventing stockouts. Extensive evaluations across different\nscenarios highlight the efficiency of our model in SCM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supply chain management (SCM) involves coordinating the flow of goods,\ninformation, and finances across various entities to deliver products\nefficiently. Effective inventory management is crucial in today's volatile and\nuncertain world. Previous research has demonstrated the superiority of\nheuristic methods and reinforcement learning applications in inventory\nmanagement. However, the application of large language models (LLMs) as\nautonomous agents in multi-agent systems for inventory management remains\nunderexplored. This study introduces a novel approach using LLMs to manage\nmulti-agent inventory systems. Leveraging their zero-shot learning\ncapabilities, our model, InvAgent, enhances resilience and improves efficiency\nacross the supply chain network. Our contributions include utilizing LLMs for\nzero-shot learning to enable adaptive and informed decision-making without\nprior training, providing explainability and clarity through chain-of-thought,\nand demonstrating dynamic adaptability to varying demand scenarios while\nreducing costs and preventing stockouts. Extensive evaluations across different\nscenarios highlight the efficiency of our model in SCM."
                },
                "authors": [
                    {
                        "name": "Yinzhu Quan"
                    },
                    {
                        "name": "Zefang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zefang Liu"
                },
                "author": "Zefang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18877v1",
                "updated": "2025-01-31T04:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    4,
                    14,
                    5,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T04:14:05Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    4,
                    14,
                    5,
                    4,
                    31,
                    0
                ],
                "title": "Distorting Embedding Space for Safety: A Defense Mechanism for\n  Adversarially Robust Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distorting Embedding Space for Safety: A Defense Mechanism for\n  Adversarially Robust Diffusion Models"
                },
                "summary": "Text-to-image diffusion models show remarkable generation performance\nfollowing text prompts, but risk generating Not Safe For Work (NSFW) contents\nfrom unsafe prompts. Existing approaches, such as prompt filtering or concept\nunlearning, fail to defend against adversarial attacks while maintaining benign\nimage quality. In this paper, we propose a novel approach called Distorting\nEmbedding Space (DES), a text encoder-based defense mechanism that effectively\ntackles these issues through innovative embedding space control. DES transforms\nunsafe embeddings, extracted from a text encoder using unsafe prompts, toward\ncarefully calculated safe embedding regions to prevent unsafe contents\ngeneration, while reproducing the original safe embeddings. DES also\nneutralizes the nudity embedding, extracted using prompt ``nudity\", by aligning\nit with neutral embedding to enhance robustness against adversarial attacks.\nThese methods ensure both robust defense and high-quality image generation.\nAdditionally, DES can be adopted in a plug-and-play manner and requires zero\ninference overhead, facilitating its deployment. Extensive experiments on\ndiverse attack types, including black-box and white-box scenarios, demonstrate\nDES's state-of-the-art performance in both defense capability and benign image\ngeneration quality. Our model is available at https://github.com/aei13/DES.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image diffusion models show remarkable generation performance\nfollowing text prompts, but risk generating Not Safe For Work (NSFW) contents\nfrom unsafe prompts. Existing approaches, such as prompt filtering or concept\nunlearning, fail to defend against adversarial attacks while maintaining benign\nimage quality. In this paper, we propose a novel approach called Distorting\nEmbedding Space (DES), a text encoder-based defense mechanism that effectively\ntackles these issues through innovative embedding space control. DES transforms\nunsafe embeddings, extracted from a text encoder using unsafe prompts, toward\ncarefully calculated safe embedding regions to prevent unsafe contents\ngeneration, while reproducing the original safe embeddings. DES also\nneutralizes the nudity embedding, extracted using prompt ``nudity\", by aligning\nit with neutral embedding to enhance robustness against adversarial attacks.\nThese methods ensure both robust defense and high-quality image generation.\nAdditionally, DES can be adopted in a plug-and-play manner and requires zero\ninference overhead, facilitating its deployment. Extensive experiments on\ndiverse attack types, including black-box and white-box scenarios, demonstrate\nDES's state-of-the-art performance in both defense capability and benign image\ngeneration quality. Our model is available at https://github.com/aei13/DES."
                },
                "authors": [
                    {
                        "name": "Jaesin Ahn"
                    },
                    {
                        "name": "Heechul Jung"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Jung"
                },
                "author": "Heechul Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18160v2",
                "updated": "2025-01-31T04:02:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    4,
                    2,
                    40,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-30T05:56:30Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    56,
                    30,
                    3,
                    30,
                    0
                ],
                "title": "RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing"
                },
                "summary": "Code auditing is a code review process with the goal of finding bugs. Large\nLanguage Models (LLMs) have shown substantial potential in this task, offering\nthe ability to analyze programs without compilation and enabling customized bug\ndetection following specified prompts. However, applying LLMs to\nrepository-level code auditing presents notable challenges. The inherent\ncontext limits and hallucinations of LLMs can lead to the low quality of bug\nreports. Meanwhile, the large size of software repositories introduces\nsubstantial time and token costs, hindering efficiency and scalability in\nreal-world scenarios. This work introduces an autonomous LLM-agent, RepoAudit,\ndesigned to enable precise and efficient repository-level code auditing.\nEquipped with the agent memory, RepoAudit explores the code repository on\ndemand, analyzing data-flow facts along different feasible program paths in\nindividual functions. It also introduces the validator to check the data-flow\nfacts for hallucination mitigation and examine the satisfiability of path\nconditions of potential buggy paths, which enables RepoAudit to discard false\npositives in the code auditing. Our experiment shows that RepoAudit powered by\nClaude 3.5 Sonnet successfully finds 38 true bugs in 15 real-world systems,\nconsuming 0.44 hours and $2.54 per project on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code auditing is a code review process with the goal of finding bugs. Large\nLanguage Models (LLMs) have shown substantial potential in this task, offering\nthe ability to analyze programs without compilation and enabling customized bug\ndetection following specified prompts. However, applying LLMs to\nrepository-level code auditing presents notable challenges. The inherent\ncontext limits and hallucinations of LLMs can lead to the low quality of bug\nreports. Meanwhile, the large size of software repositories introduces\nsubstantial time and token costs, hindering efficiency and scalability in\nreal-world scenarios. This work introduces an autonomous LLM-agent, RepoAudit,\ndesigned to enable precise and efficient repository-level code auditing.\nEquipped with the agent memory, RepoAudit explores the code repository on\ndemand, analyzing data-flow facts along different feasible program paths in\nindividual functions. It also introduces the validator to check the data-flow\nfacts for hallucination mitigation and examine the satisfiability of path\nconditions of potential buggy paths, which enables RepoAudit to discard false\npositives in the code auditing. Our experiment shows that RepoAudit powered by\nClaude 3.5 Sonnet successfully finds 38 true bugs in 15 real-world systems,\nconsuming 0.44 hours and $2.54 per project on average."
                },
                "authors": [
                    {
                        "name": "Jinyao Guo"
                    },
                    {
                        "name": "Chengpeng Wang"
                    },
                    {
                        "name": "Xiangzhe Xu"
                    },
                    {
                        "name": "Zian Su"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "arxiv_comment": "19 pages, 8 tables, 5 figures, 3 listings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]