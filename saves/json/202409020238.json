[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Benoît Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v3",
                "updated": "2024-08-21T02:32:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    2,
                    32,
                    43,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro López-García"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Büchner"
                    },
                    {
                        "name": "Leonardo Agudo Jácome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jácome"
                },
                "author": "Leonardo Agudo Jácome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adrià Armejach"
                    },
                    {
                        "name": "Miquel Moretó"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moretó"
                },
                "author": "Miquel Moretó",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19604v1",
                "updated": "2024-07-28T22:34:20Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T22:34:20Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "title": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning"
                },
                "summary": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kyle Kuan"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/IGSC48788.2019.8957182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IGSC48788.2019.8957182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: 2019 Tenth International Green and Sustainable\n  Computing Conference (IGSC)",
                "arxiv_journal_ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19318v1",
                "updated": "2024-07-27T18:26:32Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T18:26:32Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "title": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review"
                },
                "summary": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture."
                },
                "authors": [
                    {
                        "name": "Anujkumarsinh Donvir"
                    },
                    {
                        "name": "Apeksha Jain"
                    },
                    {
                        "name": "Pradeep Kumar Saraswathi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Kumar Saraswathi"
                },
                "author": "Pradeep Kumar Saraswathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v2",
                "updated": "2024-07-27T08:52:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    52,
                    39,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference"
                },
                "summary": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19205v1",
                "updated": "2024-07-27T08:21:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T08:21:14Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions"
                },
                "summary": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Ashkan Taghipour"
                    },
                    {
                        "name": "Morteza Ghahremani"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Aref Miri Rekavandi"
                    },
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Hamid Laga"
                    },
                    {
                        "name": "Farid Boussaid"
                    }
                ],
                "author_detail": {
                    "name": "Farid Boussaid"
                },
                "author": "Farid Boussaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19090v1",
                "updated": "2024-07-26T21:11:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "published": "2024-07-26T21:11:58Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores"
                },
                "summary": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance."
                },
                "authors": [
                    {
                        "name": "Alireza Heidari"
                    },
                    {
                        "name": "Amirhossein Ahmadi"
                    },
                    {
                        "name": "Zefeng Zhi"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Cloud Databases",
                "arxiv_journal_ref": "VLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18121v1",
                "updated": "2024-07-25T15:29:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T15:29:05Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache"
                },
                "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Guangyi Chen"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02750v2",
                "updated": "2024-07-25T09:16:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    16,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-02-05T06:06:47Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    6,
                    6,
                    47,
                    0,
                    36,
                    0
                ],
                "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
                },
                "summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_doi": "10.13140/RG.2.2.28167.37282",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.28167.37282",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.02750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICML2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20272v1",
                "updated": "2024-07-25T07:50:17Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T07:50:17Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Inference Framework for Early-exit Large Language Models"
                },
                "summary": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up."
                },
                "authors": [
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yihan Yan"
                    },
                    {
                        "name": "Xinshuo Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08711v3",
                "updated": "2024-07-24T13:36:03Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    13,
                    36,
                    3,
                    2,
                    206,
                    0
                ],
                "published": "2023-01-20T18:13:38Z",
                "published_parsed": [
                    2023,
                    1,
                    20,
                    18,
                    13,
                    38,
                    4,
                    20,
                    0
                ],
                "title": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers"
                },
                "summary": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers."
                },
                "authors": [
                    {
                        "name": "Qifa Yan"
                    },
                    {
                        "name": "Xiaohu Tang"
                    },
                    {
                        "name": "Zhengchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengchun Zhou"
                },
                "author": "Zhengchun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15771v2",
                "updated": "2024-07-24T12:56:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    56,
                    41,
                    2,
                    206,
                    0
                ],
                "published": "2024-03-13T17:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    47,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations"
                },
                "summary": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations."
                },
                "authors": [
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15569v2",
                "updated": "2024-07-24T08:56:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    56,
                    11,
                    2,
                    206,
                    0
                ],
                "published": "2024-01-28T05:12:09Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    5,
                    12,
                    9,
                    6,
                    28,
                    0
                ],
                "title": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs"
                },
                "summary": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted by IJCAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09636v2",
                "updated": "2024-07-23T17:55:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    55,
                    30,
                    1,
                    205,
                    0
                ],
                "published": "2024-03-14T17:59:26Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    59,
                    26,
                    3,
                    74,
                    0
                ],
                "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"
                },
                "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "David Tarjan"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024) 37396-37412",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16672v1",
                "updated": "2024-07-23T17:42:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T17:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "title": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications"
                },
                "summary": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications."
                },
                "authors": [
                    {
                        "name": "Sahan Liyanaarachchi"
                    },
                    {
                        "name": "Stavros Mitrolaris"
                    },
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16303v1",
                "updated": "2024-07-23T08:58:06Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:58:06Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "title": "Hidden Web Caches Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Web Caches Discovery"
                },
                "summary": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers."
                },
                "authors": [
                    {
                        "name": "Matteo Golinelli"
                    },
                    {
                        "name": "Bruno Crispo"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Crispo"
                },
                "author": "Bruno Crispo",
                "arxiv_doi": "10.1145/3678890.3678931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678890.3678931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.16303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The definitive Version of Record was published in The 27th\n  International Symposium on Research in Attacks, Intrusions and Defenses (RAID\n  2024), September 30-October 02, 2024, Padua, Italy,\n  https://doi.org/10.1145/3678890.3678931",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.16770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16770v1",
                "updated": "2024-08-29T17:59:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    59,
                    54,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:59:54Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    59,
                    54,
                    3,
                    242,
                    0
                ],
                "title": "3D Whole-body Grasp Synthesis with Directional Controllability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Whole-body Grasp Synthesis with Directional Controllability"
                },
                "summary": "Synthesizing 3D whole-bodies that realistically grasp objects is useful for\nanimation, mixed reality, and robotics. This is challenging, because the hands\nand body need to look natural w.r.t. each other, the grasped object, as well as\nthe local scene (i.e., a receptacle supporting the object). Only recent work\ntackles this, with a divide-and-conquer approach; it first generates a\n\"guiding\" right-hand grasp, and then searches for bodies that match this.\nHowever, the guiding-hand synthesis lacks controllability and receptacle\nawareness, so it likely has an implausible direction (i.e., a body can't match\nthis without penetrating the receptacle) and needs corrections through major\npost-processing. Moreover, the body search needs exhaustive sampling and is\nexpensive. These are strong limitations. We tackle these with a novel method\ncalled CWGrasp. Our key idea is that performing geometry-based reasoning \"early\non,\" instead of \"too late,\" provides rich \"control\" signals for inference. To\nthis end, CWGrasp first samples a plausible reaching-direction vector (used\nlater for both the arm and hand) from a probabilistic model built via\nraycasting from the object and collision checking. Then, it generates a\nreaching body with a desired arm direction, as well as a \"guiding\" grasping\nhand with a desired palm direction that complies with the arm's one.\nEventually, CWGrasp refines the body to match the \"guiding\" hand, while\nplausibly contacting the scene. Notably, generating already-compatible \"parts\"\ngreatly simplifies the \"whole.\" Moreover, CWGrasp uniquely tackles both right-\nand left-hand grasps. We evaluate on the GRAB and ReplicaGrasp datasets.\nCWGrasp outperforms baselines, at lower runtime and budget, while all\ncomponents help performance. Code and models will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing 3D whole-bodies that realistically grasp objects is useful for\nanimation, mixed reality, and robotics. This is challenging, because the hands\nand body need to look natural w.r.t. each other, the grasped object, as well as\nthe local scene (i.e., a receptacle supporting the object). Only recent work\ntackles this, with a divide-and-conquer approach; it first generates a\n\"guiding\" right-hand grasp, and then searches for bodies that match this.\nHowever, the guiding-hand synthesis lacks controllability and receptacle\nawareness, so it likely has an implausible direction (i.e., a body can't match\nthis without penetrating the receptacle) and needs corrections through major\npost-processing. Moreover, the body search needs exhaustive sampling and is\nexpensive. These are strong limitations. We tackle these with a novel method\ncalled CWGrasp. Our key idea is that performing geometry-based reasoning \"early\non,\" instead of \"too late,\" provides rich \"control\" signals for inference. To\nthis end, CWGrasp first samples a plausible reaching-direction vector (used\nlater for both the arm and hand) from a probabilistic model built via\nraycasting from the object and collision checking. Then, it generates a\nreaching body with a desired arm direction, as well as a \"guiding\" grasping\nhand with a desired palm direction that complies with the arm's one.\nEventually, CWGrasp refines the body to match the \"guiding\" hand, while\nplausibly contacting the scene. Notably, generating already-compatible \"parts\"\ngreatly simplifies the \"whole.\" Moreover, CWGrasp uniquely tackles both right-\nand left-hand grasps. We evaluate on the GRAB and ReplicaGrasp datasets.\nCWGrasp outperforms baselines, at lower runtime and budget, while all\ncomponents help performance. Code and models will be released."
                },
                "authors": [
                    {
                        "name": "Georgios Paschalidis"
                    },
                    {
                        "name": "Romana Wilschut"
                    },
                    {
                        "name": "Dimitrije Antić"
                    },
                    {
                        "name": "Omid Taheri"
                    },
                    {
                        "name": "Dimitrios Tzionas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Tzionas"
                },
                "author": "Dimitrios Tzionas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16763v1",
                "updated": "2024-08-29T17:57:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    57,
                    22,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:57:22Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    57,
                    22,
                    3,
                    242,
                    0
                ],
                "title": "Finite Sample Valid Inference via Calibrated Bootstrap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite Sample Valid Inference via Calibrated Bootstrap"
                },
                "summary": "While widely used as a general method for uncertainty quantification, the\nbootstrap method encounters difficulties that raise concerns about its validity\nin practical applications. This paper introduces a new resampling-based method,\ntermed $\\textit{calibrated bootstrap}$, designed to generate finite\nsample-valid parametric inference from a sample of size $n$. The central idea\nis to calibrate an $m$-out-of-$n$ resampling scheme, where the calibration\nparameter $m$ is determined against inferential pivotal quantities derived from\nthe cumulative distribution functions of loss functions in parameter\nestimation. The method comprises two algorithms. The first, named\n$\\textit{resampling approximation}$ (RA), employs a $\\textit{stochastic\napproximation}$ algorithm to find the value of the calibration parameter\n$m=m_\\alpha$ for a given $\\alpha$ in a manner that ensures the resulting\n$m$-out-of-$n$ bootstrapped $1-\\alpha$ confidence set is valid. The second\nalgorithm, termed $\\textit{distributional resampling}$ (DR), is developed to\nfurther select samples of bootstrapped estimates from the RA step when\nconstructing $1-\\alpha$ confidence sets for a range of $\\alpha$ values is of\ninterest. The proposed method is illustrated and compared to existing methods\nusing linear regression with and without $L_1$ penalty, within the context of a\nhigh-dimensional setting and a real-world data application. The paper concludes\nwith remarks on a few open problems worthy of consideration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While widely used as a general method for uncertainty quantification, the\nbootstrap method encounters difficulties that raise concerns about its validity\nin practical applications. This paper introduces a new resampling-based method,\ntermed $\\textit{calibrated bootstrap}$, designed to generate finite\nsample-valid parametric inference from a sample of size $n$. The central idea\nis to calibrate an $m$-out-of-$n$ resampling scheme, where the calibration\nparameter $m$ is determined against inferential pivotal quantities derived from\nthe cumulative distribution functions of loss functions in parameter\nestimation. The method comprises two algorithms. The first, named\n$\\textit{resampling approximation}$ (RA), employs a $\\textit{stochastic\napproximation}$ algorithm to find the value of the calibration parameter\n$m=m_\\alpha$ for a given $\\alpha$ in a manner that ensures the resulting\n$m$-out-of-$n$ bootstrapped $1-\\alpha$ confidence set is valid. The second\nalgorithm, termed $\\textit{distributional resampling}$ (DR), is developed to\nfurther select samples of bootstrapped estimates from the RA step when\nconstructing $1-\\alpha$ confidence sets for a range of $\\alpha$ values is of\ninterest. The proposed method is illustrated and compared to existing methods\nusing linear regression with and without $L_1$ penalty, within the context of a\nhigh-dimensional setting and a real-world data application. The paper concludes\nwith remarks on a few open problems worthy of consideration."
                },
                "authors": [
                    {
                        "name": "Yiran Jiang"
                    },
                    {
                        "name": "Chuanhai Liu"
                    },
                    {
                        "name": "Heping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Heping Zhang"
                },
                "author": "Heping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10972v2",
                "updated": "2024-08-29T17:55:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    55,
                    52,
                    3,
                    242,
                    0
                ],
                "published": "2024-07-15T17:59:55Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    59,
                    55,
                    0,
                    197,
                    0
                ],
                "title": "VGBench: Evaluating Large Language Models on Vector Graphics\n  Understanding and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VGBench: Evaluating Large Language Models on Vector Graphics\n  Understanding and Generation"
                },
                "summary": "In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons, sketches and\nscientific figures. Recent studies have shown promising results on processing\nvector graphics with capable Large Language Models (LLMs). However, such works\nfocus solely on qualitative results, understanding, or a specific type of\nvector graphics. We propose VGBench, a comprehensive benchmark for LLMs on\nhandling vector graphics through diverse aspects, including (a) both visual\nunderstanding and generation, (b) evaluation of various vector graphics\nformats, (c) diverse question types, (d) wide range of prompting techniques,\n(e) under multiple LLMs and (f) comparison with VLMs on rasterized\nrepresentations. Evaluating on our collected 4279 understanding and 5845\ngeneration samples, we find that LLMs show strong capability on both aspects\nwhile exhibiting less desirable performance on low-level formats (SVG). Both\ndata and evaluation pipeline will be open-sourced at https://vgbench.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons, sketches and\nscientific figures. Recent studies have shown promising results on processing\nvector graphics with capable Large Language Models (LLMs). However, such works\nfocus solely on qualitative results, understanding, or a specific type of\nvector graphics. We propose VGBench, a comprehensive benchmark for LLMs on\nhandling vector graphics through diverse aspects, including (a) both visual\nunderstanding and generation, (b) evaluation of various vector graphics\nformats, (c) diverse question types, (d) wide range of prompting techniques,\n(e) under multiple LLMs and (f) comparison with VLMs on rasterized\nrepresentations. Evaluating on our collected 4279 understanding and 5845\ngeneration samples, we find that LLMs show strong capability on both aspects\nwhile exhibiting less desirable performance on low-level formats (SVG). Both\ndata and evaluation pipeline will be open-sourced at https://vgbench.github.io."
                },
                "authors": [
                    {
                        "name": "Bocheng Zou"
                    },
                    {
                        "name": "Mu Cai"
                    },
                    {
                        "name": "Jianrui Zhang"
                    },
                    {
                        "name": "Yong Jae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yong Jae Lee"
                },
                "author": "Yong Jae Lee",
                "arxiv_comment": "Project Page: https://vgbench.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16756v1",
                "updated": "2024-08-29T17:54:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    54,
                    14,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:54:14Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    54,
                    14,
                    3,
                    242,
                    0
                ],
                "title": "How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of\n  Large Language Models"
                },
                "summary": "The rapid evolution of large language models (LLMs) has transformed the\ncompetitive landscape in natural language processing (NLP), particularly for\nEnglish and other data-rich languages. However, underrepresented languages like\nCantonese, spoken by over 85 million people, face significant development gaps,\nwhich is particularly concerning given the economic significance of the\nGuangdong-Hong Kong-Macau Greater Bay Area, and in substantial\nCantonese-speaking populations in places like Singapore and North America.\nDespite its wide use, Cantonese has scant representation in NLP research,\nespecially compared to other languages from similarly developed regions. To\nbridge these gaps, we outline current Cantonese NLP methods and introduce new\nbenchmarks designed to evaluate LLM performance in factual generation,\nmathematical logic, complex reasoning, and general knowledge in Cantonese,\nwhich aim to advance open-source Cantonese LLM technology. We also propose\nfuture research directions and recommended models to enhance Cantonese LLM\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs) has transformed the\ncompetitive landscape in natural language processing (NLP), particularly for\nEnglish and other data-rich languages. However, underrepresented languages like\nCantonese, spoken by over 85 million people, face significant development gaps,\nwhich is particularly concerning given the economic significance of the\nGuangdong-Hong Kong-Macau Greater Bay Area, and in substantial\nCantonese-speaking populations in places like Singapore and North America.\nDespite its wide use, Cantonese has scant representation in NLP research,\nespecially compared to other languages from similarly developed regions. To\nbridge these gaps, we outline current Cantonese NLP methods and introduce new\nbenchmarks designed to evaluate LLM performance in factual generation,\nmathematical logic, complex reasoning, and general knowledge in Cantonese,\nwhich aim to advance open-source Cantonese LLM technology. We also propose\nfuture research directions and recommended models to enhance Cantonese LLM\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Jiyue Jiang"
                    },
                    {
                        "name": "Liheng Chen"
                    },
                    {
                        "name": "Pengan Chen"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Qinghang Bao"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13154v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13154v3",
                "updated": "2024-08-29T17:47:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    47,
                    18,
                    3,
                    242,
                    0
                ],
                "published": "2024-06-19T02:09:15Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    2,
                    9,
                    15,
                    2,
                    171,
                    0
                ],
                "title": "Conditional score-based diffusion models for solving inverse problems in\n  mechanics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional score-based diffusion models for solving inverse problems in\n  mechanics"
                },
                "summary": "We propose a framework to perform Bayesian inference using conditional\nscore-based diffusion models to solve a class of inverse problems in mechanics\ninvolving the inference of a specimen's spatially varying material properties\nfrom noisy measurements of its mechanical response to loading. Conditional\nscore-based diffusion models are generative models that learn to approximate\nthe score function of a conditional distribution using samples from the joint\ndistribution. More specifically, the score functions corresponding to multiple\nrealizations of the measurement are approximated using a single neural network,\nthe so-called score network, which is subsequently used to sample the posterior\ndistribution using an appropriate Markov chain Monte Carlo scheme based on\nLangevin dynamics. Training the score network only requires simulating the\nforward model. Hence, the proposed approach can accommodate black-box forward\nmodels and complex measurement noise. Moreover, once the score network has been\ntrained, it can be re-used to solve the inverse problem for different\nrealizations of the measurements. We demonstrate the efficacy of the proposed\napproach on a suite of high-dimensional inverse problems in mechanics that\ninvolve inferring heterogeneous material properties from noisy measurements.\nSome examples we consider involve synthetic data, while others include data\ncollected from actual elastography experiments. Further, our applications\ndemonstrate that the proposed approach can handle different measurement\nmodalities, complex patterns in the inferred quantities, non-Gaussian and\nnon-additive noise models, and nonlinear black-box forward models. The results\nshow that the proposed framework can solve large-scale physics-based inverse\nproblems efficiently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a framework to perform Bayesian inference using conditional\nscore-based diffusion models to solve a class of inverse problems in mechanics\ninvolving the inference of a specimen's spatially varying material properties\nfrom noisy measurements of its mechanical response to loading. Conditional\nscore-based diffusion models are generative models that learn to approximate\nthe score function of a conditional distribution using samples from the joint\ndistribution. More specifically, the score functions corresponding to multiple\nrealizations of the measurement are approximated using a single neural network,\nthe so-called score network, which is subsequently used to sample the posterior\ndistribution using an appropriate Markov chain Monte Carlo scheme based on\nLangevin dynamics. Training the score network only requires simulating the\nforward model. Hence, the proposed approach can accommodate black-box forward\nmodels and complex measurement noise. Moreover, once the score network has been\ntrained, it can be re-used to solve the inverse problem for different\nrealizations of the measurements. We demonstrate the efficacy of the proposed\napproach on a suite of high-dimensional inverse problems in mechanics that\ninvolve inferring heterogeneous material properties from noisy measurements.\nSome examples we consider involve synthetic data, while others include data\ncollected from actual elastography experiments. Further, our applications\ndemonstrate that the proposed approach can handle different measurement\nmodalities, complex patterns in the inferred quantities, non-Gaussian and\nnon-additive noise models, and nonlinear black-box forward models. The results\nshow that the proposed framework can solve large-scale physics-based inverse\nproblems efficiently."
                },
                "authors": [
                    {
                        "name": "Agnimitra Dasgupta"
                    },
                    {
                        "name": "Harisankar Ramaswamy"
                    },
                    {
                        "name": "Javier Murgoitio-Esandi"
                    },
                    {
                        "name": "Ken Foo"
                    },
                    {
                        "name": "Runze Li"
                    },
                    {
                        "name": "Qifa Zhou"
                    },
                    {
                        "name": "Brendan Kennedy"
                    },
                    {
                        "name": "Assad Oberai"
                    }
                ],
                "author_detail": {
                    "name": "Assad Oberai"
                },
                "author": "Assad Oberai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13154v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13154v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16747v1",
                "updated": "2024-08-29T17:40:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    40,
                    6,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:40:06Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    40,
                    6,
                    3,
                    242,
                    0
                ],
                "title": "Post-innermost stable circular orbit ringdown of a rapidly spinning\n  black hole: mass ratio dependence of higher harmonic quasi-normal mode\n  excitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-innermost stable circular orbit ringdown of a rapidly spinning\n  black hole: mass ratio dependence of higher harmonic quasi-normal mode\n  excitation"
                },
                "summary": "In a binary merger with a small mass ratio, as the secondary body approaches\nthe innermost stable circular orbit (ISCO) of the primary black hole, the\nmotion transitions from the adiabatic inspiral to the plunge governed by the\ngeodesic equation. The plunge orbit is expected to excite the ringdown\ngravitational wave, which encodes information about the primary black hole's\ngeometry. The details of the transition regime depend on the binary's mass\nratio through radiation fluxes, which in turn influence the initial conditions\nfor the plunge. As such, the mass ratio affects the post-ISCO ringdown\ngravitational wave excitation. In this study, we numerically investigate the\nmass ratio dependence of higher harmonic quasi-normal mode excitations in the\npost-ISCO gravitational waves of rapidly spinning black holes, based on the\nTeukolsky-Sasaki-Nakamura formalism. We consider the effect of mass ratio on\nthe gravitational waves by accounting for the energy and angular momentum\nlosses during the transition regime following the Ori-Thorne procedure. We\nexamine two mass ratio scenarios: the intermediate mass ratio (IMR) and the\nextreme mass ratio (EMR). Our main finding is that higher harmonic quasi-normal\nmodes are significantly excited in an IMR merger involving a highly spinning\nprimary black hole. This implies that detecting an IMR merger involving such a\nprimary black hole with space-based gravitational wave interferometers can\nprovide valuable opportunities to infer black hole properties or test general\nrelativity with excellent precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a binary merger with a small mass ratio, as the secondary body approaches\nthe innermost stable circular orbit (ISCO) of the primary black hole, the\nmotion transitions from the adiabatic inspiral to the plunge governed by the\ngeodesic equation. The plunge orbit is expected to excite the ringdown\ngravitational wave, which encodes information about the primary black hole's\ngeometry. The details of the transition regime depend on the binary's mass\nratio through radiation fluxes, which in turn influence the initial conditions\nfor the plunge. As such, the mass ratio affects the post-ISCO ringdown\ngravitational wave excitation. In this study, we numerically investigate the\nmass ratio dependence of higher harmonic quasi-normal mode excitations in the\npost-ISCO gravitational waves of rapidly spinning black holes, based on the\nTeukolsky-Sasaki-Nakamura formalism. We consider the effect of mass ratio on\nthe gravitational waves by accounting for the energy and angular momentum\nlosses during the transition regime following the Ori-Thorne procedure. We\nexamine two mass ratio scenarios: the intermediate mass ratio (IMR) and the\nextreme mass ratio (EMR). Our main finding is that higher harmonic quasi-normal\nmodes are significantly excited in an IMR merger involving a highly spinning\nprimary black hole. This implies that detecting an IMR merger involving such a\nprimary black hole with space-based gravitational wave interferometers can\nprovide valuable opportunities to infer black hole properties or test general\nrelativity with excellent precision."
                },
                "authors": [
                    {
                        "name": "Daiki Watarai"
                    }
                ],
                "author_detail": {
                    "name": "Daiki Watarai"
                },
                "author": "Daiki Watarai",
                "arxiv_comment": "20 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16740v1",
                "updated": "2024-08-29T17:34:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    34,
                    10,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:34:10Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    34,
                    10,
                    3,
                    242,
                    0
                ],
                "title": "Theoretical and Methodological Framework for Studying Texts Produced by\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical and Methodological Framework for Studying Texts Produced by\n  Large Language Models"
                },
                "summary": "This paper addresses the conceptual, methodological and technical challenges\nin studying large language models (LLMs) and the texts they produce from a\nquantitative linguistics perspective. It builds on a theoretical framework that\ndistinguishes between the LLM as a substrate and the entities the model\nsimulates. The paper advocates for a strictly non-anthropomorphic approach to\nmodels while cautiously applying methodologies used in studying human\nlinguistic behavior to the simulated entities. While natural language\nprocessing researchers focus on the models themselves, their architecture,\nevaluation, and methods for improving performance, we as quantitative linguists\nshould strive to build a robust theory concerning the characteristics of texts\nproduced by LLMs, how they differ from human-produced texts, and the properties\nof simulated entities. Additionally, we should explore the potential of LLMs as\nan instrument for studying human culture, of which language is an integral\npart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the conceptual, methodological and technical challenges\nin studying large language models (LLMs) and the texts they produce from a\nquantitative linguistics perspective. It builds on a theoretical framework that\ndistinguishes between the LLM as a substrate and the entities the model\nsimulates. The paper advocates for a strictly non-anthropomorphic approach to\nmodels while cautiously applying methodologies used in studying human\nlinguistic behavior to the simulated entities. While natural language\nprocessing researchers focus on the models themselves, their architecture,\nevaluation, and methods for improving performance, we as quantitative linguists\nshould strive to build a robust theory concerning the characteristics of texts\nproduced by LLMs, how they differ from human-produced texts, and the properties\nof simulated entities. Additionally, we should explore the potential of LLMs as\nan instrument for studying human culture, of which language is an integral\npart."
                },
                "authors": [
                    {
                        "name": "Jiří Milička"
                    }
                ],
                "author_detail": {
                    "name": "Jiří Milička"
                },
                "author": "Jiří Milička",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16737v1",
                "updated": "2024-08-29T17:32:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    32,
                    35,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:32:35Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    32,
                    35,
                    3,
                    242,
                    0
                ],
                "title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal\n  Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal\n  Sampling"
                },
                "summary": "Training on high-quality synthetic data from strong language models (LMs) is\na common strategy to improve the reasoning performance of LMs. In this work, we\nrevisit whether this strategy is compute-optimal under a fixed inference budget\n(e.g., FLOPs). To do so, we investigate the trade-offs between generating\nsynthetic data using a stronger but more expensive (SE) model versus a weaker\nbut cheaper (WC) model. We evaluate the generated data across three key\nmetrics: coverage, diversity, and false positive rate, and show that the data\nfrom WC models may have higher coverage and diversity, but also exhibit higher\nfalse positive rates. We then finetune LMs on data from SE and WC models in\ndifferent settings: knowledge distillation, self-improvement, and a novel\nweak-to-strong improvement setup where a weaker LM teaches reasoning to a\nstronger LM. Our findings reveal that models finetuned on WC-generated data\nconsistently outperform those trained on SE-generated data across multiple\nbenchmarks and multiple choices of WC and SE models. These results challenge\nthe prevailing practice of relying on SE models for synthetic data generation,\nsuggesting that WC may be the compute-optimal approach for training advanced LM\nreasoners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training on high-quality synthetic data from strong language models (LMs) is\na common strategy to improve the reasoning performance of LMs. In this work, we\nrevisit whether this strategy is compute-optimal under a fixed inference budget\n(e.g., FLOPs). To do so, we investigate the trade-offs between generating\nsynthetic data using a stronger but more expensive (SE) model versus a weaker\nbut cheaper (WC) model. We evaluate the generated data across three key\nmetrics: coverage, diversity, and false positive rate, and show that the data\nfrom WC models may have higher coverage and diversity, but also exhibit higher\nfalse positive rates. We then finetune LMs on data from SE and WC models in\ndifferent settings: knowledge distillation, self-improvement, and a novel\nweak-to-strong improvement setup where a weaker LM teaches reasoning to a\nstronger LM. Our findings reveal that models finetuned on WC-generated data\nconsistently outperform those trained on SE-generated data across multiple\nbenchmarks and multiple choices of WC and SE models. These results challenge\nthe prevailing practice of relying on SE models for synthetic data generation,\nsuggesting that WC may be the compute-optimal approach for training advanced LM\nreasoners."
                },
                "authors": [
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Arian Hosseini"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Vinh Q. Tran"
                    },
                    {
                        "name": "Mehran Kazemi"
                    }
                ],
                "author_detail": {
                    "name": "Mehran Kazemi"
                },
                "author": "Mehran Kazemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16725v2",
                "updated": "2024-08-30T02:53:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    53,
                    48,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-29T17:18:53Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    18,
                    53,
                    3,
                    242,
                    0
                ],
                "title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming"
                },
                "summary": "Recent advances in language models have achieved significant progress.\nGPT-4o, as a new milestone, has enabled real-time conversations with humans,\ndemonstrating near-human natural fluency. Such human-computer interaction\nnecessitates models with the capability to perform reasoning directly with the\naudio modality and generate output in streaming. However, this remains beyond\nthe reach of current academic models, as they typically depend on extra TTS\nsystems for speech synthesis, resulting in undesirable latency. This paper\nintroduces the Mini-Omni, an audio-based end-to-end conversational model,\ncapable of real-time speech interaction. To achieve this capability, we propose\na text-instructed speech generation method, along with batch-parallel\nstrategies during inference to further boost the performance. Our method also\nhelps to retain the original model's language capabilities with minimal\ndegradation, enabling other works to establish real-time interaction\ncapabilities. We call this training method \"Any Model Can Talk\". We also\nintroduce the VoiceAssistant-400K dataset to fine-tune models optimized for\nspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,\nopen-source model for real-time speech interaction, offering valuable potential\nfor future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in language models have achieved significant progress.\nGPT-4o, as a new milestone, has enabled real-time conversations with humans,\ndemonstrating near-human natural fluency. Such human-computer interaction\nnecessitates models with the capability to perform reasoning directly with the\naudio modality and generate output in streaming. However, this remains beyond\nthe reach of current academic models, as they typically depend on extra TTS\nsystems for speech synthesis, resulting in undesirable latency. This paper\nintroduces the Mini-Omni, an audio-based end-to-end conversational model,\ncapable of real-time speech interaction. To achieve this capability, we propose\na text-instructed speech generation method, along with batch-parallel\nstrategies during inference to further boost the performance. Our method also\nhelps to retain the original model's language capabilities with minimal\ndegradation, enabling other works to establish real-time interaction\ncapabilities. We call this training method \"Any Model Can Talk\". We also\nintroduce the VoiceAssistant-400K dataset to fine-tune models optimized for\nspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,\nopen-source model for real-time speech interaction, offering valuable potential\nfor future research."
                },
                "authors": [
                    {
                        "name": "Zhifei Xie"
                    },
                    {
                        "name": "Changqiao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Changqiao Wu"
                },
                "author": "Changqiao Wu",
                "arxiv_comment": "Technical report, work in progress. Demo and code:\n  https://github.com/gpt-omni/mini-omni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16706v1",
                "updated": "2024-08-29T17:00:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    0,
                    38,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:00:38Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    0,
                    38,
                    3,
                    242,
                    0
                ],
                "title": "Incremental Context-free Grammar Inference in Black Box Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental Context-free Grammar Inference in Black Box Settings"
                },
                "summary": "Black-box context-free grammar inference presents a significant challenge in\nmany practical settings due to limited access to example programs. The\nstate-of-the-art methods, Arvada and Treevada, employ heuristic approaches to\ngeneralize grammar rules, initiating from flat parse trees and exploring\ndiverse generalization sequences. We have observed that these approaches suffer\nfrom low quality and readability, primarily because they process entire example\nstrings, adding to the complexity and substantially slowing down computations.\nTo overcome these limitations, we propose a novel method that segments example\nstrings into smaller units and incrementally infers the grammar. Our approach,\nnamed Kedavra, has demonstrated superior grammar quality (enhanced precision\nand recall), faster runtime, and improved readability through empirical\ncomparison.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box context-free grammar inference presents a significant challenge in\nmany practical settings due to limited access to example programs. The\nstate-of-the-art methods, Arvada and Treevada, employ heuristic approaches to\ngeneralize grammar rules, initiating from flat parse trees and exploring\ndiverse generalization sequences. We have observed that these approaches suffer\nfrom low quality and readability, primarily because they process entire example\nstrings, adding to the complexity and substantially slowing down computations.\nTo overcome these limitations, we propose a novel method that segments example\nstrings into smaller units and incrementally infers the grammar. Our approach,\nnamed Kedavra, has demonstrated superior grammar quality (enhanced precision\nand recall), faster runtime, and improved readability through empirical\ncomparison."
                },
                "authors": [
                    {
                        "name": "Feifei Li"
                    },
                    {
                        "name": "Xiao Chen"
                    },
                    {
                        "name": "Xi Xiao"
                    },
                    {
                        "name": "Xiaoyu Sun"
                    },
                    {
                        "name": "Chuan Chen"
                    },
                    {
                        "name": "Shaohua Wang"
                    },
                    {
                        "name": "Jitao Han"
                    }
                ],
                "author_detail": {
                    "name": "Jitao Han"
                },
                "author": "Jitao Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15409v2",
                "updated": "2024-08-29T17:00:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    0,
                    24,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-27T21:19:37Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    21,
                    19,
                    37,
                    1,
                    240,
                    0
                ],
                "title": "Awes, Laws, and Flaws From Today's LLM Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Awes, Laws, and Flaws From Today's LLM Research"
                },
                "summary": "We perform a critical examination of the scientific methodology behind\ncontemporary large language model (LLM) research. For this we assess over 2,000\nresearch works based on criteria typical of what is considered good research\n(e.g. presence of statistical tests and reproducibility) and cross-validate it\nwith arguments that are at the centre of controversy (e.g., claims of emergent\nbehaviour, the use of LLMs as evaluators). We find multiple trends, such as\ndeclines in claims of emergent behaviour and ethics disclaimers; the rise of\nLLMs as evaluators in spite of a lack of consensus from the community about\ntheir useability; and an increase of claims of LLM reasoning abilities,\ntypically without leveraging human evaluation. This paper underscores the need\nfor more scrutiny and rigour by and from this field to live up to the\nfundamentals of a responsible scientific method that is ethical, reproducible,\nsystematic, and open to criticism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We perform a critical examination of the scientific methodology behind\ncontemporary large language model (LLM) research. For this we assess over 2,000\nresearch works based on criteria typical of what is considered good research\n(e.g. presence of statistical tests and reproducibility) and cross-validate it\nwith arguments that are at the centre of controversy (e.g., claims of emergent\nbehaviour, the use of LLMs as evaluators). We find multiple trends, such as\ndeclines in claims of emergent behaviour and ethics disclaimers; the rise of\nLLMs as evaluators in spite of a lack of consensus from the community about\ntheir useability; and an increase of claims of LLM reasoning abilities,\ntypically without leveraging human evaluation. This paper underscores the need\nfor more scrutiny and rigour by and from this field to live up to the\nfundamentals of a responsible scientific method that is ethical, reproducible,\nsystematic, and open to criticism."
                },
                "authors": [
                    {
                        "name": "Adrian de Wynter"
                    }
                ],
                "author_detail": {
                    "name": "Adrian de Wynter"
                },
                "author": "Adrian de Wynter",
                "arxiv_comment": "Under review -- v1 was an old draft with an unrevised abstract (oops)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16704v1",
                "updated": "2024-08-29T16:58:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    58,
                    10,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T16:58:10Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    58,
                    10,
                    3,
                    242,
                    0
                ],
                "title": "One-Shot Learning Meets Depth Diffusion in Multi-Object Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Shot Learning Meets Depth Diffusion in Multi-Object Videos"
                },
                "summary": "Creating editable videos that depict complex interactions between multiple\nobjects in various artistic styles has long been a challenging task in\nfilmmaking. Progress is often hampered by the scarcity of data sets that\ncontain paired text descriptions and corresponding videos that showcase these\ninteractions. This paper introduces a novel depth-conditioning approach that\nsignificantly advances this field by enabling the generation of coherent and\ndiverse videos from just a single text-video pair using a pre-trained\ndepth-aware Text-to-Image (T2I) model. Our method fine-tunes the pre-trained\nmodel to capture continuous motion by employing custom-designed spatial and\ntemporal attention mechanisms. During inference, we use the DDIM inversion to\nprovide structural guidance for video generation. This innovative technique\nallows for continuously controllable depth in videos, facilitating the\ngeneration of multiobject interactions while maintaining the concept generation\nand compositional strengths of the original T2I model across various artistic\nstyles, such as photorealism, animation, and impressionism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating editable videos that depict complex interactions between multiple\nobjects in various artistic styles has long been a challenging task in\nfilmmaking. Progress is often hampered by the scarcity of data sets that\ncontain paired text descriptions and corresponding videos that showcase these\ninteractions. This paper introduces a novel depth-conditioning approach that\nsignificantly advances this field by enabling the generation of coherent and\ndiverse videos from just a single text-video pair using a pre-trained\ndepth-aware Text-to-Image (T2I) model. Our method fine-tunes the pre-trained\nmodel to capture continuous motion by employing custom-designed spatial and\ntemporal attention mechanisms. During inference, we use the DDIM inversion to\nprovide structural guidance for video generation. This innovative technique\nallows for continuously controllable depth in videos, facilitating the\ngeneration of multiobject interactions while maintaining the concept generation\nand compositional strengths of the original T2I model across various artistic\nstyles, such as photorealism, animation, and impressionism."
                },
                "authors": [
                    {
                        "name": "Anisha Jain"
                    }
                ],
                "author_detail": {
                    "name": "Anisha Jain"
                },
                "author": "Anisha Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16700v1",
                "updated": "2024-08-29T16:51:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    51,
                    7,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T16:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    51,
                    7,
                    3,
                    242,
                    0
                ],
                "title": "GradBias: Unveiling Word Influence on Bias in Text-to-Image Generative\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GradBias: Unveiling Word Influence on Bias in Text-to-Image Generative\n  Models"
                },
                "summary": "Recent progress in Text-to-Image (T2I) generative models has enabled\nhigh-quality image generation. As performance and accessibility increase, these\nmodels are gaining significant attraction and popularity: ensuring their\nfairness and safety is a priority to prevent the dissemination and perpetuation\nof biases. However, existing studies in bias detection focus on closed sets of\npredefined biases (e.g., gender, ethnicity). In this paper, we propose a\ngeneral framework to identify, quantify, and explain biases in an open set\nsetting, i.e. without requiring a predefined set. This pipeline leverages a\nLarge Language Model (LLM) to propose biases starting from a set of captions.\nNext, these captions are used by the target generative model for generating a\nset of images. Finally, Vision Question Answering (VQA) is leveraged for bias\nevaluation. We show two variations of this framework: OpenBias and GradBias.\nOpenBias detects and quantifies biases, while GradBias determines the\ncontribution of individual prompt words on biases. OpenBias effectively detects\nboth well-known and novel biases related to people, objects, and animals and\nhighly aligns with existing closed-set bias detection methods and human\njudgment. GradBias shows that neutral words can significantly influence biases\nand it outperforms several baselines, including state-of-the-art foundation\nmodels. Code available here: https://github.com/Moreno98/GradBias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in Text-to-Image (T2I) generative models has enabled\nhigh-quality image generation. As performance and accessibility increase, these\nmodels are gaining significant attraction and popularity: ensuring their\nfairness and safety is a priority to prevent the dissemination and perpetuation\nof biases. However, existing studies in bias detection focus on closed sets of\npredefined biases (e.g., gender, ethnicity). In this paper, we propose a\ngeneral framework to identify, quantify, and explain biases in an open set\nsetting, i.e. without requiring a predefined set. This pipeline leverages a\nLarge Language Model (LLM) to propose biases starting from a set of captions.\nNext, these captions are used by the target generative model for generating a\nset of images. Finally, Vision Question Answering (VQA) is leveraged for bias\nevaluation. We show two variations of this framework: OpenBias and GradBias.\nOpenBias detects and quantifies biases, while GradBias determines the\ncontribution of individual prompt words on biases. OpenBias effectively detects\nboth well-known and novel biases related to people, objects, and animals and\nhighly aligns with existing closed-set bias detection methods and human\njudgment. GradBias shows that neutral words can significantly influence biases\nand it outperforms several baselines, including state-of-the-art foundation\nmodels. Code available here: https://github.com/Moreno98/GradBias."
                },
                "authors": [
                    {
                        "name": "Moreno D'Incà"
                    },
                    {
                        "name": "Elia Peruzzo"
                    },
                    {
                        "name": "Massimiliano Mancini"
                    },
                    {
                        "name": "Xingqian Xu"
                    },
                    {
                        "name": "Humphrey Shi"
                    },
                    {
                        "name": "Nicu Sebe"
                    }
                ],
                "author_detail": {
                    "name": "Nicu Sebe"
                },
                "author": "Nicu Sebe",
                "arxiv_comment": "Under review. Code: https://github.com/Moreno98/GradBias",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04952v2",
                "updated": "2024-08-29T16:49:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    49,
                    29,
                    3,
                    242,
                    0
                ],
                "published": "2024-06-07T14:16:37Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    14,
                    16,
                    37,
                    4,
                    159,
                    0
                ],
                "title": "Quantifying Geospatial in the Common Crawl Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Geospatial in the Common Crawl Corpus"
                },
                "summary": "Large language models (LLMs) exhibit emerging geospatial capabilities,\nstemming from their pre-training on vast unlabelled text datasets that are\noften derived from the Common Crawl (CC) corpus. However, the geospatial\ncontent within CC remains largely unexplored, impacting our understanding of\nLLMs' spatial reasoning. This paper investigates the prevalence of geospatial\ndata in recent Common Crawl releases using Gemini 1.5, a powerful language\nmodel. By analyzing a sample of documents and manually revising the results, we\nestimate that 18.7% of web documents in CC contain geospatial information such\nas coordinates and addresses. We find little difference in prevalence between\nEnlgish- and non-English-language documents. Our findings provide quantitative\ninsights into the nature and extent of geospatial data in CC, and lay the\ngroundwork for future studies of geospatial biases of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit emerging geospatial capabilities,\nstemming from their pre-training on vast unlabelled text datasets that are\noften derived from the Common Crawl (CC) corpus. However, the geospatial\ncontent within CC remains largely unexplored, impacting our understanding of\nLLMs' spatial reasoning. This paper investigates the prevalence of geospatial\ndata in recent Common Crawl releases using Gemini 1.5, a powerful language\nmodel. By analyzing a sample of documents and manually revising the results, we\nestimate that 18.7% of web documents in CC contain geospatial information such\nas coordinates and addresses. We find little difference in prevalence between\nEnlgish- and non-English-language documents. Our findings provide quantitative\ninsights into the nature and extent of geospatial data in CC, and lay the\ngroundwork for future studies of geospatial biases of LLMs."
                },
                "authors": [
                    {
                        "name": "Ilya Ilyankou"
                    },
                    {
                        "name": "Meihui Wang"
                    },
                    {
                        "name": "Stefano Cavazzi"
                    },
                    {
                        "name": "James Haworth"
                    }
                ],
                "author_detail": {
                    "name": "James Haworth"
                },
                "author": "James Haworth",
                "arxiv_comment": "Accepted as a poster to ACM SIGSPATIAL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16672v1",
                "updated": "2024-08-29T16:21:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    21,
                    0,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T16:21:00Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    21,
                    0,
                    3,
                    242,
                    0
                ],
                "title": "Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction\n  Retriever",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction\n  Retriever"
                },
                "summary": "Multi-vector dense models, such as ColBERT, have proven highly effective in\ninformation retrieval. ColBERT's late interaction scoring approximates the\njoint query-document attention seen in cross-encoders while maintaining\ninference efficiency closer to traditional dense retrieval models, thanks to\nits bi-encoder architecture and recent optimizations in indexing and search. In\nthis paper, we introduce several improvements to the ColBERT model architecture\nand training pipeline, leveraging techniques successful in the more established\nsingle-vector embedding model paradigm, particularly those suited for\nheterogeneous multilingual data. Our new model, Jina-ColBERT-v2, demonstrates\nstrong performance across a range of English and multilingual retrieval tasks,\nwhile also cutting storage requirements by up to 50% compared to previous\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-vector dense models, such as ColBERT, have proven highly effective in\ninformation retrieval. ColBERT's late interaction scoring approximates the\njoint query-document attention seen in cross-encoders while maintaining\ninference efficiency closer to traditional dense retrieval models, thanks to\nits bi-encoder architecture and recent optimizations in indexing and search. In\nthis paper, we introduce several improvements to the ColBERT model architecture\nand training pipeline, leveraging techniques successful in the more established\nsingle-vector embedding model paradigm, particularly those suited for\nheterogeneous multilingual data. Our new model, Jina-ColBERT-v2, demonstrates\nstrong performance across a range of English and multilingual retrieval tasks,\nwhile also cutting storage requirements by up to 50% compared to previous\nmodels."
                },
                "authors": [
                    {
                        "name": "Rohan Jha"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Michael Günther"
                    },
                    {
                        "name": "Saba Sturua"
                    },
                    {
                        "name": "Mohammad Kalim Akram"
                    },
                    {
                        "name": "Han Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Han Xiao"
                },
                "author": "Han Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16673v1",
                "updated": "2024-08-29T16:21:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    21,
                    0,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T16:21:00Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    21,
                    0,
                    3,
                    242,
                    0
                ],
                "title": "Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less\n  Overfitting and Better Diversity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less\n  Overfitting and Better Diversity"
                },
                "summary": "Large language models rely on Supervised Fine-Tuning (SFT) to specialize in\ndownstream tasks. Cross Entropy (CE) loss is the de facto choice in SFT, but it\noften leads to overfitting and limited output diversity due to its aggressive\nupdates to the data distribution. This paper aim to address these issues by\nintroducing the maximum entropy principle, which favors models with flatter\ndistributions that still effectively capture the data. Specifically, we develop\na new distribution matching method called GEM, which solves reverse\nKullback-Leibler divergence minimization with an entropy regularizer.\n  For the SFT of Llama-3-8B models, GEM outperforms CE in several aspects.\nFirst, when applied to the UltraFeedback dataset to develop general\ninstruction-following abilities, GEM exhibits reduced overfitting, evidenced by\nlower perplexity and better performance on the IFEval benchmark. Furthermore,\nGEM enhances output diversity, leading to performance gains of up to 7 points\non math reasoning and code generation tasks using best-of-n sampling, even\nwithout domain-specific data. Second, when fine-tuning with domain-specific\ndatasets for math reasoning and code generation, GEM also shows less\noverfitting and improvements of up to 10 points compared with CE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models rely on Supervised Fine-Tuning (SFT) to specialize in\ndownstream tasks. Cross Entropy (CE) loss is the de facto choice in SFT, but it\noften leads to overfitting and limited output diversity due to its aggressive\nupdates to the data distribution. This paper aim to address these issues by\nintroducing the maximum entropy principle, which favors models with flatter\ndistributions that still effectively capture the data. Specifically, we develop\na new distribution matching method called GEM, which solves reverse\nKullback-Leibler divergence minimization with an entropy regularizer.\n  For the SFT of Llama-3-8B models, GEM outperforms CE in several aspects.\nFirst, when applied to the UltraFeedback dataset to develop general\ninstruction-following abilities, GEM exhibits reduced overfitting, evidenced by\nlower perplexity and better performance on the IFEval benchmark. Furthermore,\nGEM enhances output diversity, leading to performance gains of up to 7 points\non math reasoning and code generation tasks using best-of-n sampling, even\nwithout domain-specific data. Second, when fine-tuning with domain-specific\ndatasets for math reasoning and code generation, GEM also shows less\noverfitting and improvements of up to 10 points compared with CE."
                },
                "authors": [
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Congliang Chen"
                    },
                    {
                        "name": "Tian Xu"
                    },
                    {
                        "name": "Zeyu Qin"
                    },
                    {
                        "name": "Jiancong Xiao"
                    },
                    {
                        "name": "Ruoyu Sun"
                    },
                    {
                        "name": "Zhi-Quan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Quan Luo"
                },
                "author": "Zhi-Quan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16667v1",
                "updated": "2024-08-29T16:15:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    15,
                    1,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T16:15:01Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    15,
                    1,
                    3,
                    242,
                    0
                ],
                "title": "Iterative Graph Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Graph Alignment"
                },
                "summary": "By compressing diverse narratives, LLMs go beyond memorization, achieving\nintelligence by capturing generalizable causal relationships. However, they\nsuffer from local 'representation gaps' due to insufficient training data\ndiversity, limiting their real-world utility, especially in tasks requiring\nstrict alignment to rules. Traditional alignment methods relying on heavy human\nannotations are inefficient and unscalable. Recent self-alignment techniques\nalso fall short, as they often depend on self-selection based prompting and\nmemorization-based learning. To address these issues, we introduce Iterative\nGraph Alignment (IGA), an annotation-free rule-based alignment algorithm. A\nteacher model (VLM) employs Iterative Graph Prompting (IGP) to create logical\ngraphs and reference answers. The student model (LLM) identifies local\nknowledge gaps by attempting to align its responses with these references,\ncollaborating with helper models to generate diverse answers. These aligned\nresponses are then used for iterative supervised fine-tuning (SFT). Our\nevaluations across five rule-based scenarios demonstrate IGP's effectiveness,\nwith a 73.12\\% alignment improvement in Claude Sonnet 3.5, and\nLlama3-8B-Instruct achieving an 86.20\\% improvement, outperforming Claude\nSonnet 3.5 in rule-based alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By compressing diverse narratives, LLMs go beyond memorization, achieving\nintelligence by capturing generalizable causal relationships. However, they\nsuffer from local 'representation gaps' due to insufficient training data\ndiversity, limiting their real-world utility, especially in tasks requiring\nstrict alignment to rules. Traditional alignment methods relying on heavy human\nannotations are inefficient and unscalable. Recent self-alignment techniques\nalso fall short, as they often depend on self-selection based prompting and\nmemorization-based learning. To address these issues, we introduce Iterative\nGraph Alignment (IGA), an annotation-free rule-based alignment algorithm. A\nteacher model (VLM) employs Iterative Graph Prompting (IGP) to create logical\ngraphs and reference answers. The student model (LLM) identifies local\nknowledge gaps by attempting to align its responses with these references,\ncollaborating with helper models to generate diverse answers. These aligned\nresponses are then used for iterative supervised fine-tuning (SFT). Our\nevaluations across five rule-based scenarios demonstrate IGP's effectiveness,\nwith a 73.12\\% alignment improvement in Claude Sonnet 3.5, and\nLlama3-8B-Instruct achieving an 86.20\\% improvement, outperforming Claude\nSonnet 3.5 in rule-based alignment."
                },
                "authors": [
                    {
                        "name": "Fangyuan Yu"
                    },
                    {
                        "name": "Hardeep Singh Arora"
                    },
                    {
                        "name": "Matt Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Matt Johnson"
                },
                "author": "Matt Johnson",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15969v2",
                "updated": "2024-08-29T15:49:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    15,
                    49,
                    55,
                    3,
                    242,
                    0
                ],
                "published": "2024-05-24T22:41:38Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    22,
                    41,
                    38,
                    4,
                    145,
                    0
                ],
                "title": "Massive Digital Over-the-Air Computation for Communication-Efficient\n  Federated Edge Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive Digital Over-the-Air Computation for Communication-Efficient\n  Federated Edge Learning"
                },
                "summary": "Over-the-air computation (AirComp) is a promising technology converging\ncommunication and computation over wireless networks, which can be particularly\neffective in model training, inference, and more emerging edge intelligence\napplications. AirComp relies on uncoded transmission of individual signals,\nwhich are added naturally over the multiple access channel thanks to the\nsuperposition property of the wireless medium. Despite significantly improved\ncommunication efficiency, how to accommodate AirComp in the existing and future\ndigital communication networks, that are based on discrete modulation schemes,\nremains a challenge. This paper proposes a massive digital AirComp (MD-AirComp)\nscheme, that leverages an unsourced massive access protocol, to enhance\ncompatibility with both current and next-generation wireless networks.\nMD-AirComp utilizes vector quantization to reduce the uplink communication\noverhead, and employs shared quantization and modulation codebooks. At the\nreceiver, we propose a near-optimal approximate message passing-based algorithm\nto compute the model aggregation results from the superposed sequences, which\nrelies on estimating the number of devices transmitting each code sequence,\nrather than trying to decode the messages of individual transmitters. We apply\nMD-AirComp to the federated edge learning (FEEL), and show that it\nsignificantly accelerates FEEL convergence compared to state-of-the-art while\nusing the same amount of communication resources. To support further research\nand ensure reproducibility, we have made our code available at\nhttps://github.com/liqiao19/MD-AirComp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over-the-air computation (AirComp) is a promising technology converging\ncommunication and computation over wireless networks, which can be particularly\neffective in model training, inference, and more emerging edge intelligence\napplications. AirComp relies on uncoded transmission of individual signals,\nwhich are added naturally over the multiple access channel thanks to the\nsuperposition property of the wireless medium. Despite significantly improved\ncommunication efficiency, how to accommodate AirComp in the existing and future\ndigital communication networks, that are based on discrete modulation schemes,\nremains a challenge. This paper proposes a massive digital AirComp (MD-AirComp)\nscheme, that leverages an unsourced massive access protocol, to enhance\ncompatibility with both current and next-generation wireless networks.\nMD-AirComp utilizes vector quantization to reduce the uplink communication\noverhead, and employs shared quantization and modulation codebooks. At the\nreceiver, we propose a near-optimal approximate message passing-based algorithm\nto compute the model aggregation results from the superposed sequences, which\nrelies on estimating the number of devices transmitting each code sequence,\nrather than trying to decode the messages of individual transmitters. We apply\nMD-AirComp to the federated edge learning (FEEL), and show that it\nsignificantly accelerates FEEL convergence compared to state-of-the-art while\nusing the same amount of communication resources. To support further research\nand ensure reproducibility, we have made our code available at\nhttps://github.com/liqiao19/MD-AirComp."
                },
                "authors": [
                    {
                        "name": "Li Qiao"
                    },
                    {
                        "name": "Zhen Gao"
                    },
                    {
                        "name": "Mahdi Boloursaz Mashhadi"
                    },
                    {
                        "name": "Deniz Gündüz"
                    }
                ],
                "author_detail": {
                    "name": "Deniz Gündüz"
                },
                "author": "Deniz Gündüz",
                "arxiv_comment": "IEEE Journal on Selected Areas in Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16642v1",
                "updated": "2024-08-29T15:46:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    15,
                    46,
                    10,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T15:46:10Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    15,
                    46,
                    10,
                    3,
                    242,
                    0
                ],
                "title": "A machine learning approach for computing solar flare locations in\n  X-rays on-board Solar Orbiter/STIX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A machine learning approach for computing solar flare locations in\n  X-rays on-board Solar Orbiter/STIX"
                },
                "summary": "The Spectrometer/Telescope for Imaging X-rays (STIX) on-board the ESA Solar\nOrbiter mission retrieves the coordinates of solar flare locations by means of\na specific sub-collimator, named the Coarse Flare Locator (CFL). When a solar\nflare occurs on the Sun, the emitted X-ray radiation casts the shadow of a\npeculiar \"H-shaped\" tungsten grid over the CFL X-ray detector. From\nmeasurements of the areas of the detector that are illuminated by the X-ray\nradiation, it is possible to retrieve the $(x,y)$ coordinates of the flare\nlocation on the solar disk.\n  In this paper, we train a neural network on a dataset of real CFL\nobservations to estimate the coordinates of solar flare locations. Further, we\napply a post-training quantization technique specifically tailored to the\nadopted model architecture. This technique allows all computations to be in\ninteger arithmetic at inference time, making the model compatible with the STIX\ncomputational requirements. We show that our model outperforms the currently\nadopted algorithm for estimating the flare locations from CFL data regarding\nprediction accuracy while requiring fewer parameters. We finally discuss\npossible future applications of the proposed model on-board STIX.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Spectrometer/Telescope for Imaging X-rays (STIX) on-board the ESA Solar\nOrbiter mission retrieves the coordinates of solar flare locations by means of\na specific sub-collimator, named the Coarse Flare Locator (CFL). When a solar\nflare occurs on the Sun, the emitted X-ray radiation casts the shadow of a\npeculiar \"H-shaped\" tungsten grid over the CFL X-ray detector. From\nmeasurements of the areas of the detector that are illuminated by the X-ray\nradiation, it is possible to retrieve the $(x,y)$ coordinates of the flare\nlocation on the solar disk.\n  In this paper, we train a neural network on a dataset of real CFL\nobservations to estimate the coordinates of solar flare locations. Further, we\napply a post-training quantization technique specifically tailored to the\nadopted model architecture. This technique allows all computations to be in\ninteger arithmetic at inference time, making the model compatible with the STIX\ncomputational requirements. We show that our model outperforms the currently\nadopted algorithm for estimating the flare locations from CFL data regarding\nprediction accuracy while requiring fewer parameters. We finally discuss\npossible future applications of the proposed model on-board STIX."
                },
                "authors": [
                    {
                        "name": "Paolo Massa"
                    },
                    {
                        "name": "Simon Felix"
                    },
                    {
                        "name": "László István Etesi"
                    },
                    {
                        "name": "Ewan C. M. Dickson"
                    },
                    {
                        "name": "Hualin Xiao"
                    },
                    {
                        "name": "Francesco P. Ramunno"
                    },
                    {
                        "name": "Merve Selcuk-Simsek"
                    },
                    {
                        "name": "Brandon Panos"
                    },
                    {
                        "name": "André Csillaghy"
                    },
                    {
                        "name": "Säm Krucker"
                    }
                ],
                "author_detail": {
                    "name": "Säm Krucker"
                },
                "author": "Säm Krucker",
                "arxiv_comment": "Conference paper accepted for an poster presentation to the ESA\n  SPAICE CONFERENCE 17 19 September 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16629v1",
                "updated": "2024-08-29T15:36:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    15,
                    36,
                    52,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T15:36:52Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    15,
                    36,
                    52,
                    3,
                    242,
                    0
                ],
                "title": "LLMs generate structurally realistic social networks but overestimate\n  political homophily",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs generate structurally realistic social networks but overestimate\n  political homophily"
                },
                "summary": "Generating social networks is essential for many applications, such as\nepidemic modeling and social simulations. Prior approaches either involve deep\nlearning models, which require many observed networks for training, or stylized\nmodels, which are limited in their realism and flexibility. In contrast, LLMs\noffer the potential for zero-shot and flexible network generation. However, two\nkey questions are: (1) are LLM's generated networks realistic, and (2) what are\nrisks of bias, given the importance of demographics in forming social ties? To\nanswer these questions, we develop three prompting methods for network\ngeneration and compare the generated networks to real social networks. We find\nthat more realistic networks are generated with \"local\" methods, where the LLM\nconstructs relations for one persona at a time, compared to \"global\" methods\nthat construct the entire network at once. We also find that the generated\nnetworks match real networks on many characteristics, including density,\nclustering, community structure, and degree. However, we find that LLMs\nemphasize political homophily over all other types of homophily and\noverestimate political homophily relative to real-world measures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating social networks is essential for many applications, such as\nepidemic modeling and social simulations. Prior approaches either involve deep\nlearning models, which require many observed networks for training, or stylized\nmodels, which are limited in their realism and flexibility. In contrast, LLMs\noffer the potential for zero-shot and flexible network generation. However, two\nkey questions are: (1) are LLM's generated networks realistic, and (2) what are\nrisks of bias, given the importance of demographics in forming social ties? To\nanswer these questions, we develop three prompting methods for network\ngeneration and compare the generated networks to real social networks. We find\nthat more realistic networks are generated with \"local\" methods, where the LLM\nconstructs relations for one persona at a time, compared to \"global\" methods\nthat construct the entire network at once. We also find that the generated\nnetworks match real networks on many characteristics, including density,\nclustering, community structure, and degree. However, we find that LLMs\nemphasize political homophily over all other types of homophily and\noverestimate political homophily relative to real-world measures."
                },
                "authors": [
                    {
                        "name": "Serina Chang"
                    },
                    {
                        "name": "Alicja Chaszczewicz"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Maya Josifovska"
                    },
                    {
                        "name": "Emma Pierson"
                    },
                    {
                        "name": "Jure Leskovec"
                    }
                ],
                "author_detail": {
                    "name": "Jure Leskovec"
                },
                "author": "Jure Leskovec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.07181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.07181v2",
                "updated": "2024-08-29T15:33:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    15,
                    33,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2023-06-12T15:27:50Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    15,
                    27,
                    50,
                    0,
                    163,
                    0
                ],
                "title": "Bayesian estimation of covariate assisted principal regression for brain\n  functional connectivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian estimation of covariate assisted principal regression for brain\n  functional connectivity"
                },
                "summary": "This paper presents a Bayesian reformulation of covariate-assisted principal\n(CAP) regression of Zhao and others (2021), which aims to identify components\nin the covariance of response signal that are associated with covariates in a\nregression framework. We introduce a geometric formulation and\nreparameterization of individual covariance matrices in their tangent space. By\nmapping the covariance matrices to the tangent space, we leverage Euclidean\ngeometry to perform posterior inference. This approach enables joint estimation\nof all parameters and uncertainty quantification within a unified framework,\nfusing dimension reduction for covariance matrices with regression model\nestimation. We validate the proposed method through simulation studies and\napply it to analyze associations between covariates and brain functional\nconnectivity, utilizing data from the Human Connectome Project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a Bayesian reformulation of covariate-assisted principal\n(CAP) regression of Zhao and others (2021), which aims to identify components\nin the covariance of response signal that are associated with covariates in a\nregression framework. We introduce a geometric formulation and\nreparameterization of individual covariance matrices in their tangent space. By\nmapping the covariance matrices to the tangent space, we leverage Euclidean\ngeometry to perform posterior inference. This approach enables joint estimation\nof all parameters and uncertainty quantification within a unified framework,\nfusing dimension reduction for covariance matrices with regression model\nestimation. We validate the proposed method through simulation studies and\napply it to analyze associations between covariates and brain functional\nconnectivity, utilizing data from the Human Connectome Project."
                },
                "authors": [
                    {
                        "name": "Hyung G. Park"
                    }
                ],
                "author_detail": {
                    "name": "Hyung G. Park"
                },
                "author": "Hyung G. Park",
                "arxiv_comment": "20 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.07181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.07181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16601v1",
                "updated": "2024-08-29T15:12:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    15,
                    12,
                    16,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T15:12:16Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    15,
                    12,
                    16,
                    3,
                    242,
                    0
                ],
                "title": "Examination of Code generated by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examination of Code generated by Large Language Models"
                },
                "summary": "Large language models (LLMs), such as ChatGPT and Copilot, are transforming\nsoftware development by automating code generation and, arguably, enable rapid\nprototyping, support education, and boost productivity. Therefore, correctness\nand quality of the generated code should be on par with manually written code.\nTo assess the current state of LLMs in generating correct code of high quality,\nwe conducted controlled experiments with ChatGPT and Copilot: we let the LLMs\ngenerate simple algorithms in Java and Python along with the corresponding unit\ntests and assessed the correctness and the quality (coverage) of the generated\n(test) codes. We observed significant differences between the LLMs, between the\nlanguages, between algorithm and test codes, and over time. The present paper\nreports these results together with the experimental methods allowing repeated\nand comparable assessments for more algorithms, languages, and LLMs over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), such as ChatGPT and Copilot, are transforming\nsoftware development by automating code generation and, arguably, enable rapid\nprototyping, support education, and boost productivity. Therefore, correctness\nand quality of the generated code should be on par with manually written code.\nTo assess the current state of LLMs in generating correct code of high quality,\nwe conducted controlled experiments with ChatGPT and Copilot: we let the LLMs\ngenerate simple algorithms in Java and Python along with the corresponding unit\ntests and assessed the correctness and the quality (coverage) of the generated\n(test) codes. We observed significant differences between the LLMs, between the\nlanguages, between algorithm and test codes, and over time. The present paper\nreports these results together with the experimental methods allowing repeated\nand comparable assessments for more algorithms, languages, and LLMs over time."
                },
                "authors": [
                    {
                        "name": "Robin Beer"
                    },
                    {
                        "name": "Alexander Feix"
                    },
                    {
                        "name": "Tim Guttzeit"
                    },
                    {
                        "name": "Tamara Muras"
                    },
                    {
                        "name": "Vincent Müller"
                    },
                    {
                        "name": "Maurice Rauscher"
                    },
                    {
                        "name": "Florian Schäffler"
                    },
                    {
                        "name": "Welf Löwe"
                    }
                ],
                "author_detail": {
                    "name": "Welf Löwe"
                },
                "author": "Welf Löwe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16592v1",
                "updated": "2024-08-29T14:55:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    55,
                    33,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T14:55:33Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    55,
                    33,
                    3,
                    242,
                    0
                ],
                "title": "High-Dimensional Sparse Data Low-rank Representation via Accelerated\n  Asynchronous Parallel Stochastic Gradient Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Dimensional Sparse Data Low-rank Representation via Accelerated\n  Asynchronous Parallel Stochastic Gradient Descent"
                },
                "summary": "Data characterized by high dimensionality and sparsity are commonly used to\ndescribe real-world node interactions. Low-rank representation (LR) can map\nhigh-dimensional sparse (HDS) data to low-dimensional feature spaces and infer\nnode interactions via modeling data latent associations. Unfortunately,\nexisting optimization algorithms for LR models are computationally inefficient\nand slowly convergent on large-scale datasets. To address this issue, this\npaper proposes an Accelerated Asynchronous Parallel Stochastic Gradient Descent\nA2PSGD for High-Dimensional Sparse Data Low-rank Representation with three\nfold-ideas: a) establishing a lock-free scheduler to simultaneously respond to\nscheduling requests from multiple threads; b) introducing a greedy\nalgorithm-based load balancing strategy for balancing the computational load\namong threads; c) incorporating Nesterov's accelerated gradient into the\nlearning scheme to accelerate model convergence. Empirical studies show that\nA2PSGD outperforms existing optimization algorithms for HDS data LR in both\naccuracy and training time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data characterized by high dimensionality and sparsity are commonly used to\ndescribe real-world node interactions. Low-rank representation (LR) can map\nhigh-dimensional sparse (HDS) data to low-dimensional feature spaces and infer\nnode interactions via modeling data latent associations. Unfortunately,\nexisting optimization algorithms for LR models are computationally inefficient\nand slowly convergent on large-scale datasets. To address this issue, this\npaper proposes an Accelerated Asynchronous Parallel Stochastic Gradient Descent\nA2PSGD for High-Dimensional Sparse Data Low-rank Representation with three\nfold-ideas: a) establishing a lock-free scheduler to simultaneously respond to\nscheduling requests from multiple threads; b) introducing a greedy\nalgorithm-based load balancing strategy for balancing the computational load\namong threads; c) incorporating Nesterov's accelerated gradient into the\nlearning scheme to accelerate model convergence. Empirical studies show that\nA2PSGD outperforms existing optimization algorithms for HDS data LR in both\naccuracy and training time."
                },
                "authors": [
                    {
                        "name": "Qicong Hu"
                    },
                    {
                        "name": "Hao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wu"
                },
                "author": "Hao Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16624v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16624v2",
                "updated": "2024-08-29T14:54:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    54,
                    29,
                    3,
                    242,
                    0
                ],
                "published": "2024-05-26T16:40:07Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    16,
                    40,
                    7,
                    6,
                    147,
                    0
                ],
                "title": "Constraining the physical properties of large-scale jets from black hole\n  X-ray binaries and their impact on the local environment with blast-wave\n  dynamical models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining the physical properties of large-scale jets from black hole\n  X-ray binaries and their impact on the local environment with blast-wave\n  dynamical models"
                },
                "summary": "Relativistic discrete ejecta launched by black hole X-ray binaries (BH XRBs)\ncan be observed to propagate up to parsec-scales from the central object.\nObserving the final deceleration phase of these jets is crucial to estimate\ntheir physical parameters and to reconstruct their full trajectory, with\nimplications for the jet powering mechanism, composition and formation. In this\npaper we present the results of the modelling of the motion of the ejecta from\nthree BH XRBs: MAXI J1820+070, MAXI J1535$-$571 and XTE J1752$-$223, for which\nhigh-resolution radio and X-ray observations of jets propagating up to $\\sim$15\narcsec ($\\sim$0.6 pc at 3 kpc) from the core have been published in the recent\nyears. For each jet, we modeled its entire motion with a dynamical blast-wave\nmodel, inferring robust values for the jet Lorentz factor, inclination angle\nand ejection time. Under several assumptions associated to the ejection\nduration, the jet opening angle and the available accretion power, we are able\nto derive stringent constraints on the maximum jet kinetic energy for each\nsource (between $10^{43}$ and $10^{44}$ erg, including also H1743$-$322), as\nwell as placing interesting upper limits on the density of the ISM through\nwhich the jets are propagating (from $n_{\\rm ISM} \\lesssim 0.4$ cm$^{-3}$ down\nto $n_{\\rm ISM} \\lesssim 10^{-4}$ cm$^{-3}$). Overall, our results highlight\nthe potential of applying models derived from gamma-ray bursts to the physics\nof jets from BH XRBs and support the emerging picture of these sources as\npreferentially embedded in low-density environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relativistic discrete ejecta launched by black hole X-ray binaries (BH XRBs)\ncan be observed to propagate up to parsec-scales from the central object.\nObserving the final deceleration phase of these jets is crucial to estimate\ntheir physical parameters and to reconstruct their full trajectory, with\nimplications for the jet powering mechanism, composition and formation. In this\npaper we present the results of the modelling of the motion of the ejecta from\nthree BH XRBs: MAXI J1820+070, MAXI J1535$-$571 and XTE J1752$-$223, for which\nhigh-resolution radio and X-ray observations of jets propagating up to $\\sim$15\narcsec ($\\sim$0.6 pc at 3 kpc) from the core have been published in the recent\nyears. For each jet, we modeled its entire motion with a dynamical blast-wave\nmodel, inferring robust values for the jet Lorentz factor, inclination angle\nand ejection time. Under several assumptions associated to the ejection\nduration, the jet opening angle and the available accretion power, we are able\nto derive stringent constraints on the maximum jet kinetic energy for each\nsource (between $10^{43}$ and $10^{44}$ erg, including also H1743$-$322), as\nwell as placing interesting upper limits on the density of the ISM through\nwhich the jets are propagating (from $n_{\\rm ISM} \\lesssim 0.4$ cm$^{-3}$ down\nto $n_{\\rm ISM} \\lesssim 10^{-4}$ cm$^{-3}$). Overall, our results highlight\nthe potential of applying models derived from gamma-ray bursts to the physics\nof jets from BH XRBs and support the emerging picture of these sources as\npreferentially embedded in low-density environments."
                },
                "authors": [
                    {
                        "name": "Francesco Carotenuto"
                    },
                    {
                        "name": "Rob Fender"
                    },
                    {
                        "name": "Alexandra J. Tetarenko"
                    },
                    {
                        "name": "Stéphane Corbel"
                    },
                    {
                        "name": "Andrzej A. Zdziarski"
                    },
                    {
                        "name": "Gulzar Shaik"
                    },
                    {
                        "name": "Alex J. Cooper"
                    },
                    {
                        "name": "Irene Di Palma"
                    }
                ],
                "author_detail": {
                    "name": "Irene Di Palma"
                },
                "author": "Irene Di Palma",
                "arxiv_comment": "22 pages, 9 figures. Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16624v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16624v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05418v2",
                "updated": "2024-08-29T14:50:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    50,
                    10,
                    3,
                    242,
                    0
                ],
                "published": "2024-05-08T20:39:54Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    20,
                    39,
                    54,
                    2,
                    129,
                    0
                ],
                "title": "Mitigating Exaggerated Safety in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Exaggerated Safety in Large Language Models"
                },
                "summary": "As the popularity of Large Language Models (LLMs) grow, combining model\nsafety with utility becomes increasingly important. The challenge is making\nsure that LLMs can recognize and decline dangerous prompts without sacrificing\ntheir ability to be helpful. The problem of \"exaggerated safety\" demonstrates\nhow difficult this can be. To reduce excessive safety behaviours -- which was\ndiscovered to be 26.1% of safe prompts being misclassified as dangerous and\nrefused -- we use a combination of XSTest dataset prompts as well as\ninteractive, contextual, and few-shot prompting to examine the decision bounds\nof LLMs such as Llama2, Gemma Command R+, and Phi-3. We find that few-shot\nprompting works best for Llama2, interactive prompting works best Gemma, and\ncontextual prompting works best for Command R+ and Phi-3. Using a combination\nof these prompting strategies, we are able to mitigate exaggerated safety\nbehaviors by an overall 92.9% across all LLMs. Our work presents a multiple\nprompting strategies to jailbreak LLMs' decision-making processes, allowing\nthem to navigate the tight line between refusing unsafe prompts and remaining\nhelpful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the popularity of Large Language Models (LLMs) grow, combining model\nsafety with utility becomes increasingly important. The challenge is making\nsure that LLMs can recognize and decline dangerous prompts without sacrificing\ntheir ability to be helpful. The problem of \"exaggerated safety\" demonstrates\nhow difficult this can be. To reduce excessive safety behaviours -- which was\ndiscovered to be 26.1% of safe prompts being misclassified as dangerous and\nrefused -- we use a combination of XSTest dataset prompts as well as\ninteractive, contextual, and few-shot prompting to examine the decision bounds\nof LLMs such as Llama2, Gemma Command R+, and Phi-3. We find that few-shot\nprompting works best for Llama2, interactive prompting works best Gemma, and\ncontextual prompting works best for Command R+ and Phi-3. Using a combination\nof these prompting strategies, we are able to mitigate exaggerated safety\nbehaviors by an overall 92.9% across all LLMs. Our work presents a multiple\nprompting strategies to jailbreak LLMs' decision-making processes, allowing\nthem to navigate the tight line between refusing unsafe prompts and remaining\nhelpful."
                },
                "authors": [
                    {
                        "name": "Ruchira Ray"
                    },
                    {
                        "name": "Ruchi Bhalani"
                    }
                ],
                "author_detail": {
                    "name": "Ruchi Bhalani"
                },
                "author": "Ruchi Bhalani",
                "arxiv_comment": "17 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16586v1",
                "updated": "2024-08-29T14:49:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    49,
                    13,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T14:49:13Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    49,
                    13,
                    3,
                    242,
                    0
                ],
                "title": "Enhancing Dialogue Generation in Werewolf Game Through Situation\n  Analysis and Persuasion Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Dialogue Generation in Werewolf Game Through Situation\n  Analysis and Persuasion Strategies"
                },
                "summary": "Recent advancements in natural language processing, particularly with large\nlanguage models (LLMs) like GPT-4, have significantly enhanced dialogue\nsystems, enabling them to generate more natural and fluent conversations.\nDespite these improvements, challenges persist, such as managing continuous\ndialogues, memory retention, and minimizing hallucinations. The AIWolfDial2024\naddresses these challenges by employing the Werewolf Game, an incomplete\ninformation game, to test the capabilities of LLMs in complex interactive\nenvironments. This paper introduces a LLM-based Werewolf Game AI, where each\nrole is supported by situation analysis to aid response generation.\nAdditionally, for the werewolf role, various persuasion strategies, including\nlogical appeal, credibility appeal, and emotional appeal, are employed to\neffectively persuade other players to align with its actions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in natural language processing, particularly with large\nlanguage models (LLMs) like GPT-4, have significantly enhanced dialogue\nsystems, enabling them to generate more natural and fluent conversations.\nDespite these improvements, challenges persist, such as managing continuous\ndialogues, memory retention, and minimizing hallucinations. The AIWolfDial2024\naddresses these challenges by employing the Werewolf Game, an incomplete\ninformation game, to test the capabilities of LLMs in complex interactive\nenvironments. This paper introduces a LLM-based Werewolf Game AI, where each\nrole is supported by situation analysis to aid response generation.\nAdditionally, for the werewolf role, various persuasion strategies, including\nlogical appeal, credibility appeal, and emotional appeal, are employed to\neffectively persuade other players to align with its actions."
                },
                "authors": [
                    {
                        "name": "Zhiyang Qi"
                    },
                    {
                        "name": "Michimasa Inaba"
                    }
                ],
                "author_detail": {
                    "name": "Michimasa Inaba"
                },
                "author": "Michimasa Inaba",
                "arxiv_comment": "Accepted to the AIWolfDial2024 workshop at INLG 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11455v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11455v2",
                "updated": "2024-08-29T14:48:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    48,
                    10,
                    3,
                    242,
                    0
                ],
                "published": "2024-06-17T12:11:01Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    12,
                    11,
                    1,
                    0,
                    169,
                    0
                ],
                "title": "Adaptive Reinforcement Learning Planning: Harnessing Large Language\n  Models for Complex Information Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Reinforcement Learning Planning: Harnessing Large Language\n  Models for Complex Information Extraction"
                },
                "summary": "Existing research on large language models (LLMs) shows that they can solve\ninformation extraction tasks through multi-step planning. However, their\nextraction behavior on complex sentences and tasks is unstable, emerging issues\nsuch as false positives and missing elements. We observe that decomposing\ncomplex extraction tasks and extracting them step by step can effectively\nimprove LLMs' performance, and the extraction orders of entities significantly\naffect the final results of LLMs. This paper proposes a two-stage multi-step\nmethod for LLM-based information extraction and adopts the RL framework to\nexecute the multi-step planning. We regard sequential extraction as a Markov\ndecision process, build an LLM-based extraction environment, design a decision\nmodule to adaptively provide the optimal order for sequential entity extraction\non different sentences, and utilize the DDQN algorithm to train the decision\nmodel. We also design the rewards and evaluation metrics suitable for the\nextraction results of LLMs. We conduct extensive experiments on multiple public\ndatasets to demonstrate the effectiveness of our method in improving the\ninformation extraction capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing research on large language models (LLMs) shows that they can solve\ninformation extraction tasks through multi-step planning. However, their\nextraction behavior on complex sentences and tasks is unstable, emerging issues\nsuch as false positives and missing elements. We observe that decomposing\ncomplex extraction tasks and extracting them step by step can effectively\nimprove LLMs' performance, and the extraction orders of entities significantly\naffect the final results of LLMs. This paper proposes a two-stage multi-step\nmethod for LLM-based information extraction and adopts the RL framework to\nexecute the multi-step planning. We regard sequential extraction as a Markov\ndecision process, build an LLM-based extraction environment, design a decision\nmodule to adaptively provide the optimal order for sequential entity extraction\non different sentences, and utilize the DDQN algorithm to train the decision\nmodel. We also design the rewards and evaluation metrics suitable for the\nextraction results of LLMs. We conduct extensive experiments on multiple public\ndatasets to demonstrate the effectiveness of our method in improving the\ninformation extraction capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Zepeng Ding"
                    },
                    {
                        "name": "Ruiyang Ke"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Guochao Jiang"
                    },
                    {
                        "name": "Yanda Li"
                    },
                    {
                        "name": "Deqing Yang"
                    },
                    {
                        "name": "Jiaqing Liang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqing Liang"
                },
                "author": "Jiaqing Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11455v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11455v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15710v2",
                "updated": "2024-08-29T14:47:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    47,
                    37,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-28T11:18:06Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    11,
                    18,
                    6,
                    2,
                    241,
                    0
                ],
                "title": "Conan-embedding: General Text Embedding with More and Better Negative\n  Samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conan-embedding: General Text Embedding with More and Better Negative\n  Samples"
                },
                "summary": "With the growing popularity of RAG, the capabilities of embedding models are\ngaining increasing attention. Embedding models are primarily trained through\ncontrastive loss learning, with negative examples being a key component.\nPrevious work has proposed various hard negative mining strategies, but these\nstrategies are typically employed as preprocessing steps. In this paper, we\npropose the conan-embedding model, which maximizes the utilization of more and\nhigher-quality negative examples. Specifically, since the model's ability to\nhandle preprocessed negative examples evolves during training, we propose\ndynamic hard negative mining method to expose the model to more challenging\nnegative examples throughout the training process. Secondly, contrastive\nlearning requires as many negative examples as possible but is limited by GPU\nmemory constraints. Therefore, we use a Cross-GPU balancing Loss to provide\nmore negative examples for embedding training and balance the batch size across\nmultiple tasks. Moreover, we also discovered that the prompt-response pairs\nfrom LLMs can be used for embedding training. Our approach effectively enhances\nthe capabilities of embedding models, currently ranking first on the Chinese\nleaderboard of Massive text embedding benchmark",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing popularity of RAG, the capabilities of embedding models are\ngaining increasing attention. Embedding models are primarily trained through\ncontrastive loss learning, with negative examples being a key component.\nPrevious work has proposed various hard negative mining strategies, but these\nstrategies are typically employed as preprocessing steps. In this paper, we\npropose the conan-embedding model, which maximizes the utilization of more and\nhigher-quality negative examples. Specifically, since the model's ability to\nhandle preprocessed negative examples evolves during training, we propose\ndynamic hard negative mining method to expose the model to more challenging\nnegative examples throughout the training process. Secondly, contrastive\nlearning requires as many negative examples as possible but is limited by GPU\nmemory constraints. Therefore, we use a Cross-GPU balancing Loss to provide\nmore negative examples for embedding training and balance the batch size across\nmultiple tasks. Moreover, we also discovered that the prompt-response pairs\nfrom LLMs can be used for embedding training. Our approach effectively enhances\nthe capabilities of embedding models, currently ranking first on the Chinese\nleaderboard of Massive text embedding benchmark"
                },
                "authors": [
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Tang"
                    },
                    {
                        "name": "Shizhe Chen"
                    },
                    {
                        "name": "Xi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xi Chen"
                },
                "author": "Xi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16534v1",
                "updated": "2024-08-29T14:17:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    17,
                    39,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T14:17:39Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    17,
                    39,
                    3,
                    242,
                    0
                ],
                "title": "Tight bound on neutron-star radius with quasiperiodic oscillations in\n  short gamma-ray bursts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight bound on neutron-star radius with quasiperiodic oscillations in\n  short gamma-ray bursts"
                },
                "summary": "Quasiperiodic oscillations (QPOs) have been recently discovered in the short\ngamma-ray bursts (GRBs) 910711 and 931101B. Their frequencies are consistent\nwith those of the radial and quadrupolar oscillations of binary neutron star\nmerger remnants, as obtained in numerical relativity simulations. These\nsimulations reveal quasiuniversal relations between the remnant oscillation\nfrequencies and the tidal coupling constant of the binaries. Under the\nassumption that the observed QPOs are due to these postmerger oscillations, we\nuse the frequency-tide relations in a Bayesian framework to infer the source\nredshift, as well as the chirp mass and the binary tidal deformability of the\nbinary neutron star progenitors for GRBs 910711 and 931101B. We further use\nthis inference to estimate bounds on the mass-radius relation for neutron\nstars. By combining the estimates from the two GRBs, we find a 68\\% credible\nrange $R_{1.4}=12.48^{+0.41}_{-0.41}$~km for the radius of a neutron star with\nmass $M=1.4$~M$_\\odot$, which is one of the tightest bounds to date.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasiperiodic oscillations (QPOs) have been recently discovered in the short\ngamma-ray bursts (GRBs) 910711 and 931101B. Their frequencies are consistent\nwith those of the radial and quadrupolar oscillations of binary neutron star\nmerger remnants, as obtained in numerical relativity simulations. These\nsimulations reveal quasiuniversal relations between the remnant oscillation\nfrequencies and the tidal coupling constant of the binaries. Under the\nassumption that the observed QPOs are due to these postmerger oscillations, we\nuse the frequency-tide relations in a Bayesian framework to infer the source\nredshift, as well as the chirp mass and the binary tidal deformability of the\nbinary neutron star progenitors for GRBs 910711 and 931101B. We further use\nthis inference to estimate bounds on the mass-radius relation for neutron\nstars. By combining the estimates from the two GRBs, we find a 68\\% credible\nrange $R_{1.4}=12.48^{+0.41}_{-0.41}$~km for the radius of a neutron star with\nmass $M=1.4$~M$_\\odot$, which is one of the tightest bounds to date."
                },
                "authors": [
                    {
                        "name": "Victor Guedes"
                    },
                    {
                        "name": "David Radice"
                    },
                    {
                        "name": "Cecilia Chirenti"
                    },
                    {
                        "name": "Kent Yagi"
                    }
                ],
                "author_detail": {
                    "name": "Kent Yagi"
                },
                "author": "Kent Yagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16546v1",
                "updated": "2024-08-29T14:09:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    9,
                    37,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T14:09:37Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    9,
                    37,
                    3,
                    242,
                    0
                ],
                "title": "RAVE for Speech: Efficient Voice Conversion at High Sampling Rates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAVE for Speech: Efficient Voice Conversion at High Sampling Rates"
                },
                "summary": "Voice conversion has gained increasing popularity within the field of audio\nmanipulation and speech synthesis. Often, the main objective is to transfer the\ninput identity to that of a target speaker without changing its linguistic\ncontent. While current work provides high-fidelity solutions they rarely focus\non model simplicity, high-sampling rate environments or stream-ability. By\nincorporating speech representation learning into a generative timbre transfer\nmodel, traditionally created for musical purposes, we investigate the realm of\nvoice conversion generated directly in the time domain at high sampling rates.\nMore specifically, we guide the latent space of a baseline model towards\nlinguistically relevant representations and condition it on external speaker\ninformation. Through objective and subjective assessments, we demonstrate that\nthe proposed solution can attain levels of naturalness, quality, and\nintelligibility comparable to those of a state-of-the-art solution for seen\nspeakers, while significantly decreasing inference time. However, despite the\npresence of target speaker characteristics in the converted output, the actual\nsimilarity to unseen speakers remains a challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voice conversion has gained increasing popularity within the field of audio\nmanipulation and speech synthesis. Often, the main objective is to transfer the\ninput identity to that of a target speaker without changing its linguistic\ncontent. While current work provides high-fidelity solutions they rarely focus\non model simplicity, high-sampling rate environments or stream-ability. By\nincorporating speech representation learning into a generative timbre transfer\nmodel, traditionally created for musical purposes, we investigate the realm of\nvoice conversion generated directly in the time domain at high sampling rates.\nMore specifically, we guide the latent space of a baseline model towards\nlinguistically relevant representations and condition it on external speaker\ninformation. Through objective and subjective assessments, we demonstrate that\nthe proposed solution can attain levels of naturalness, quality, and\nintelligibility comparable to those of a state-of-the-art solution for seen\nspeakers, while significantly decreasing inference time. However, despite the\npresence of target speaker characteristics in the converted output, the actual\nsimilarity to unseen speakers remains a challenge."
                },
                "authors": [
                    {
                        "name": "Anders R. Bargum"
                    },
                    {
                        "name": "Simon Lajboschitz"
                    },
                    {
                        "name": "Cumhur Erkut"
                    }
                ],
                "author_detail": {
                    "name": "Cumhur Erkut"
                },
                "author": "Cumhur Erkut",
                "arxiv_comment": "Accepted for publication in Proceedings of the 27th International\n  Conference on Digital Audio Effects (DAFx24), Guildford, United Kingdom, 3 -\n  7 September 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01805v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01805v4",
                "updated": "2024-08-29T14:05:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    5,
                    44,
                    3,
                    242,
                    0
                ],
                "published": "2024-02-02T09:45:33Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    9,
                    45,
                    33,
                    4,
                    33,
                    0
                ],
                "title": "Can LLMs perform structured graph reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs perform structured graph reasoning?"
                },
                "summary": "Pretrained Large Language Models (LLMs) have demonstrated various reasoning\ncapabilities through language-based prompts alone, particularly in unstructured\ntask settings (tasks purely based on language semantics). However, LLMs often\nstruggle with structured tasks, because of the inherent incompatibility of\ninput representation. Reducing structured tasks to uni-dimensional language\nsemantics often renders the problem trivial. Keeping the trade-off between LLM\ncompatibility and structure complexity in mind, we design various graph\nreasoning tasks as a proxy to semi-structured tasks in this paper, in order to\ntest the ability to navigate through representations beyond plain text in\nvarious LLMs. Particularly, we design 10 distinct problems of graph traversal,\neach representing increasing levels of complexity, and benchmark 5 different\ninstruct-finetuned LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) on the\naforementioned tasks. Further, we analyse the performance of models across\nvarious settings such as varying sizes of graphs as well as different forms of\nk-shot prompting. We highlight various limitations, biases and properties of\nLLMs through this benchmarking process, such as an inverse relation to the\naverage degrees of freedom of traversal per node in graphs, the overall\nnegative impact of k-shot prompting on graph reasoning tasks, and a positive\nresponse bias which prevents LLMs from identifying the absence of a valid\nsolution. Finally, we introduce a new prompting technique specially designed\nfor graph traversal tasks (PathCompare), which demonstrates a notable increase\nin the performance of LLMs in comparison to standard prompting techniques such\nas Chain-of-Thought (CoT).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained Large Language Models (LLMs) have demonstrated various reasoning\ncapabilities through language-based prompts alone, particularly in unstructured\ntask settings (tasks purely based on language semantics). However, LLMs often\nstruggle with structured tasks, because of the inherent incompatibility of\ninput representation. Reducing structured tasks to uni-dimensional language\nsemantics often renders the problem trivial. Keeping the trade-off between LLM\ncompatibility and structure complexity in mind, we design various graph\nreasoning tasks as a proxy to semi-structured tasks in this paper, in order to\ntest the ability to navigate through representations beyond plain text in\nvarious LLMs. Particularly, we design 10 distinct problems of graph traversal,\neach representing increasing levels of complexity, and benchmark 5 different\ninstruct-finetuned LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) on the\naforementioned tasks. Further, we analyse the performance of models across\nvarious settings such as varying sizes of graphs as well as different forms of\nk-shot prompting. We highlight various limitations, biases and properties of\nLLMs through this benchmarking process, such as an inverse relation to the\naverage degrees of freedom of traversal per node in graphs, the overall\nnegative impact of k-shot prompting on graph reasoning tasks, and a positive\nresponse bias which prevents LLMs from identifying the absence of a valid\nsolution. Finally, we introduce a new prompting technique specially designed\nfor graph traversal tasks (PathCompare), which demonstrates a notable increase\nin the performance of LLMs in comparison to standard prompting techniques such\nas Chain-of-Thought (CoT)."
                },
                "authors": [
                    {
                        "name": "Palaash Agrawal"
                    },
                    {
                        "name": "Shavak Vasania"
                    },
                    {
                        "name": "Cheston Tan"
                    }
                ],
                "author_detail": {
                    "name": "Cheston Tan"
                },
                "author": "Cheston Tan",
                "arxiv_comment": "International Conference on Pattern Recognition (ICPR), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01805v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01805v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16544v1",
                "updated": "2024-08-29T14:02:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    2,
                    47,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T14:02:47Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    2,
                    47,
                    3,
                    242,
                    0
                ],
                "title": "Spurfies: Sparse Surface Reconstruction using Local Geometry Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spurfies: Sparse Surface Reconstruction using Local Geometry Priors"
                },
                "summary": "We introduce Spurfies, a novel method for sparse-view surface reconstruction\nthat disentangles appearance and geometry information to utilize local geometry\npriors trained on synthetic data. Recent research heavily focuses on 3D\nreconstruction using dense multi-view setups, typically requiring hundreds of\nimages. However, these methods often struggle with few-view scenarios. Existing\nsparse-view reconstruction techniques often rely on multi-view stereo networks\nthat need to learn joint priors for geometry and appearance from a large amount\nof data. In contrast, we introduce a neural point representation that\ndisentangles geometry and appearance to train a local geometry prior using a\nsubset of the synthetic ShapeNet dataset only. During inference, we utilize\nthis surface prior as additional constraint for surface and appearance\nreconstruction from sparse input views via differentiable volume rendering,\nrestricting the space of possible solutions. We validate the effectiveness of\nour method on the DTU dataset and demonstrate that it outperforms previous\nstate of the art by 35% in surface quality while achieving competitive novel\nview synthesis quality. Moreover, in contrast to previous works, our method can\nbe applied to larger, unbounded scenes, such as Mip-NeRF 360.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Spurfies, a novel method for sparse-view surface reconstruction\nthat disentangles appearance and geometry information to utilize local geometry\npriors trained on synthetic data. Recent research heavily focuses on 3D\nreconstruction using dense multi-view setups, typically requiring hundreds of\nimages. However, these methods often struggle with few-view scenarios. Existing\nsparse-view reconstruction techniques often rely on multi-view stereo networks\nthat need to learn joint priors for geometry and appearance from a large amount\nof data. In contrast, we introduce a neural point representation that\ndisentangles geometry and appearance to train a local geometry prior using a\nsubset of the synthetic ShapeNet dataset only. During inference, we utilize\nthis surface prior as additional constraint for surface and appearance\nreconstruction from sparse input views via differentiable volume rendering,\nrestricting the space of possible solutions. We validate the effectiveness of\nour method on the DTU dataset and demonstrate that it outperforms previous\nstate of the art by 35% in surface quality while achieving competitive novel\nview synthesis quality. Moreover, in contrast to previous works, our method can\nbe applied to larger, unbounded scenes, such as Mip-NeRF 360."
                },
                "authors": [
                    {
                        "name": "Kevin Raj"
                    },
                    {
                        "name": "Christopher Wewer"
                    },
                    {
                        "name": "Raza Yunus"
                    },
                    {
                        "name": "Eddy Ilg"
                    },
                    {
                        "name": "Jan Eric Lenssen"
                    }
                ],
                "author_detail": {
                    "name": "Jan Eric Lenssen"
                },
                "author": "Jan Eric Lenssen",
                "arxiv_comment": "https://geometric-rl.mpi-inf.mpg.de/spurfies/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16542v1",
                "updated": "2024-08-29T14:00:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    0,
                    57,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T14:00:57Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    0,
                    57,
                    3,
                    242,
                    0
                ],
                "title": "SALSA: Speedy ASR-LLM Synchronous Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALSA: Speedy ASR-LLM Synchronous Aggregation"
                },
                "summary": "Harnessing pre-trained LLMs to improve ASR systems, particularly for\nlow-resource languages, is now an emerging area of research. Existing methods\nrange from using LLMs for ASR error correction to tightly coupled systems that\nreplace the ASR decoder with the LLM. These approaches either increase decoding\ntime or require expensive training of the cross-attention layers. We propose\nSALSA, which couples the decoder layers of the ASR to the LLM decoder, while\nsynchronously advancing both decoders. Such coupling is performed with a simple\nprojection of the last decoder state, and is thus significantly more training\nefficient than earlier approaches. A challenge of our proposed coupling is\nhandling the mismatch between the tokenizers of the LLM and ASR systems. We\nhandle this mismatch using cascading tokenization with respect to the LLM and\nASR vocabularies. We evaluate SALSA on 8 low-resource languages in the FLEURS\nbenchmark, yielding substantial WER reductions of up to 38%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing pre-trained LLMs to improve ASR systems, particularly for\nlow-resource languages, is now an emerging area of research. Existing methods\nrange from using LLMs for ASR error correction to tightly coupled systems that\nreplace the ASR decoder with the LLM. These approaches either increase decoding\ntime or require expensive training of the cross-attention layers. We propose\nSALSA, which couples the decoder layers of the ASR to the LLM decoder, while\nsynchronously advancing both decoders. Such coupling is performed with a simple\nprojection of the last decoder state, and is thus significantly more training\nefficient than earlier approaches. A challenge of our proposed coupling is\nhandling the mismatch between the tokenizers of the LLM and ASR systems. We\nhandle this mismatch using cascading tokenization with respect to the LLM and\nASR vocabularies. We evaluate SALSA on 8 low-resource languages in the FLEURS\nbenchmark, yielding substantial WER reductions of up to 38%."
                },
                "authors": [
                    {
                        "name": "Ashish Mittal"
                    },
                    {
                        "name": "Darshan Prabhu"
                    },
                    {
                        "name": "Sunita Sarawagi"
                    },
                    {
                        "name": "Preethi Jyothi"
                    }
                ],
                "author_detail": {
                    "name": "Preethi Jyothi"
                },
                "author": "Preethi Jyothi",
                "arxiv_comment": "Accepted to INTERSPEECH 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16541v1",
                "updated": "2024-08-29T13:58:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    58,
                    38,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T13:58:38Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    58,
                    38,
                    3,
                    242,
                    0
                ],
                "title": "Subpolar Gyre Variability in CMIP6 Models: Is there a Mechanism for\n  Bistability?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subpolar Gyre Variability in CMIP6 Models: Is there a Mechanism for\n  Bistability?"
                },
                "summary": "The subpolar gyre is at risk of crossing a tipping point which would result\nin the collapse of convection in the Labrador Sea. It is important to\nunderstand the mechanisms at play and how they are represented in climate\nmodels. In this study we use causal inference to verify whether the proposed\nmechanism for bistability of the subpolar gyre is represented in CMIP6 models.\nIn many models an increase of sea surface salinity leads to a deepening of the\nmixed layer resulting in a cooling of the water at intermediate depth, in line\nwith theory. The feedback from the subsurface temperature through density to\nthe strength of the gyre circulation is more ambiguous, with fewer models\nindicating a significant link. Those that do show a significant link do not\nagree on its sign. One model (CESM2) contains all interactions, with both a\nnegative and delayed positive feedback loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The subpolar gyre is at risk of crossing a tipping point which would result\nin the collapse of convection in the Labrador Sea. It is important to\nunderstand the mechanisms at play and how they are represented in climate\nmodels. In this study we use causal inference to verify whether the proposed\nmechanism for bistability of the subpolar gyre is represented in CMIP6 models.\nIn many models an increase of sea surface salinity leads to a deepening of the\nmixed layer resulting in a cooling of the water at intermediate depth, in line\nwith theory. The feedback from the subsurface temperature through density to\nthe strength of the gyre circulation is more ambiguous, with fewer models\nindicating a significant link. Those that do show a significant link do not\nagree on its sign. One model (CESM2) contains all interactions, with both a\nnegative and delayed positive feedback loop."
                },
                "authors": [
                    {
                        "name": "Swinda K. J. Falkena"
                    },
                    {
                        "name": "Anna S. von der Heydt"
                    }
                ],
                "author_detail": {
                    "name": "Anna S. von der Heydt"
                },
                "author": "Anna S. von der Heydt",
                "arxiv_comment": "Submitted to Geophysical Research Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16527v1",
                "updated": "2024-08-29T13:39:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    39,
                    1,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T13:39:01Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    39,
                    1,
                    3,
                    242,
                    0
                ],
                "title": "Multitask learning for improved scour detection: A dynamic wave tank\n  study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multitask learning for improved scour detection: A dynamic wave tank\n  study"
                },
                "summary": "Population-based structural health monitoring (PBSHM), aims to share\ninformation between members of a population. An offshore wind (OW) farm could\nbe considered as a population of nominally-identical wind-turbine structures.\nHowever, benign variations exist among members, such as geometry, sea-bed\nconditions and temperature differences. These factors could influence\nstructural properties and therefore the dynamic response, making it more\ndifficult to detect structural problems via traditional SHM techniques.\n  This paper explores the use of a Bayesian hierarchical model as a means of\nmultitask learning, to infer foundation stiffness distribution parameters at\nboth population and local levels. To do this, observations of natural frequency\nfrom populations of structures were first generated from both numerical and\nexperimental models. These observations were then used in a partially-pooled\nBayesian hierarchical model in tandem with surrogate FE models of the\nstructures to infer foundation stiffness parameters. Finally, it is\ndemonstrated how the learned parameters may be used as a basis to perform more\nrobust anomaly detection (as compared to a no-pooling approach) e.g. as a\nresult of scour.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Population-based structural health monitoring (PBSHM), aims to share\ninformation between members of a population. An offshore wind (OW) farm could\nbe considered as a population of nominally-identical wind-turbine structures.\nHowever, benign variations exist among members, such as geometry, sea-bed\nconditions and temperature differences. These factors could influence\nstructural properties and therefore the dynamic response, making it more\ndifficult to detect structural problems via traditional SHM techniques.\n  This paper explores the use of a Bayesian hierarchical model as a means of\nmultitask learning, to infer foundation stiffness distribution parameters at\nboth population and local levels. To do this, observations of natural frequency\nfrom populations of structures were first generated from both numerical and\nexperimental models. These observations were then used in a partially-pooled\nBayesian hierarchical model in tandem with surrogate FE models of the\nstructures to infer foundation stiffness parameters. Finally, it is\ndemonstrated how the learned parameters may be used as a basis to perform more\nrobust anomaly detection (as compared to a no-pooling approach) e.g. as a\nresult of scour."
                },
                "authors": [
                    {
                        "name": "Simon M. Brealy"
                    },
                    {
                        "name": "Aidan J. Hughes"
                    },
                    {
                        "name": "Tina A. Dardeno"
                    },
                    {
                        "name": "Lawrence A. Bull"
                    },
                    {
                        "name": "Robin S. Mills"
                    },
                    {
                        "name": "Nikolaos Dervilis"
                    },
                    {
                        "name": "Keith Worden"
                    }
                ],
                "author_detail": {
                    "name": "Keith Worden"
                },
                "author": "Keith Worden",
                "arxiv_comment": "25 pages, 12 figures, early work features in ISWHM 2023 conference\n  proceedings and available here: arXiv:2402.19295. Submitted to the Renewable\n  Energy journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12347v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12347v3",
                "updated": "2024-08-29T13:33:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    33,
                    40,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-22T12:43:14Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    43,
                    14,
                    3,
                    235,
                    0
                ],
                "title": "Preregistration does not improve the transparent evaluation of severity\n  in Popper's philosophy of science or when deviations are allowed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preregistration does not improve the transparent evaluation of severity\n  in Popper's philosophy of science or when deviations are allowed"
                },
                "summary": "One justification for preregistering research hypotheses, methods, and\nanalyses is that it improves the transparent evaluation of the severity of\nhypothesis tests. In this article, I consider two cases in which\npreregistration does not improve this evaluation. First, I argue that, although\npreregistration can facilitate the transparent evaluation of severity in Mayo's\nerror statistical philosophy of science, it does not facilitate this evaluation\nin Popper's theory-centric approach. To illustrate, I show that associated\nconcerns about Type I error rate inflation are only relevant in the error\nstatistical approach and not in a theory-centric approach. Second, I argue that\na preregistered test procedure that allows deviations in its implementation\ndoes not provide a more transparent evaluation of Mayoian severity than a\nnon-preregistered procedure. In particular, I argue that sample-based\nvalidity-enhancing deviations cause an unknown inflation of the test\nprocedure's Type I (familywise) error rate and, consequently, an unknown\nreduction in its capability to license inferences severely. I conclude that\npreregistration does not improve the transparent evaluation of severity in\nPopper's philosophy of science or when deviations are allowed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One justification for preregistering research hypotheses, methods, and\nanalyses is that it improves the transparent evaluation of the severity of\nhypothesis tests. In this article, I consider two cases in which\npreregistration does not improve this evaluation. First, I argue that, although\npreregistration can facilitate the transparent evaluation of severity in Mayo's\nerror statistical philosophy of science, it does not facilitate this evaluation\nin Popper's theory-centric approach. To illustrate, I show that associated\nconcerns about Type I error rate inflation are only relevant in the error\nstatistical approach and not in a theory-centric approach. Second, I argue that\na preregistered test procedure that allows deviations in its implementation\ndoes not provide a more transparent evaluation of Mayoian severity than a\nnon-preregistered procedure. In particular, I argue that sample-based\nvalidity-enhancing deviations cause an unknown inflation of the test\nprocedure's Type I (familywise) error rate and, consequently, an unknown\nreduction in its capability to license inferences severely. I conclude that\npreregistration does not improve the transparent evaluation of severity in\nPopper's philosophy of science or when deviations are allowed."
                },
                "authors": [
                    {
                        "name": "Mark Rubin"
                    }
                ],
                "author_detail": {
                    "name": "Mark Rubin"
                },
                "author": "Mark Rubin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12347v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12347v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16515v1",
                "updated": "2024-08-29T13:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    26,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T13:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    26,
                    26,
                    3,
                    242,
                    0
                ],
                "title": "CanCal: Towards Real-time and Lightweight Ransomware Detection and\n  Response in Industrial Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CanCal: Towards Real-time and Lightweight Ransomware Detection and\n  Response in Industrial Environments"
                },
                "summary": "Ransomware attacks have emerged as one of the most significant cybersecurity\nthreats. Despite numerous proposed detection and defense methods, existing\napproaches face two fundamental limitations in large-scale industrial\napplications: intolerable system overheads and notorious alert fatigue. To\naddress these challenges, we propose CanCal, a real-time and lightweight\nransomware detection system. Specifically, CanCal selectively filters\nsuspicious processes by the monitoring layers and then performs in-depth\nbehavioral analysis to isolate ransomware activities from benign operations,\nminimizing alert fatigue while ensuring lightweight computational and storage\noverhead. The experimental results on a large-scale industrial\nenvironment~(1,761 ransomware, ~3 million events, continuous test over 5\nmonths) indicate that CanCal is as effective as state-of-the-art techniques\nwhile enabling rapid inference within 30ms and real-time response within a\nmaximum of 3 seconds. CanCal dramatically reduces average CPU utilization by\n91.04% (from 6.7% to 0.6%) and peak CPU utilization by 76.69% (from 26.6% to\n6.2%), while avoiding 76.50% (from 3,192 to 750) of the inspection efforts from\nsecurity analysts. By the time of this writing, CanCal has been integrated into\na commercial product and successfully deployed on 3.32 million endpoints for\nover a year. From March 2023 to April 2024, CanCal successfully detected and\nthwarted 61 ransomware attacks, demonstrating the effectiveness of CanCal in\ncombating sophisticated ransomware threats in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ransomware attacks have emerged as one of the most significant cybersecurity\nthreats. Despite numerous proposed detection and defense methods, existing\napproaches face two fundamental limitations in large-scale industrial\napplications: intolerable system overheads and notorious alert fatigue. To\naddress these challenges, we propose CanCal, a real-time and lightweight\nransomware detection system. Specifically, CanCal selectively filters\nsuspicious processes by the monitoring layers and then performs in-depth\nbehavioral analysis to isolate ransomware activities from benign operations,\nminimizing alert fatigue while ensuring lightweight computational and storage\noverhead. The experimental results on a large-scale industrial\nenvironment~(1,761 ransomware, ~3 million events, continuous test over 5\nmonths) indicate that CanCal is as effective as state-of-the-art techniques\nwhile enabling rapid inference within 30ms and real-time response within a\nmaximum of 3 seconds. CanCal dramatically reduces average CPU utilization by\n91.04% (from 6.7% to 0.6%) and peak CPU utilization by 76.69% (from 26.6% to\n6.2%), while avoiding 76.50% (from 3,192 to 750) of the inspection efforts from\nsecurity analysts. By the time of this writing, CanCal has been integrated into\na commercial product and successfully deployed on 3.32 million endpoints for\nover a year. From March 2023 to April 2024, CanCal successfully detected and\nthwarted 61 ransomware attacks, demonstrating the effectiveness of CanCal in\ncombating sophisticated ransomware threats in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Feng Dong"
                    },
                    {
                        "name": "Hangfeng Yang"
                    },
                    {
                        "name": "Jingheng Xu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_doi": "10.1145/3658644.3690269",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690269",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.16515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the 2024 ACM SIGSAC Conference on Computer and\n  Communications Security (CCS'24), October 14--18, 2024, Salt Lake City",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11160v2",
                "updated": "2024-08-29T13:23:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    23,
                    15,
                    3,
                    242,
                    0
                ],
                "published": "2024-04-17T08:16:48Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    8,
                    16,
                    48,
                    2,
                    108,
                    0
                ],
                "title": "Low-Cost Language Models: Survey and Performance Evaluation on Python\n  Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Cost Language Models: Survey and Performance Evaluation on Python\n  Code Generation"
                },
                "summary": "Large Language Models (LLMs) have become a popular choice for many Natural\nLanguage Processing (NLP) tasks due to their versatility and ability to produce\nhigh-quality results. Specifically, they are increasingly used for automatic\ncode generation to help developers tackle repetitive coding tasks. However,\nLLMs' substantial computational and memory requirements often make them\ninaccessible to users with limited resources. This paper focuses on very\nlow-cost models which offer a more accessible alternative to resource-intensive\nLLMs. We notably: (1) propose a thorough semi-manual evaluation of their\nperformance in generating Python code, (2) introduce a Chain-of-Thought (CoT)\nprompting strategy to improve model reasoning and code quality, and (3) propose\na new dataset of 60 programming problems, with varied difficulty levels,\ndesigned to extend existing benchmarks like HumanEval and EvalPlus. Our\nfindings show that some low-cost compatible models achieve competitive results\ncompared to larger models like ChatGPT despite using significantly fewer\nresources. We will make our dataset and prompts publicly available to support\nfurther research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become a popular choice for many Natural\nLanguage Processing (NLP) tasks due to their versatility and ability to produce\nhigh-quality results. Specifically, they are increasingly used for automatic\ncode generation to help developers tackle repetitive coding tasks. However,\nLLMs' substantial computational and memory requirements often make them\ninaccessible to users with limited resources. This paper focuses on very\nlow-cost models which offer a more accessible alternative to resource-intensive\nLLMs. We notably: (1) propose a thorough semi-manual evaluation of their\nperformance in generating Python code, (2) introduce a Chain-of-Thought (CoT)\nprompting strategy to improve model reasoning and code quality, and (3) propose\na new dataset of 60 programming problems, with varied difficulty levels,\ndesigned to extend existing benchmarks like HumanEval and EvalPlus. Our\nfindings show that some low-cost compatible models achieve competitive results\ncompared to larger models like ChatGPT despite using significantly fewer\nresources. We will make our dataset and prompts publicly available to support\nfurther research."
                },
                "authors": [
                    {
                        "name": "Jessica López Espejel"
                    },
                    {
                        "name": "Mahaman Sanoussi Yahaya Alassan"
                    },
                    {
                        "name": "Merieme Bouhandi"
                    },
                    {
                        "name": "Walid Dahhane"
                    },
                    {
                        "name": "El Hassane Ettifouri"
                    }
                ],
                "author_detail": {
                    "name": "El Hassane Ettifouri"
                },
                "author": "El Hassane Ettifouri",
                "arxiv_comment": "Under review at Elsevier's Engineering Applications of Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15126v2",
                "updated": "2024-08-29T13:21:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    21,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-27T15:07:27Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    7,
                    27,
                    1,
                    240,
                    0
                ],
                "title": "Force-Guided Bridge Matching for Full-Atom Time-Coarsened Dynamics of\n  Peptides",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Force-Guided Bridge Matching for Full-Atom Time-Coarsened Dynamics of\n  Peptides"
                },
                "summary": "Molecular Dynamics (MD) simulations are irreplaceable and ubiquitous in\nfields of materials science, chemistry, pharmacology just to name a few.\nConventional MD simulations are plagued by numerical stability as well as long\nequilibration time issues, which limits broader applications of MD simulations.\nRecently, a surge of deep learning approaches have been devised for\ntime-coarsened dynamics, which learns the state transition mechanism over much\nlarger time scales to overcome these limitations. However, only a few methods\ntarget the underlying Boltzmann distribution by resampling techniques, where\nproposals are rarely accepted as new states with low efficiency. In this work,\nwe propose a force-guided bridge matching model, FBM, a novel framework that\nfirst incorporates physical priors into bridge matching for full-atom\ntime-coarsened dynamics. With the guidance of our well-designed intermediate\nforce field, FBM is feasible to target the Boltzmann-like distribution by\ndirect inference without extra steps. Experiments on small peptides verify our\nsuperiority in terms of comprehensive metrics and demonstrate transferability\nto unseen peptide systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular Dynamics (MD) simulations are irreplaceable and ubiquitous in\nfields of materials science, chemistry, pharmacology just to name a few.\nConventional MD simulations are plagued by numerical stability as well as long\nequilibration time issues, which limits broader applications of MD simulations.\nRecently, a surge of deep learning approaches have been devised for\ntime-coarsened dynamics, which learns the state transition mechanism over much\nlarger time scales to overcome these limitations. However, only a few methods\ntarget the underlying Boltzmann distribution by resampling techniques, where\nproposals are rarely accepted as new states with low efficiency. In this work,\nwe propose a force-guided bridge matching model, FBM, a novel framework that\nfirst incorporates physical priors into bridge matching for full-atom\ntime-coarsened dynamics. With the guidance of our well-designed intermediate\nforce field, FBM is feasible to target the Boltzmann-like distribution by\ndirect inference without extra steps. Experiments on small peptides verify our\nsuperiority in terms of comprehensive metrics and demonstrate transferability\nto unseen peptide systems."
                },
                "authors": [
                    {
                        "name": "Ziyang Yu"
                    },
                    {
                        "name": "Wenbing Huang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00287v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00287v4",
                "updated": "2024-08-29T13:17:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    17,
                    52,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-01T05:14:37Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    5,
                    14,
                    37,
                    4,
                    61,
                    0
                ],
                "title": "Neural Simulation-Based Inference of the Neutron Star Equation of State\n  directly from Telescope Spectra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Simulation-Based Inference of the Neutron Star Equation of State\n  directly from Telescope Spectra"
                },
                "summary": "Neutron stars provide a unique opportunity to study strongly interacting\nmatter under extreme density conditions. The intricacies of matter inside\nneutron stars and their equation of state are not directly visible, but\ndetermine bulk properties, such as mass and radius, which affect the star's\nthermal X-ray emissions. However, the telescope spectra of these emissions are\nalso affected by the stellar distance, hydrogen column, and effective surface\ntemperature, which are not always well-constrained. Uncertainties on these\nnuisance parameters must be accounted for when making a robust estimation of\nthe equation of state. In this study, we develop a novel methodology that, for\nthe first time, can infer the full posterior distribution of both the equation\nof state and nuisance parameters directly from telescope observations. This\nmethod relies on the use of neural likelihood estimation, in which normalizing\nflows use samples of simulated telescope data to learn the likelihood of the\nneutron star spectra as a function of these parameters, coupled with\nHamiltonian Monte Carlo methods to efficiently sample from the corresponding\nposterior distribution. Our approach surpasses the accuracy of previous\nmethods, improves the interpretability of the results by providing access to\nthe full posterior distribution, and naturally scales to a growing number of\nneutron star observations expected in the coming years.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron stars provide a unique opportunity to study strongly interacting\nmatter under extreme density conditions. The intricacies of matter inside\nneutron stars and their equation of state are not directly visible, but\ndetermine bulk properties, such as mass and radius, which affect the star's\nthermal X-ray emissions. However, the telescope spectra of these emissions are\nalso affected by the stellar distance, hydrogen column, and effective surface\ntemperature, which are not always well-constrained. Uncertainties on these\nnuisance parameters must be accounted for when making a robust estimation of\nthe equation of state. In this study, we develop a novel methodology that, for\nthe first time, can infer the full posterior distribution of both the equation\nof state and nuisance parameters directly from telescope observations. This\nmethod relies on the use of neural likelihood estimation, in which normalizing\nflows use samples of simulated telescope data to learn the likelihood of the\nneutron star spectra as a function of these parameters, coupled with\nHamiltonian Monte Carlo methods to efficiently sample from the corresponding\nposterior distribution. Our approach surpasses the accuracy of previous\nmethods, improves the interpretability of the results by providing access to\nthe full posterior distribution, and naturally scales to a growing number of\nneutron star observations expected in the coming years."
                },
                "authors": [
                    {
                        "name": "Len Brandes"
                    },
                    {
                        "name": "Chirag Modi"
                    },
                    {
                        "name": "Aishik Ghosh"
                    },
                    {
                        "name": "Delaney Farrell"
                    },
                    {
                        "name": "Lee Lindblom"
                    },
                    {
                        "name": "Lukas Heinrich"
                    },
                    {
                        "name": "Andrew W. Steiner"
                    },
                    {
                        "name": "Fridolin Weber"
                    },
                    {
                        "name": "Daniel Whiteson"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Whiteson"
                },
                "author": "Daniel Whiteson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00287v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00287v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16502v1",
                "updated": "2024-08-29T13:01:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    1,
                    42,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T13:01:42Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    1,
                    42,
                    3,
                    242,
                    0
                ],
                "title": "LLMs vs Established Text Augmentation Techniques for Classification:\n  When do the Benefits Outweight the Costs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs vs Established Text Augmentation Techniques for Classification:\n  When do the Benefits Outweight the Costs?"
                },
                "summary": "The generative large language models (LLMs) are increasingly being used for\ndata augmentation tasks, where text samples are LLM-paraphrased and then used\nfor classifier fine-tuning. However, a research that would confirm a clear\ncost-benefit advantage of LLMs over more established augmentation methods is\nlargely missing. To study if (and when) is the LLM-based augmentation\nadvantageous, we compared the effects of recent LLM augmentation methods with\nestablished ones on 6 datasets, 3 classifiers and 2 fine-tuning methods. We\nalso varied the number of seeds and collected samples to better explore the\ndownstream model accuracy space. Finally, we performed a cost-benefit analysis\nand show that LLM-based methods are worthy of deployment only when very small\nnumber of seeds is used. Moreover, in many cases, established methods lead to\nsimilar or better model accuracies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generative large language models (LLMs) are increasingly being used for\ndata augmentation tasks, where text samples are LLM-paraphrased and then used\nfor classifier fine-tuning. However, a research that would confirm a clear\ncost-benefit advantage of LLMs over more established augmentation methods is\nlargely missing. To study if (and when) is the LLM-based augmentation\nadvantageous, we compared the effects of recent LLM augmentation methods with\nestablished ones on 6 datasets, 3 classifiers and 2 fine-tuning methods. We\nalso varied the number of seeds and collected samples to better explore the\ndownstream model accuracy space. Finally, we performed a cost-benefit analysis\nand show that LLM-based methods are worthy of deployment only when very small\nnumber of seeds is used. Moreover, in many cases, established methods lead to\nsimilar or better model accuracies."
                },
                "authors": [
                    {
                        "name": "Jan Cegin"
                    },
                    {
                        "name": "Jakub Simko"
                    },
                    {
                        "name": "Peter Brusilovsky"
                    }
                ],
                "author_detail": {
                    "name": "Peter Brusilovsky"
                },
                "author": "Peter Brusilovsky",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16498v1",
                "updated": "2024-08-29T12:56:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    56,
                    6,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T12:56:06Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    56,
                    6,
                    3,
                    242,
                    0
                ],
                "title": "A Survey on Evaluating Large Language Models in Code Generation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Evaluating Large Language Models in Code Generation Tasks"
                },
                "summary": "This paper provides a comprehensive review of the current methods and metrics\nused to evaluate the performance of Large Language Models (LLMs) in code\ngeneration tasks. With the rapid growth in demand for automated software\ndevelopment, LLMs have demonstrated significant potential in the field of code\ngeneration. The paper begins by reviewing the historical development of LLMs\nand their applications in code generation. Next, it details various methods and\nmetrics for assessing the code generation capabilities of LLMs, including code\ncorrectness, efficiency, readability, and evaluation methods based on expert\nreview and user experience. The paper also evaluates the widely used benchmark\ndatasets, identifying their limitations and proposing directions for future\nimprovements. Specifically, the paper analyzes the performance of code\ngeneration models across different tasks by combining multiple evaluation\nmetrics, such as code compilation/interpretation success rates, unit test pass\nrates, and performance and efficiency metrics, to comprehensively assess the\npractical application of LLMs in code generation. Finally, the paper discusses\nthe challenges faced in evaluating LLMs in code generation, particularly how to\nensure the comprehensiveness and accuracy of evaluation methods and how to\nadapt to the evolving practices of software development. These analyses and\ndiscussions provide valuable insights for further optimizing and improving the\napplication of LLMs in code generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a comprehensive review of the current methods and metrics\nused to evaluate the performance of Large Language Models (LLMs) in code\ngeneration tasks. With the rapid growth in demand for automated software\ndevelopment, LLMs have demonstrated significant potential in the field of code\ngeneration. The paper begins by reviewing the historical development of LLMs\nand their applications in code generation. Next, it details various methods and\nmetrics for assessing the code generation capabilities of LLMs, including code\ncorrectness, efficiency, readability, and evaluation methods based on expert\nreview and user experience. The paper also evaluates the widely used benchmark\ndatasets, identifying their limitations and proposing directions for future\nimprovements. Specifically, the paper analyzes the performance of code\ngeneration models across different tasks by combining multiple evaluation\nmetrics, such as code compilation/interpretation success rates, unit test pass\nrates, and performance and efficiency metrics, to comprehensively assess the\npractical application of LLMs in code generation. Finally, the paper discusses\nthe challenges faced in evaluating LLMs in code generation, particularly how to\nensure the comprehensiveness and accuracy of evaluation methods and how to\nadapt to the evolving practices of software development. These analyses and\ndiscussions provide valuable insights for further optimizing and improving the\napplication of LLMs in code generation tasks."
                },
                "authors": [
                    {
                        "name": "Liguo Chen"
                    },
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Hongrui Jia"
                    },
                    {
                        "name": "Zhengran Zeng"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yijiang Xu"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Qing Gao"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Shikun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shikun Zhang"
                },
                "author": "Shikun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06702v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06702v3",
                "updated": "2024-08-29T12:27:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    27,
                    12,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-11T13:17:55Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    13,
                    17,
                    55,
                    0,
                    71,
                    0
                ],
                "title": "Fast Text-to-3D-Aware Face Generation and Manipulation via Direct\n  Cross-modal Mapping and Geometric Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Text-to-3D-Aware Face Generation and Manipulation via Direct\n  Cross-modal Mapping and Geometric Regularization"
                },
                "summary": "Text-to-3D-aware face (T3D Face) generation and manipulation is an emerging\nresearch hot spot in machine learning, which still suffers from low efficiency\nand poor quality. In this paper, we propose an End-to-End Efficient and\nEffective network for fast and accurate T3D face generation and manipulation,\ntermed $E^3$-FaceNet. Different from existing complex generation paradigms,\n$E^3$-FaceNet resorts to a direct mapping from text instructions to 3D-aware\nvisual space. We introduce a novel Style Code Enhancer to enhance cross-modal\nsemantic alignment, alongside an innovative Geometric Regularization objective\nto maintain consistency across multi-view generations. Extensive experiments on\nthree benchmark datasets demonstrate that $E^3$-FaceNet can not only achieve\npicture-like 3D face generation and manipulation, but also improve inference\nspeed by orders of magnitudes. For instance, compared with Latent3D,\n$E^3$-FaceNet speeds up the five-view generations by almost 470 times, while\nstill exceeding in generation quality. Our code is released at\nhttps://github.com/Aria-Zhangjl/E3-FaceNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-3D-aware face (T3D Face) generation and manipulation is an emerging\nresearch hot spot in machine learning, which still suffers from low efficiency\nand poor quality. In this paper, we propose an End-to-End Efficient and\nEffective network for fast and accurate T3D face generation and manipulation,\ntermed $E^3$-FaceNet. Different from existing complex generation paradigms,\n$E^3$-FaceNet resorts to a direct mapping from text instructions to 3D-aware\nvisual space. We introduce a novel Style Code Enhancer to enhance cross-modal\nsemantic alignment, alongside an innovative Geometric Regularization objective\nto maintain consistency across multi-view generations. Extensive experiments on\nthree benchmark datasets demonstrate that $E^3$-FaceNet can not only achieve\npicture-like 3D face generation and manipulation, but also improve inference\nspeed by orders of magnitudes. For instance, compared with Latent3D,\n$E^3$-FaceNet speeds up the five-view generations by almost 470 times, while\nstill exceeding in generation quality. Our code is released at\nhttps://github.com/Aria-Zhangjl/E3-FaceNet."
                },
                "authors": [
                    {
                        "name": "Jinlu Zhang"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Qiancheng Zheng"
                    },
                    {
                        "name": "Xiaoxiong Du"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06702v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06702v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11512v2",
                "updated": "2024-08-29T12:25:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    25,
                    14,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-21T10:44:10Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    44,
                    10,
                    2,
                    234,
                    0
                ],
                "title": "IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine\n  Translation"
                },
                "summary": "This paper introduces two multilingual systems, IKUN and IKUN-C, developed\nfor the general machine translation task in WMT24. IKUN and IKUN-C represent an\nopen system and a constrained system, respectively, built on Llama-3-8b and\nMistral-7B-v0.3. Both systems are designed to handle all 11 language directions\nusing a single model. According to automatic evaluation metrics, IKUN-C\nachieved 6 first-place and 3 second-place finishes among all constrained\nsystems, while IKUN secured 1 first-place and 2 second-place finishes across\nboth open and constrained systems. These encouraging results suggest that large\nlanguage models (LLMs) are nearing the level of proficiency required for\neffective multilingual machine translation. The systems are based on a\ntwo-stage approach: first, continuous pre-training on monolingual data in 10\nlanguages, followed by fine-tuning on high-quality parallel data for 11\nlanguage directions. The primary difference between IKUN and IKUN-C lies in\ntheir monolingual pre-training strategy. IKUN-C is pre-trained using\nconstrained monolingual data, whereas IKUN leverages monolingual data from the\nOSCAR dataset. In the second phase, both systems are fine-tuned on parallel\ndata sourced from NTREX, Flores, and WMT16-23 for all 11 language pairs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces two multilingual systems, IKUN and IKUN-C, developed\nfor the general machine translation task in WMT24. IKUN and IKUN-C represent an\nopen system and a constrained system, respectively, built on Llama-3-8b and\nMistral-7B-v0.3. Both systems are designed to handle all 11 language directions\nusing a single model. According to automatic evaluation metrics, IKUN-C\nachieved 6 first-place and 3 second-place finishes among all constrained\nsystems, while IKUN secured 1 first-place and 2 second-place finishes across\nboth open and constrained systems. These encouraging results suggest that large\nlanguage models (LLMs) are nearing the level of proficiency required for\neffective multilingual machine translation. The systems are based on a\ntwo-stage approach: first, continuous pre-training on monolingual data in 10\nlanguages, followed by fine-tuning on high-quality parallel data for 11\nlanguage directions. The primary difference between IKUN and IKUN-C lies in\ntheir monolingual pre-training strategy. IKUN-C is pre-trained using\nconstrained monolingual data, whereas IKUN leverages monolingual data from the\nOSCAR dataset. In the second phase, both systems are fine-tuned on parallel\ndata sourced from NTREX, Flores, and WMT16-23 for all 11 language pairs."
                },
                "authors": [
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Shahram Khadivi"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "arxiv_comment": "typo: 120K -> 12K vocabulary size",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09128v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09128v2",
                "updated": "2024-08-29T12:24:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    24,
                    54,
                    3,
                    242,
                    0
                ],
                "published": "2024-04-14T02:41:25Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    2,
                    41,
                    25,
                    6,
                    105,
                    0
                ],
                "title": "Data-driven AC Optimal Power Flow with Physics-informed Learning and\n  Calibrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven AC Optimal Power Flow with Physics-informed Learning and\n  Calibrations"
                },
                "summary": "The modern power grid is witnessing a shift in operations from traditional\ncontrol methods to more advanced operational mechanisms. Due to the nonconvex\nnature of the Alternating Current Optimal Power Flow (ACOPF) problem and the\nneed for operations with better granularity in the modern smart grid, system\noperators require a more efficient and reliable ACOPF solver. While data-driven\nACOPF methods excel in directly inferring the optimal solution based on power\ngrid demand, achieving both feasibility and optimality remains a challenge due\nto the NP-hardness of the problem. In this paper, we propose a physics-informed\nmachine learning model and a feasibility calibration algorithm to produce\nsolutions for the ACOPF problem. Notably, the machine learning model produces\nsolutions with a 0.5\\% and 1.4\\% optimality gap for IEEE bus 14 and 118 grids,\nrespectively. The feasibility correction algorithm converges for all test\nscenarios on bus 14 and achieves a 92.2% convergence rate on bus 118.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The modern power grid is witnessing a shift in operations from traditional\ncontrol methods to more advanced operational mechanisms. Due to the nonconvex\nnature of the Alternating Current Optimal Power Flow (ACOPF) problem and the\nneed for operations with better granularity in the modern smart grid, system\noperators require a more efficient and reliable ACOPF solver. While data-driven\nACOPF methods excel in directly inferring the optimal solution based on power\ngrid demand, achieving both feasibility and optimality remains a challenge due\nto the NP-hardness of the problem. In this paper, we propose a physics-informed\nmachine learning model and a feasibility calibration algorithm to produce\nsolutions for the ACOPF problem. Notably, the machine learning model produces\nsolutions with a 0.5\\% and 1.4\\% optimality gap for IEEE bus 14 and 118 grids,\nrespectively. The feasibility correction algorithm converges for all test\nscenarios on bus 14 and achieves a 92.2% convergence rate on bus 118."
                },
                "authors": [
                    {
                        "name": "Junfei Wang"
                    },
                    {
                        "name": "Pirathayini Srikantha"
                    }
                ],
                "author_detail": {
                    "name": "Pirathayini Srikantha"
                },
                "author": "Pirathayini Srikantha",
                "arxiv_comment": "6 pages, 3 figures, 1 algorithm and 2 tables. Submitted to\n  SmartGridComm2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09128v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09128v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08928v2",
                "updated": "2024-08-29T12:23:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    23,
                    4,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-13T19:36:03Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    19,
                    36,
                    3,
                    2,
                    73,
                    0
                ],
                "title": "Neuromorphic force-control in an industrial task: validating energy and\n  latency benefits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic force-control in an industrial task: validating energy and\n  latency benefits"
                },
                "summary": "As robots become smarter and more ubiquitous, optimizing the power\nconsumption of intelligent compute becomes imperative towards ensuring the\nsustainability of technological advancements. Neuromorphic computing hardware\nmakes use of biologically inspired neural architectures to achieve energy and\nlatency improvements compared to conventional von Neumann computing\narchitecture. Applying these benefits to robots has been demonstrated in\nseveral works in the field of neurorobotics, typically on relatively simple\ncontrol tasks. Here, we introduce an example of neuromorphic computing applied\nto the real-world industrial task of object insertion. We trained a spiking\nneural network (SNN) to perform force-torque feedback control using a\nreinforcement learning approach in simulation. We then ported the SNN to the\nIntel neuromorphic research chip Loihi interfaced with a KUKA robotic arm. At\ninference time we show latency competitive with current CPU/GPU architectures,\nand one order of magnitude less energy usage in comparison to state-of-the-art\nlow-energy edge-hardware. We offer this example as a proof of concept\nimplementation of a neuromoprhic controller in real-world robotic setting,\nhighlighting the benefits of neuromorphic hardware for the development of\nintelligent controllers for robots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As robots become smarter and more ubiquitous, optimizing the power\nconsumption of intelligent compute becomes imperative towards ensuring the\nsustainability of technological advancements. Neuromorphic computing hardware\nmakes use of biologically inspired neural architectures to achieve energy and\nlatency improvements compared to conventional von Neumann computing\narchitecture. Applying these benefits to robots has been demonstrated in\nseveral works in the field of neurorobotics, typically on relatively simple\ncontrol tasks. Here, we introduce an example of neuromorphic computing applied\nto the real-world industrial task of object insertion. We trained a spiking\nneural network (SNN) to perform force-torque feedback control using a\nreinforcement learning approach in simulation. We then ported the SNN to the\nIntel neuromorphic research chip Loihi interfaced with a KUKA robotic arm. At\ninference time we show latency competitive with current CPU/GPU architectures,\nand one order of magnitude less energy usage in comparison to state-of-the-art\nlow-energy edge-hardware. We offer this example as a proof of concept\nimplementation of a neuromoprhic controller in real-world robotic setting,\nhighlighting the benefits of neuromorphic hardware for the development of\nintelligent controllers for robots."
                },
                "authors": [
                    {
                        "name": "Camilo Amaya"
                    },
                    {
                        "name": "Evan Eames"
                    },
                    {
                        "name": "Gintautas Palinauskas"
                    },
                    {
                        "name": "Alexander Perzylo"
                    },
                    {
                        "name": "Yulia Sandamirskaya"
                    },
                    {
                        "name": "Axel von Arnim"
                    }
                ],
                "author_detail": {
                    "name": "Axel von Arnim"
                },
                "author": "Axel von Arnim",
                "arxiv_comment": "Accepted at IROS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16482v1",
                "updated": "2024-08-29T12:18:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    18,
                    4,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T12:18:04Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    18,
                    4,
                    3,
                    242,
                    0
                ],
                "title": "Self-Alignment: Improving Alignment of Cultural Values in LLMs via\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Alignment: Improving Alignment of Cultural Values in LLMs via\n  In-Context Learning"
                },
                "summary": "Improving the alignment of Large Language Models (LLMs) with respect to the\ncultural values that they encode has become an increasingly important topic. In\nthis work, we study whether we can exploit existing knowledge about cultural\nvalues at inference time to adjust model responses to cultural value probes. We\npresent a simple and inexpensive method that uses a combination of in-context\nlearning (ICL) and human survey data, and show that we can improve the\nalignment to cultural values across 5 models that include both English-centric\nand multilingual LLMs. Importantly, we show that our method could prove useful\nin test languages other than English and can improve alignment to the cultural\nvalues that correspond to a range of culturally diverse countries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the alignment of Large Language Models (LLMs) with respect to the\ncultural values that they encode has become an increasingly important topic. In\nthis work, we study whether we can exploit existing knowledge about cultural\nvalues at inference time to adjust model responses to cultural value probes. We\npresent a simple and inexpensive method that uses a combination of in-context\nlearning (ICL) and human survey data, and show that we can improve the\nalignment to cultural values across 5 models that include both English-centric\nand multilingual LLMs. Importantly, we show that our method could prove useful\nin test languages other than English and can improve alignment to the cultural\nvalues that correspond to a range of culturally diverse countries."
                },
                "authors": [
                    {
                        "name": "Rochelle Choenni"
                    },
                    {
                        "name": "Ekaterina Shutova"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Shutova"
                },
                "author": "Ekaterina Shutova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09173v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09173v2",
                "updated": "2024-08-29T11:58:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    58,
                    59,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-17T11:39:57Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    11,
                    39,
                    57,
                    5,
                    230,
                    0
                ],
                "title": "Spectral properties of high dimensional rescaled sample correlation\n  matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral properties of high dimensional rescaled sample correlation\n  matrices"
                },
                "summary": "High-dimensional sample correlation matrices are a crucial class of random\nmatrices in multivariate statistical analysis. The central limit theorem (CLT)\nprovides a theoretical foundation for statistical inference. In this paper,\nassuming that the data dimension increases proportionally with the sample size,\nwe derive the limiting spectral distribution of the matrix\n$\\widehat{\\mathbf{R}}_n\\mathbf{M}$ and establish the CLTs for the linear\nspectral statistics (LSS) of $\\widehat{\\mathbf{R}}_n\\mathbf{M}$ in two\nstructures: linear independent component structure and elliptical structure. In\ncontrast to existing literature, our proposed spectral properties do not\nrequire $\\mathbf{M}$ to be an identity matrix. Moreover, we also derive the\njoint limiting distribution of LSSs of $\\widehat{\\mathbf{R}}_n\n\\mathbf{M}_1,\\ldots,\\widehat{\\mathbf{R}}_n \\mathbf{M}_K$. As an illustration,\nan application is given for the CLT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional sample correlation matrices are a crucial class of random\nmatrices in multivariate statistical analysis. The central limit theorem (CLT)\nprovides a theoretical foundation for statistical inference. In this paper,\nassuming that the data dimension increases proportionally with the sample size,\nwe derive the limiting spectral distribution of the matrix\n$\\widehat{\\mathbf{R}}_n\\mathbf{M}$ and establish the CLTs for the linear\nspectral statistics (LSS) of $\\widehat{\\mathbf{R}}_n\\mathbf{M}$ in two\nstructures: linear independent component structure and elliptical structure. In\ncontrast to existing literature, our proposed spectral properties do not\nrequire $\\mathbf{M}$ to be an identity matrix. Moreover, we also derive the\njoint limiting distribution of LSSs of $\\widehat{\\mathbf{R}}_n\n\\mathbf{M}_1,\\ldots,\\widehat{\\mathbf{R}}_n \\mathbf{M}_K$. As an illustration,\nan application is given for the CLT."
                },
                "authors": [
                    {
                        "name": "Weijiang Chen"
                    },
                    {
                        "name": "Shurong Zheng"
                    },
                    {
                        "name": "Tingting Zou"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Zou"
                },
                "author": "Tingting Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09173v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09173v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17915v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17915v3",
                "updated": "2024-08-29T11:58:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    58,
                    46,
                    3,
                    242,
                    0
                ],
                "published": "2024-07-25T10:09:21Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    10,
                    9,
                    21,
                    3,
                    207,
                    0
                ],
                "title": "The Dark Side of Function Calling: Pathways to Jailbreaking Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Side of Function Calling: Pathways to Jailbreaking Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir power comes with significant security considerations. While extensive\nresearch has been conducted on the safety of LLMs in chat mode, the security\nimplications of their function calling feature have been largely overlooked.\nThis paper uncovers a critical vulnerability in the function calling process of\nLLMs, introducing a novel \"jailbreak function\" attack method that exploits\nalignment discrepancies, user coercion, and the absence of rigorous safety\nfilters. Our empirical study, conducted on six state-of-the-art LLMs including\nGPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average\nsuccess rate of over 90\\% for this attack. We provide a comprehensive analysis\nof why function calls are susceptible to such attacks and propose defensive\nstrategies, including the use of defensive prompts. Our findings highlight the\nurgent need for enhanced security measures in the function calling capabilities\nof LLMs, contributing to the field of AI safety by identifying a previously\nunexplored risk, designing an effective attack method, and suggesting practical\ndefensive measures. Our code is available at\nhttps://github.com/wooozihui/jailbreakfunction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir power comes with significant security considerations. While extensive\nresearch has been conducted on the safety of LLMs in chat mode, the security\nimplications of their function calling feature have been largely overlooked.\nThis paper uncovers a critical vulnerability in the function calling process of\nLLMs, introducing a novel \"jailbreak function\" attack method that exploits\nalignment discrepancies, user coercion, and the absence of rigorous safety\nfilters. Our empirical study, conducted on six state-of-the-art LLMs including\nGPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average\nsuccess rate of over 90\\% for this attack. We provide a comprehensive analysis\nof why function calls are susceptible to such attacks and propose defensive\nstrategies, including the use of defensive prompts. Our findings highlight the\nurgent need for enhanced security measures in the function calling capabilities\nof LLMs, contributing to the field of AI safety by identifying a previously\nunexplored risk, designing an effective attack method, and suggesting practical\ndefensive measures. Our code is available at\nhttps://github.com/wooozihui/jailbreakfunction."
                },
                "authors": [
                    {
                        "name": "Zihui Wu"
                    },
                    {
                        "name": "Haichang Gao"
                    },
                    {
                        "name": "Jianping He"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17915v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17915v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16465v1",
                "updated": "2024-08-29T11:54:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    54,
                    2,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T11:54:02Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    54,
                    2,
                    3,
                    242,
                    0
                ],
                "title": "Human and LLM-Based Voice Assistant Interaction: An Analytical Framework\n  for User Verbal and Nonverbal Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human and LLM-Based Voice Assistant Interaction: An Analytical Framework\n  for User Verbal and Nonverbal Behaviors"
                },
                "summary": "Recent progress in large language model (LLM) technology has significantly\nenhanced the interaction experience between humans and voice assistants (VAs).\nThis project aims to explore a user's continuous interaction with LLM-based VA\n(LLM-VA) during a complex task. We recruited 12 participants to interact with\nan LLM-VA during a cooking task, selected for its complexity and the\nrequirement for continuous interaction. We observed that users show both verbal\nand nonverbal behaviors, though they know that the LLM-VA can not capture those\nnonverbal signals. Despite the prevalence of nonverbal behavior in human-human\ncommunication, there is no established analytical methodology or framework for\nexploring it in human-VA interactions. After analyzing 3 hours and 39 minutes\nof video recordings, we developed an analytical framework with three\ndimensions: 1) behavior characteristics, including both verbal and nonverbal\nbehaviors, 2) interaction stages--exploration, conflict, and integration--that\nillustrate the progression of user interactions, and 3) stage transition\nthroughout the task. This analytical framework identifies key verbal and\nnonverbal behaviors that provide a foundation for future research and practical\napplications in optimizing human and LLM-VA interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language model (LLM) technology has significantly\nenhanced the interaction experience between humans and voice assistants (VAs).\nThis project aims to explore a user's continuous interaction with LLM-based VA\n(LLM-VA) during a complex task. We recruited 12 participants to interact with\nan LLM-VA during a cooking task, selected for its complexity and the\nrequirement for continuous interaction. We observed that users show both verbal\nand nonverbal behaviors, though they know that the LLM-VA can not capture those\nnonverbal signals. Despite the prevalence of nonverbal behavior in human-human\ncommunication, there is no established analytical methodology or framework for\nexploring it in human-VA interactions. After analyzing 3 hours and 39 minutes\nof video recordings, we developed an analytical framework with three\ndimensions: 1) behavior characteristics, including both verbal and nonverbal\nbehaviors, 2) interaction stages--exploration, conflict, and integration--that\nillustrate the progression of user interactions, and 3) stage transition\nthroughout the task. This analytical framework identifies key verbal and\nnonverbal behaviors that provide a foundation for future research and practical\napplications in optimizing human and LLM-VA interactions."
                },
                "authors": [
                    {
                        "name": "Szeyi Chan"
                    },
                    {
                        "name": "Shihan Fu"
                    },
                    {
                        "name": "Jiachen Li"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Smit Desai"
                    },
                    {
                        "name": "Mirjana Prpa"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15911v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15911v2",
                "updated": "2024-08-29T11:51:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    51,
                    34,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-28T16:25:04Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    25,
                    4,
                    2,
                    241,
                    0
                ],
                "title": "Accelerating Image-based Pest Detection on a Heterogeneous Multi-core\n  Microcontroller",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Image-based Pest Detection on a Heterogeneous Multi-core\n  Microcontroller"
                },
                "summary": "The codling moth pest poses a significant threat to global crop production,\nwith potential losses of up to 80% in apple orchards. Special camera-based\nsensor nodes are deployed in the field to record and transmit images of trapped\ninsects to monitor the presence of the pest. This paper investigates the\nembedding of computer vision algorithms in the sensor node using a novel\nState-of-the-Art Microcontroller Unit (MCU), the GreenWaves Technologies' GAP9\nSystem-on-Chip, which combines 10 RISC-V general purposes cores with a\nconvolution hardware accelerator. We compare the performance of a lightweight\nViola-Jones detector algorithm with a Convolutional Neural Network (CNN),\nMobileNetV3-SSDLite, trained for the pest detection task. On two datasets that\ndifferentiate for the distance between the camera sensor and the pest targets,\nthe CNN generalizes better than the other method and achieves a detection\naccuracy between 83% and 72%. Thanks to the GAP9's CNN accelerator, the CNN\ninference task takes only 147 ms to process a 320$\\times$240 image. Compared to\nthe GAP8 MCU, which only relies on general-purpose cores for processing, we\nachieved 9.5$\\times$ faster inference speed. When running on a 1000 mAh battery\nat 3.7 V, the estimated lifetime is approximately 199 days, processing an image\nevery 30 seconds. Our study demonstrates that the novel heterogeneous MCU can\nperform end-to-end CNN inference with an energy consumption of just 4.85 mJ,\nmatching the efficiency of the simpler Viola-Jones algorithm and offering power\nconsumption up to 15$\\times$ lower than previous methods. Code at:\nhttps://github.com/Bomps4/TAFE_Pest_Detection",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The codling moth pest poses a significant threat to global crop production,\nwith potential losses of up to 80% in apple orchards. Special camera-based\nsensor nodes are deployed in the field to record and transmit images of trapped\ninsects to monitor the presence of the pest. This paper investigates the\nembedding of computer vision algorithms in the sensor node using a novel\nState-of-the-Art Microcontroller Unit (MCU), the GreenWaves Technologies' GAP9\nSystem-on-Chip, which combines 10 RISC-V general purposes cores with a\nconvolution hardware accelerator. We compare the performance of a lightweight\nViola-Jones detector algorithm with a Convolutional Neural Network (CNN),\nMobileNetV3-SSDLite, trained for the pest detection task. On two datasets that\ndifferentiate for the distance between the camera sensor and the pest targets,\nthe CNN generalizes better than the other method and achieves a detection\naccuracy between 83% and 72%. Thanks to the GAP9's CNN accelerator, the CNN\ninference task takes only 147 ms to process a 320$\\times$240 image. Compared to\nthe GAP8 MCU, which only relies on general-purpose cores for processing, we\nachieved 9.5$\\times$ faster inference speed. When running on a 1000 mAh battery\nat 3.7 V, the estimated lifetime is approximately 199 days, processing an image\nevery 30 seconds. Our study demonstrates that the novel heterogeneous MCU can\nperform end-to-end CNN inference with an energy consumption of just 4.85 mJ,\nmatching the efficiency of the simpler Viola-Jones algorithm and offering power\nconsumption up to 15$\\times$ lower than previous methods. Code at:\nhttps://github.com/Bomps4/TAFE_Pest_Detection"
                },
                "authors": [
                    {
                        "name": "Luca Bompani"
                    },
                    {
                        "name": "Luca Crupi"
                    },
                    {
                        "name": "Daniele Palossi"
                    },
                    {
                        "name": "Olmo Baldoni"
                    },
                    {
                        "name": "Davide Brunelli"
                    },
                    {
                        "name": "Francesco Conti"
                    },
                    {
                        "name": "Manuele Rusci"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "11 pages, 7 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15911v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.3; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16450v1",
                "updated": "2024-08-29T11:30:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    30,
                    21,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T11:30:21Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    30,
                    21,
                    3,
                    242,
                    0
                ],
                "title": "What to Preserve and What to Transfer: Faithful, Identity-Preserving\n  Diffusion-based Hairstyle Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What to Preserve and What to Transfer: Faithful, Identity-Preserving\n  Diffusion-based Hairstyle Transfer"
                },
                "summary": "Hairstyle transfer is a challenging task in the image editing field that\nmodifies the hairstyle of a given face image while preserving its other\nappearance and background features. The existing hairstyle transfer approaches\nheavily rely on StyleGAN, which is pre-trained on cropped and aligned face\nimages. Hence, they struggle to generalize under challenging conditions such as\nextreme variations of head poses or focal lengths. To address this issue, we\npropose a one-stage hairstyle transfer diffusion model, HairFusion, that\napplies to real-world scenarios. Specifically, we carefully design a\nhair-agnostic representation as the input of the model, where the original hair\ninformation is thoroughly eliminated. Next, we introduce a hair align\ncross-attention (Align-CA) to accurately align the reference hairstyle with the\nface image while considering the difference in their face shape. To enhance the\npreservation of the face image's original features, we leverage adaptive hair\nblending during the inference, where the output's hair regions are estimated by\nthe cross-attention map in Align-CA and blended with non-hair areas of the face\nimage. Our experimental results show that our method achieves state-of-the-art\nperformance compared to the existing methods in preserving the integrity of\nboth the transferred hairstyle and the surrounding features. The codes are\navailable at https://github.com/cychungg/HairFusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hairstyle transfer is a challenging task in the image editing field that\nmodifies the hairstyle of a given face image while preserving its other\nappearance and background features. The existing hairstyle transfer approaches\nheavily rely on StyleGAN, which is pre-trained on cropped and aligned face\nimages. Hence, they struggle to generalize under challenging conditions such as\nextreme variations of head poses or focal lengths. To address this issue, we\npropose a one-stage hairstyle transfer diffusion model, HairFusion, that\napplies to real-world scenarios. Specifically, we carefully design a\nhair-agnostic representation as the input of the model, where the original hair\ninformation is thoroughly eliminated. Next, we introduce a hair align\ncross-attention (Align-CA) to accurately align the reference hairstyle with the\nface image while considering the difference in their face shape. To enhance the\npreservation of the face image's original features, we leverage adaptive hair\nblending during the inference, where the output's hair regions are estimated by\nthe cross-attention map in Align-CA and blended with non-hair areas of the face\nimage. Our experimental results show that our method achieves state-of-the-art\nperformance compared to the existing methods in preserving the integrity of\nboth the transferred hairstyle and the surrounding features. The codes are\navailable at https://github.com/cychungg/HairFusion."
                },
                "authors": [
                    {
                        "name": "Chaeyeon Chung"
                    },
                    {
                        "name": "Sunghyun Park"
                    },
                    {
                        "name": "Jeongho Kim"
                    },
                    {
                        "name": "Jaegul Choo"
                    }
                ],
                "author_detail": {
                    "name": "Jaegul Choo"
                },
                "author": "Jaegul Choo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08453v2",
                "updated": "2024-08-29T11:22:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    22,
                    14,
                    3,
                    242,
                    0
                ],
                "published": "2024-02-13T13:39:38Z",
                "published_parsed": [
                    2024,
                    2,
                    13,
                    13,
                    39,
                    38,
                    1,
                    44,
                    0
                ],
                "title": "The SRG/eROSITA All-Sky Survey. Optical identification and properties of\n  galaxy clusters and groups in the western galactic hemisphere",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The SRG/eROSITA All-Sky Survey. Optical identification and properties of\n  galaxy clusters and groups in the western galactic hemisphere"
                },
                "summary": "The first SRG/eROSITA All-Sky Survey (eRASS1) provides the largest\nintracluster medium-selected galaxy cluster and group catalog covering the\nwestern galactic hemisphere. Compared to samples selected purely on X-ray\nextent, the sample purity can be enhanced by identifying cluster candidates\nusing optical and near-infrared data from the DESI Legacy Imaging Surveys.\nUsing the red-sequence-based cluster finder eROMaPPer, we measured individual\nphotometric properties (redshift $z_\\lambda$, richness $\\lambda$, optical\ncenter, and BCG position) for 12,000 eRASS1 clusters over a sky area of 13,116\ndeg$^2$, augmented by 247 cases identified by matching the candidates with\nknown clusters from the literature. The median redshift of the identified\neRASS1 sample is $z=0.31$, with 10% of the clusters at $z>0.72$. The\nphotometric redshifts have an accuracy of $\\delta z/(1+z)<0.005$ for\n$0.05<z<0.9$. Spectroscopic cluster properties (redshift $z_{\\rm spec}$ and\nvelocity dispersion $\\sigma$) are measured a posteriori for a subsample of\n3,210 and 1,499 eRASS1 clusters, respectively, using an extensive compilation\nof spectroscopic redshifts of galaxies from the literature. We infer that the\nprimary eRASS1 sample has a purity of 86% and optical completeness >95% for\n$z>0.05$. For these and further quality assessments of the eRASS1 identified\ncatalog, we applied our identification method to a collection of galaxy cluster\ncatalogs in the literature, as well as blindly on the full Legacy Surveys\ncovering 24,069 deg$^2$. Using a combination of these cluster samples, we\ninvestigated the velocity dispersion-richness relation, finding\n$\\log(\\lambda)=2.401\\times\\log(\\sigma)-5.074$ with an intrinsic scatter of\n$0.10\\pm0.01$ dex. Our main result is the identified eRASS1 cluster catalog\nwith a high purity and a well-defined X-ray selection process, enabling precise\ncosmological analyses presented in companion papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The first SRG/eROSITA All-Sky Survey (eRASS1) provides the largest\nintracluster medium-selected galaxy cluster and group catalog covering the\nwestern galactic hemisphere. Compared to samples selected purely on X-ray\nextent, the sample purity can be enhanced by identifying cluster candidates\nusing optical and near-infrared data from the DESI Legacy Imaging Surveys.\nUsing the red-sequence-based cluster finder eROMaPPer, we measured individual\nphotometric properties (redshift $z_\\lambda$, richness $\\lambda$, optical\ncenter, and BCG position) for 12,000 eRASS1 clusters over a sky area of 13,116\ndeg$^2$, augmented by 247 cases identified by matching the candidates with\nknown clusters from the literature. The median redshift of the identified\neRASS1 sample is $z=0.31$, with 10% of the clusters at $z>0.72$. The\nphotometric redshifts have an accuracy of $\\delta z/(1+z)<0.005$ for\n$0.05<z<0.9$. Spectroscopic cluster properties (redshift $z_{\\rm spec}$ and\nvelocity dispersion $\\sigma$) are measured a posteriori for a subsample of\n3,210 and 1,499 eRASS1 clusters, respectively, using an extensive compilation\nof spectroscopic redshifts of galaxies from the literature. We infer that the\nprimary eRASS1 sample has a purity of 86% and optical completeness >95% for\n$z>0.05$. For these and further quality assessments of the eRASS1 identified\ncatalog, we applied our identification method to a collection of galaxy cluster\ncatalogs in the literature, as well as blindly on the full Legacy Surveys\ncovering 24,069 deg$^2$. Using a combination of these cluster samples, we\ninvestigated the velocity dispersion-richness relation, finding\n$\\log(\\lambda)=2.401\\times\\log(\\sigma)-5.074$ with an intrinsic scatter of\n$0.10\\pm0.01$ dex. Our main result is the identified eRASS1 cluster catalog\nwith a high purity and a well-defined X-ray selection process, enabling precise\ncosmological analyses presented in companion papers."
                },
                "authors": [
                    {
                        "name": "M. Kluge"
                    },
                    {
                        "name": "J. Comparat"
                    },
                    {
                        "name": "A. Liu"
                    },
                    {
                        "name": "F. Balzer"
                    },
                    {
                        "name": "E. Bulbul"
                    },
                    {
                        "name": "J. Ider Chitham"
                    },
                    {
                        "name": "V. Ghirardini"
                    },
                    {
                        "name": "C. Garrel"
                    },
                    {
                        "name": "Y. E. Bahar"
                    },
                    {
                        "name": "E. Artis"
                    },
                    {
                        "name": "R. Bender"
                    },
                    {
                        "name": "N. Clerc"
                    },
                    {
                        "name": "T. Dwelly"
                    },
                    {
                        "name": "M. H. Fabricius"
                    },
                    {
                        "name": "S. Grandis"
                    },
                    {
                        "name": "D. Hernández-Lang"
                    },
                    {
                        "name": "G. J. Hill"
                    },
                    {
                        "name": "J. Joshi"
                    },
                    {
                        "name": "G. Lamer"
                    },
                    {
                        "name": "A. Merloni"
                    },
                    {
                        "name": "K. Nandra"
                    },
                    {
                        "name": "F. Pacaud"
                    },
                    {
                        "name": "P. Predehl"
                    },
                    {
                        "name": "M. E. Ramos-Ceja"
                    },
                    {
                        "name": "T. H. Reiprich"
                    },
                    {
                        "name": "M. Salvato"
                    },
                    {
                        "name": "J. S. Sanders"
                    },
                    {
                        "name": "T. Schrabback"
                    },
                    {
                        "name": "R. Seppi"
                    },
                    {
                        "name": "S. Zelmer"
                    },
                    {
                        "name": "A. Zenteno"
                    },
                    {
                        "name": "X. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "X. Zhang"
                },
                "author": "X. Zhang",
                "arxiv_doi": "10.1051/0004-6361/202349031",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202349031",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.08453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "36 pages, 23 figures, 6 tables. Accepted for publication in A&A",
                "arxiv_journal_ref": "A&A 688, A210 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12862v2",
                "updated": "2024-08-29T11:18:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    18,
                    16,
                    3,
                    242,
                    0
                ],
                "published": "2024-04-19T13:01:59Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    13,
                    1,
                    59,
                    4,
                    110,
                    0
                ],
                "title": "A Guide to Feature Importance Methods for Scientific Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Guide to Feature Importance Methods for Scientific Inference"
                },
                "summary": "While machine learning (ML) models are increasingly used due to their high\npredictive power, their use in understanding the data-generating process (DGP)\nis limited. Understanding the DGP requires insights into feature-target\nassociations, which many ML models cannot directly provide due to their opaque\ninternal mechanisms. Feature importance (FI) methods provide useful insights\ninto the DGP under certain conditions. Since the results of different FI\nmethods have different interpretations, selecting the correct FI method for a\nconcrete use case is crucial and still requires expert knowledge. This paper\nserves as a comprehensive guide to help understand the different\ninterpretations of global FI methods. Through an extensive review of FI methods\nand providing new proofs regarding their interpretation, we facilitate a\nthorough understanding of these methods and formulate concrete recommendations\nfor scientific inference. We conclude by discussing options for FI uncertainty\nestimation and point to directions for future research aiming at full\nstatistical inference from black-box ML models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While machine learning (ML) models are increasingly used due to their high\npredictive power, their use in understanding the data-generating process (DGP)\nis limited. Understanding the DGP requires insights into feature-target\nassociations, which many ML models cannot directly provide due to their opaque\ninternal mechanisms. Feature importance (FI) methods provide useful insights\ninto the DGP under certain conditions. Since the results of different FI\nmethods have different interpretations, selecting the correct FI method for a\nconcrete use case is crucial and still requires expert knowledge. This paper\nserves as a comprehensive guide to help understand the different\ninterpretations of global FI methods. Through an extensive review of FI methods\nand providing new proofs regarding their interpretation, we facilitate a\nthorough understanding of these methods and formulate concrete recommendations\nfor scientific inference. We conclude by discussing options for FI uncertainty\nestimation and point to directions for future research aiming at full\nstatistical inference from black-box ML models."
                },
                "authors": [
                    {
                        "name": "Fiona Katharina Ewald"
                    },
                    {
                        "name": "Ludwig Bothmann"
                    },
                    {
                        "name": "Marvin N. Wright"
                    },
                    {
                        "name": "Bernd Bischl"
                    },
                    {
                        "name": "Giuseppe Casalicchio"
                    },
                    {
                        "name": "Gunnar König"
                    }
                ],
                "author_detail": {
                    "name": "Gunnar König"
                },
                "author": "Gunnar König",
                "arxiv_doi": "10.1007/978-3-031-63797-1_22",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-63797-1_22",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.12862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Longo, L., Lapuschkin, S., Seifert, C. (eds) Explainable\n  Artificial Intelligence. xAI 2024. Communications in Computer and Information\n  Science, vol 2154. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16440v1",
                "updated": "2024-08-29T11:05:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    5,
                    54,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T11:05:54Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    5,
                    54,
                    3,
                    242,
                    0
                ],
                "title": "Instruction-tuned Large Language Models for Machine Translation in the\n  Medical Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuned Large Language Models for Machine Translation in the\n  Medical Domain"
                },
                "summary": "Large Language Models (LLMs) have shown promising results on machine\ntranslation for high resource language pairs and domains. However, in\nspecialised domains (e.g. medical) LLMs have shown lower performance compared\nto standard neural machine translation models. The consistency in the machine\ntranslation of terminology is crucial for users, researchers, and translators\nin specialised domains. In this study, we compare the performance between\nbaseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we\nintroduce terminology from specialised medical dictionaries into the\ninstruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs\nsignificantly outperform the baseline models with automatic metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promising results on machine\ntranslation for high resource language pairs and domains. However, in\nspecialised domains (e.g. medical) LLMs have shown lower performance compared\nto standard neural machine translation models. The consistency in the machine\ntranslation of terminology is crucial for users, researchers, and translators\nin specialised domains. In this study, we compare the performance between\nbaseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we\nintroduce terminology from specialised medical dictionaries into the\ninstruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs\nsignificantly outperform the baseline models with automatic metrics."
                },
                "authors": [
                    {
                        "name": "Miguel Rios"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Rios"
                },
                "author": "Miguel Rios",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15381v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15381v3",
                "updated": "2024-08-29T11:03:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    3,
                    57,
                    3,
                    242,
                    0
                ],
                "published": "2024-04-23T09:44:58Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    9,
                    44,
                    58,
                    1,
                    114,
                    0
                ],
                "title": "Advances and Open Challenges in Federated Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances and Open Challenges in Federated Foundation Models"
                },
                "summary": "The integration of Foundation Models (FMs) with Federated Learning (FL)\npresents a transformative paradigm in Artificial Intelligence (AI). This\nintegration offers enhanced capabilities while addressing concerns of privacy,\ndata decentralization, and computational efficiency. This paper provides a\ncomprehensive survey of the emerging field of Federated Foundation Models\n(FedFM), elucidating their synergistic relationship and exploring novel\nmethodologies, challenges, and future directions that the FL research field\nneeds to focus on in order to thrive in the age of FMs. A systematic\nmulti-tiered taxonomy is proposed, categorizing existing FedFM approaches for\nmodel training, aggregation, trustworthiness, and incentivization. Key\nchallenges, including how to enable FL to deal with high complexity of\ncomputational demands, privacy considerations, contribution evaluation, and\ncommunication efficiency, are thoroughly discussed. Moreover, the paper\nexplores the intricate challenges of communication, scalability, and security\ninherent in training/fine-tuning FMs via FL. It highlights the potential of\nquantum computing to revolutionize the processes of training, inference,\noptimization, and data encryption. This survey also introduces the\nimplementation requirement of FedFM and some practical FedFM applications.\nThen, this survey provides the lessons with a clear understanding of our\nfindings for FedFM. Finally, this survey not only provides insights into the\ncurrent state and challenges of FedFM but also paves the way for future\nresearch directions, emphasizing the need for developing trustworthy solutions.\nIt serves as a foundational guide for researchers and practitioners interested\nin contributing to this interdisciplinary and rapidly advancing field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Foundation Models (FMs) with Federated Learning (FL)\npresents a transformative paradigm in Artificial Intelligence (AI). This\nintegration offers enhanced capabilities while addressing concerns of privacy,\ndata decentralization, and computational efficiency. This paper provides a\ncomprehensive survey of the emerging field of Federated Foundation Models\n(FedFM), elucidating their synergistic relationship and exploring novel\nmethodologies, challenges, and future directions that the FL research field\nneeds to focus on in order to thrive in the age of FMs. A systematic\nmulti-tiered taxonomy is proposed, categorizing existing FedFM approaches for\nmodel training, aggregation, trustworthiness, and incentivization. Key\nchallenges, including how to enable FL to deal with high complexity of\ncomputational demands, privacy considerations, contribution evaluation, and\ncommunication efficiency, are thoroughly discussed. Moreover, the paper\nexplores the intricate challenges of communication, scalability, and security\ninherent in training/fine-tuning FMs via FL. It highlights the potential of\nquantum computing to revolutionize the processes of training, inference,\noptimization, and data encryption. This survey also introduces the\nimplementation requirement of FedFM and some practical FedFM applications.\nThen, this survey provides the lessons with a clear understanding of our\nfindings for FedFM. Finally, this survey not only provides insights into the\ncurrent state and challenges of FedFM but also paves the way for future\nresearch directions, emphasizing the need for developing trustworthy solutions.\nIt serves as a foundational guide for researchers and practitioners interested\nin contributing to this interdisciplinary and rapidly advancing field."
                },
                "authors": [
                    {
                        "name": "Chao Ren"
                    },
                    {
                        "name": "Han Yu"
                    },
                    {
                        "name": "Hongyi Peng"
                    },
                    {
                        "name": "Xiaoli Tang"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Liping Yi"
                    },
                    {
                        "name": "Alysa Ziying Tan"
                    },
                    {
                        "name": "Yulan Gao"
                    },
                    {
                        "name": "Anran Li"
                    },
                    {
                        "name": "Xiaoxiao Li"
                    },
                    {
                        "name": "Zengxiang Li"
                    },
                    {
                        "name": "Qiang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yang"
                },
                "author": "Qiang Yang",
                "arxiv_comment": "Survey of Federated Foundation Models (FedFM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15381v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15381v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16429v1",
                "updated": "2024-08-29T10:43:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    43,
                    55,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T10:43:55Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    43,
                    55,
                    3,
                    242,
                    0
                ],
                "title": "Gradient-free variational learning with conditional mixture networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient-free variational learning with conditional mixture networks"
                },
                "summary": "Balancing computational efficiency with robust predictive performance is\ncrucial in supervised learning, especially for critical applications. Standard\ndeep learning models, while accurate and scalable, often lack probabilistic\nfeatures like calibrated predictions and uncertainty quantification. Bayesian\nmethods address these issues but can be computationally expensive as model and\ndata complexity increase. Previous work shows that fast variational methods can\nreduce the compute requirements of Bayesian methods by eliminating the need for\ngradient computation or sampling, but are often limited to simple models. We\ndemonstrate that conditional mixture networks (CMNs), a probabilistic variant\nof the mixture-of-experts (MoE) model, are suitable for fast, gradient-free\ninference and can solve complex classification tasks. CMNs employ linear\nexperts and a softmax gating network. By exploiting conditional conjugacy and\nP\\'olya-Gamma augmentation, we furnish Gaussian likelihoods for the weights of\nboth the linear experts and the gating network. This enables efficient\nvariational updates using coordinate ascent variational inference (CAVI),\navoiding traditional gradient-based optimization. We validate this approach by\ntraining two-layer CMNs on standard benchmarks from the UCI repository. Our\nmethod, CAVI-CMN, achieves competitive and often superior predictive accuracy\ncompared to maximum likelihood estimation (MLE) with backpropagation, while\nmaintaining competitive runtime and full posterior distributions over all model\nparameters. Moreover, as input size or the number of experts increases,\ncomputation time scales competitively with MLE and other gradient-based\nsolutions like black-box variational inference (BBVI), making CAVI-CMN a\npromising tool for deep, fast, and gradient-free Bayesian networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing computational efficiency with robust predictive performance is\ncrucial in supervised learning, especially for critical applications. Standard\ndeep learning models, while accurate and scalable, often lack probabilistic\nfeatures like calibrated predictions and uncertainty quantification. Bayesian\nmethods address these issues but can be computationally expensive as model and\ndata complexity increase. Previous work shows that fast variational methods can\nreduce the compute requirements of Bayesian methods by eliminating the need for\ngradient computation or sampling, but are often limited to simple models. We\ndemonstrate that conditional mixture networks (CMNs), a probabilistic variant\nof the mixture-of-experts (MoE) model, are suitable for fast, gradient-free\ninference and can solve complex classification tasks. CMNs employ linear\nexperts and a softmax gating network. By exploiting conditional conjugacy and\nP\\'olya-Gamma augmentation, we furnish Gaussian likelihoods for the weights of\nboth the linear experts and the gating network. This enables efficient\nvariational updates using coordinate ascent variational inference (CAVI),\navoiding traditional gradient-based optimization. We validate this approach by\ntraining two-layer CMNs on standard benchmarks from the UCI repository. Our\nmethod, CAVI-CMN, achieves competitive and often superior predictive accuracy\ncompared to maximum likelihood estimation (MLE) with backpropagation, while\nmaintaining competitive runtime and full posterior distributions over all model\nparameters. Moreover, as input size or the number of experts increases,\ncomputation time scales competitively with MLE and other gradient-based\nsolutions like black-box variational inference (BBVI), making CAVI-CMN a\npromising tool for deep, fast, and gradient-free Bayesian networks."
                },
                "authors": [
                    {
                        "name": "Conor Heins"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Dimitrije Markovic"
                    },
                    {
                        "name": "Alexander Tschantz"
                    },
                    {
                        "name": "Jeff Beck"
                    },
                    {
                        "name": "Christopher Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Buckley"
                },
                "author": "Christopher Buckley",
                "arxiv_comment": "16 pages main text (3 figures), including references. 9 pages\n  supplementary material (5 figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15496v2",
                "updated": "2024-08-29T10:35:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    35,
                    52,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-28T02:47:27Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    2,
                    47,
                    27,
                    2,
                    241,
                    0
                ],
                "title": "ReMamba: Equip Mamba with Effective Long-Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReMamba: Equip Mamba with Effective Long-Sequence Modeling"
                },
                "summary": "While the Mamba architecture demonstrates superior inference efficiency and\ncompetitive performance on short-context natural language processing (NLP)\ntasks, empirical evidence suggests its capacity to comprehend long contexts is\nlimited compared to transformer-based models. In this study, we investigate the\nlong-context efficiency issues of the Mamba models and propose ReMamba, which\nenhances Mamba's ability to comprehend long contexts. ReMamba incorporates\nselective compression and adaptation techniques within a two-stage re-forward\nprocess, incurring minimal additional inference costs overhead. Experimental\nresults on the LongBench and L-Eval benchmarks demonstrate ReMamba's efficacy,\nimproving over the baselines by 3.2 and 1.6 points, respectively, and attaining\nperformance almost on par with same-size transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the Mamba architecture demonstrates superior inference efficiency and\ncompetitive performance on short-context natural language processing (NLP)\ntasks, empirical evidence suggests its capacity to comprehend long contexts is\nlimited compared to transformer-based models. In this study, we investigate the\nlong-context efficiency issues of the Mamba models and propose ReMamba, which\nenhances Mamba's ability to comprehend long contexts. ReMamba incorporates\nselective compression and adaptation techniques within a two-stage re-forward\nprocess, incurring minimal additional inference costs overhead. Experimental\nresults on the LongBench and L-Eval benchmarks demonstrate ReMamba's efficacy,\nimproving over the baselines by 3.2 and 1.6 points, respectively, and attaining\nperformance almost on par with same-size transformer models."
                },
                "authors": [
                    {
                        "name": "Danlong Yuan"
                    },
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Huishuai Zhang"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Dongyan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongyan Zhao"
                },
                "author": "Dongyan Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16423v1",
                "updated": "2024-08-29T10:31:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    31,
                    52,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T10:31:52Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    31,
                    52,
                    3,
                    242,
                    0
                ],
                "title": "WHISMA: A Speech-LLM to Perform Zero-shot Spoken Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WHISMA: A Speech-LLM to Perform Zero-shot Spoken Language Understanding"
                },
                "summary": "Speech large language models (speech-LLMs) integrate speech and text-based\nfoundation models to provide a unified framework for handling a wide range of\ndownstream tasks. In this paper, we introduce WHISMA, a speech-LLM tailored for\nspoken language understanding (SLU) that demonstrates robust performance in\nvarious zero-shot settings. WHISMA combines the speech encoder from Whisper\nwith the Llama-3 LLM, and is fine-tuned in a parameter-efficient manner on a\ncomprehensive collection of SLU-related datasets. Our experiments show that\nWHISMA significantly improves the zero-shot slot filling performance on the\nSLURP benchmark, achieving a relative gain of 26.6% compared to the current\nstate-of-the-art model. Furthermore, to evaluate WHISMA's generalisation\ncapabilities to unseen domains, we develop a new task-agnostic benchmark named\nSLU-GLUE. The evaluation results indicate that WHISMA outperforms an existing\nspeech-LLM (Qwen-Audio) with a relative gain of 33.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech large language models (speech-LLMs) integrate speech and text-based\nfoundation models to provide a unified framework for handling a wide range of\ndownstream tasks. In this paper, we introduce WHISMA, a speech-LLM tailored for\nspoken language understanding (SLU) that demonstrates robust performance in\nvarious zero-shot settings. WHISMA combines the speech encoder from Whisper\nwith the Llama-3 LLM, and is fine-tuned in a parameter-efficient manner on a\ncomprehensive collection of SLU-related datasets. Our experiments show that\nWHISMA significantly improves the zero-shot slot filling performance on the\nSLURP benchmark, achieving a relative gain of 26.6% compared to the current\nstate-of-the-art model. Furthermore, to evaluate WHISMA's generalisation\ncapabilities to unseen domains, we develop a new task-agnostic benchmark named\nSLU-GLUE. The evaluation results indicate that WHISMA outperforms an existing\nspeech-LLM (Qwen-Audio) with a relative gain of 33.0%."
                },
                "authors": [
                    {
                        "name": "Mohan Li"
                    },
                    {
                        "name": "Cong-Thanh Do"
                    },
                    {
                        "name": "Simon Keizer"
                    },
                    {
                        "name": "Youmna Farag"
                    },
                    {
                        "name": "Svetlana Stoyanchev"
                    },
                    {
                        "name": "Rama Doddipatla"
                    }
                ],
                "author_detail": {
                    "name": "Rama Doddipatla"
                },
                "author": "Rama Doddipatla",
                "arxiv_comment": "accepted to SLT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11288v2",
                "updated": "2024-08-29T10:10:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    10,
                    55,
                    3,
                    242,
                    0
                ],
                "published": "2024-04-17T11:52:47Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    52,
                    47,
                    2,
                    108,
                    0
                ],
                "title": "A Preference-driven Paradigm for Enhanced Translation with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Preference-driven Paradigm for Enhanced Translation with Large\n  Language Models"
                },
                "summary": "Recent research has shown that large language models (LLMs) can achieve\nremarkable translation performance through supervised fine-tuning (SFT) using\nonly a small amount of parallel data. However, SFT simply instructs the model\nto imitate the reference translations at the token level, making it vulnerable\nto the noise present in the references. Hence, the assistance from SFT often\nreaches a plateau once the LLMs have achieved a certain level of translation\ncapability, and further increasing the size of parallel data does not provide\nadditional benefits. To overcome this plateau associated with imitation-based\nSFT, we propose a preference-based approach built upon the Plackett-Luce model.\nThe objective is to steer LLMs towards a more nuanced understanding of\ntranslation preferences from a holistic view, while also being more resilient\nin the absence of gold translations. We further build a dataset named MAPLE to\nverify the effectiveness of our approach, which includes multiple translations\nof varying quality for each source sentence. Extensive experiments demonstrate\nthe superiority of our approach in \"breaking the plateau\" across diverse LLMs\nand test settings. Our in-depth analysis underscores the pivotal role of\ndiverse translations and accurate preference scores in the success of our\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that large language models (LLMs) can achieve\nremarkable translation performance through supervised fine-tuning (SFT) using\nonly a small amount of parallel data. However, SFT simply instructs the model\nto imitate the reference translations at the token level, making it vulnerable\nto the noise present in the references. Hence, the assistance from SFT often\nreaches a plateau once the LLMs have achieved a certain level of translation\ncapability, and further increasing the size of parallel data does not provide\nadditional benefits. To overcome this plateau associated with imitation-based\nSFT, we propose a preference-based approach built upon the Plackett-Luce model.\nThe objective is to steer LLMs towards a more nuanced understanding of\ntranslation preferences from a holistic view, while also being more resilient\nin the absence of gold translations. We further build a dataset named MAPLE to\nverify the effectiveness of our approach, which includes multiple translations\nof varying quality for each source sentence. Extensive experiments demonstrate\nthe superiority of our approach in \"breaking the plateau\" across diverse LLMs\nand test settings. Our in-depth analysis underscores the pivotal role of\ndiverse translations and accurate preference scores in the success of our\napproach."
                },
                "authors": [
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Sony Trenous"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    },
                    {
                        "name": "Dietrich Klakow"
                    },
                    {
                        "name": "Bill Byrne"
                    },
                    {
                        "name": "Eva Hasler"
                    }
                ],
                "author_detail": {
                    "name": "Eva Hasler"
                },
                "author": "Eva Hasler",
                "arxiv_comment": "Accepted to NAACL 2024 (long, main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16400v1",
                "updated": "2024-08-29T10:00:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    0,
                    57,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T10:00:57Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    0,
                    57,
                    3,
                    242,
                    0
                ],
                "title": "Outside the Comfort Zone: Analysing LLM Capabilities in Software\n  Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outside the Comfort Zone: Analysing LLM Capabilities in Software\n  Vulnerability Detection"
                },
                "summary": "The significant increase in software production driven by automation and\nfaster development lifecycles has resulted in a corresponding surge in software\nvulnerabilities. In parallel, the evolving landscape of software vulnerability\ndetection, highlighting the shift from traditional methods to machine learning\nand large language models (LLMs), provides massive opportunities at the cost of\nresource-demanding computations. This paper thoroughly analyses LLMs'\ncapabilities in detecting vulnerabilities within source code by testing models\nbeyond their usual applications to study their potential in cybersecurity\ntasks. We evaluate the performance of six open-source models that are\nspecifically trained for vulnerability detection against six general-purpose\nLLMs, three of which were further fine-tuned on a dataset that we compiled. Our\ndataset, alongside five state-of-the-art benchmark datasets, were used to\ncreate a pipeline to leverage a binary classification task, namely classifying\ncode into vulnerable and non-vulnerable. The findings highlight significant\nvariations in classification accuracy across benchmarks, revealing the critical\ninfluence of fine-tuning in enhancing the detection capabilities of small LLMs\nover their larger counterparts, yet only in the specific scenarios in which\nthey were trained. Further experiments and analysis also underscore the issues\nwith current benchmark datasets, particularly around mislabeling and their\nimpact on model training and performance, which raises concerns about the\ncurrent state of practice. We also discuss the road ahead in the field\nsuggesting strategies for improved model training and dataset curation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant increase in software production driven by automation and\nfaster development lifecycles has resulted in a corresponding surge in software\nvulnerabilities. In parallel, the evolving landscape of software vulnerability\ndetection, highlighting the shift from traditional methods to machine learning\nand large language models (LLMs), provides massive opportunities at the cost of\nresource-demanding computations. This paper thoroughly analyses LLMs'\ncapabilities in detecting vulnerabilities within source code by testing models\nbeyond their usual applications to study their potential in cybersecurity\ntasks. We evaluate the performance of six open-source models that are\nspecifically trained for vulnerability detection against six general-purpose\nLLMs, three of which were further fine-tuned on a dataset that we compiled. Our\ndataset, alongside five state-of-the-art benchmark datasets, were used to\ncreate a pipeline to leverage a binary classification task, namely classifying\ncode into vulnerable and non-vulnerable. The findings highlight significant\nvariations in classification accuracy across benchmarks, revealing the critical\ninfluence of fine-tuning in enhancing the detection capabilities of small LLMs\nover their larger counterparts, yet only in the specific scenarios in which\nthey were trained. Further experiments and analysis also underscore the issues\nwith current benchmark datasets, particularly around mislabeling and their\nimpact on model training and performance, which raises concerns about the\ncurrent state of practice. We also discuss the road ahead in the field\nsuggesting strategies for improved model training and dataset curation."
                },
                "authors": [
                    {
                        "name": "Yuejun Guo"
                    },
                    {
                        "name": "Constantinos Patsakis"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Qiang Tang"
                    },
                    {
                        "name": "Fran Casino"
                    }
                ],
                "author_detail": {
                    "name": "Fran Casino"
                },
                "author": "Fran Casino",
                "arxiv_comment": "Accepted to ESORICS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16387v1",
                "updated": "2024-08-29T09:50:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    9,
                    50,
                    21,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T09:50:21Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    9,
                    50,
                    21,
                    3,
                    242,
                    0
                ],
                "title": "Enhancing MOTION2NX for Efficient, Scalable and Secure Image Inference\n  using Convolutional Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing MOTION2NX for Efficient, Scalable and Secure Image Inference\n  using Convolutional Neural Networks"
                },
                "summary": "This work contributes towards the development of an efficient and scalable\nopen-source Secure Multi-Party Computation (SMPC) protocol on machines with\nmoderate computational resources. We use the ABY2.0 SMPC protocol implemented\non the C++ based MOTION2NX framework for secure convolutional neural network\n(CNN) inference application with semi-honest security. Our list of\ncontributions are as follows. Firstly, we enhance MOTION2NX by providing a\ntensorized version of several primitive functions including the Hadamard\nproduct, indicator function and argmax function. Our design of secure indicator\nfunction based on a novel approach that uses secure Relu function available in\nthe baseline MOTION2NX implementation. The secure indicator function is used,\nin turn, as a building block for a novel implementation of secure argmax.\nSecondly, we also develop a novel splitting of the computations at each CNN\nlayer into multiple configurable chunks thereby resulting in significant\nreduction in RAM usage. Thirdly, we adapt an existing Helper node algorithm,\nworking in tandem with the ABY2.0 protocol, for efficient convolution\ncomputation. This algorithm not only reduces execution time but also reduces\nthe RAM usage required to execute CNN models, but comes at a cost of an\nadditional compute server. Moreover, the ideas presented in this paper can also\nbe applied to secure neural network training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work contributes towards the development of an efficient and scalable\nopen-source Secure Multi-Party Computation (SMPC) protocol on machines with\nmoderate computational resources. We use the ABY2.0 SMPC protocol implemented\non the C++ based MOTION2NX framework for secure convolutional neural network\n(CNN) inference application with semi-honest security. Our list of\ncontributions are as follows. Firstly, we enhance MOTION2NX by providing a\ntensorized version of several primitive functions including the Hadamard\nproduct, indicator function and argmax function. Our design of secure indicator\nfunction based on a novel approach that uses secure Relu function available in\nthe baseline MOTION2NX implementation. The secure indicator function is used,\nin turn, as a building block for a novel implementation of secure argmax.\nSecondly, we also develop a novel splitting of the computations at each CNN\nlayer into multiple configurable chunks thereby resulting in significant\nreduction in RAM usage. Thirdly, we adapt an existing Helper node algorithm,\nworking in tandem with the ABY2.0 protocol, for efficient convolution\ncomputation. This algorithm not only reduces execution time but also reduces\nthe RAM usage required to execute CNN models, but comes at a cost of an\nadditional compute server. Moreover, the ideas presented in this paper can also\nbe applied to secure neural network training."
                },
                "authors": [
                    {
                        "name": "Haritha K"
                    },
                    {
                        "name": "Ramya Burra"
                    },
                    {
                        "name": "Srishti Mittal"
                    },
                    {
                        "name": "Sarthak Sharma"
                    },
                    {
                        "name": "Abhilash Venkatesh"
                    },
                    {
                        "name": "Anshoo Tandon"
                    }
                ],
                "author_detail": {
                    "name": "Anshoo Tandon"
                },
                "author": "Anshoo Tandon",
                "arxiv_comment": "20 pages, 1 figure. arXiv admin note: text overlap with\n  arXiv:2310.10133",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14700v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14700v2",
                "updated": "2024-08-29T09:44:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    9,
                    44,
                    53,
                    3,
                    242,
                    0
                ],
                "published": "2024-05-23T15:34:53Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    15,
                    34,
                    53,
                    3,
                    144,
                    0
                ],
                "title": "Sparse-Tuning: Adapting Vision Transformers with Efficient Fine-tuning\n  and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-Tuning: Adapting Vision Transformers with Efficient Fine-tuning\n  and Inference"
                },
                "summary": "Parameter-efficient fine-tuning (PEFT) has emerged as a popular solution for\nadapting pre-trained Vision Transformer (ViT) models to downstream\napplications. While current PEFT methods have achieved parameter efficiency,\nthey overlook the efficiency of computation and GPU memory during both\nfine-tuning and inference, falling short of practical requirements. In this\npaper, we propose \\textbf{Sparse-Tuning}, a novel PEFT method that accounts for\nthe information redundancy in images and videos to boost the above efficiency.\nBy sparsely preserving the semantic-relevant tokens and merging irrelevant\nones, Sparse-Tuning minimizes the quantity of tokens processed at each layer,\nleading to a quadratic reduction in computational and memory overhead. To align\nour token sparsification strategy suitably with fine-tuning purposes, we\nfurther design Dense Adapters that establish dense connections from shallow\nlayers to deeper layers. These Dense Adapters integrate multi-level local\nfeatures to enrich the current tokens, improving both token preservation and\nmodel adaptation. Empirical results on VTAB-1K, three image datasets, and two\nvideo datasets show that our Sparse-Tuning reduces GFLOPs to \\textbf{62\\%-70\\%}\nof the original ViT-B while achieving state-of-the-art performance. Source code\nis available at \\url{https://github.com/liuting20/Sparse-Tuning}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning (PEFT) has emerged as a popular solution for\nadapting pre-trained Vision Transformer (ViT) models to downstream\napplications. While current PEFT methods have achieved parameter efficiency,\nthey overlook the efficiency of computation and GPU memory during both\nfine-tuning and inference, falling short of practical requirements. In this\npaper, we propose \\textbf{Sparse-Tuning}, a novel PEFT method that accounts for\nthe information redundancy in images and videos to boost the above efficiency.\nBy sparsely preserving the semantic-relevant tokens and merging irrelevant\nones, Sparse-Tuning minimizes the quantity of tokens processed at each layer,\nleading to a quadratic reduction in computational and memory overhead. To align\nour token sparsification strategy suitably with fine-tuning purposes, we\nfurther design Dense Adapters that establish dense connections from shallow\nlayers to deeper layers. These Dense Adapters integrate multi-level local\nfeatures to enrich the current tokens, improving both token preservation and\nmodel adaptation. Empirical results on VTAB-1K, three image datasets, and two\nvideo datasets show that our Sparse-Tuning reduces GFLOPs to \\textbf{62\\%-70\\%}\nof the original ViT-B while achieving state-of-the-art performance. Source code\nis available at \\url{https://github.com/liuting20/Sparse-Tuning}."
                },
                "authors": [
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Liangtao Shi"
                    },
                    {
                        "name": "Zunnan Xu"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Quanjun Yin"
                    },
                    {
                        "name": "Xiaohong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohong Liu"
                },
                "author": "Xiaohong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14700v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14700v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15256v2",
                "updated": "2024-08-29T09:34:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    9,
                    34,
                    48,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-09T19:21:14Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    19,
                    21,
                    14,
                    4,
                    222,
                    0
                ],
                "title": "Improving Ontology Requirements Engineering with OntoChat and\n  Participatory Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Ontology Requirements Engineering with OntoChat and\n  Participatory Prompting"
                },
                "summary": "Past ontology requirements engineering (ORE) has primarily relied on manual\nmethods, such as interviews and collaborative forums, to gather user\nrequirements from domain experts, especially in large projects. Current\nOntoChat offers a framework for ORE that utilises large language models (LLMs)\nto streamline the process through four key functions: user story creation,\ncompetency question (CQ) extraction, CQ filtration and analysis, and ontology\ntesting support. In OntoChat, users are expected to prompt the chatbot to\ngenerate user stories. However, preliminary evaluations revealed that they\nstruggle to do this effectively. To address this issue, we experimented with a\nresearch method called participatory prompting, which involves\nresearcher-mediated interactions to help users without deep knowledge of LLMs\nuse the chatbot more effectively. This participatory prompting user study\nproduces pre-defined prompt templates based on user queries, focusing on\ncreating and refining personas, goals, scenarios, sample data, and data\nresources for user stories. These refined user stories will subsequently be\nconverted into CQs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Past ontology requirements engineering (ORE) has primarily relied on manual\nmethods, such as interviews and collaborative forums, to gather user\nrequirements from domain experts, especially in large projects. Current\nOntoChat offers a framework for ORE that utilises large language models (LLMs)\nto streamline the process through four key functions: user story creation,\ncompetency question (CQ) extraction, CQ filtration and analysis, and ontology\ntesting support. In OntoChat, users are expected to prompt the chatbot to\ngenerate user stories. However, preliminary evaluations revealed that they\nstruggle to do this effectively. To address this issue, we experimented with a\nresearch method called participatory prompting, which involves\nresearcher-mediated interactions to help users without deep knowledge of LLMs\nuse the chatbot more effectively. This participatory prompting user study\nproduces pre-defined prompt templates based on user queries, focusing on\ncreating and refining personas, goals, scenarios, sample data, and data\nresources for user stories. These refined user stories will subsequently be\nconverted into CQs."
                },
                "authors": [
                    {
                        "name": "Yihang Zhao"
                    },
                    {
                        "name": "Bohui Zhang"
                    },
                    {
                        "name": "Xi Hu"
                    },
                    {
                        "name": "Shuyin Ouyang"
                    },
                    {
                        "name": "Jongmo Kim"
                    },
                    {
                        "name": "Nitisha Jain"
                    },
                    {
                        "name": "Jacopo de Berardinis"
                    },
                    {
                        "name": "Albert Meroño-Peñuela"
                    },
                    {
                        "name": "Elena Simperl"
                    }
                ],
                "author_detail": {
                    "name": "Elena Simperl"
                },
                "author": "Elena Simperl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16368v1",
                "updated": "2024-08-29T09:25:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    9,
                    25,
                    10,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T09:25:10Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    9,
                    25,
                    10,
                    3,
                    242,
                    0
                ],
                "title": "Shape matters: Inferring the motility of confluent cells from static\n  images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shape matters: Inferring the motility of confluent cells from static\n  images"
                },
                "summary": "Cell motility in dense cell collectives is pivotal in various diseases like\ncancer metastasis and asthma. A central aspect in these phenomena is the\nheterogeneity in cell motility, but identifying the motility of individual\ncells is challenging. Previous work has established the importance of the\naverage cell shape in predicting cell dynamics. Here, we aim to identify the\nimportance of individual cell shape features, rather than collective features,\nto distinguish between high-motility (active) and low-motility (passive) cells\nin heterogeneous cell layers. Employing the Cellular Potts Model, we generate\nsimulation snapshots and extract static features as inputs for a simple\nmachine-learning model. Our results show that when the passive cells are\nnon-motile, this machine-learning model can accurately predict whether a cell\nis passive or active using only single-cell shape features. Furthermore, we\nexplore scenarios where passive cells also exhibit some degree of motility,\nalbeit less than active cells. In such cases, our findings indicate that a\nneural network trained on shape features can accurately classify cell motility,\nparticularly when the number of active cells is low, and the motility of active\ncells is significantly higher compared to passive cells. This work offers\npotential for physics-inspired predictions of single-cell properties with\nimplications for inferring cell dynamics from static histological images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell motility in dense cell collectives is pivotal in various diseases like\ncancer metastasis and asthma. A central aspect in these phenomena is the\nheterogeneity in cell motility, but identifying the motility of individual\ncells is challenging. Previous work has established the importance of the\naverage cell shape in predicting cell dynamics. Here, we aim to identify the\nimportance of individual cell shape features, rather than collective features,\nto distinguish between high-motility (active) and low-motility (passive) cells\nin heterogeneous cell layers. Employing the Cellular Potts Model, we generate\nsimulation snapshots and extract static features as inputs for a simple\nmachine-learning model. Our results show that when the passive cells are\nnon-motile, this machine-learning model can accurately predict whether a cell\nis passive or active using only single-cell shape features. Furthermore, we\nexplore scenarios where passive cells also exhibit some degree of motility,\nalbeit less than active cells. In such cases, our findings indicate that a\nneural network trained on shape features can accurately classify cell motility,\nparticularly when the number of active cells is low, and the motility of active\ncells is significantly higher compared to passive cells. This work offers\npotential for physics-inspired predictions of single-cell properties with\nimplications for inferring cell dynamics from static histological images."
                },
                "authors": [
                    {
                        "name": "Quirine J. S. Braat"
                    },
                    {
                        "name": "Giulia Janzen"
                    },
                    {
                        "name": "Bas C. Jansen"
                    },
                    {
                        "name": "Vincent E. Debets"
                    },
                    {
                        "name": "Simone Ciarella"
                    },
                    {
                        "name": "Liesbeth M. C. Janssen"
                    }
                ],
                "author_detail": {
                    "name": "Liesbeth M. C. Janssen"
                },
                "author": "Liesbeth M. C. Janssen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13744v2",
                "updated": "2024-08-29T09:08:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    9,
                    8,
                    54,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-25T07:08:58Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    7,
                    8,
                    58,
                    6,
                    238,
                    0
                ],
                "title": "Enhancing Adaptive Deep Networks for Image Classification via\n  Uncertainty-aware Decision Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Adaptive Deep Networks for Image Classification via\n  Uncertainty-aware Decision Fusion"
                },
                "summary": "Handling varying computational resources is a critical issue in modern AI\napplications. Adaptive deep networks, featuring the dynamic employment of\nmultiple classifier heads among different layers, have been proposed to address\nclassification tasks under varying computing resources. Existing approaches\ntypically utilize the last classifier supported by the available resources for\ninference, as they believe that the last classifier always performs better\nacross all classes. However, our findings indicate that earlier classifier\nheads can outperform the last head for certain classes. Based on this\nobservation, we introduce the Collaborative Decision Making (CDM) module, which\nfuses the multiple classifier heads to enhance the inference performance of\nadaptive deep networks. CDM incorporates an uncertainty-aware fusion method\nbased on evidential deep learning (EDL), that utilizes the reliability\n(uncertainty values) from the first c-1 classifiers to improve the c-th\nclassifier' accuracy. We also design a balance term that reduces fusion\nsaturation and unfairness issues caused by EDL constraints to improve the\nfusion quality of CDM. Finally, a regularized training strategy that uses the\nlast classifier to guide the learning process of early classifiers is proposed\nto further enhance the CDM module's effect, called the Guided Collaborative\nDecision Making (GCDM) framework. The experimental evaluation demonstrates the\neffectiveness of our approaches. Results on ImageNet datasets show CDM and GCDM\nobtain 0.4% to 2.8% accuracy improvement (under varying computing resources) on\npopular adaptive networks. The code is available at the link\nhttps://github.com/Meteor-Stars/GCDM_AdaptiveNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling varying computational resources is a critical issue in modern AI\napplications. Adaptive deep networks, featuring the dynamic employment of\nmultiple classifier heads among different layers, have been proposed to address\nclassification tasks under varying computing resources. Existing approaches\ntypically utilize the last classifier supported by the available resources for\ninference, as they believe that the last classifier always performs better\nacross all classes. However, our findings indicate that earlier classifier\nheads can outperform the last head for certain classes. Based on this\nobservation, we introduce the Collaborative Decision Making (CDM) module, which\nfuses the multiple classifier heads to enhance the inference performance of\nadaptive deep networks. CDM incorporates an uncertainty-aware fusion method\nbased on evidential deep learning (EDL), that utilizes the reliability\n(uncertainty values) from the first c-1 classifiers to improve the c-th\nclassifier' accuracy. We also design a balance term that reduces fusion\nsaturation and unfairness issues caused by EDL constraints to improve the\nfusion quality of CDM. Finally, a regularized training strategy that uses the\nlast classifier to guide the learning process of early classifiers is proposed\nto further enhance the CDM module's effect, called the Guided Collaborative\nDecision Making (GCDM) framework. The experimental evaluation demonstrates the\neffectiveness of our approaches. Results on ImageNet datasets show CDM and GCDM\nobtain 0.4% to 2.8% accuracy improvement (under varying computing resources) on\npopular adaptive networks. The code is available at the link\nhttps://github.com/Meteor-Stars/GCDM_AdaptiveNet."
                },
                "authors": [
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhipeng Xie"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Qitong Wang"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_doi": "10.1145/3664647.3681368",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681368",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.13744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 27 figures. In ACM Multimedia 2024",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18638v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18638v3",
                "updated": "2024-08-29T08:51:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    51,
                    4,
                    3,
                    242,
                    0
                ],
                "published": "2024-02-28T19:00:02Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    0,
                    2,
                    2,
                    59,
                    0
                ],
                "title": "Constraining the hadronic properties of star-forming galaxies above $1\\,\n  \\rm GeV$ with 15-years Fermi-LAT data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining the hadronic properties of star-forming galaxies above $1\\,\n  \\rm GeV$ with 15-years Fermi-LAT data"
                },
                "summary": "Star-forming and starburst galaxies (SFGs and SBGs) are powerful emitters of\nnon-thermal $\\gamma$-rays and neutrinos, due to their intense phases of\nstar-formation activity, which should confine high-energy Cosmic-Rays (CRs)\ninside their environments. In this paper, using the publicly-available\n\\texttt{fermitools}, we analyse 15.3 years of $\\gamma$-ray between $1-1000\\,\n\\rm GeV$ data for 70 sources, 56 of which were not previously detected. We find\nat~$4\\sigma$ level an indication of $\\gamma$-ray emission for other two SBGs,\nnamely M 83 and NGC 1365. By contrast, we find that, even with the new\ndescription of background, the significance for the $\\gamma$-ray emission of M\n33~(initially reported as discovered) still stands at $\\sim \\, 4\\sigma$ (as\nalready reported by previous works). Along with previous findings, the flux of\neach detected source is consistent with a $\\sim E^{-2.3/2.4}$ spectrum,\ncompatible with the injected CR flux inferred for CRs in the Milky-Way. We\nnotice that the correlation between the calorimetric fraction~$F_{\\rm cal}$ of\nhigh-energy protons in SFGs and SBGs (the fraction of high-energy protons\nactually producing high-energy $\\gamma$-rays and neutrinos) and the SFR is in\naccordance with the expected scaling relation for CR escape dominated by\nadvection. We remark that undiscovered sources strongly constrain $F_{\\rm cal}$\nat 95\\% CL, providing fundamental information when we interpret the results as\ncommon properties of SFGs and SBGs. Finally, we find that these sources might\ncontribute $(12\\pm 3)\\%$ to the EGB, while the corresponding diffuse neutrino\nflux strongly depends on the spectral index distribution along the source\nclass.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star-forming and starburst galaxies (SFGs and SBGs) are powerful emitters of\nnon-thermal $\\gamma$-rays and neutrinos, due to their intense phases of\nstar-formation activity, which should confine high-energy Cosmic-Rays (CRs)\ninside their environments. In this paper, using the publicly-available\n\\texttt{fermitools}, we analyse 15.3 years of $\\gamma$-ray between $1-1000\\,\n\\rm GeV$ data for 70 sources, 56 of which were not previously detected. We find\nat~$4\\sigma$ level an indication of $\\gamma$-ray emission for other two SBGs,\nnamely M 83 and NGC 1365. By contrast, we find that, even with the new\ndescription of background, the significance for the $\\gamma$-ray emission of M\n33~(initially reported as discovered) still stands at $\\sim \\, 4\\sigma$ (as\nalready reported by previous works). Along with previous findings, the flux of\neach detected source is consistent with a $\\sim E^{-2.3/2.4}$ spectrum,\ncompatible with the injected CR flux inferred for CRs in the Milky-Way. We\nnotice that the correlation between the calorimetric fraction~$F_{\\rm cal}$ of\nhigh-energy protons in SFGs and SBGs (the fraction of high-energy protons\nactually producing high-energy $\\gamma$-rays and neutrinos) and the SFR is in\naccordance with the expected scaling relation for CR escape dominated by\nadvection. We remark that undiscovered sources strongly constrain $F_{\\rm cal}$\nat 95\\% CL, providing fundamental information when we interpret the results as\ncommon properties of SFGs and SBGs. Finally, we find that these sources might\ncontribute $(12\\pm 3)\\%$ to the EGB, while the corresponding diffuse neutrino\nflux strongly depends on the spectral index distribution along the source\nclass."
                },
                "authors": [
                    {
                        "name": "Antonio Ambrosone"
                    },
                    {
                        "name": "Marco Chianese"
                    },
                    {
                        "name": "Antonio Marinelli"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Marinelli"
                },
                "author": "Antonio Marinelli",
                "arxiv_doi": "10.1088/1475-7516/2024/08/040",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1475-7516/2024/08/040",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.18638v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18638v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "32 pages, 9 figures, 16 Tables. Version 3",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01602v2",
                "updated": "2024-08-29T08:49:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    49,
                    14,
                    3,
                    242,
                    0
                ],
                "published": "2024-04-02T02:46:18Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    2,
                    46,
                    18,
                    1,
                    93,
                    0
                ],
                "title": "Helmsman of the Masses? Evaluate the Opinion Leadership of Large\n  Language Models in the Werewolf Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helmsman of the Masses? Evaluate the Opinion Leadership of Large\n  Language Models in the Werewolf Game"
                },
                "summary": "Large language models (LLMs) have exhibited memorable strategic behaviors in\nsocial deductive games. However, the significance of opinion leadership\nexhibited by LLM-based agents has been largely overlooked, which is crucial for\npractical applications in multi-agent and human-AI interaction settings.\nOpinion leaders are individuals who have a noticeable impact on the beliefs and\nbehaviors of others within a social group. In this work, we employ the Werewolf\ngame as a simulation platform to assess the opinion leadership of LLMs. The\ngame includes the role of the Sheriff, tasked with summarizing arguments and\nrecommending decision options, and therefore serves as a credible proxy for an\nopinion leader. We develop a framework integrating the Sheriff role and devise\ntwo novel metrics based on the critical characteristics of opinion leaders. The\nfirst metric measures the reliability of the opinion leader, and the second\nassesses the influence of the opinion leader on other players' decisions. We\nconduct extensive experiments to evaluate LLMs of different scales. In\naddition, we collect a Werewolf question-answering dataset (WWQA) to assess and\nenhance LLM's grasp of the game rules, and we also incorporate human\nparticipants for further analysis. The results suggest that the Werewolf game\nis a suitable test bed to evaluate the opinion leadership of LLMs, and few LLMs\npossess the capacity for opinion leadership.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited memorable strategic behaviors in\nsocial deductive games. However, the significance of opinion leadership\nexhibited by LLM-based agents has been largely overlooked, which is crucial for\npractical applications in multi-agent and human-AI interaction settings.\nOpinion leaders are individuals who have a noticeable impact on the beliefs and\nbehaviors of others within a social group. In this work, we employ the Werewolf\ngame as a simulation platform to assess the opinion leadership of LLMs. The\ngame includes the role of the Sheriff, tasked with summarizing arguments and\nrecommending decision options, and therefore serves as a credible proxy for an\nopinion leader. We develop a framework integrating the Sheriff role and devise\ntwo novel metrics based on the critical characteristics of opinion leaders. The\nfirst metric measures the reliability of the opinion leader, and the second\nassesses the influence of the opinion leader on other players' decisions. We\nconduct extensive experiments to evaluate LLMs of different scales. In\naddition, we collect a Werewolf question-answering dataset (WWQA) to assess and\nenhance LLM's grasp of the game rules, and we also incorporate human\nparticipants for further analysis. The results suggest that the Werewolf game\nis a suitable test bed to evaluate the opinion leadership of LLMs, and few LLMs\npossess the capacity for opinion leadership."
                },
                "authors": [
                    {
                        "name": "Silin Du"
                    },
                    {
                        "name": "Xiaowei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Zhang"
                },
                "author": "Xiaowei Zhang",
                "arxiv_comment": "Published as a conference paper at COLM 2024. 37 pages, 6 figures, 27\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05482v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05482v4",
                "updated": "2024-08-29T08:46:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    46,
                    46,
                    3,
                    242,
                    0
                ],
                "published": "2024-06-08T14:14:19Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    14,
                    14,
                    19,
                    5,
                    160,
                    0
                ],
                "title": "Efficient Topology-aware Data Augmentation for High-Degree Graph Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Topology-aware Data Augmentation for High-Degree Graph Neural\n  Networks"
                },
                "summary": "In recent years, graph neural networks (GNNs) have emerged as a potent tool\nfor learning on graph-structured data and won fruitful successes in varied\nfields. The majority of GNNs follow the message-passing paradigm, where\nrepresentations of each node are learned by recursively aggregating features of\nits neighbors. However, this mechanism brings severe over-smoothing and\nefficiency issues over high-degree graphs (HDGs), wherein most nodes have\ndozens (or even hundreds) of neighbors, such as social networks, transaction\ngraphs, power grids, etc. Additionally, such graphs usually encompass rich and\ncomplex structure semantics, which are hard to capture merely by feature\naggregations in GNNs. Motivated by the above limitations, we propose TADA, an\nefficient and effective front-mounted data augmentation framework for GNNs on\nHDGs. Under the hood, TADA includes two key modules: (i) feature expansion with\nstructure embeddings, and (ii) topology- and attribute-aware graph\nsparsification. The former obtains augmented node features and enhanced model\ncapacity by encoding the graph structure into high-quality structure embeddings\nwith our highly-efficient sketching method. Further, by exploiting\ntask-relevant features extracted from graph structures and attributes, the\nsecond module enables the accurate identification and reduction of numerous\nredundant/noisy edges from the input graph, thereby alleviating over-smoothing\nand facilitating faster feature aggregations over HDGs. Empirically, TADA\nconsiderably improves the predictive performance of mainstream GNN models on 8\nreal homophilic/heterophilic HDGs in terms of node classification, while\nachieving efficient training and inference processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, graph neural networks (GNNs) have emerged as a potent tool\nfor learning on graph-structured data and won fruitful successes in varied\nfields. The majority of GNNs follow the message-passing paradigm, where\nrepresentations of each node are learned by recursively aggregating features of\nits neighbors. However, this mechanism brings severe over-smoothing and\nefficiency issues over high-degree graphs (HDGs), wherein most nodes have\ndozens (or even hundreds) of neighbors, such as social networks, transaction\ngraphs, power grids, etc. Additionally, such graphs usually encompass rich and\ncomplex structure semantics, which are hard to capture merely by feature\naggregations in GNNs. Motivated by the above limitations, we propose TADA, an\nefficient and effective front-mounted data augmentation framework for GNNs on\nHDGs. Under the hood, TADA includes two key modules: (i) feature expansion with\nstructure embeddings, and (ii) topology- and attribute-aware graph\nsparsification. The former obtains augmented node features and enhanced model\ncapacity by encoding the graph structure into high-quality structure embeddings\nwith our highly-efficient sketching method. Further, by exploiting\ntask-relevant features extracted from graph structures and attributes, the\nsecond module enables the accurate identification and reduction of numerous\nredundant/noisy edges from the input graph, thereby alleviating over-smoothing\nand facilitating faster feature aggregations over HDGs. Empirically, TADA\nconsiderably improves the predictive performance of mainstream GNN models on 8\nreal homophilic/heterophilic HDGs in terms of node classification, while\nachieving efficient training and inference processes."
                },
                "authors": [
                    {
                        "name": "Yurui Lai"
                    },
                    {
                        "name": "Xiaoyang Lin"
                    },
                    {
                        "name": "Renchi Yang"
                    },
                    {
                        "name": "Hongtao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongtao Wang"
                },
                "author": "Hongtao Wang",
                "arxiv_comment": "This is the technical report for the paper accepted to KDD 2024. 16\n  pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05482v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05482v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09816v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09816v4",
                "updated": "2024-08-29T08:45:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    45,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-07-13T09:22:33Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    9,
                    22,
                    33,
                    5,
                    195,
                    0
                ],
                "title": "MaskMoE: Boosting Token-Level Learning via Routing Mask in\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaskMoE: Boosting Token-Level Learning via Routing Mask in\n  Mixture-of-Experts"
                },
                "summary": "Scaling the size of a model enhances its capabilities but significantly\nincreases computation complexity. Mixture-of-Experts models (MoE) address the\nissue by allowing model size to scale up without substantially increasing\ntraining or inference costs. In MoE, there is an important module called the\nrouter, which is used to distribute each token to the experts. Currently, the\nmainstream routing methods include dynamic routing and fixed routing. Despite\ntheir promising results, MoE models encounter several challenges. Primarily,\nfor dynamic routing methods, the dispersion of training tokens across multiple\nexperts can lead to underfitting, particularly for infrequent tokens.\nAdditionally, though fixed routing methods can mitigate that issue, they\ncompromise on the diversity of representations. In this paper, we propose\n\\textbf{MaskMoE}, a method designed to enhance token-level learning by\nemploying a routing \\textbf{mask}ing technique within the\n\\textbf{M}ixture-\\textbf{o}f-\\textbf{E}xperts model. MaskMoE is capable of\nmaintaining representation diversity while achieving more comprehensive\ntraining. Experimental results demonstrate that our method outperforms previous\ndominant Mixture-of-Experts models in terms of both perplexity (PPL) and\ndownstream task performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the size of a model enhances its capabilities but significantly\nincreases computation complexity. Mixture-of-Experts models (MoE) address the\nissue by allowing model size to scale up without substantially increasing\ntraining or inference costs. In MoE, there is an important module called the\nrouter, which is used to distribute each token to the experts. Currently, the\nmainstream routing methods include dynamic routing and fixed routing. Despite\ntheir promising results, MoE models encounter several challenges. Primarily,\nfor dynamic routing methods, the dispersion of training tokens across multiple\nexperts can lead to underfitting, particularly for infrequent tokens.\nAdditionally, though fixed routing methods can mitigate that issue, they\ncompromise on the diversity of representations. In this paper, we propose\n\\textbf{MaskMoE}, a method designed to enhance token-level learning by\nemploying a routing \\textbf{mask}ing technique within the\n\\textbf{M}ixture-\\textbf{o}f-\\textbf{E}xperts model. MaskMoE is capable of\nmaintaining representation diversity while achieving more comprehensive\ntraining. Experimental results demonstrate that our method outperforms previous\ndominant Mixture-of-Experts models in terms of both perplexity (PPL) and\ndownstream task performance."
                },
                "authors": [
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Xue Bai"
                    },
                    {
                        "name": "Xing Wu"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Haoran Lian"
                    },
                    {
                        "name": "Guangyuan Ma"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Guiguang Ding"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09816v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09816v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15533v2",
                "updated": "2024-08-29T08:45:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    45,
                    30,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-28T04:44:43Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    4,
                    44,
                    43,
                    2,
                    241,
                    0
                ],
                "title": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via\n  Layer-wise Relevance Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via\n  Layer-wise Relevance Propagation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines."
                },
                "authors": [
                    {
                        "name": "Haichuan Hu"
                    },
                    {
                        "name": "Yuhan Sun"
                    },
                    {
                        "name": "Quanjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanjun Zhang"
                },
                "author": "Quanjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16345v1",
                "updated": "2024-08-29T08:30:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    30,
                    33,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T08:30:33Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    30,
                    33,
                    3,
                    242,
                    0
                ],
                "title": "The Unreasonable Ineffectiveness of Nucleus Sampling on Mitigating Text\n  Memorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Unreasonable Ineffectiveness of Nucleus Sampling on Mitigating Text\n  Memorization"
                },
                "summary": "This work analyses the text memorization behavior of large language models\n(LLMs) when subjected to nucleus sampling. Stochastic decoding methods like\nnucleus sampling are typically applied to overcome issues such as monotonous\nand repetitive text generation, which are often observed with\nmaximization-based decoding techniques. We hypothesize that nucleus sampling\nmight also reduce the occurrence of memorization patterns, because it could\nlead to the selection of tokens outside the memorized sequence. To test this\nhypothesis we create a diagnostic dataset with a known distribution of\nduplicates that gives us some control over the likelihood of memorization of\ncertain parts of the training data. Our analysis of two GPT-Neo models\nfine-tuned on this dataset interestingly shows that (i) an increase of the\nnucleus size reduces memorization only modestly, and (ii) even when models do\nnot engage in \"hard\" memorization -- a verbatim reproduction of training\nsamples -- they may still display \"soft\" memorization whereby they generate\noutputs that echo the training data but without a complete one-by-one\nresemblance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work analyses the text memorization behavior of large language models\n(LLMs) when subjected to nucleus sampling. Stochastic decoding methods like\nnucleus sampling are typically applied to overcome issues such as monotonous\nand repetitive text generation, which are often observed with\nmaximization-based decoding techniques. We hypothesize that nucleus sampling\nmight also reduce the occurrence of memorization patterns, because it could\nlead to the selection of tokens outside the memorized sequence. To test this\nhypothesis we create a diagnostic dataset with a known distribution of\nduplicates that gives us some control over the likelihood of memorization of\ncertain parts of the training data. Our analysis of two GPT-Neo models\nfine-tuned on this dataset interestingly shows that (i) an increase of the\nnucleus size reduces memorization only modestly, and (ii) even when models do\nnot engage in \"hard\" memorization -- a verbatim reproduction of training\nsamples -- they may still display \"soft\" memorization whereby they generate\noutputs that echo the training data but without a complete one-by-one\nresemblance."
                },
                "authors": [
                    {
                        "name": "Luka Borec"
                    },
                    {
                        "name": "Philipp Sadler"
                    },
                    {
                        "name": "David Schlangen"
                    }
                ],
                "author_detail": {
                    "name": "David Schlangen"
                },
                "author": "David Schlangen",
                "arxiv_comment": "9 pages, Accepted at INLG 2024 (International Natural Language\n  Generation Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12326v2",
                "updated": "2024-08-29T08:27:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    27,
                    27,
                    3,
                    242,
                    0
                ],
                "published": "2024-02-19T18:00:30Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    18,
                    0,
                    30,
                    0,
                    50,
                    0
                ],
                "title": "PsychoGAT: A Novel Psychological Measurement Paradigm through\n  Interactive Fiction Games with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsychoGAT: A Novel Psychological Measurement Paradigm through\n  Interactive Fiction Games with LLM Agents"
                },
                "summary": "Psychological measurement is essential for mental health, self-understanding,\nand personal development. Traditional methods, such as self-report scales and\npsychologist interviews, often face challenges with engagement and\naccessibility. While game-based and LLM-based tools have been explored to\nimprove user interest and automate assessment, they struggle to balance\nengagement with generalizability. In this work, we propose PsychoGAT\n(Psychological Game AgenTs) to achieve a generic gamification of psychological\nassessment. The main insight is that powerful LLMs can function both as adept\npsychologists and innovative game designers. By incorporating LLM agents into\ndesignated roles and carefully managing their interactions, PsychoGAT can\ntransform any standardized scales into personalized and engaging interactive\nfiction games. To validate the proposed method, we conduct psychometric\nevaluations to assess its effectiveness and employ human evaluators to examine\nthe generated content across various psychological constructs, including\ndepression, cognitive distortions, and personality traits. Results demonstrate\nthat PsychoGAT serves as an effective assessment tool, achieving statistically\nsignificant excellence in psychometric metrics such as reliability, convergent\nvalidity, and discriminant validity. Moreover, human evaluations confirm\nPsychoGAT's enhancements in content coherence, interactivity, interest,\nimmersion, and satisfaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psychological measurement is essential for mental health, self-understanding,\nand personal development. Traditional methods, such as self-report scales and\npsychologist interviews, often face challenges with engagement and\naccessibility. While game-based and LLM-based tools have been explored to\nimprove user interest and automate assessment, they struggle to balance\nengagement with generalizability. In this work, we propose PsychoGAT\n(Psychological Game AgenTs) to achieve a generic gamification of psychological\nassessment. The main insight is that powerful LLMs can function both as adept\npsychologists and innovative game designers. By incorporating LLM agents into\ndesignated roles and carefully managing their interactions, PsychoGAT can\ntransform any standardized scales into personalized and engaging interactive\nfiction games. To validate the proposed method, we conduct psychometric\nevaluations to assess its effectiveness and employ human evaluators to examine\nthe generated content across various psychological constructs, including\ndepression, cognitive distortions, and personality traits. Results demonstrate\nthat PsychoGAT serves as an effective assessment tool, achieving statistically\nsignificant excellence in psychometric metrics such as reliability, convergent\nvalidity, and discriminant validity. Moreover, human evaluations confirm\nPsychoGAT's enhancements in content coherence, interactivity, interest,\nimmersion, and satisfaction."
                },
                "authors": [
                    {
                        "name": "Qisen Yang"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Honghui Chen"
                    },
                    {
                        "name": "Shenzhi Wang"
                    },
                    {
                        "name": "Yifan Pu"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Shiji Song"
                    },
                    {
                        "name": "Gao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gao Huang"
                },
                "author": "Gao Huang",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14507v2",
                "updated": "2024-08-29T08:24:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    24,
                    42,
                    3,
                    242,
                    0
                ],
                "published": "2024-07-19T17:59:03Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    17,
                    59,
                    3,
                    4,
                    201,
                    0
                ],
                "title": "Internal Consistency and Self-Feedback in Large Language Models: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal Consistency and Self-Feedback in Large Language Models: A\n  Survey"
                },
                "summary": "Large language models (LLMs) often exhibit deficient reasoning or generate\nhallucinations. To address these, studies prefixed with \"Self-\" such as\nSelf-Consistency, Self-Improve, and Self-Refine have been initiated. They share\na commonality: involving LLMs evaluating and updating themselves. Nonetheless,\nthese efforts lack a unified perspective on summarization, as existing surveys\npredominantly focus on categorization.\n  In this paper, we summarize a theoretical framework, Internal Consistency,\noffering explanations for reasoning deficiencies and hallucinations. Internal\nConsistency refers to the consistency in expressions among LLMs' latent,\ndecoding, or response layers based on sampling methodologies. Then, we\nintroduce another effective theoretical framework capable of mining Internal\nConsistency, named Self-Feedback. This framework consists of two modules:\nSelf-Evaluation and Self-Update. The former captures Internal Consistency\nSignals, while the latter leverages the signals to enhance either the model's\nresponse or the model itself. This framework has been employed in numerous\nstudies.\n  We systematically classify these studies by tasks and lines of work;\nsummarize relevant evaluation methods and benchmarks; and delve into the\nconcern, \"Does Self-Feedback Really Work?\" We also propose several critical\nviewpoints, including the \"Hourglass Evolution of Internal Consistency\",\n\"Consistency Is (Almost) Correctness\" hypothesis, and \"The Paradox of Latent\nand Explicit Reasoning\". The relevant resources are open-sourced at\nhttps://github.com/IAAR-Shanghai/ICSFSurvey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit deficient reasoning or generate\nhallucinations. To address these, studies prefixed with \"Self-\" such as\nSelf-Consistency, Self-Improve, and Self-Refine have been initiated. They share\na commonality: involving LLMs evaluating and updating themselves. Nonetheless,\nthese efforts lack a unified perspective on summarization, as existing surveys\npredominantly focus on categorization.\n  In this paper, we summarize a theoretical framework, Internal Consistency,\noffering explanations for reasoning deficiencies and hallucinations. Internal\nConsistency refers to the consistency in expressions among LLMs' latent,\ndecoding, or response layers based on sampling methodologies. Then, we\nintroduce another effective theoretical framework capable of mining Internal\nConsistency, named Self-Feedback. This framework consists of two modules:\nSelf-Evaluation and Self-Update. The former captures Internal Consistency\nSignals, while the latter leverages the signals to enhance either the model's\nresponse or the model itself. This framework has been employed in numerous\nstudies.\n  We systematically classify these studies by tasks and lines of work;\nsummarize relevant evaluation methods and benchmarks; and delve into the\nconcern, \"Does Self-Feedback Really Work?\" We also propose several critical\nviewpoints, including the \"Hourglass Evolution of Internal Consistency\",\n\"Consistency Is (Almost) Correctness\" hypothesis, and \"The Paradox of Latent\nand Explicit Reasoning\". The relevant resources are open-sourced at\nhttps://github.com/IAAR-Shanghai/ICSFSurvey."
                },
                "authors": [
                    {
                        "name": "Xun Liang"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Zifan Zheng"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Qingchen Yu"
                    },
                    {
                        "name": "Xunkai Li"
                    },
                    {
                        "name": "Rong-Hua Li"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhonghao Wang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "24 pages, 9 figures, 7 tables, 14 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16326v1",
                "updated": "2024-08-29T08:02:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    2,
                    9,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T08:02:09Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    2,
                    9,
                    3,
                    242,
                    0
                ],
                "title": "Critic-CoT: Boosting the reasoning abilities of large language model via\n  Chain-of-thoughts Critic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critic-CoT: Boosting the reasoning abilities of large language model via\n  Chain-of-thoughts Critic"
                },
                "summary": "Self-critic has become an important mechanism for enhancing the reasoning\nperformance of LLMs. However, current approaches mainly involve basic prompts\nwithout further training, which tend to be over-simplified, leading to limited\naccuracy.Moreover, there is a lack of in-depth investigation of the\nrelationship between LLM's ability to criticism and its task-solving\nperformance.To address these issues, we propose Critic-CoT, a novel framework\nthat pushes LLMs toward System-2-like critic capability, via step-wise CoT\nreasoning format and distant-supervision data construction, without the need\nfor human annotation. Experiments on GSM8K and MATH show that via filtering out\ninvalid solutions or iterative refinement, our enhanced model boosts\ntask-solving performance, which demonstrates the effectiveness of our method.\nFurther, we find that training on critique and refinement alone improves the\ngeneration. We hope our work could shed light on future research on improving\nthe reasoning and critic ability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-critic has become an important mechanism for enhancing the reasoning\nperformance of LLMs. However, current approaches mainly involve basic prompts\nwithout further training, which tend to be over-simplified, leading to limited\naccuracy.Moreover, there is a lack of in-depth investigation of the\nrelationship between LLM's ability to criticism and its task-solving\nperformance.To address these issues, we propose Critic-CoT, a novel framework\nthat pushes LLMs toward System-2-like critic capability, via step-wise CoT\nreasoning format and distant-supervision data construction, without the need\nfor human annotation. Experiments on GSM8K and MATH show that via filtering out\ninvalid solutions or iterative refinement, our enhanced model boosts\ntask-solving performance, which demonstrates the effectiveness of our method.\nFurther, we find that training on critique and refinement alone improves the\ngeneration. We hope our work could shed light on future research on improving\nthe reasoning and critic ability of LLMs."
                },
                "authors": [
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Jie Lou"
                    },
                    {
                        "name": "Boxi Cao"
                    },
                    {
                        "name": "Xueru Wen"
                    },
                    {
                        "name": "Yuqiu Ji"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Debing Zhang"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10039v2",
                "updated": "2024-08-29T07:21:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    7,
                    21,
                    54,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-19T14:31:57Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    31,
                    57,
                    0,
                    232,
                    0
                ],
                "title": "MSDiagnosis: An EMR-based Dataset for Clinical Multi-Step Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSDiagnosis: An EMR-based Dataset for Clinical Multi-Step Diagnosis"
                },
                "summary": "Clinical diagnosis is critical in medical practice, typically requiring a\ncontinuous and evolving process that includes primary diagnosis, differential\ndiagnosis, and final diagnosis. However, most existing clinical diagnostic\ntasks are single-step processes, which does not align with the complex\nmulti-step diagnostic procedures found in real-world clinical settings. In this\npaper, we propose a multi-step diagnostic task and annotate a clinical\ndiagnostic dataset (MSDiagnosis). This dataset includes primary diagnosis,\ndifferential diagnosis, and final diagnosis questions. Additionally, we propose\na novel and effective framework. This framework combines forward inference,\nbackward inference, reflection, and refinement, enabling the LLM to\nself-evaluate and adjust its diagnostic results. To assess the effectiveness of\nour proposed method, we design and conduct extensive experiments. The\nexperimental results demonstrate the effectiveness of the proposed method. We\nalso provide a comprehensive experimental analysis and suggest future research\ndirections for this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical diagnosis is critical in medical practice, typically requiring a\ncontinuous and evolving process that includes primary diagnosis, differential\ndiagnosis, and final diagnosis. However, most existing clinical diagnostic\ntasks are single-step processes, which does not align with the complex\nmulti-step diagnostic procedures found in real-world clinical settings. In this\npaper, we propose a multi-step diagnostic task and annotate a clinical\ndiagnostic dataset (MSDiagnosis). This dataset includes primary diagnosis,\ndifferential diagnosis, and final diagnosis questions. Additionally, we propose\na novel and effective framework. This framework combines forward inference,\nbackward inference, reflection, and refinement, enabling the LLM to\nself-evaluate and adjust its diagnostic results. To assess the effectiveness of\nour proposed method, we design and conduct extensive experiments. The\nexperimental results demonstrate the effectiveness of the proposed method. We\nalso provide a comprehensive experimental analysis and suggest future research\ndirections for this task."
                },
                "authors": [
                    {
                        "name": "Ruihui Hou"
                    },
                    {
                        "name": "Shencheng Chen"
                    },
                    {
                        "name": "Yongqi Fan"
                    },
                    {
                        "name": "Lifeng Zhu"
                    },
                    {
                        "name": "Jing Sun"
                    },
                    {
                        "name": "Jingping Liu"
                    },
                    {
                        "name": "Tong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Tong Ruan"
                },
                "author": "Tong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16312v2",
                "updated": "2024-08-30T11:48:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    11,
                    48,
                    40,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-29T07:20:56Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    7,
                    20,
                    56,
                    3,
                    242,
                    0
                ],
                "title": "SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval"
                },
                "summary": "Large-scale test collections play a crucial role in Information Retrieval\n(IR) research. However, according to the Cranfield paradigm and the research\ninto publicly available datasets, the existing information retrieval research\nstudies are commonly developed on small-scale datasets that rely on human\nassessors for relevance judgments - a time-intensive and expensive process.\nRecent studies have shown the strong capability of Large Language Models (LLMs)\nin producing reliable relevance judgments with human accuracy but at a greatly\nreduced cost. In this paper, to address the missing large-scale ad-hoc document\nretrieval dataset, we extend the TREC Deep Learning Track (DL) test collection\nvia additional language model synthetic labels to enable researchers to test\nand evaluate their search systems at a large scale. Specifically, such a test\ncollection includes more than 1,900 test queries from the previous years of\ntracks. We compare system evaluation with past human labels from past years and\nfind that our synthetically created large-scale test collection can lead to\nhighly correlated system rankings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale test collections play a crucial role in Information Retrieval\n(IR) research. However, according to the Cranfield paradigm and the research\ninto publicly available datasets, the existing information retrieval research\nstudies are commonly developed on small-scale datasets that rely on human\nassessors for relevance judgments - a time-intensive and expensive process.\nRecent studies have shown the strong capability of Large Language Models (LLMs)\nin producing reliable relevance judgments with human accuracy but at a greatly\nreduced cost. In this paper, to address the missing large-scale ad-hoc document\nretrieval dataset, we extend the TREC Deep Learning Track (DL) test collection\nvia additional language model synthetic labels to enable researchers to test\nand evaluate their search systems at a large scale. Specifically, such a test\ncollection includes more than 1,900 test queries from the previous years of\ntracks. We compare system evaluation with past human labels from past years and\nfind that our synthetically created large-scale test collection can lead to\nhighly correlated system rankings."
                },
                "authors": [
                    {
                        "name": "Hossein A. Rahmani"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Emine Yilmaz"
                    },
                    {
                        "name": "Nick Craswell"
                    },
                    {
                        "name": "Bhaskar Mitra"
                    },
                    {
                        "name": "Paul Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Paul Thomas"
                },
                "author": "Paul Thomas",
                "arxiv_comment": "9 pages, resource paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16304v1",
                "updated": "2024-08-29T07:11:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    7,
                    11,
                    9,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T07:11:09Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    7,
                    11,
                    9,
                    3,
                    242,
                    0
                ],
                "title": "Understanding Privacy Norms through Web Forms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Privacy Norms through Web Forms"
                },
                "summary": "Web forms are one of the primary ways to collect personal information online,\nyet they are relatively under-studied. Unlike web tracking, data collection\nthrough web forms is explicit and contextualized. Users (i) are asked to input\nspecific personal information types, and (ii) know the specific context (i.e.,\non which website and for what purpose). For web forms to be trusted by users,\nthey must meet the common sense standards of appropriate data collection\npractices within a particular context (i.e., privacy norms). In this paper, we\nextract the privacy norms embedded within web forms through a measurement\nstudy. First, we build a specialized crawler to discover web forms on websites.\nWe run it on 11,500 popular websites, and we create a dataset of 293K web\nforms. Second, to process data of this scale, we develop a cost-efficient way\nto annotate web forms with form types and personal information types, using\ntext classifiers trained with assistance of large language models (LLMs).\nThird, by analyzing the annotated dataset, we reveal common patterns of data\ncollection practices. We find that (i) these patterns are explained by\nfunctional necessities and legal obligations, thus reflecting privacy norms,\nand that (ii) deviations from the observed norms often signal unnecessary data\ncollection. In addition, we analyze the privacy policies that accompany web\nforms. We show that, despite their wide adoption and use, there is a disconnect\nbetween privacy policy disclosures and the observed privacy norms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web forms are one of the primary ways to collect personal information online,\nyet they are relatively under-studied. Unlike web tracking, data collection\nthrough web forms is explicit and contextualized. Users (i) are asked to input\nspecific personal information types, and (ii) know the specific context (i.e.,\non which website and for what purpose). For web forms to be trusted by users,\nthey must meet the common sense standards of appropriate data collection\npractices within a particular context (i.e., privacy norms). In this paper, we\nextract the privacy norms embedded within web forms through a measurement\nstudy. First, we build a specialized crawler to discover web forms on websites.\nWe run it on 11,500 popular websites, and we create a dataset of 293K web\nforms. Second, to process data of this scale, we develop a cost-efficient way\nto annotate web forms with form types and personal information types, using\ntext classifiers trained with assistance of large language models (LLMs).\nThird, by analyzing the annotated dataset, we reveal common patterns of data\ncollection practices. We find that (i) these patterns are explained by\nfunctional necessities and legal obligations, thus reflecting privacy norms,\nand that (ii) deviations from the observed norms often signal unnecessary data\ncollection. In addition, we analyze the privacy policies that accompany web\nforms. We show that, despite their wide adoption and use, there is a disconnect\nbetween privacy policy disclosures and the observed privacy norms."
                },
                "authors": [
                    {
                        "name": "Hao Cui"
                    },
                    {
                        "name": "Rahmadi Trimananda"
                    },
                    {
                        "name": "Athina Markopoulou"
                    }
                ],
                "author_detail": {
                    "name": "Athina Markopoulou"
                },
                "author": "Athina Markopoulou",
                "arxiv_comment": "18 pages, 7 figures, to be published in the Proceedings on Privacy\n  Enhancing Technologies (PoPETs) 2025.1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16296v1",
                "updated": "2024-08-29T06:54:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    6,
                    54,
                    3,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T06:54:03Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    6,
                    54,
                    3,
                    3,
                    242,
                    0
                ],
                "title": "Rethinking Sparse Lexical Representations for Image Retrieval in the Age\n  of Rising Multi-Modal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Sparse Lexical Representations for Image Retrieval in the Age\n  of Rising Multi-Modal Large Language Models"
                },
                "summary": "In this paper, we rethink sparse lexical representations for image retrieval.\nBy utilizing multi-modal large language models (M-LLMs) that support visual\nprompting, we can extract image features and convert them into textual data,\nenabling us to utilize efficient sparse retrieval algorithms employed in\nnatural language processing for image retrieval tasks. To assist the LLM in\nextracting image features, we apply data augmentation techniques for key\nexpansion and analyze the impact with a metric for relevance between images and\ntextual data. We empirically show the superior precision and recall performance\nof our image retrieval method compared to conventional vision-language\nmodel-based methods on the MS-COCO, PASCAL VOC, and NUS-WIDE datasets in a\nkeyword-based image retrieval scenario, where keywords serve as search queries.\nWe also demonstrate that the retrieval performance can be improved by\niteratively incorporating keywords into search queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we rethink sparse lexical representations for image retrieval.\nBy utilizing multi-modal large language models (M-LLMs) that support visual\nprompting, we can extract image features and convert them into textual data,\nenabling us to utilize efficient sparse retrieval algorithms employed in\nnatural language processing for image retrieval tasks. To assist the LLM in\nextracting image features, we apply data augmentation techniques for key\nexpansion and analyze the impact with a metric for relevance between images and\ntextual data. We empirically show the superior precision and recall performance\nof our image retrieval method compared to conventional vision-language\nmodel-based methods on the MS-COCO, PASCAL VOC, and NUS-WIDE datasets in a\nkeyword-based image retrieval scenario, where keywords serve as search queries.\nWe also demonstrate that the retrieval performance can be improved by\niteratively incorporating keywords into search queries."
                },
                "authors": [
                    {
                        "name": "Kengo Nakata"
                    },
                    {
                        "name": "Daisuke Miyashita"
                    },
                    {
                        "name": "Youyang Ng"
                    },
                    {
                        "name": "Yasuto Hoshi"
                    },
                    {
                        "name": "Jun Deguchi"
                    }
                ],
                "author_detail": {
                    "name": "Jun Deguchi"
                },
                "author": "Jun Deguchi",
                "arxiv_comment": "Accepted to ECCV 2024 Workshops: 2nd Workshop on Traditional Computer\n  Vision in the Age of Deep Learning (TradiCV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16276v1",
                "updated": "2024-08-29T05:47:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    5,
                    47,
                    14,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T05:47:14Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    5,
                    47,
                    14,
                    3,
                    242,
                    0
                ],
                "title": "Enhancing AI-Driven Psychological Consultation: Layered Prompts with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing AI-Driven Psychological Consultation: Layered Prompts with\n  Large Language Models"
                },
                "summary": "Psychological consultation is essential for improving mental health and\nwell-being, yet challenges such as the shortage of qualified professionals and\nscalability issues limit its accessibility. To address these challenges, we\nexplore the use of large language models (LLMs) like GPT-4 to augment\npsychological consultation services. Our approach introduces a novel layered\nprompting system that dynamically adapts to user input, enabling comprehensive\nand relevant information gathering. We also develop empathy-driven and\nscenario-based prompts to enhance the LLM's emotional intelligence and\ncontextual understanding in therapeutic settings. We validated our approach\nthrough experiments using a newly collected dataset of psychological\nconsultation dialogues, demonstrating significant improvements in response\nquality. The results highlight the potential of our prompt engineering\ntechniques to enhance AI-driven psychological consultation, offering a scalable\nand accessible solution to meet the growing demand for mental health support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psychological consultation is essential for improving mental health and\nwell-being, yet challenges such as the shortage of qualified professionals and\nscalability issues limit its accessibility. To address these challenges, we\nexplore the use of large language models (LLMs) like GPT-4 to augment\npsychological consultation services. Our approach introduces a novel layered\nprompting system that dynamically adapts to user input, enabling comprehensive\nand relevant information gathering. We also develop empathy-driven and\nscenario-based prompts to enhance the LLM's emotional intelligence and\ncontextual understanding in therapeutic settings. We validated our approach\nthrough experiments using a newly collected dataset of psychological\nconsultation dialogues, demonstrating significant improvements in response\nquality. The results highlight the potential of our prompt engineering\ntechniques to enhance AI-driven psychological consultation, offering a scalable\nand accessible solution to meet the growing demand for mental health support."
                },
                "authors": [
                    {
                        "name": "Rafael Souza"
                    },
                    {
                        "name": "Jia-Hao Lim"
                    },
                    {
                        "name": "Alexander Davis"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Davis"
                },
                "author": "Alexander Davis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.11911v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.11911v6",
                "updated": "2024-08-29T05:14:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    5,
                    14,
                    36,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-21T09:22:07Z",
                "published_parsed": [
                    2023,
                    9,
                    21,
                    9,
                    22,
                    7,
                    3,
                    264,
                    0
                ],
                "title": "InstructERC: Reforming Emotion Recognition in Conversation with\n  Multi-task Retrieval-Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructERC: Reforming Emotion Recognition in Conversation with\n  Multi-task Retrieval-Augmented Large Language Models"
                },
                "summary": "The field of emotion recognition of conversation (ERC) has been focusing on\nseparating sentence feature encoding and context modeling, lacking exploration\nin generative paradigms based on unified designs. In this study, we propose a\nnovel approach, InstructERC, to reformulate the ERC task from a discriminative\nframework to a generative framework based on Large Language Models (LLMs).\nInstructERC makes three significant contributions: (1) it introduces a simple\nyet effective retrieval template module, which helps the model explicitly\nintegrate multi-granularity dialogue supervision information. (2) We introduce\ntwo additional emotion alignment tasks, namely speaker identification and\nemotion prediction tasks, to implicitly model the dialogue role relationships\nand future emotional tendencies in conversations. (3) Pioneeringly, we unify\nemotion labels across benchmarks through the feeling wheel to fit real\napplication scenarios. InstructERC still perform impressively on this unified\ndataset. Our LLM-based plugin framework significantly outperforms all previous\nmodels and achieves comprehensive SOTA on three commonly used ERC datasets.\nExtensive analysis of parameter-efficient and data-scaling experiments provides\nempirical guidance for applying it in practical scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of emotion recognition of conversation (ERC) has been focusing on\nseparating sentence feature encoding and context modeling, lacking exploration\nin generative paradigms based on unified designs. In this study, we propose a\nnovel approach, InstructERC, to reformulate the ERC task from a discriminative\nframework to a generative framework based on Large Language Models (LLMs).\nInstructERC makes three significant contributions: (1) it introduces a simple\nyet effective retrieval template module, which helps the model explicitly\nintegrate multi-granularity dialogue supervision information. (2) We introduce\ntwo additional emotion alignment tasks, namely speaker identification and\nemotion prediction tasks, to implicitly model the dialogue role relationships\nand future emotional tendencies in conversations. (3) Pioneeringly, we unify\nemotion labels across benchmarks through the feeling wheel to fit real\napplication scenarios. InstructERC still perform impressively on this unified\ndataset. Our LLM-based plugin framework significantly outperforms all previous\nmodels and achieves comprehensive SOTA on three commonly used ERC datasets.\nExtensive analysis of parameter-efficient and data-scaling experiments provides\nempirical guidance for applying it in practical scenarios."
                },
                "authors": [
                    {
                        "name": "Shanglin Lei"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Xiaoping Wang"
                    },
                    {
                        "name": "Keheng Wang"
                    },
                    {
                        "name": "Runqi Qiao"
                    },
                    {
                        "name": "Sirui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sirui Wang"
                },
                "author": "Sirui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.11911v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.11911v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16264v1",
                "updated": "2024-08-29T05:02:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    5,
                    2,
                    52,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T05:02:52Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    5,
                    2,
                    52,
                    3,
                    242,
                    0
                ],
                "title": "LoraMap: Harnessing the Power of LoRA Connections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoraMap: Harnessing the Power of LoRA Connections"
                },
                "summary": "Large Language Models (LLMs) can benefit from mitigating hallucinations\nthrough fact-checking and overcoming substantial computational overhead with\nparameter-efficient techniques such as Low-Rank Adaptation (LoRA). While some\nstudies have explored the parallel integration of multiple LoRAs, these\napproaches need attention to the connections between them. This paper\ninvestigates methods to establish connections among multiple LoRAs. We create\nthree reasoning datasets tailored to fact-checking and fine-tune individual\nLoRAs, allowing them to view and reason from diverse perspectives. Then, we\nexplore strategies for allocating these reasoning LoRAs and introduce LoraMap,\nan approach to map connections between them. The results on the fact-checking\ntask demonstrate that the performance of LoraMap is superior to LoraHub, an\nexisting LoRA composition method. LoraMap also outperforms with significantly\nfewer parameters than LoraConcat, which concatenates LoRAs and further\nfine-tunes them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can benefit from mitigating hallucinations\nthrough fact-checking and overcoming substantial computational overhead with\nparameter-efficient techniques such as Low-Rank Adaptation (LoRA). While some\nstudies have explored the parallel integration of multiple LoRAs, these\napproaches need attention to the connections between them. This paper\ninvestigates methods to establish connections among multiple LoRAs. We create\nthree reasoning datasets tailored to fact-checking and fine-tune individual\nLoRAs, allowing them to view and reason from diverse perspectives. Then, we\nexplore strategies for allocating these reasoning LoRAs and introduce LoraMap,\nan approach to map connections between them. The results on the fact-checking\ntask demonstrate that the performance of LoraMap is superior to LoraHub, an\nexisting LoRA composition method. LoraMap also outperforms with significantly\nfewer parameters than LoraConcat, which concatenates LoRAs and further\nfine-tunes them."
                },
                "authors": [
                    {
                        "name": "Hyeryun Park"
                    },
                    {
                        "name": "Jeongwon Kwak"
                    },
                    {
                        "name": "Dongsuk Jang"
                    },
                    {
                        "name": "Sumin Park"
                    },
                    {
                        "name": "Jinwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinwook Choi"
                },
                "author": "Jinwook Choi",
                "arxiv_comment": "13 pages, 9 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16249v1",
                "updated": "2024-08-29T04:06:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    4,
                    6,
                    34,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T04:06:34Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    4,
                    6,
                    34,
                    3,
                    242,
                    0
                ],
                "title": "Iterated Energy-based Flow Matching for Sampling from Boltzmann\n  Densities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterated Energy-based Flow Matching for Sampling from Boltzmann\n  Densities"
                },
                "summary": "In this work, we consider the problem of training a generator from\nevaluations of energy functions or unnormalized densities. This is a\nfundamental problem in probabilistic inference, which is crucial for scientific\napplications such as learning the 3D coordinate distribution of a molecule. To\nsolve this problem, we propose iterated energy-based flow matching (iEFM), the\nfirst off-policy approach to train continuous normalizing flow (CNF) models\nfrom unnormalized densities. We introduce the simulation-free energy-based flow\nmatching objective, which trains the model to predict the Monte Carlo\nestimation of the marginal vector field constructed from known energy\nfunctions. Our framework is general and can be extended to variance-exploding\n(VE) and optimal transport (OT) conditional probability paths. We evaluate iEFM\non a two-dimensional Gaussian mixture model (GMM) and an eight-dimensional\nfour-particle double-well potential (DW-4) energy function. Our results\ndemonstrate that iEFM outperforms existing methods, showcasing its potential\nfor efficient and scalable probabilistic modeling in complex high-dimensional\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider the problem of training a generator from\nevaluations of energy functions or unnormalized densities. This is a\nfundamental problem in probabilistic inference, which is crucial for scientific\napplications such as learning the 3D coordinate distribution of a molecule. To\nsolve this problem, we propose iterated energy-based flow matching (iEFM), the\nfirst off-policy approach to train continuous normalizing flow (CNF) models\nfrom unnormalized densities. We introduce the simulation-free energy-based flow\nmatching objective, which trains the model to predict the Monte Carlo\nestimation of the marginal vector field constructed from known energy\nfunctions. Our framework is general and can be extended to variance-exploding\n(VE) and optimal transport (OT) conditional probability paths. We evaluate iEFM\non a two-dimensional Gaussian mixture model (GMM) and an eight-dimensional\nfour-particle double-well potential (DW-4) energy function. Our results\ndemonstrate that iEFM outperforms existing methods, showcasing its potential\nfor efficient and scalable probabilistic modeling in complex high-dimensional\nsystems."
                },
                "authors": [
                    {
                        "name": "Dongyeop Woo"
                    },
                    {
                        "name": "Sungsoo Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Sungsoo Ahn"
                },
                "author": "Sungsoo Ahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16241v1",
                "updated": "2024-08-29T03:50:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    3,
                    50,
                    24,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T03:50:24Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    3,
                    50,
                    24,
                    3,
                    242,
                    0
                ],
                "title": "Making the Most of your Model: Methods for Finetuning and Applying\n  Pretrained Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making the Most of your Model: Methods for Finetuning and Applying\n  Pretrained Transformers"
                },
                "summary": "This thesis provides methods and analysis of models which make progress on\nthis goal. The techniques outlined are task agnostic, and should provide\nbenefit when used with nearly any transformer LM. We introduce two new\nfinetuning methods which add new capabilities to the models they are used on.\nThe first adds a recurrence mechanism, which removes the fixed-window sized\nconstraint and improves the efficiency of a transformer decoder. The second\nallows masked language models (MLMs) to be used for initialization of both the\nencoder and decoder of a non-autoregressive sequence-to-sequence transformer,\nopening up generative applications of models which were previously only used\nfor natural language understanding tasks.\n  We also introduce two new techniques for improving the quality of predictions\nof any transformer decoder without additional finetuning. One, hidden state\noptimization, can be applied to any transformer decoder to improve the quality\nof predictions at inference time, especially for few-shot classification. The\nother, conditional beam search, allows practitioners to search for natural\nlanguage generation (NLG) model outputs with high likelihood while conditioning\non the event that the output is not degenerate (e.g. empty, repetitive, etc.).\n  Finally, we provide theoretical and empirical insights on the divergence of\nmodel-likelihood and output quality which has widely been observed in prior\nwork. These insights apply to any model which represents a distribution over\ntext, and apply to language models which are not transformers or even\nautoregressive. We argue that the NLP community has, to some extent,\nmisunderstood the implications of these findings, and encourage a point of view\nwhich has more nuance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This thesis provides methods and analysis of models which make progress on\nthis goal. The techniques outlined are task agnostic, and should provide\nbenefit when used with nearly any transformer LM. We introduce two new\nfinetuning methods which add new capabilities to the models they are used on.\nThe first adds a recurrence mechanism, which removes the fixed-window sized\nconstraint and improves the efficiency of a transformer decoder. The second\nallows masked language models (MLMs) to be used for initialization of both the\nencoder and decoder of a non-autoregressive sequence-to-sequence transformer,\nopening up generative applications of models which were previously only used\nfor natural language understanding tasks.\n  We also introduce two new techniques for improving the quality of predictions\nof any transformer decoder without additional finetuning. One, hidden state\noptimization, can be applied to any transformer decoder to improve the quality\nof predictions at inference time, especially for few-shot classification. The\nother, conditional beam search, allows practitioners to search for natural\nlanguage generation (NLG) model outputs with high likelihood while conditioning\non the event that the output is not degenerate (e.g. empty, repetitive, etc.).\n  Finally, we provide theoretical and empirical insights on the divergence of\nmodel-likelihood and output quality which has widely been observed in prior\nwork. These insights apply to any model which represents a distribution over\ntext, and apply to language models which are not transformers or even\nautoregressive. We argue that the NLP community has, to some extent,\nmisunderstood the implications of these findings, and encourage a point of view\nwhich has more nuance."
                },
                "authors": [
                    {
                        "name": "Davis Yoshida"
                    }
                ],
                "author_detail": {
                    "name": "Davis Yoshida"
                },
                "author": "Davis Yoshida",
                "arxiv_comment": "PhD thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13385v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13385v4",
                "updated": "2024-08-29T03:11:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    3,
                    11,
                    14,
                    3,
                    242,
                    0
                ],
                "published": "2023-11-22T13:27:36Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    13,
                    27,
                    36,
                    2,
                    326,
                    0
                ],
                "title": "SegVol: Universal and Interactive Volumetric Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SegVol: Universal and Interactive Volumetric Medical Image Segmentation"
                },
                "summary": "Precise image segmentation provides clinical study with instructive\ninformation. Despite the remarkable progress achieved in medical image\nsegmentation, there is still an absence of a 3D foundation segmentation model\nthat can segment a wide range of anatomical categories with easy user\ninteraction. In this paper, we propose a 3D foundation segmentation model,\nnamed SegVol, supporting universal and interactive volumetric medical image\nsegmentation. By scaling up training data to 90K unlabeled Computed Tomography\n(CT) volumes and 6K labeled CT volumes, this foundation model supports the\nsegmentation of over 200 anatomical categories using semantic and spatial\nprompts. To facilitate efficient and precise inference on volumetric images, we\ndesign a zoom-out-zoom-in mechanism. Extensive experiments on 22 anatomical\nsegmentation tasks verify that SegVol outperforms the competitors in 19 tasks,\nwith improvements up to 37.24% compared to the runner-up methods. We\ndemonstrate the effectiveness and importance of specific designs by ablation\nstudy. We expect this foundation model can promote the development of\nvolumetric medical image analysis. The model and code are publicly available\nat: https://github.com/BAAI-DCAI/SegVol.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise image segmentation provides clinical study with instructive\ninformation. Despite the remarkable progress achieved in medical image\nsegmentation, there is still an absence of a 3D foundation segmentation model\nthat can segment a wide range of anatomical categories with easy user\ninteraction. In this paper, we propose a 3D foundation segmentation model,\nnamed SegVol, supporting universal and interactive volumetric medical image\nsegmentation. By scaling up training data to 90K unlabeled Computed Tomography\n(CT) volumes and 6K labeled CT volumes, this foundation model supports the\nsegmentation of over 200 anatomical categories using semantic and spatial\nprompts. To facilitate efficient and precise inference on volumetric images, we\ndesign a zoom-out-zoom-in mechanism. Extensive experiments on 22 anatomical\nsegmentation tasks verify that SegVol outperforms the competitors in 19 tasks,\nwith improvements up to 37.24% compared to the runner-up methods. We\ndemonstrate the effectiveness and importance of specific designs by ablation\nstudy. We expect this foundation model can promote the development of\nvolumetric medical image analysis. The model and code are publicly available\nat: https://github.com/BAAI-DCAI/SegVol."
                },
                "authors": [
                    {
                        "name": "Yuxin Du"
                    },
                    {
                        "name": "Fan Bai"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13385v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13385v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13985v2",
                "updated": "2024-08-29T02:40:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    40,
                    12,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-26T02:35:37Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    35,
                    37,
                    0,
                    239,
                    0
                ],
                "title": "TF-Attack: Transferable and Fast Adversarial Attacks on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TF-Attack: Transferable and Fast Adversarial Attacks on Large Language\n  Models"
                },
                "summary": "With the great advancements in large language models (LLMs), adversarial\nattacks against LLMs have recently attracted increasing attention. We found\nthat pre-existing adversarial attack methodologies exhibit limited\ntransferability and are notably inefficient, particularly when applied to LLMs.\nIn this paper, we analyze the core mechanisms of previous predominant\nadversarial attack methods, revealing that 1) the distributions of importance\nscore differ markedly among victim models, restricting the transferability; 2)\nthe sequential attack processes induces substantial time overheads. Based on\nthe above two insights, we introduce a new scheme, named TF-Attack, for\nTransferable and Fast adversarial attacks on LLMs. TF-Attack employs an\nexternal LLM as a third-party overseer rather than the victim model to identify\ncritical units within sentences. Moreover, TF-Attack introduces the concept of\nImportance Level, which allows for parallel substitutions of attacks. We\nconduct extensive experiments on 6 widely adopted benchmarks, evaluating the\nproposed method through both automatic and human metrics. Results show that our\nmethod consistently surpasses previous methods in transferability and delivers\nsignificant speed improvements, up to 20 times faster than earlier attack\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the great advancements in large language models (LLMs), adversarial\nattacks against LLMs have recently attracted increasing attention. We found\nthat pre-existing adversarial attack methodologies exhibit limited\ntransferability and are notably inefficient, particularly when applied to LLMs.\nIn this paper, we analyze the core mechanisms of previous predominant\nadversarial attack methods, revealing that 1) the distributions of importance\nscore differ markedly among victim models, restricting the transferability; 2)\nthe sequential attack processes induces substantial time overheads. Based on\nthe above two insights, we introduce a new scheme, named TF-Attack, for\nTransferable and Fast adversarial attacks on LLMs. TF-Attack employs an\nexternal LLM as a third-party overseer rather than the victim model to identify\ncritical units within sentences. Moreover, TF-Attack introduces the concept of\nImportance Level, which allows for parallel substitutions of attacks. We\nconduct extensive experiments on 6 widely adopted benchmarks, evaluating the\nproposed method through both automatic and human metrics. Results show that our\nmethod consistently surpasses previous methods in transferability and delivers\nsignificant speed improvements, up to 20 times faster than earlier attack\nstrategies."
                },
                "authors": [
                    {
                        "name": "Zelin Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Mingming Yang"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10903v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10903v5",
                "updated": "2024-08-29T02:38:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    38,
                    5,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-20T14:47:38Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    47,
                    38,
                    1,
                    233,
                    0
                ],
                "title": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model"
                },
                "summary": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue."
                },
                "authors": [
                    {
                        "name": "Yeyong Yu"
                    },
                    {
                        "name": "Runsheng Yu"
                    },
                    {
                        "name": "Haojie Wei"
                    },
                    {
                        "name": "Zhanqiu Zhang"
                    },
                    {
                        "name": "Quan Qian"
                    }
                ],
                "author_detail": {
                    "name": "Quan Qian"
                },
                "author": "Quan Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10903v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10903v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16221v1",
                "updated": "2024-08-29T02:35:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    35,
                    53,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:35:53Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    35,
                    53,
                    3,
                    242,
                    0
                ],
                "title": "SSDM: Scalable Speech Dysfluency Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSDM: Scalable Speech Dysfluency Modeling"
                },
                "summary": "Speech dysfluency modeling is the core module for spoken language learning,\nand speech therapy. However, there are three challenges. First, current\nstate-of-the-art solutions suffer from poor scalability. Second, there is a\nlack of a large-scale dysfluency corpus. Third, there is not an effective\nlearning framework. In this paper, we propose \\textit{SSDM: Scalable Speech\nDysfluency Modeling}, which (1) adopts articulatory gestures as scalable forced\nalignment; (2) introduces connectionist subsequence aligner (CSA) to achieve\ndysfluency alignment; (3) introduces a large-scale simulated dysfluency corpus\ncalled Libri-Dys; and (4) develops an end-to-end system by leveraging the power\nof large language models (LLMs). We expect SSDM to serve as a standard in the\narea of dysfluency modeling. Demo is available at\n\\url{https://eureka235.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech dysfluency modeling is the core module for spoken language learning,\nand speech therapy. However, there are three challenges. First, current\nstate-of-the-art solutions suffer from poor scalability. Second, there is a\nlack of a large-scale dysfluency corpus. Third, there is not an effective\nlearning framework. In this paper, we propose \\textit{SSDM: Scalable Speech\nDysfluency Modeling}, which (1) adopts articulatory gestures as scalable forced\nalignment; (2) introduces connectionist subsequence aligner (CSA) to achieve\ndysfluency alignment; (3) introduces a large-scale simulated dysfluency corpus\ncalled Libri-Dys; and (4) develops an end-to-end system by leveraging the power\nof large language models (LLMs). We expect SSDM to serve as a standard in the\narea of dysfluency modeling. Demo is available at\n\\url{https://eureka235.github.io}."
                },
                "authors": [
                    {
                        "name": "Jiachen Lian"
                    },
                    {
                        "name": "Xuanru Zhou"
                    },
                    {
                        "name": "Zoe Ezzes"
                    },
                    {
                        "name": "Jet Vonk"
                    },
                    {
                        "name": "Brittany Morin"
                    },
                    {
                        "name": "David Baquirin"
                    },
                    {
                        "name": "Zachary Mille"
                    },
                    {
                        "name": "Maria Luisa Gorno Tempini"
                    },
                    {
                        "name": "Gopala Anumanchipalli"
                    }
                ],
                "author_detail": {
                    "name": "Gopala Anumanchipalli"
                },
                "author": "Gopala Anumanchipalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16219v1",
                "updated": "2024-08-29T02:25:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    25,
                    12,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:25:12Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    25,
                    12,
                    3,
                    242,
                    0
                ],
                "title": "Training-free Video Temporal Grounding using Large-scale Pre-trained\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free Video Temporal Grounding using Large-scale Pre-trained\n  Models"
                },
                "summary": "Video temporal grounding aims to identify video segments within untrimmed\nvideos that are most relevant to a given natural language query. Existing video\ntemporal localization models rely on specific datasets for training and have\nhigh data collection costs, but they exhibit poor generalization capability\nunder the across-dataset and out-of-distribution (OOD) settings. In this paper,\nwe propose a Training-Free Video Temporal Grounding (TFVTG) approach that\nleverages the ability of pre-trained large models. A naive baseline is to\nenumerate proposals in the video and use the pre-trained visual language models\n(VLMs) to select the best proposal according to the vision-language alignment.\nHowever, most existing VLMs are trained on image-text pairs or trimmed video\nclip-text pairs, making it struggle to (1) grasp the relationship and\ndistinguish the temporal boundaries of multiple events within the same video;\n(2) comprehend and be sensitive to the dynamic transition of events (the\ntransition from one event to another) in the video. To address these issues, we\npropose leveraging large language models (LLMs) to analyze multiple sub-events\ncontained in the query text and analyze the temporal order and relationships\nbetween these events. Secondly, we split a sub-event into dynamic transition\nand static status parts and propose the dynamic and static scoring functions\nusing VLMs to better evaluate the relevance between the event and the\ndescription. Finally, for each sub-event description, we use VLMs to locate the\ntop-k proposals and leverage the order and relationships between sub-events\nprovided by LLMs to filter and integrate these proposals. Our method achieves\nthe best performance on zero-shot video temporal grounding on Charades-STA and\nActivityNet Captions datasets without any training and demonstrates better\ngeneralization capabilities in cross-dataset and OOD settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video temporal grounding aims to identify video segments within untrimmed\nvideos that are most relevant to a given natural language query. Existing video\ntemporal localization models rely on specific datasets for training and have\nhigh data collection costs, but they exhibit poor generalization capability\nunder the across-dataset and out-of-distribution (OOD) settings. In this paper,\nwe propose a Training-Free Video Temporal Grounding (TFVTG) approach that\nleverages the ability of pre-trained large models. A naive baseline is to\nenumerate proposals in the video and use the pre-trained visual language models\n(VLMs) to select the best proposal according to the vision-language alignment.\nHowever, most existing VLMs are trained on image-text pairs or trimmed video\nclip-text pairs, making it struggle to (1) grasp the relationship and\ndistinguish the temporal boundaries of multiple events within the same video;\n(2) comprehend and be sensitive to the dynamic transition of events (the\ntransition from one event to another) in the video. To address these issues, we\npropose leveraging large language models (LLMs) to analyze multiple sub-events\ncontained in the query text and analyze the temporal order and relationships\nbetween these events. Secondly, we split a sub-event into dynamic transition\nand static status parts and propose the dynamic and static scoring functions\nusing VLMs to better evaluate the relevance between the event and the\ndescription. Finally, for each sub-event description, we use VLMs to locate the\ntop-k proposals and leverage the order and relationships between sub-events\nprovided by LLMs to filter and integrate these proposals. Our method achieves\nthe best performance on zero-shot video temporal grounding on Charades-STA and\nActivityNet Captions datasets without any training and demonstrates better\ngeneralization capabilities in cross-dataset and OOD settings."
                },
                "authors": [
                    {
                        "name": "Minghang Zheng"
                    },
                    {
                        "name": "Xinhao Cai"
                    },
                    {
                        "name": "Qingchao Chen"
                    },
                    {
                        "name": "Yuxin Peng"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "Accepted by ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16218v1",
                "updated": "2024-08-29T02:21:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    21,
                    11,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:21:11Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    21,
                    11,
                    3,
                    242,
                    0
                ],
                "title": "Targeted Cause Discovery with Data-Driven Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted Cause Discovery with Data-Driven Learning"
                },
                "summary": "We propose a novel machine learning approach for inferring causal variables\nof a target variable from observations. Our goal is to identify both direct and\nindirect causes within a system, thereby efficiently regulating the target\nvariable when the difficulty and cost of intervening on each causal variable\nvary. Our method employs a neural network trained to identify causality through\nsupervised learning on simulated data. By implementing a local-inference\nstrategy, we achieve linear complexity with respect to the number of variables,\nefficiently scaling up to thousands of variables. Empirical results demonstrate\nthe effectiveness of our method in identifying causal relationships within\nlarge-scale gene regulatory networks, outperforming existing causal discovery\nmethods that primarily focus on direct causality. We validate our model's\ngeneralization capability across novel graph structures and generating\nmechanisms, including gene regulatory networks of E. coli and the human K562\ncell line. Implementation codes are available at\nhttps://github.com/snu-mllab/Targeted-Cause-Discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel machine learning approach for inferring causal variables\nof a target variable from observations. Our goal is to identify both direct and\nindirect causes within a system, thereby efficiently regulating the target\nvariable when the difficulty and cost of intervening on each causal variable\nvary. Our method employs a neural network trained to identify causality through\nsupervised learning on simulated data. By implementing a local-inference\nstrategy, we achieve linear complexity with respect to the number of variables,\nefficiently scaling up to thousands of variables. Empirical results demonstrate\nthe effectiveness of our method in identifying causal relationships within\nlarge-scale gene regulatory networks, outperforming existing causal discovery\nmethods that primarily focus on direct causality. We validate our model's\ngeneralization capability across novel graph structures and generating\nmechanisms, including gene regulatory networks of E. coli and the human K562\ncell line. Implementation codes are available at\nhttps://github.com/snu-mllab/Targeted-Cause-Discovery."
                },
                "authors": [
                    {
                        "name": "Jang-Hyun Kim"
                    },
                    {
                        "name": "Claudia Skok Gibbs"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Hyun Oh Song"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    }
                ],
                "author_detail": {
                    "name": "Kyunghyun Cho"
                },
                "author": "Kyunghyun Cho",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13989v2",
                "updated": "2024-08-29T02:17:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    17,
                    51,
                    3,
                    242,
                    0
                ],
                "published": "2024-07-19T02:34:10Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    2,
                    34,
                    10,
                    4,
                    201,
                    0
                ],
                "title": "Enhancing Data-Limited Graph Neural Networks by Actively Distilling\n  Knowledge from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Data-Limited Graph Neural Networks by Actively Distilling\n  Knowledge from Large Language Models"
                },
                "summary": "Graphs are pervasive in the real-world, such as social network analysis,\nbioinformatics, and knowledge graphs. Graph neural networks (GNNs) have great\nability in node classification, a fundamental task on graphs. Unfortunately,\nconventional GNNs still face challenges in scenarios with few labeled nodes,\ndespite the prevalence of few-shot node classification tasks in real-world\napplications. To address this challenge, various approaches have been proposed,\nincluding graph meta-learning, transfer learning, and methods based on Large\nLanguage Models (LLMs). However, traditional meta-learning and transfer\nlearning methods often require prior knowledge from base classes or fail to\nexploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based\nmethods may overlook the zero-shot capabilities of LLMs and rely heavily on the\nquality of generated contexts. In this paper, we propose a novel approach that\nintegrates LLMs and GNNs, leveraging the zero-shot inference and reasoning\ncapabilities of LLMs and employing a Graph-LLM-based active learning paradigm\nto enhance GNNs' performance. Extensive experiments demonstrate the\neffectiveness of our model in improving node classification accuracy with\nconsiderably limited labeled data, surpassing state-of-the-art baselines by\nsignificant margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are pervasive in the real-world, such as social network analysis,\nbioinformatics, and knowledge graphs. Graph neural networks (GNNs) have great\nability in node classification, a fundamental task on graphs. Unfortunately,\nconventional GNNs still face challenges in scenarios with few labeled nodes,\ndespite the prevalence of few-shot node classification tasks in real-world\napplications. To address this challenge, various approaches have been proposed,\nincluding graph meta-learning, transfer learning, and methods based on Large\nLanguage Models (LLMs). However, traditional meta-learning and transfer\nlearning methods often require prior knowledge from base classes or fail to\nexploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based\nmethods may overlook the zero-shot capabilities of LLMs and rely heavily on the\nquality of generated contexts. In this paper, we propose a novel approach that\nintegrates LLMs and GNNs, leveraging the zero-shot inference and reasoning\ncapabilities of LLMs and employing a Graph-LLM-based active learning paradigm\nto enhance GNNs' performance. Extensive experiments demonstrate the\neffectiveness of our model in improving node classification accuracy with\nconsiderably limited labeled data, surpassing state-of-the-art baselines by\nsignificant margins."
                },
                "authors": [
                    {
                        "name": "Quan Li"
                    },
                    {
                        "name": "Tianxiang Zhao"
                    },
                    {
                        "name": "Lingwei Chen"
                    },
                    {
                        "name": "Junjie Xu"
                    },
                    {
                        "name": "Suhang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Suhang Wang"
                },
                "author": "Suhang Wang",
                "arxiv_comment": "10 pages, 3 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16213v1",
                "updated": "2024-08-29T02:12:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    12,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:12:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    12,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language\n  Models for Chest X-ray Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language\n  Models for Chest X-ray Interpretation"
                },
                "summary": "The rapid evolution of artificial intelligence, especially in large language\nmodels (LLMs), has significantly impacted various domains, including\nhealthcare. In chest X-ray (CXR) analysis, previous studies have employed LLMs,\nbut with limitations: either underutilizing the multi-tasking capabilities of\nLLMs or lacking clinical accuracy. This paper presents M4CXR, a multi-modal LLM\ndesigned to enhance CXR interpretation. The model is trained on a visual\ninstruction-following dataset that integrates various task-specific datasets in\na conversational format. As a result, the model supports multiple tasks such as\nmedical report generation (MRG), visual grounding, and visual question\nanswering (VQA). M4CXR achieves state-of-the-art clinical accuracy in MRG by\nemploying a chain-of-thought prompting strategy, in which it identifies\nfindings in CXR images and subsequently generates corresponding reports. The\nmodel is adaptable to various MRG scenarios depending on the available inputs,\nsuch as single-image, multi-image, and multi-study contexts. In addition to\nMRG, M4CXR performs visual grounding at a level comparable to specialized\nmodels and also demonstrates outstanding performance in VQA. Both quantitative\nand qualitative assessments reveal M4CXR's versatility in MRG, visual\ngrounding, and VQA, while consistently maintaining clinical accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of artificial intelligence, especially in large language\nmodels (LLMs), has significantly impacted various domains, including\nhealthcare. In chest X-ray (CXR) analysis, previous studies have employed LLMs,\nbut with limitations: either underutilizing the multi-tasking capabilities of\nLLMs or lacking clinical accuracy. This paper presents M4CXR, a multi-modal LLM\ndesigned to enhance CXR interpretation. The model is trained on a visual\ninstruction-following dataset that integrates various task-specific datasets in\na conversational format. As a result, the model supports multiple tasks such as\nmedical report generation (MRG), visual grounding, and visual question\nanswering (VQA). M4CXR achieves state-of-the-art clinical accuracy in MRG by\nemploying a chain-of-thought prompting strategy, in which it identifies\nfindings in CXR images and subsequently generates corresponding reports. The\nmodel is adaptable to various MRG scenarios depending on the available inputs,\nsuch as single-image, multi-image, and multi-study contexts. In addition to\nMRG, M4CXR performs visual grounding at a level comparable to specialized\nmodels and also demonstrates outstanding performance in VQA. Both quantitative\nand qualitative assessments reveal M4CXR's versatility in MRG, visual\ngrounding, and VQA, while consistently maintaining clinical accuracy."
                },
                "authors": [
                    {
                        "name": "Jonggwon Park"
                    },
                    {
                        "name": "Soobum Kim"
                    },
                    {
                        "name": "Byungmu Yoon"
                    },
                    {
                        "name": "Jihun Hyun"
                    },
                    {
                        "name": "Kyoyun Choi"
                    }
                ],
                "author_detail": {
                    "name": "Kyoyun Choi"
                },
                "author": "Kyoyun Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16550v2",
                "updated": "2024-08-29T02:06:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    6,
                    7,
                    3,
                    242,
                    0
                ],
                "published": "2024-07-23T15:04:38Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    15,
                    4,
                    38,
                    1,
                    205,
                    0
                ],
                "title": "A Kernel-Based Conditional Two-Sample Test Using Nearest Neighbors (with\n  Applications to Calibration, Regression Curves, and Simulation-Based\n  Inference)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Kernel-Based Conditional Two-Sample Test Using Nearest Neighbors (with\n  Applications to Calibration, Regression Curves, and Simulation-Based\n  Inference)"
                },
                "summary": "In this paper we introduce a kernel-based measure for detecting differences\nbetween two conditional distributions. Using the `kernel trick' and\nnearest-neighbor graphs, we propose a consistent estimate of this measure which\ncan be computed in nearly linear time (for a fixed number of nearest\nneighbors). Moreover, when the two conditional distributions are the same, the\nestimate has a Gaussian limit and its asymptotic variance has a simple form\nthat can be easily estimated from the data. The resulting test attains precise\nasymptotic level and is universally consistent for detecting differences\nbetween two conditional distributions. We also provide a resampling based test\nusing our estimate that applies to the conditional goodness-of-fit problem,\nwhich controls Type I error in finite samples and is asymptotically consistent\nwith only a finite number of resamples. A method to de-randomize the resampling\ntest is also presented. The proposed methods can be readily applied to a broad\nrange of problems, ranging from classical nonparametric statistics to modern\nmachine learning. Specifically, we explore three applications: testing model\ncalibration, regression curve evaluation, and validation of emulator models in\nsimulation-based inference. We illustrate the superior performance of our\nmethod for these tasks, both in simulations as well as on real data. In\nparticular, we apply our method to (1) assess the calibration of neural network\nmodels trained on the CIFAR-10 dataset, (2) compare regression functions for\nwind power generation across two different turbines, and (3) validate emulator\nmodels on benchmark examples with intractable posteriors and for generating\nsynthetic `redshift' associated with galaxy images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we introduce a kernel-based measure for detecting differences\nbetween two conditional distributions. Using the `kernel trick' and\nnearest-neighbor graphs, we propose a consistent estimate of this measure which\ncan be computed in nearly linear time (for a fixed number of nearest\nneighbors). Moreover, when the two conditional distributions are the same, the\nestimate has a Gaussian limit and its asymptotic variance has a simple form\nthat can be easily estimated from the data. The resulting test attains precise\nasymptotic level and is universally consistent for detecting differences\nbetween two conditional distributions. We also provide a resampling based test\nusing our estimate that applies to the conditional goodness-of-fit problem,\nwhich controls Type I error in finite samples and is asymptotically consistent\nwith only a finite number of resamples. A method to de-randomize the resampling\ntest is also presented. The proposed methods can be readily applied to a broad\nrange of problems, ranging from classical nonparametric statistics to modern\nmachine learning. Specifically, we explore three applications: testing model\ncalibration, regression curve evaluation, and validation of emulator models in\nsimulation-based inference. We illustrate the superior performance of our\nmethod for these tasks, both in simulations as well as on real data. In\nparticular, we apply our method to (1) assess the calibration of neural network\nmodels trained on the CIFAR-10 dataset, (2) compare regression functions for\nwind power generation across two different turbines, and (3) validate emulator\nmodels on benchmark examples with intractable posteriors and for generating\nsynthetic `redshift' associated with galaxy images."
                },
                "authors": [
                    {
                        "name": "Anirban Chatterjee"
                    },
                    {
                        "name": "Ziang Niu"
                    },
                    {
                        "name": "Bhaswar B. Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Bhaswar B. Bhattacharya"
                },
                "author": "Bhaswar B. Bhattacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16208v1",
                "updated": "2024-08-29T02:03:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    3,
                    5,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:03:05Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    3,
                    5,
                    3,
                    242,
                    0
                ],
                "title": "ReXamine-Global: A Framework for Uncovering Inconsistencies in Radiology\n  Report Generation Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReXamine-Global: A Framework for Uncovering Inconsistencies in Radiology\n  Report Generation Metrics"
                },
                "summary": "Given the rapidly expanding capabilities of generative AI models for\nradiology, there is a need for robust metrics that can accurately measure the\nquality of AI-generated radiology reports across diverse hospitals. We develop\nReXamine-Global, a LLM-powered, multi-site framework that tests metrics across\ndifferent writing styles and patient populations, exposing gaps in their\ngeneralization. First, our method tests whether a metric is undesirably\nsensitive to reporting style, providing different scores depending on whether\nAI-generated reports are stylistically similar to ground-truth reports or not.\nSecond, our method measures whether a metric reliably agrees with experts, or\nwhether metric and expert scores of AI-generated report quality diverge for\nsome sites. Using 240 reports from 6 hospitals around the world, we apply\nReXamine-Global to 7 established report evaluation metrics and uncover serious\ngaps in their generalizability. Developers can apply ReXamine-Global when\ndesigning new report evaluation metrics, ensuring their robustness across\nsites. Additionally, our analysis of existing metrics can guide users of those\nmetrics towards evaluation procedures that work reliably at their sites of\ninterest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the rapidly expanding capabilities of generative AI models for\nradiology, there is a need for robust metrics that can accurately measure the\nquality of AI-generated radiology reports across diverse hospitals. We develop\nReXamine-Global, a LLM-powered, multi-site framework that tests metrics across\ndifferent writing styles and patient populations, exposing gaps in their\ngeneralization. First, our method tests whether a metric is undesirably\nsensitive to reporting style, providing different scores depending on whether\nAI-generated reports are stylistically similar to ground-truth reports or not.\nSecond, our method measures whether a metric reliably agrees with experts, or\nwhether metric and expert scores of AI-generated report quality diverge for\nsome sites. Using 240 reports from 6 hospitals around the world, we apply\nReXamine-Global to 7 established report evaluation metrics and uncover serious\ngaps in their generalizability. Developers can apply ReXamine-Global when\ndesigning new report evaluation metrics, ensuring their robustness across\nsites. Additionally, our analysis of existing metrics can guide users of those\nmetrics towards evaluation procedures that work reliably at their sites of\ninterest."
                },
                "authors": [
                    {
                        "name": "Oishi Banerjee"
                    },
                    {
                        "name": "Agustina Saenz"
                    },
                    {
                        "name": "Kay Wu"
                    },
                    {
                        "name": "Warren Clements"
                    },
                    {
                        "name": "Adil Zia"
                    },
                    {
                        "name": "Dominic Buensalido"
                    },
                    {
                        "name": "Helen Kavnoudias"
                    },
                    {
                        "name": "Alain S. Abi-Ghanem"
                    },
                    {
                        "name": "Nour El Ghawi"
                    },
                    {
                        "name": "Cibele Luna"
                    },
                    {
                        "name": "Patricia Castillo"
                    },
                    {
                        "name": "Khaled Al-Surimi"
                    },
                    {
                        "name": "Rayyan A. Daghistani"
                    },
                    {
                        "name": "Yuh-Min Chen"
                    },
                    {
                        "name": "Heng-sheng Chao"
                    },
                    {
                        "name": "Lars Heiliger"
                    },
                    {
                        "name": "Moon Kim"
                    },
                    {
                        "name": "Johannes Haubold"
                    },
                    {
                        "name": "Frederic Jonske"
                    },
                    {
                        "name": "Pranav Rajpurkar"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Rajpurkar"
                },
                "author": "Pranav Rajpurkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15591v2",
                "updated": "2024-08-29T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    1,
                    56,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-28T07:31:32Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    7,
                    31,
                    32,
                    2,
                    241,
                    0
                ],
                "title": "VFLIP: A Backdoor Defense for Vertical Federated Learning via\n  Identification and Purification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFLIP: A Backdoor Defense for Vertical Federated Learning via\n  Identification and Purification"
                },
                "summary": "Vertical Federated Learning (VFL) focuses on handling vertically partitioned\ndata over FL participants. Recent studies have discovered a significant\nvulnerability in VFL to backdoor attacks which specifically target the distinct\ncharacteristics of VFL. Therefore, these attacks may neutralize existing\ndefense mechanisms designed primarily for Horizontal Federated Learning (HFL)\nand deep neural networks. In this paper, we present the first backdoor defense,\ncalled VFLIP, specialized for VFL. VFLIP employs the identification and\npurification techniques that operate at the inference stage, consequently\nimproving the robustness against backdoor attacks to a great extent. VFLIP\nfirst identifies backdoor-triggered embeddings by adopting a participant-wise\nanomaly detection approach. Subsequently, VFLIP conducts purification which\nremoves the embeddings identified as malicious and reconstructs all the\nembeddings based on the remaining embeddings. We conduct extensive experiments\non CIFAR10, CINIC10, Imagenette, NUS-WIDE, and BankMarketing to demonstrate\nthat VFLIP can effectively mitigate backdoor attacks in VFL.\nhttps://github.com/blingcho/VFLIP-esorics24",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertical Federated Learning (VFL) focuses on handling vertically partitioned\ndata over FL participants. Recent studies have discovered a significant\nvulnerability in VFL to backdoor attacks which specifically target the distinct\ncharacteristics of VFL. Therefore, these attacks may neutralize existing\ndefense mechanisms designed primarily for Horizontal Federated Learning (HFL)\nand deep neural networks. In this paper, we present the first backdoor defense,\ncalled VFLIP, specialized for VFL. VFLIP employs the identification and\npurification techniques that operate at the inference stage, consequently\nimproving the robustness against backdoor attacks to a great extent. VFLIP\nfirst identifies backdoor-triggered embeddings by adopting a participant-wise\nanomaly detection approach. Subsequently, VFLIP conducts purification which\nremoves the embeddings identified as malicious and reconstructs all the\nembeddings based on the remaining embeddings. We conduct extensive experiments\non CIFAR10, CINIC10, Imagenette, NUS-WIDE, and BankMarketing to demonstrate\nthat VFLIP can effectively mitigate backdoor attacks in VFL.\nhttps://github.com/blingcho/VFLIP-esorics24"
                },
                "authors": [
                    {
                        "name": "Yungi Cho"
                    },
                    {
                        "name": "Woorim Han"
                    },
                    {
                        "name": "Miseon Yu"
                    },
                    {
                        "name": "Younghan Lee"
                    },
                    {
                        "name": "Ho Bae"
                    },
                    {
                        "name": "Yunheung Paek"
                    }
                ],
                "author_detail": {
                    "name": "Yunheung Paek"
                },
                "author": "Yunheung Paek",
                "arxiv_comment": "Accepted by 29th European Symposium on Research in Computer Security\n  (ESORICS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10064v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10064v3",
                "updated": "2024-08-29T01:44:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    1,
                    44,
                    8,
                    3,
                    242,
                    0
                ],
                "published": "2024-07-14T03:31:33Z",
                "published_parsed": [
                    2024,
                    7,
                    14,
                    3,
                    31,
                    33,
                    6,
                    196,
                    0
                ],
                "title": "Revolutionizing Bridge Operation and maintenance with LLM-based Agents:\n  An Overview of Applications and Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Bridge Operation and maintenance with LLM-based Agents:\n  An Overview of Applications and Insights"
                },
                "summary": "In various industrial fields of human social development, people have been\nexploring methods aimed at freeing human labor. Constructing LLM-based agents\nis considered to be one of the most effective tools to achieve this goal.\nAgent, as a kind of human-like intelligent entity with the ability of\nperception, planning, decision-making, and action, has created great production\nvalue in many fields. However, the bridge O\\&M field shows a relatively low\nlevel of intelligence compared to other industries. Nevertheless, the bridge\nO\\&M field has developed numerous intelligent inspection devices, machine\nlearning algorithms, and autonomous evaluation and decision-making methods,\nwhich provide a feasible basis for breakthroughs in artificial intelligence in\nthis field. The aim of this study is to explore the impact of AI bodies based\non large-scale language models on the field of bridge O\\&M and to analyze the\npotential challenges and opportunities it brings to the core tasks of bridge\nO\\&M. Through in-depth research and analysis, this paper expects to provide a\nmore comprehensive perspective for understanding the application of\nintelligentsia in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In various industrial fields of human social development, people have been\nexploring methods aimed at freeing human labor. Constructing LLM-based agents\nis considered to be one of the most effective tools to achieve this goal.\nAgent, as a kind of human-like intelligent entity with the ability of\nperception, planning, decision-making, and action, has created great production\nvalue in many fields. However, the bridge O\\&M field shows a relatively low\nlevel of intelligence compared to other industries. Nevertheless, the bridge\nO\\&M field has developed numerous intelligent inspection devices, machine\nlearning algorithms, and autonomous evaluation and decision-making methods,\nwhich provide a feasible basis for breakthroughs in artificial intelligence in\nthis field. The aim of this study is to explore the impact of AI bodies based\non large-scale language models on the field of bridge O\\&M and to analyze the\npotential challenges and opportunities it brings to the core tasks of bridge\nO\\&M. Through in-depth research and analysis, this paper expects to provide a\nmore comprehensive perspective for understanding the application of\nintelligentsia in this field."
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Lianzhen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lianzhen Zhang"
                },
                "author": "Lianzhen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10064v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10064v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16198v1",
                "updated": "2024-08-29T01:32:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    1,
                    32,
                    49,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T01:32:49Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    1,
                    32,
                    49,
                    3,
                    242,
                    0
                ],
                "title": "Chain-of-Experts (CoE): Reverse Engineering Software Bills of Materials\n  for JavaScript Application Bundles through Code Clone Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Experts (CoE): Reverse Engineering Software Bills of Materials\n  for JavaScript Application Bundles through Code Clone Search"
                },
                "summary": "A Software Bill of Materials (SBoM) is a detailed inventory of all\ncomponents, libraries, and modules in a software artifact, providing\ntraceability throughout the software supply chain. With the increasing\npopularity of JavaScript in software engineering due to its dynamic syntax and\nseamless supply chain integration, the exposure to vulnerabilities and attacks\nhas risen significantly. A JavaScript application bundle, which is a\nconsolidated, symbol-stripped, and optimized assembly of code for deployment\npurpose. Generating a SBoM from a JavaScript application bundle through a\nreverse-engineering process ensures the integrity, security, and compliance of\nthe supplier's software release, even without access to the original dependency\ngraphs.\n  This paper presents the first study on SBoM generation for JavaScript\napplication bundles. We identify three key challenges for this task, i.e.,\nnested code scopes, extremely long sequences, and large retrieval spaces. To\naddress these challenges, we introduce Chain-of-Experts (CoE), a multi-task\ndeep learning model designed to generate SBoMs through three tasks: code\nsegmentation, code classification, and code clone retrieval. We evaluate CoE\nagainst individual task-specific solutions on 500 web application bundles with\nover 66,000 dependencies. Our experimental results demonstrate that CoE offers\ncompetitive outcomes with less training and inference time when compared with\ncombined individual task-specific solutions. Consequently, CoE provides the\nfirst scalable, efficient, and end-to-end solution for the SBoM generation of\nreal-world JavaScript application bundles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Software Bill of Materials (SBoM) is a detailed inventory of all\ncomponents, libraries, and modules in a software artifact, providing\ntraceability throughout the software supply chain. With the increasing\npopularity of JavaScript in software engineering due to its dynamic syntax and\nseamless supply chain integration, the exposure to vulnerabilities and attacks\nhas risen significantly. A JavaScript application bundle, which is a\nconsolidated, symbol-stripped, and optimized assembly of code for deployment\npurpose. Generating a SBoM from a JavaScript application bundle through a\nreverse-engineering process ensures the integrity, security, and compliance of\nthe supplier's software release, even without access to the original dependency\ngraphs.\n  This paper presents the first study on SBoM generation for JavaScript\napplication bundles. We identify three key challenges for this task, i.e.,\nnested code scopes, extremely long sequences, and large retrieval spaces. To\naddress these challenges, we introduce Chain-of-Experts (CoE), a multi-task\ndeep learning model designed to generate SBoMs through three tasks: code\nsegmentation, code classification, and code clone retrieval. We evaluate CoE\nagainst individual task-specific solutions on 500 web application bundles with\nover 66,000 dependencies. Our experimental results demonstrate that CoE offers\ncompetitive outcomes with less training and inference time when compared with\ncombined individual task-specific solutions. Consequently, CoE provides the\nfirst scalable, efficient, and end-to-end solution for the SBoM generation of\nreal-world JavaScript application bundles."
                },
                "authors": [
                    {
                        "name": "Leo Song"
                    },
                    {
                        "name": "Steven H. H. Ding"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Li Tao Li"
                    },
                    {
                        "name": "Philippe Charland"
                    },
                    {
                        "name": "Andrew Walenstein"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Walenstein"
                },
                "author": "Andrew Walenstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16191v1",
                "updated": "2024-08-29T01:09:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    1,
                    9,
                    30,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T01:09:30Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    1,
                    9,
                    30,
                    3,
                    242,
                    0
                ],
                "title": "Variational Mode-Driven Graph Convolutional Network for Spatiotemporal\n  Traffic Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Mode-Driven Graph Convolutional Network for Spatiotemporal\n  Traffic Forecasting"
                },
                "summary": "This paper focuses on spatio-temporal (ST) traffic prediction traffic using\ngraph neural networks. Given that ST data consists of non-stationary and\ncomplex time events, interpreting and predicting such trends is comparatively\ncomplicated. Representation of ST data in modes helps us infer behavior and\nassess the impact of noise on prediction applications. We propose a framework\nthat decomposes ST data into modes using the variational mode decomposition\n(VMD) method, which is then fed into the neural network for forecasting future\nstates. This hybrid approach is known as a variational mode graph convolutional\nnetwork (VMGCN). Instead of exhaustively searching for the number of modes,\nthey are determined using the reconstruction loss from the real-time\napplication data. We also study the significance of each mode and the impact of\nbandwidth constraints on different horizon predictions in traffic flow data. We\nevaluate the performance of our proposed network on the LargeST dataset for\nboth short and long-term predictions. Our framework yields better results\ncompared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper focuses on spatio-temporal (ST) traffic prediction traffic using\ngraph neural networks. Given that ST data consists of non-stationary and\ncomplex time events, interpreting and predicting such trends is comparatively\ncomplicated. Representation of ST data in modes helps us infer behavior and\nassess the impact of noise on prediction applications. We propose a framework\nthat decomposes ST data into modes using the variational mode decomposition\n(VMD) method, which is then fed into the neural network for forecasting future\nstates. This hybrid approach is known as a variational mode graph convolutional\nnetwork (VMGCN). Instead of exhaustively searching for the number of modes,\nthey are determined using the reconstruction loss from the real-time\napplication data. We also study the significance of each mode and the impact of\nbandwidth constraints on different horizon predictions in traffic flow data. We\nevaluate the performance of our proposed network on the LargeST dataset for\nboth short and long-term predictions. Our framework yields better results\ncompared to state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Osama Ahmad"
                    },
                    {
                        "name": "Zubair Khalid"
                    }
                ],
                "author_detail": {
                    "name": "Zubair Khalid"
                },
                "author": "Zubair Khalid",
                "arxiv_comment": "IEEE Transactions on Intelligent Transportation Systems Submission,\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2407.10972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10972v2",
                "updated": "2024-08-29T17:55:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    55,
                    52,
                    3,
                    242,
                    0
                ],
                "published": "2024-07-15T17:59:55Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    59,
                    55,
                    0,
                    197,
                    0
                ],
                "title": "VGBench: Evaluating Large Language Models on Vector Graphics\n  Understanding and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VGBench: Evaluating Large Language Models on Vector Graphics\n  Understanding and Generation"
                },
                "summary": "In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons, sketches and\nscientific figures. Recent studies have shown promising results on processing\nvector graphics with capable Large Language Models (LLMs). However, such works\nfocus solely on qualitative results, understanding, or a specific type of\nvector graphics. We propose VGBench, a comprehensive benchmark for LLMs on\nhandling vector graphics through diverse aspects, including (a) both visual\nunderstanding and generation, (b) evaluation of various vector graphics\nformats, (c) diverse question types, (d) wide range of prompting techniques,\n(e) under multiple LLMs and (f) comparison with VLMs on rasterized\nrepresentations. Evaluating on our collected 4279 understanding and 5845\ngeneration samples, we find that LLMs show strong capability on both aspects\nwhile exhibiting less desirable performance on low-level formats (SVG). Both\ndata and evaluation pipeline will be open-sourced at https://vgbench.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons, sketches and\nscientific figures. Recent studies have shown promising results on processing\nvector graphics with capable Large Language Models (LLMs). However, such works\nfocus solely on qualitative results, understanding, or a specific type of\nvector graphics. We propose VGBench, a comprehensive benchmark for LLMs on\nhandling vector graphics through diverse aspects, including (a) both visual\nunderstanding and generation, (b) evaluation of various vector graphics\nformats, (c) diverse question types, (d) wide range of prompting techniques,\n(e) under multiple LLMs and (f) comparison with VLMs on rasterized\nrepresentations. Evaluating on our collected 4279 understanding and 5845\ngeneration samples, we find that LLMs show strong capability on both aspects\nwhile exhibiting less desirable performance on low-level formats (SVG). Both\ndata and evaluation pipeline will be open-sourced at https://vgbench.github.io."
                },
                "authors": [
                    {
                        "name": "Bocheng Zou"
                    },
                    {
                        "name": "Mu Cai"
                    },
                    {
                        "name": "Jianrui Zhang"
                    },
                    {
                        "name": "Yong Jae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yong Jae Lee"
                },
                "author": "Yong Jae Lee",
                "arxiv_comment": "Project Page: https://vgbench.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16756v1",
                "updated": "2024-08-29T17:54:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    54,
                    14,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:54:14Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    54,
                    14,
                    3,
                    242,
                    0
                ],
                "title": "How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of\n  Large Language Models"
                },
                "summary": "The rapid evolution of large language models (LLMs) has transformed the\ncompetitive landscape in natural language processing (NLP), particularly for\nEnglish and other data-rich languages. However, underrepresented languages like\nCantonese, spoken by over 85 million people, face significant development gaps,\nwhich is particularly concerning given the economic significance of the\nGuangdong-Hong Kong-Macau Greater Bay Area, and in substantial\nCantonese-speaking populations in places like Singapore and North America.\nDespite its wide use, Cantonese has scant representation in NLP research,\nespecially compared to other languages from similarly developed regions. To\nbridge these gaps, we outline current Cantonese NLP methods and introduce new\nbenchmarks designed to evaluate LLM performance in factual generation,\nmathematical logic, complex reasoning, and general knowledge in Cantonese,\nwhich aim to advance open-source Cantonese LLM technology. We also propose\nfuture research directions and recommended models to enhance Cantonese LLM\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs) has transformed the\ncompetitive landscape in natural language processing (NLP), particularly for\nEnglish and other data-rich languages. However, underrepresented languages like\nCantonese, spoken by over 85 million people, face significant development gaps,\nwhich is particularly concerning given the economic significance of the\nGuangdong-Hong Kong-Macau Greater Bay Area, and in substantial\nCantonese-speaking populations in places like Singapore and North America.\nDespite its wide use, Cantonese has scant representation in NLP research,\nespecially compared to other languages from similarly developed regions. To\nbridge these gaps, we outline current Cantonese NLP methods and introduce new\nbenchmarks designed to evaluate LLM performance in factual generation,\nmathematical logic, complex reasoning, and general knowledge in Cantonese,\nwhich aim to advance open-source Cantonese LLM technology. We also propose\nfuture research directions and recommended models to enhance Cantonese LLM\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Jiyue Jiang"
                    },
                    {
                        "name": "Liheng Chen"
                    },
                    {
                        "name": "Pengan Chen"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Qinghang Bao"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16754v1",
                "updated": "2024-08-29T17:53:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    53,
                    0,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:53:00Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    53,
                    0,
                    3,
                    242,
                    0
                ],
                "title": "A compact neuromorphic system for ultra energy-efficient, on-device\n  robot localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A compact neuromorphic system for ultra energy-efficient, on-device\n  robot localization"
                },
                "summary": "Neuromorphic computing offers a transformative pathway to overcome the\ncomputational and energy challenges faced in deploying robotic localization and\nnavigation systems at the edge. Visual place recognition, a critical component\nfor navigation, is often hampered by the high resource demands of conventional\nsystems, making them unsuitable for small-scale robotic platforms which still\nrequire to perform complex, long-range tasks. Although neuromorphic approaches\noffer potential for greater efficiency, real-time edge deployment remains\nconstrained by the complexity and limited scalability of bio-realistic\nnetworks. Here, we demonstrate a neuromorphic localization system that performs\naccurate place recognition in up to 8km of traversal using models as small as\n180 KB with 44k parameters, while consuming less than 1% of the energy required\nby conventional methods. Our Locational Encoding with Neuromorphic Systems\n(LENS) integrates spiking neural networks, an event-based dynamic vision\nsensor, and a neuromorphic processor within a single SPECK(TM) chip, enabling\nreal-time, energy-efficient localization on a hexapod robot. LENS represents\nthe first fully neuromorphic localization system capable of large-scale,\non-device deployment, setting a new benchmark for energy efficient robotic\nplace recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic computing offers a transformative pathway to overcome the\ncomputational and energy challenges faced in deploying robotic localization and\nnavigation systems at the edge. Visual place recognition, a critical component\nfor navigation, is often hampered by the high resource demands of conventional\nsystems, making them unsuitable for small-scale robotic platforms which still\nrequire to perform complex, long-range tasks. Although neuromorphic approaches\noffer potential for greater efficiency, real-time edge deployment remains\nconstrained by the complexity and limited scalability of bio-realistic\nnetworks. Here, we demonstrate a neuromorphic localization system that performs\naccurate place recognition in up to 8km of traversal using models as small as\n180 KB with 44k parameters, while consuming less than 1% of the energy required\nby conventional methods. Our Locational Encoding with Neuromorphic Systems\n(LENS) integrates spiking neural networks, an event-based dynamic vision\nsensor, and a neuromorphic processor within a single SPECK(TM) chip, enabling\nreal-time, energy-efficient localization on a hexapod robot. LENS represents\nthe first fully neuromorphic localization system capable of large-scale,\non-device deployment, setting a new benchmark for energy efficient robotic\nplace recognition."
                },
                "authors": [
                    {
                        "name": "Adam D. Hines"
                    },
                    {
                        "name": "Michael Milford"
                    },
                    {
                        "name": "Tobias Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Fischer"
                },
                "author": "Tobias Fischer",
                "arxiv_comment": "28 pages, 4 main figures, 4 supplementary figures, 1 supplementary\n  table, and 1 movie. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16740v1",
                "updated": "2024-08-29T17:34:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    34,
                    10,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:34:10Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    34,
                    10,
                    3,
                    242,
                    0
                ],
                "title": "Theoretical and Methodological Framework for Studying Texts Produced by\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical and Methodological Framework for Studying Texts Produced by\n  Large Language Models"
                },
                "summary": "This paper addresses the conceptual, methodological and technical challenges\nin studying large language models (LLMs) and the texts they produce from a\nquantitative linguistics perspective. It builds on a theoretical framework that\ndistinguishes between the LLM as a substrate and the entities the model\nsimulates. The paper advocates for a strictly non-anthropomorphic approach to\nmodels while cautiously applying methodologies used in studying human\nlinguistic behavior to the simulated entities. While natural language\nprocessing researchers focus on the models themselves, their architecture,\nevaluation, and methods for improving performance, we as quantitative linguists\nshould strive to build a robust theory concerning the characteristics of texts\nproduced by LLMs, how they differ from human-produced texts, and the properties\nof simulated entities. Additionally, we should explore the potential of LLMs as\nan instrument for studying human culture, of which language is an integral\npart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the conceptual, methodological and technical challenges\nin studying large language models (LLMs) and the texts they produce from a\nquantitative linguistics perspective. It builds on a theoretical framework that\ndistinguishes between the LLM as a substrate and the entities the model\nsimulates. The paper advocates for a strictly non-anthropomorphic approach to\nmodels while cautiously applying methodologies used in studying human\nlinguistic behavior to the simulated entities. While natural language\nprocessing researchers focus on the models themselves, their architecture,\nevaluation, and methods for improving performance, we as quantitative linguists\nshould strive to build a robust theory concerning the characteristics of texts\nproduced by LLMs, how they differ from human-produced texts, and the properties\nof simulated entities. Additionally, we should explore the potential of LLMs as\nan instrument for studying human culture, of which language is an integral\npart."
                },
                "authors": [
                    {
                        "name": "Jiří Milička"
                    }
                ],
                "author_detail": {
                    "name": "Jiří Milička"
                },
                "author": "Jiří Milička",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16737v1",
                "updated": "2024-08-29T17:32:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    32,
                    35,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:32:35Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    32,
                    35,
                    3,
                    242,
                    0
                ],
                "title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal\n  Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal\n  Sampling"
                },
                "summary": "Training on high-quality synthetic data from strong language models (LMs) is\na common strategy to improve the reasoning performance of LMs. In this work, we\nrevisit whether this strategy is compute-optimal under a fixed inference budget\n(e.g., FLOPs). To do so, we investigate the trade-offs between generating\nsynthetic data using a stronger but more expensive (SE) model versus a weaker\nbut cheaper (WC) model. We evaluate the generated data across three key\nmetrics: coverage, diversity, and false positive rate, and show that the data\nfrom WC models may have higher coverage and diversity, but also exhibit higher\nfalse positive rates. We then finetune LMs on data from SE and WC models in\ndifferent settings: knowledge distillation, self-improvement, and a novel\nweak-to-strong improvement setup where a weaker LM teaches reasoning to a\nstronger LM. Our findings reveal that models finetuned on WC-generated data\nconsistently outperform those trained on SE-generated data across multiple\nbenchmarks and multiple choices of WC and SE models. These results challenge\nthe prevailing practice of relying on SE models for synthetic data generation,\nsuggesting that WC may be the compute-optimal approach for training advanced LM\nreasoners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training on high-quality synthetic data from strong language models (LMs) is\na common strategy to improve the reasoning performance of LMs. In this work, we\nrevisit whether this strategy is compute-optimal under a fixed inference budget\n(e.g., FLOPs). To do so, we investigate the trade-offs between generating\nsynthetic data using a stronger but more expensive (SE) model versus a weaker\nbut cheaper (WC) model. We evaluate the generated data across three key\nmetrics: coverage, diversity, and false positive rate, and show that the data\nfrom WC models may have higher coverage and diversity, but also exhibit higher\nfalse positive rates. We then finetune LMs on data from SE and WC models in\ndifferent settings: knowledge distillation, self-improvement, and a novel\nweak-to-strong improvement setup where a weaker LM teaches reasoning to a\nstronger LM. Our findings reveal that models finetuned on WC-generated data\nconsistently outperform those trained on SE-generated data across multiple\nbenchmarks and multiple choices of WC and SE models. These results challenge\nthe prevailing practice of relying on SE models for synthetic data generation,\nsuggesting that WC may be the compute-optimal approach for training advanced LM\nreasoners."
                },
                "authors": [
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Arian Hosseini"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Vinh Q. Tran"
                    },
                    {
                        "name": "Mehran Kazemi"
                    }
                ],
                "author_detail": {
                    "name": "Mehran Kazemi"
                },
                "author": "Mehran Kazemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15409v2",
                "updated": "2024-08-29T17:00:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    0,
                    24,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-27T21:19:37Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    21,
                    19,
                    37,
                    1,
                    240,
                    0
                ],
                "title": "Awes, Laws, and Flaws From Today's LLM Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Awes, Laws, and Flaws From Today's LLM Research"
                },
                "summary": "We perform a critical examination of the scientific methodology behind\ncontemporary large language model (LLM) research. For this we assess over 2,000\nresearch works based on criteria typical of what is considered good research\n(e.g. presence of statistical tests and reproducibility) and cross-validate it\nwith arguments that are at the centre of controversy (e.g., claims of emergent\nbehaviour, the use of LLMs as evaluators). We find multiple trends, such as\ndeclines in claims of emergent behaviour and ethics disclaimers; the rise of\nLLMs as evaluators in spite of a lack of consensus from the community about\ntheir useability; and an increase of claims of LLM reasoning abilities,\ntypically without leveraging human evaluation. This paper underscores the need\nfor more scrutiny and rigour by and from this field to live up to the\nfundamentals of a responsible scientific method that is ethical, reproducible,\nsystematic, and open to criticism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We perform a critical examination of the scientific methodology behind\ncontemporary large language model (LLM) research. For this we assess over 2,000\nresearch works based on criteria typical of what is considered good research\n(e.g. presence of statistical tests and reproducibility) and cross-validate it\nwith arguments that are at the centre of controversy (e.g., claims of emergent\nbehaviour, the use of LLMs as evaluators). We find multiple trends, such as\ndeclines in claims of emergent behaviour and ethics disclaimers; the rise of\nLLMs as evaluators in spite of a lack of consensus from the community about\ntheir useability; and an increase of claims of LLM reasoning abilities,\ntypically without leveraging human evaluation. This paper underscores the need\nfor more scrutiny and rigour by and from this field to live up to the\nfundamentals of a responsible scientific method that is ethical, reproducible,\nsystematic, and open to criticism."
                },
                "authors": [
                    {
                        "name": "Adrian de Wynter"
                    }
                ],
                "author_detail": {
                    "name": "Adrian de Wynter"
                },
                "author": "Adrian de Wynter",
                "arxiv_comment": "Under review -- v1 was an old draft with an unrevised abstract (oops)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16703v1",
                "updated": "2024-08-29T16:56:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    56,
                    40,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T16:56:40Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    56,
                    40,
                    3,
                    242,
                    0
                ],
                "title": "RoboMNIST: A Multimodal Dataset for Multi-Robot Activity Recognition\n  Using WiFi Sensing, Video, and Audio",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboMNIST: A Multimodal Dataset for Multi-Robot Activity Recognition\n  Using WiFi Sensing, Video, and Audio"
                },
                "summary": "We introduce a novel dataset for multi-robot activity recognition (MRAR)\nusing two robotic arms integrating WiFi channel state information (CSI), video,\nand audio data. This multimodal dataset utilizes signals of opportunity,\nleveraging existing WiFi infrastructure to provide detailed indoor\nenvironmental sensing without additional sensor deployment. Data were collected\nusing two Franka Emika robotic arms, complemented by three cameras, three WiFi\nsniffers to collect CSI, and three microphones capturing distinct yet\ncomplementary audio data streams. The combination of CSI, visual, and auditory\ndata can enhance robustness and accuracy in MRAR. This comprehensive dataset\nenables a holistic understanding of robotic environments, facilitating advanced\nautonomous operations that mimic human-like perception and interaction. By\nrepurposing ubiquitous WiFi signals for environmental sensing, this dataset\noffers significant potential aiming to advance robotic perception and\nautonomous systems. It provides a valuable resource for developing\nsophisticated decision-making and adaptive capabilities in dynamic\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel dataset for multi-robot activity recognition (MRAR)\nusing two robotic arms integrating WiFi channel state information (CSI), video,\nand audio data. This multimodal dataset utilizes signals of opportunity,\nleveraging existing WiFi infrastructure to provide detailed indoor\nenvironmental sensing without additional sensor deployment. Data were collected\nusing two Franka Emika robotic arms, complemented by three cameras, three WiFi\nsniffers to collect CSI, and three microphones capturing distinct yet\ncomplementary audio data streams. The combination of CSI, visual, and auditory\ndata can enhance robustness and accuracy in MRAR. This comprehensive dataset\nenables a holistic understanding of robotic environments, facilitating advanced\nautonomous operations that mimic human-like perception and interaction. By\nrepurposing ubiquitous WiFi signals for environmental sensing, this dataset\noffers significant potential aiming to advance robotic perception and\nautonomous systems. It provides a valuable resource for developing\nsophisticated decision-making and adaptive capabilities in dynamic\nenvironments."
                },
                "authors": [
                    {
                        "name": "Kian Behzad"
                    },
                    {
                        "name": "Rojin Zandi"
                    },
                    {
                        "name": "Elaheh Motamedi"
                    },
                    {
                        "name": "Hojjat Salehinejad"
                    },
                    {
                        "name": "Milad Siami"
                    }
                ],
                "author_detail": {
                    "name": "Milad Siami"
                },
                "author": "Milad Siami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16700v1",
                "updated": "2024-08-29T16:51:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    51,
                    7,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T16:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    51,
                    7,
                    3,
                    242,
                    0
                ],
                "title": "GradBias: Unveiling Word Influence on Bias in Text-to-Image Generative\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GradBias: Unveiling Word Influence on Bias in Text-to-Image Generative\n  Models"
                },
                "summary": "Recent progress in Text-to-Image (T2I) generative models has enabled\nhigh-quality image generation. As performance and accessibility increase, these\nmodels are gaining significant attraction and popularity: ensuring their\nfairness and safety is a priority to prevent the dissemination and perpetuation\nof biases. However, existing studies in bias detection focus on closed sets of\npredefined biases (e.g., gender, ethnicity). In this paper, we propose a\ngeneral framework to identify, quantify, and explain biases in an open set\nsetting, i.e. without requiring a predefined set. This pipeline leverages a\nLarge Language Model (LLM) to propose biases starting from a set of captions.\nNext, these captions are used by the target generative model for generating a\nset of images. Finally, Vision Question Answering (VQA) is leveraged for bias\nevaluation. We show two variations of this framework: OpenBias and GradBias.\nOpenBias detects and quantifies biases, while GradBias determines the\ncontribution of individual prompt words on biases. OpenBias effectively detects\nboth well-known and novel biases related to people, objects, and animals and\nhighly aligns with existing closed-set bias detection methods and human\njudgment. GradBias shows that neutral words can significantly influence biases\nand it outperforms several baselines, including state-of-the-art foundation\nmodels. Code available here: https://github.com/Moreno98/GradBias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in Text-to-Image (T2I) generative models has enabled\nhigh-quality image generation. As performance and accessibility increase, these\nmodels are gaining significant attraction and popularity: ensuring their\nfairness and safety is a priority to prevent the dissemination and perpetuation\nof biases. However, existing studies in bias detection focus on closed sets of\npredefined biases (e.g., gender, ethnicity). In this paper, we propose a\ngeneral framework to identify, quantify, and explain biases in an open set\nsetting, i.e. without requiring a predefined set. This pipeline leverages a\nLarge Language Model (LLM) to propose biases starting from a set of captions.\nNext, these captions are used by the target generative model for generating a\nset of images. Finally, Vision Question Answering (VQA) is leveraged for bias\nevaluation. We show two variations of this framework: OpenBias and GradBias.\nOpenBias detects and quantifies biases, while GradBias determines the\ncontribution of individual prompt words on biases. OpenBias effectively detects\nboth well-known and novel biases related to people, objects, and animals and\nhighly aligns with existing closed-set bias detection methods and human\njudgment. GradBias shows that neutral words can significantly influence biases\nand it outperforms several baselines, including state-of-the-art foundation\nmodels. Code available here: https://github.com/Moreno98/GradBias."
                },
                "authors": [
                    {
                        "name": "Moreno D'Incà"
                    },
                    {
                        "name": "Elia Peruzzo"
                    },
                    {
                        "name": "Massimiliano Mancini"
                    },
                    {
                        "name": "Xingqian Xu"
                    },
                    {
                        "name": "Humphrey Shi"
                    },
                    {
                        "name": "Nicu Sebe"
                    }
                ],
                "author_detail": {
                    "name": "Nicu Sebe"
                },
                "author": "Nicu Sebe",
                "arxiv_comment": "Under review. Code: https://github.com/Moreno98/GradBias",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04952v2",
                "updated": "2024-08-29T16:49:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    49,
                    29,
                    3,
                    242,
                    0
                ],
                "published": "2024-06-07T14:16:37Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    14,
                    16,
                    37,
                    4,
                    159,
                    0
                ],
                "title": "Quantifying Geospatial in the Common Crawl Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Geospatial in the Common Crawl Corpus"
                },
                "summary": "Large language models (LLMs) exhibit emerging geospatial capabilities,\nstemming from their pre-training on vast unlabelled text datasets that are\noften derived from the Common Crawl (CC) corpus. However, the geospatial\ncontent within CC remains largely unexplored, impacting our understanding of\nLLMs' spatial reasoning. This paper investigates the prevalence of geospatial\ndata in recent Common Crawl releases using Gemini 1.5, a powerful language\nmodel. By analyzing a sample of documents and manually revising the results, we\nestimate that 18.7% of web documents in CC contain geospatial information such\nas coordinates and addresses. We find little difference in prevalence between\nEnlgish- and non-English-language documents. Our findings provide quantitative\ninsights into the nature and extent of geospatial data in CC, and lay the\ngroundwork for future studies of geospatial biases of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit emerging geospatial capabilities,\nstemming from their pre-training on vast unlabelled text datasets that are\noften derived from the Common Crawl (CC) corpus. However, the geospatial\ncontent within CC remains largely unexplored, impacting our understanding of\nLLMs' spatial reasoning. This paper investigates the prevalence of geospatial\ndata in recent Common Crawl releases using Gemini 1.5, a powerful language\nmodel. By analyzing a sample of documents and manually revising the results, we\nestimate that 18.7% of web documents in CC contain geospatial information such\nas coordinates and addresses. We find little difference in prevalence between\nEnlgish- and non-English-language documents. Our findings provide quantitative\ninsights into the nature and extent of geospatial data in CC, and lay the\ngroundwork for future studies of geospatial biases of LLMs."
                },
                "authors": [
                    {
                        "name": "Ilya Ilyankou"
                    },
                    {
                        "name": "Meihui Wang"
                    },
                    {
                        "name": "Stefano Cavazzi"
                    },
                    {
                        "name": "James Haworth"
                    }
                ],
                "author_detail": {
                    "name": "James Haworth"
                },
                "author": "James Haworth",
                "arxiv_comment": "Accepted as a poster to ACM SIGSPATIAL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16683v1",
                "updated": "2024-08-29T16:28:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    28,
                    43,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T16:28:43Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    28,
                    43,
                    3,
                    242,
                    0
                ],
                "title": "A Catalog of Fairness-Aware Practices in Machine Learning Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Catalog of Fairness-Aware Practices in Machine Learning Engineering"
                },
                "summary": "Machine learning's widespread adoption in decision-making processes raises\nconcerns about fairness, particularly regarding the treatment of sensitive\nfeatures and potential discrimination against minorities. The software\nengineering community has responded by developing fairness-oriented metrics,\nempirical studies, and approaches. However, there remains a gap in\nunderstanding and categorizing practices for engineering fairness throughout\nthe machine learning lifecycle. This paper presents a novel catalog of\npractices for addressing fairness in machine learning derived from a systematic\nmapping study. The study identifies and categorizes 28 practices from existing\nliterature, mapping them onto different stages of the machine learning\nlifecycle. From this catalog, the authors extract actionable items and\nimplications for both researchers and practitioners in software engineering.\nThis work aims to provide a comprehensive resource for integrating fairness\nconsiderations into the development and deployment of machine learning systems,\nenhancing their reliability, accountability, and credibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning's widespread adoption in decision-making processes raises\nconcerns about fairness, particularly regarding the treatment of sensitive\nfeatures and potential discrimination against minorities. The software\nengineering community has responded by developing fairness-oriented metrics,\nempirical studies, and approaches. However, there remains a gap in\nunderstanding and categorizing practices for engineering fairness throughout\nthe machine learning lifecycle. This paper presents a novel catalog of\npractices for addressing fairness in machine learning derived from a systematic\nmapping study. The study identifies and categorizes 28 practices from existing\nliterature, mapping them onto different stages of the machine learning\nlifecycle. From this catalog, the authors extract actionable items and\nimplications for both researchers and practitioners in software engineering.\nThis work aims to provide a comprehensive resource for integrating fairness\nconsiderations into the development and deployment of machine learning systems,\nenhancing their reliability, accountability, and credibility."
                },
                "authors": [
                    {
                        "name": "Gianmario Voria"
                    },
                    {
                        "name": "Giulia Sellitto"
                    },
                    {
                        "name": "Carmine Ferrara"
                    },
                    {
                        "name": "Francesco Abate"
                    },
                    {
                        "name": "Andrea De Lucia"
                    },
                    {
                        "name": "Filomena Ferrucci"
                    },
                    {
                        "name": "Gemma Catolino"
                    },
                    {
                        "name": "Fabio Palomba"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Palomba"
                },
                "author": "Fabio Palomba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16673v1",
                "updated": "2024-08-29T16:21:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    21,
                    0,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T16:21:00Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    21,
                    0,
                    3,
                    242,
                    0
                ],
                "title": "Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less\n  Overfitting and Better Diversity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less\n  Overfitting and Better Diversity"
                },
                "summary": "Large language models rely on Supervised Fine-Tuning (SFT) to specialize in\ndownstream tasks. Cross Entropy (CE) loss is the de facto choice in SFT, but it\noften leads to overfitting and limited output diversity due to its aggressive\nupdates to the data distribution. This paper aim to address these issues by\nintroducing the maximum entropy principle, which favors models with flatter\ndistributions that still effectively capture the data. Specifically, we develop\na new distribution matching method called GEM, which solves reverse\nKullback-Leibler divergence minimization with an entropy regularizer.\n  For the SFT of Llama-3-8B models, GEM outperforms CE in several aspects.\nFirst, when applied to the UltraFeedback dataset to develop general\ninstruction-following abilities, GEM exhibits reduced overfitting, evidenced by\nlower perplexity and better performance on the IFEval benchmark. Furthermore,\nGEM enhances output diversity, leading to performance gains of up to 7 points\non math reasoning and code generation tasks using best-of-n sampling, even\nwithout domain-specific data. Second, when fine-tuning with domain-specific\ndatasets for math reasoning and code generation, GEM also shows less\noverfitting and improvements of up to 10 points compared with CE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models rely on Supervised Fine-Tuning (SFT) to specialize in\ndownstream tasks. Cross Entropy (CE) loss is the de facto choice in SFT, but it\noften leads to overfitting and limited output diversity due to its aggressive\nupdates to the data distribution. This paper aim to address these issues by\nintroducing the maximum entropy principle, which favors models with flatter\ndistributions that still effectively capture the data. Specifically, we develop\na new distribution matching method called GEM, which solves reverse\nKullback-Leibler divergence minimization with an entropy regularizer.\n  For the SFT of Llama-3-8B models, GEM outperforms CE in several aspects.\nFirst, when applied to the UltraFeedback dataset to develop general\ninstruction-following abilities, GEM exhibits reduced overfitting, evidenced by\nlower perplexity and better performance on the IFEval benchmark. Furthermore,\nGEM enhances output diversity, leading to performance gains of up to 7 points\non math reasoning and code generation tasks using best-of-n sampling, even\nwithout domain-specific data. Second, when fine-tuning with domain-specific\ndatasets for math reasoning and code generation, GEM also shows less\noverfitting and improvements of up to 10 points compared with CE."
                },
                "authors": [
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Congliang Chen"
                    },
                    {
                        "name": "Tian Xu"
                    },
                    {
                        "name": "Zeyu Qin"
                    },
                    {
                        "name": "Jiancong Xiao"
                    },
                    {
                        "name": "Ruoyu Sun"
                    },
                    {
                        "name": "Zhi-Quan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Quan Luo"
                },
                "author": "Zhi-Quan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16667v1",
                "updated": "2024-08-29T16:15:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    15,
                    1,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T16:15:01Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    15,
                    1,
                    3,
                    242,
                    0
                ],
                "title": "Iterative Graph Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Graph Alignment"
                },
                "summary": "By compressing diverse narratives, LLMs go beyond memorization, achieving\nintelligence by capturing generalizable causal relationships. However, they\nsuffer from local 'representation gaps' due to insufficient training data\ndiversity, limiting their real-world utility, especially in tasks requiring\nstrict alignment to rules. Traditional alignment methods relying on heavy human\nannotations are inefficient and unscalable. Recent self-alignment techniques\nalso fall short, as they often depend on self-selection based prompting and\nmemorization-based learning. To address these issues, we introduce Iterative\nGraph Alignment (IGA), an annotation-free rule-based alignment algorithm. A\nteacher model (VLM) employs Iterative Graph Prompting (IGP) to create logical\ngraphs and reference answers. The student model (LLM) identifies local\nknowledge gaps by attempting to align its responses with these references,\ncollaborating with helper models to generate diverse answers. These aligned\nresponses are then used for iterative supervised fine-tuning (SFT). Our\nevaluations across five rule-based scenarios demonstrate IGP's effectiveness,\nwith a 73.12\\% alignment improvement in Claude Sonnet 3.5, and\nLlama3-8B-Instruct achieving an 86.20\\% improvement, outperforming Claude\nSonnet 3.5 in rule-based alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By compressing diverse narratives, LLMs go beyond memorization, achieving\nintelligence by capturing generalizable causal relationships. However, they\nsuffer from local 'representation gaps' due to insufficient training data\ndiversity, limiting their real-world utility, especially in tasks requiring\nstrict alignment to rules. Traditional alignment methods relying on heavy human\nannotations are inefficient and unscalable. Recent self-alignment techniques\nalso fall short, as they often depend on self-selection based prompting and\nmemorization-based learning. To address these issues, we introduce Iterative\nGraph Alignment (IGA), an annotation-free rule-based alignment algorithm. A\nteacher model (VLM) employs Iterative Graph Prompting (IGP) to create logical\ngraphs and reference answers. The student model (LLM) identifies local\nknowledge gaps by attempting to align its responses with these references,\ncollaborating with helper models to generate diverse answers. These aligned\nresponses are then used for iterative supervised fine-tuning (SFT). Our\nevaluations across five rule-based scenarios demonstrate IGP's effectiveness,\nwith a 73.12\\% alignment improvement in Claude Sonnet 3.5, and\nLlama3-8B-Instruct achieving an 86.20\\% improvement, outperforming Claude\nSonnet 3.5 in rule-based alignment."
                },
                "authors": [
                    {
                        "name": "Fangyuan Yu"
                    },
                    {
                        "name": "Hardeep Singh Arora"
                    },
                    {
                        "name": "Matt Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Matt Johnson"
                },
                "author": "Matt Johnson",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16629v1",
                "updated": "2024-08-29T15:36:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    15,
                    36,
                    52,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T15:36:52Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    15,
                    36,
                    52,
                    3,
                    242,
                    0
                ],
                "title": "LLMs generate structurally realistic social networks but overestimate\n  political homophily",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs generate structurally realistic social networks but overestimate\n  political homophily"
                },
                "summary": "Generating social networks is essential for many applications, such as\nepidemic modeling and social simulations. Prior approaches either involve deep\nlearning models, which require many observed networks for training, or stylized\nmodels, which are limited in their realism and flexibility. In contrast, LLMs\noffer the potential for zero-shot and flexible network generation. However, two\nkey questions are: (1) are LLM's generated networks realistic, and (2) what are\nrisks of bias, given the importance of demographics in forming social ties? To\nanswer these questions, we develop three prompting methods for network\ngeneration and compare the generated networks to real social networks. We find\nthat more realistic networks are generated with \"local\" methods, where the LLM\nconstructs relations for one persona at a time, compared to \"global\" methods\nthat construct the entire network at once. We also find that the generated\nnetworks match real networks on many characteristics, including density,\nclustering, community structure, and degree. However, we find that LLMs\nemphasize political homophily over all other types of homophily and\noverestimate political homophily relative to real-world measures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating social networks is essential for many applications, such as\nepidemic modeling and social simulations. Prior approaches either involve deep\nlearning models, which require many observed networks for training, or stylized\nmodels, which are limited in their realism and flexibility. In contrast, LLMs\noffer the potential for zero-shot and flexible network generation. However, two\nkey questions are: (1) are LLM's generated networks realistic, and (2) what are\nrisks of bias, given the importance of demographics in forming social ties? To\nanswer these questions, we develop three prompting methods for network\ngeneration and compare the generated networks to real social networks. We find\nthat more realistic networks are generated with \"local\" methods, where the LLM\nconstructs relations for one persona at a time, compared to \"global\" methods\nthat construct the entire network at once. We also find that the generated\nnetworks match real networks on many characteristics, including density,\nclustering, community structure, and degree. However, we find that LLMs\nemphasize political homophily over all other types of homophily and\noverestimate political homophily relative to real-world measures."
                },
                "authors": [
                    {
                        "name": "Serina Chang"
                    },
                    {
                        "name": "Alicja Chaszczewicz"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Maya Josifovska"
                    },
                    {
                        "name": "Emma Pierson"
                    },
                    {
                        "name": "Jure Leskovec"
                    }
                ],
                "author_detail": {
                    "name": "Jure Leskovec"
                },
                "author": "Jure Leskovec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16601v1",
                "updated": "2024-08-29T15:12:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    15,
                    12,
                    16,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T15:12:16Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    15,
                    12,
                    16,
                    3,
                    242,
                    0
                ],
                "title": "Examination of Code generated by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examination of Code generated by Large Language Models"
                },
                "summary": "Large language models (LLMs), such as ChatGPT and Copilot, are transforming\nsoftware development by automating code generation and, arguably, enable rapid\nprototyping, support education, and boost productivity. Therefore, correctness\nand quality of the generated code should be on par with manually written code.\nTo assess the current state of LLMs in generating correct code of high quality,\nwe conducted controlled experiments with ChatGPT and Copilot: we let the LLMs\ngenerate simple algorithms in Java and Python along with the corresponding unit\ntests and assessed the correctness and the quality (coverage) of the generated\n(test) codes. We observed significant differences between the LLMs, between the\nlanguages, between algorithm and test codes, and over time. The present paper\nreports these results together with the experimental methods allowing repeated\nand comparable assessments for more algorithms, languages, and LLMs over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), such as ChatGPT and Copilot, are transforming\nsoftware development by automating code generation and, arguably, enable rapid\nprototyping, support education, and boost productivity. Therefore, correctness\nand quality of the generated code should be on par with manually written code.\nTo assess the current state of LLMs in generating correct code of high quality,\nwe conducted controlled experiments with ChatGPT and Copilot: we let the LLMs\ngenerate simple algorithms in Java and Python along with the corresponding unit\ntests and assessed the correctness and the quality (coverage) of the generated\n(test) codes. We observed significant differences between the LLMs, between the\nlanguages, between algorithm and test codes, and over time. The present paper\nreports these results together with the experimental methods allowing repeated\nand comparable assessments for more algorithms, languages, and LLMs over time."
                },
                "authors": [
                    {
                        "name": "Robin Beer"
                    },
                    {
                        "name": "Alexander Feix"
                    },
                    {
                        "name": "Tim Guttzeit"
                    },
                    {
                        "name": "Tamara Muras"
                    },
                    {
                        "name": "Vincent Müller"
                    },
                    {
                        "name": "Maurice Rauscher"
                    },
                    {
                        "name": "Florian Schäffler"
                    },
                    {
                        "name": "Welf Löwe"
                    }
                ],
                "author_detail": {
                    "name": "Welf Löwe"
                },
                "author": "Welf Löwe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05418v2",
                "updated": "2024-08-29T14:50:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    50,
                    10,
                    3,
                    242,
                    0
                ],
                "published": "2024-05-08T20:39:54Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    20,
                    39,
                    54,
                    2,
                    129,
                    0
                ],
                "title": "Mitigating Exaggerated Safety in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Exaggerated Safety in Large Language Models"
                },
                "summary": "As the popularity of Large Language Models (LLMs) grow, combining model\nsafety with utility becomes increasingly important. The challenge is making\nsure that LLMs can recognize and decline dangerous prompts without sacrificing\ntheir ability to be helpful. The problem of \"exaggerated safety\" demonstrates\nhow difficult this can be. To reduce excessive safety behaviours -- which was\ndiscovered to be 26.1% of safe prompts being misclassified as dangerous and\nrefused -- we use a combination of XSTest dataset prompts as well as\ninteractive, contextual, and few-shot prompting to examine the decision bounds\nof LLMs such as Llama2, Gemma Command R+, and Phi-3. We find that few-shot\nprompting works best for Llama2, interactive prompting works best Gemma, and\ncontextual prompting works best for Command R+ and Phi-3. Using a combination\nof these prompting strategies, we are able to mitigate exaggerated safety\nbehaviors by an overall 92.9% across all LLMs. Our work presents a multiple\nprompting strategies to jailbreak LLMs' decision-making processes, allowing\nthem to navigate the tight line between refusing unsafe prompts and remaining\nhelpful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the popularity of Large Language Models (LLMs) grow, combining model\nsafety with utility becomes increasingly important. The challenge is making\nsure that LLMs can recognize and decline dangerous prompts without sacrificing\ntheir ability to be helpful. The problem of \"exaggerated safety\" demonstrates\nhow difficult this can be. To reduce excessive safety behaviours -- which was\ndiscovered to be 26.1% of safe prompts being misclassified as dangerous and\nrefused -- we use a combination of XSTest dataset prompts as well as\ninteractive, contextual, and few-shot prompting to examine the decision bounds\nof LLMs such as Llama2, Gemma Command R+, and Phi-3. We find that few-shot\nprompting works best for Llama2, interactive prompting works best Gemma, and\ncontextual prompting works best for Command R+ and Phi-3. Using a combination\nof these prompting strategies, we are able to mitigate exaggerated safety\nbehaviors by an overall 92.9% across all LLMs. Our work presents a multiple\nprompting strategies to jailbreak LLMs' decision-making processes, allowing\nthem to navigate the tight line between refusing unsafe prompts and remaining\nhelpful."
                },
                "authors": [
                    {
                        "name": "Ruchira Ray"
                    },
                    {
                        "name": "Ruchi Bhalani"
                    }
                ],
                "author_detail": {
                    "name": "Ruchi Bhalani"
                },
                "author": "Ruchi Bhalani",
                "arxiv_comment": "17 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16586v1",
                "updated": "2024-08-29T14:49:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    49,
                    13,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T14:49:13Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    49,
                    13,
                    3,
                    242,
                    0
                ],
                "title": "Enhancing Dialogue Generation in Werewolf Game Through Situation\n  Analysis and Persuasion Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Dialogue Generation in Werewolf Game Through Situation\n  Analysis and Persuasion Strategies"
                },
                "summary": "Recent advancements in natural language processing, particularly with large\nlanguage models (LLMs) like GPT-4, have significantly enhanced dialogue\nsystems, enabling them to generate more natural and fluent conversations.\nDespite these improvements, challenges persist, such as managing continuous\ndialogues, memory retention, and minimizing hallucinations. The AIWolfDial2024\naddresses these challenges by employing the Werewolf Game, an incomplete\ninformation game, to test the capabilities of LLMs in complex interactive\nenvironments. This paper introduces a LLM-based Werewolf Game AI, where each\nrole is supported by situation analysis to aid response generation.\nAdditionally, for the werewolf role, various persuasion strategies, including\nlogical appeal, credibility appeal, and emotional appeal, are employed to\neffectively persuade other players to align with its actions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in natural language processing, particularly with large\nlanguage models (LLMs) like GPT-4, have significantly enhanced dialogue\nsystems, enabling them to generate more natural and fluent conversations.\nDespite these improvements, challenges persist, such as managing continuous\ndialogues, memory retention, and minimizing hallucinations. The AIWolfDial2024\naddresses these challenges by employing the Werewolf Game, an incomplete\ninformation game, to test the capabilities of LLMs in complex interactive\nenvironments. This paper introduces a LLM-based Werewolf Game AI, where each\nrole is supported by situation analysis to aid response generation.\nAdditionally, for the werewolf role, various persuasion strategies, including\nlogical appeal, credibility appeal, and emotional appeal, are employed to\neffectively persuade other players to align with its actions."
                },
                "authors": [
                    {
                        "name": "Zhiyang Qi"
                    },
                    {
                        "name": "Michimasa Inaba"
                    }
                ],
                "author_detail": {
                    "name": "Michimasa Inaba"
                },
                "author": "Michimasa Inaba",
                "arxiv_comment": "Accepted to the AIWolfDial2024 workshop at INLG 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11455v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11455v2",
                "updated": "2024-08-29T14:48:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    48,
                    10,
                    3,
                    242,
                    0
                ],
                "published": "2024-06-17T12:11:01Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    12,
                    11,
                    1,
                    0,
                    169,
                    0
                ],
                "title": "Adaptive Reinforcement Learning Planning: Harnessing Large Language\n  Models for Complex Information Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Reinforcement Learning Planning: Harnessing Large Language\n  Models for Complex Information Extraction"
                },
                "summary": "Existing research on large language models (LLMs) shows that they can solve\ninformation extraction tasks through multi-step planning. However, their\nextraction behavior on complex sentences and tasks is unstable, emerging issues\nsuch as false positives and missing elements. We observe that decomposing\ncomplex extraction tasks and extracting them step by step can effectively\nimprove LLMs' performance, and the extraction orders of entities significantly\naffect the final results of LLMs. This paper proposes a two-stage multi-step\nmethod for LLM-based information extraction and adopts the RL framework to\nexecute the multi-step planning. We regard sequential extraction as a Markov\ndecision process, build an LLM-based extraction environment, design a decision\nmodule to adaptively provide the optimal order for sequential entity extraction\non different sentences, and utilize the DDQN algorithm to train the decision\nmodel. We also design the rewards and evaluation metrics suitable for the\nextraction results of LLMs. We conduct extensive experiments on multiple public\ndatasets to demonstrate the effectiveness of our method in improving the\ninformation extraction capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing research on large language models (LLMs) shows that they can solve\ninformation extraction tasks through multi-step planning. However, their\nextraction behavior on complex sentences and tasks is unstable, emerging issues\nsuch as false positives and missing elements. We observe that decomposing\ncomplex extraction tasks and extracting them step by step can effectively\nimprove LLMs' performance, and the extraction orders of entities significantly\naffect the final results of LLMs. This paper proposes a two-stage multi-step\nmethod for LLM-based information extraction and adopts the RL framework to\nexecute the multi-step planning. We regard sequential extraction as a Markov\ndecision process, build an LLM-based extraction environment, design a decision\nmodule to adaptively provide the optimal order for sequential entity extraction\non different sentences, and utilize the DDQN algorithm to train the decision\nmodel. We also design the rewards and evaluation metrics suitable for the\nextraction results of LLMs. We conduct extensive experiments on multiple public\ndatasets to demonstrate the effectiveness of our method in improving the\ninformation extraction capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Zepeng Ding"
                    },
                    {
                        "name": "Ruiyang Ke"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Guochao Jiang"
                    },
                    {
                        "name": "Yanda Li"
                    },
                    {
                        "name": "Deqing Yang"
                    },
                    {
                        "name": "Jiaqing Liang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqing Liang"
                },
                "author": "Jiaqing Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11455v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11455v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15710v2",
                "updated": "2024-08-29T14:47:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    47,
                    37,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-28T11:18:06Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    11,
                    18,
                    6,
                    2,
                    241,
                    0
                ],
                "title": "Conan-embedding: General Text Embedding with More and Better Negative\n  Samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conan-embedding: General Text Embedding with More and Better Negative\n  Samples"
                },
                "summary": "With the growing popularity of RAG, the capabilities of embedding models are\ngaining increasing attention. Embedding models are primarily trained through\ncontrastive loss learning, with negative examples being a key component.\nPrevious work has proposed various hard negative mining strategies, but these\nstrategies are typically employed as preprocessing steps. In this paper, we\npropose the conan-embedding model, which maximizes the utilization of more and\nhigher-quality negative examples. Specifically, since the model's ability to\nhandle preprocessed negative examples evolves during training, we propose\ndynamic hard negative mining method to expose the model to more challenging\nnegative examples throughout the training process. Secondly, contrastive\nlearning requires as many negative examples as possible but is limited by GPU\nmemory constraints. Therefore, we use a Cross-GPU balancing Loss to provide\nmore negative examples for embedding training and balance the batch size across\nmultiple tasks. Moreover, we also discovered that the prompt-response pairs\nfrom LLMs can be used for embedding training. Our approach effectively enhances\nthe capabilities of embedding models, currently ranking first on the Chinese\nleaderboard of Massive text embedding benchmark",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing popularity of RAG, the capabilities of embedding models are\ngaining increasing attention. Embedding models are primarily trained through\ncontrastive loss learning, with negative examples being a key component.\nPrevious work has proposed various hard negative mining strategies, but these\nstrategies are typically employed as preprocessing steps. In this paper, we\npropose the conan-embedding model, which maximizes the utilization of more and\nhigher-quality negative examples. Specifically, since the model's ability to\nhandle preprocessed negative examples evolves during training, we propose\ndynamic hard negative mining method to expose the model to more challenging\nnegative examples throughout the training process. Secondly, contrastive\nlearning requires as many negative examples as possible but is limited by GPU\nmemory constraints. Therefore, we use a Cross-GPU balancing Loss to provide\nmore negative examples for embedding training and balance the batch size across\nmultiple tasks. Moreover, we also discovered that the prompt-response pairs\nfrom LLMs can be used for embedding training. Our approach effectively enhances\nthe capabilities of embedding models, currently ranking first on the Chinese\nleaderboard of Massive text embedding benchmark"
                },
                "authors": [
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Tang"
                    },
                    {
                        "name": "Shizhe Chen"
                    },
                    {
                        "name": "Xi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xi Chen"
                },
                "author": "Xi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16567v1",
                "updated": "2024-08-29T14:35:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    35,
                    14,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T14:35:14Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    35,
                    14,
                    3,
                    242,
                    0
                ],
                "title": "Identifying Terrain Physical Parameters from Vision -- Towards\n  Physical-Parameter-Aware Locomotion and Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Terrain Physical Parameters from Vision -- Towards\n  Physical-Parameter-Aware Locomotion and Navigation"
                },
                "summary": "Identifying the physical properties of the surrounding environment is\nessential for robotic locomotion and navigation to deal with non-geometric\nhazards, such as slippery and deformable terrains. It would be of great benefit\nfor robots to anticipate these extreme physical properties before contact;\nhowever, estimating environmental physical parameters from vision is still an\nopen challenge. Animals can achieve this by using their prior experience and\nknowledge of what they have seen and how it felt. In this work, we propose a\ncross-modal self-supervised learning framework for vision-based environmental\nphysical parameter estimation, which paves the way for future\nphysical-property-aware locomotion and navigation. We bridge the gap between\nexisting policies trained in simulation and identification of physical terrain\nparameters from vision. We propose to train a physical decoder in simulation to\npredict friction and stiffness from multi-modal input. The trained network\nallows the labeling of real-world images with physical parameters in a\nself-supervised manner to further train a visual network during deployment,\nwhich can densely predict the friction and stiffness from image data. We\nvalidate our physical decoder in simulation and the real world using a\nquadruped ANYmal robot, outperforming an existing baseline method. We show that\nour visual network can predict the physical properties in indoor and outdoor\nexperiments while allowing fast adaptation to new environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying the physical properties of the surrounding environment is\nessential for robotic locomotion and navigation to deal with non-geometric\nhazards, such as slippery and deformable terrains. It would be of great benefit\nfor robots to anticipate these extreme physical properties before contact;\nhowever, estimating environmental physical parameters from vision is still an\nopen challenge. Animals can achieve this by using their prior experience and\nknowledge of what they have seen and how it felt. In this work, we propose a\ncross-modal self-supervised learning framework for vision-based environmental\nphysical parameter estimation, which paves the way for future\nphysical-property-aware locomotion and navigation. We bridge the gap between\nexisting policies trained in simulation and identification of physical terrain\nparameters from vision. We propose to train a physical decoder in simulation to\npredict friction and stiffness from multi-modal input. The trained network\nallows the labeling of real-world images with physical parameters in a\nself-supervised manner to further train a visual network during deployment,\nwhich can densely predict the friction and stiffness from image data. We\nvalidate our physical decoder in simulation and the real world using a\nquadruped ANYmal robot, outperforming an existing baseline method. We show that\nour visual network can predict the physical properties in indoor and outdoor\nexperiments while allowing fast adaptation to new environments."
                },
                "authors": [
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Jonas Frey"
                    },
                    {
                        "name": "Ruyi Zhou"
                    },
                    {
                        "name": "Takahiro Miki"
                    },
                    {
                        "name": "Georg Martius"
                    },
                    {
                        "name": "Marco Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Marco Hutter"
                },
                "author": "Marco Hutter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01805v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01805v4",
                "updated": "2024-08-29T14:05:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    5,
                    44,
                    3,
                    242,
                    0
                ],
                "published": "2024-02-02T09:45:33Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    9,
                    45,
                    33,
                    4,
                    33,
                    0
                ],
                "title": "Can LLMs perform structured graph reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs perform structured graph reasoning?"
                },
                "summary": "Pretrained Large Language Models (LLMs) have demonstrated various reasoning\ncapabilities through language-based prompts alone, particularly in unstructured\ntask settings (tasks purely based on language semantics). However, LLMs often\nstruggle with structured tasks, because of the inherent incompatibility of\ninput representation. Reducing structured tasks to uni-dimensional language\nsemantics often renders the problem trivial. Keeping the trade-off between LLM\ncompatibility and structure complexity in mind, we design various graph\nreasoning tasks as a proxy to semi-structured tasks in this paper, in order to\ntest the ability to navigate through representations beyond plain text in\nvarious LLMs. Particularly, we design 10 distinct problems of graph traversal,\neach representing increasing levels of complexity, and benchmark 5 different\ninstruct-finetuned LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) on the\naforementioned tasks. Further, we analyse the performance of models across\nvarious settings such as varying sizes of graphs as well as different forms of\nk-shot prompting. We highlight various limitations, biases and properties of\nLLMs through this benchmarking process, such as an inverse relation to the\naverage degrees of freedom of traversal per node in graphs, the overall\nnegative impact of k-shot prompting on graph reasoning tasks, and a positive\nresponse bias which prevents LLMs from identifying the absence of a valid\nsolution. Finally, we introduce a new prompting technique specially designed\nfor graph traversal tasks (PathCompare), which demonstrates a notable increase\nin the performance of LLMs in comparison to standard prompting techniques such\nas Chain-of-Thought (CoT).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained Large Language Models (LLMs) have demonstrated various reasoning\ncapabilities through language-based prompts alone, particularly in unstructured\ntask settings (tasks purely based on language semantics). However, LLMs often\nstruggle with structured tasks, because of the inherent incompatibility of\ninput representation. Reducing structured tasks to uni-dimensional language\nsemantics often renders the problem trivial. Keeping the trade-off between LLM\ncompatibility and structure complexity in mind, we design various graph\nreasoning tasks as a proxy to semi-structured tasks in this paper, in order to\ntest the ability to navigate through representations beyond plain text in\nvarious LLMs. Particularly, we design 10 distinct problems of graph traversal,\neach representing increasing levels of complexity, and benchmark 5 different\ninstruct-finetuned LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) on the\naforementioned tasks. Further, we analyse the performance of models across\nvarious settings such as varying sizes of graphs as well as different forms of\nk-shot prompting. We highlight various limitations, biases and properties of\nLLMs through this benchmarking process, such as an inverse relation to the\naverage degrees of freedom of traversal per node in graphs, the overall\nnegative impact of k-shot prompting on graph reasoning tasks, and a positive\nresponse bias which prevents LLMs from identifying the absence of a valid\nsolution. Finally, we introduce a new prompting technique specially designed\nfor graph traversal tasks (PathCompare), which demonstrates a notable increase\nin the performance of LLMs in comparison to standard prompting techniques such\nas Chain-of-Thought (CoT)."
                },
                "authors": [
                    {
                        "name": "Palaash Agrawal"
                    },
                    {
                        "name": "Shavak Vasania"
                    },
                    {
                        "name": "Cheston Tan"
                    }
                ],
                "author_detail": {
                    "name": "Cheston Tan"
                },
                "author": "Cheston Tan",
                "arxiv_comment": "International Conference on Pattern Recognition (ICPR), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01805v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01805v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16542v1",
                "updated": "2024-08-29T14:00:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    0,
                    57,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T14:00:57Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    0,
                    57,
                    3,
                    242,
                    0
                ],
                "title": "SALSA: Speedy ASR-LLM Synchronous Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALSA: Speedy ASR-LLM Synchronous Aggregation"
                },
                "summary": "Harnessing pre-trained LLMs to improve ASR systems, particularly for\nlow-resource languages, is now an emerging area of research. Existing methods\nrange from using LLMs for ASR error correction to tightly coupled systems that\nreplace the ASR decoder with the LLM. These approaches either increase decoding\ntime or require expensive training of the cross-attention layers. We propose\nSALSA, which couples the decoder layers of the ASR to the LLM decoder, while\nsynchronously advancing both decoders. Such coupling is performed with a simple\nprojection of the last decoder state, and is thus significantly more training\nefficient than earlier approaches. A challenge of our proposed coupling is\nhandling the mismatch between the tokenizers of the LLM and ASR systems. We\nhandle this mismatch using cascading tokenization with respect to the LLM and\nASR vocabularies. We evaluate SALSA on 8 low-resource languages in the FLEURS\nbenchmark, yielding substantial WER reductions of up to 38%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing pre-trained LLMs to improve ASR systems, particularly for\nlow-resource languages, is now an emerging area of research. Existing methods\nrange from using LLMs for ASR error correction to tightly coupled systems that\nreplace the ASR decoder with the LLM. These approaches either increase decoding\ntime or require expensive training of the cross-attention layers. We propose\nSALSA, which couples the decoder layers of the ASR to the LLM decoder, while\nsynchronously advancing both decoders. Such coupling is performed with a simple\nprojection of the last decoder state, and is thus significantly more training\nefficient than earlier approaches. A challenge of our proposed coupling is\nhandling the mismatch between the tokenizers of the LLM and ASR systems. We\nhandle this mismatch using cascading tokenization with respect to the LLM and\nASR vocabularies. We evaluate SALSA on 8 low-resource languages in the FLEURS\nbenchmark, yielding substantial WER reductions of up to 38%."
                },
                "authors": [
                    {
                        "name": "Ashish Mittal"
                    },
                    {
                        "name": "Darshan Prabhu"
                    },
                    {
                        "name": "Sunita Sarawagi"
                    },
                    {
                        "name": "Preethi Jyothi"
                    }
                ],
                "author_detail": {
                    "name": "Preethi Jyothi"
                },
                "author": "Preethi Jyothi",
                "arxiv_comment": "Accepted to INTERSPEECH 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11160v2",
                "updated": "2024-08-29T13:23:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    23,
                    15,
                    3,
                    242,
                    0
                ],
                "published": "2024-04-17T08:16:48Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    8,
                    16,
                    48,
                    2,
                    108,
                    0
                ],
                "title": "Low-Cost Language Models: Survey and Performance Evaluation on Python\n  Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Cost Language Models: Survey and Performance Evaluation on Python\n  Code Generation"
                },
                "summary": "Large Language Models (LLMs) have become a popular choice for many Natural\nLanguage Processing (NLP) tasks due to their versatility and ability to produce\nhigh-quality results. Specifically, they are increasingly used for automatic\ncode generation to help developers tackle repetitive coding tasks. However,\nLLMs' substantial computational and memory requirements often make them\ninaccessible to users with limited resources. This paper focuses on very\nlow-cost models which offer a more accessible alternative to resource-intensive\nLLMs. We notably: (1) propose a thorough semi-manual evaluation of their\nperformance in generating Python code, (2) introduce a Chain-of-Thought (CoT)\nprompting strategy to improve model reasoning and code quality, and (3) propose\na new dataset of 60 programming problems, with varied difficulty levels,\ndesigned to extend existing benchmarks like HumanEval and EvalPlus. Our\nfindings show that some low-cost compatible models achieve competitive results\ncompared to larger models like ChatGPT despite using significantly fewer\nresources. We will make our dataset and prompts publicly available to support\nfurther research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become a popular choice for many Natural\nLanguage Processing (NLP) tasks due to their versatility and ability to produce\nhigh-quality results. Specifically, they are increasingly used for automatic\ncode generation to help developers tackle repetitive coding tasks. However,\nLLMs' substantial computational and memory requirements often make them\ninaccessible to users with limited resources. This paper focuses on very\nlow-cost models which offer a more accessible alternative to resource-intensive\nLLMs. We notably: (1) propose a thorough semi-manual evaluation of their\nperformance in generating Python code, (2) introduce a Chain-of-Thought (CoT)\nprompting strategy to improve model reasoning and code quality, and (3) propose\na new dataset of 60 programming problems, with varied difficulty levels,\ndesigned to extend existing benchmarks like HumanEval and EvalPlus. Our\nfindings show that some low-cost compatible models achieve competitive results\ncompared to larger models like ChatGPT despite using significantly fewer\nresources. We will make our dataset and prompts publicly available to support\nfurther research."
                },
                "authors": [
                    {
                        "name": "Jessica López Espejel"
                    },
                    {
                        "name": "Mahaman Sanoussi Yahaya Alassan"
                    },
                    {
                        "name": "Merieme Bouhandi"
                    },
                    {
                        "name": "Walid Dahhane"
                    },
                    {
                        "name": "El Hassane Ettifouri"
                    }
                ],
                "author_detail": {
                    "name": "El Hassane Ettifouri"
                },
                "author": "El Hassane Ettifouri",
                "arxiv_comment": "Under review at Elsevier's Engineering Applications of Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16502v1",
                "updated": "2024-08-29T13:01:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    1,
                    42,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T13:01:42Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    13,
                    1,
                    42,
                    3,
                    242,
                    0
                ],
                "title": "LLMs vs Established Text Augmentation Techniques for Classification:\n  When do the Benefits Outweight the Costs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs vs Established Text Augmentation Techniques for Classification:\n  When do the Benefits Outweight the Costs?"
                },
                "summary": "The generative large language models (LLMs) are increasingly being used for\ndata augmentation tasks, where text samples are LLM-paraphrased and then used\nfor classifier fine-tuning. However, a research that would confirm a clear\ncost-benefit advantage of LLMs over more established augmentation methods is\nlargely missing. To study if (and when) is the LLM-based augmentation\nadvantageous, we compared the effects of recent LLM augmentation methods with\nestablished ones on 6 datasets, 3 classifiers and 2 fine-tuning methods. We\nalso varied the number of seeds and collected samples to better explore the\ndownstream model accuracy space. Finally, we performed a cost-benefit analysis\nand show that LLM-based methods are worthy of deployment only when very small\nnumber of seeds is used. Moreover, in many cases, established methods lead to\nsimilar or better model accuracies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generative large language models (LLMs) are increasingly being used for\ndata augmentation tasks, where text samples are LLM-paraphrased and then used\nfor classifier fine-tuning. However, a research that would confirm a clear\ncost-benefit advantage of LLMs over more established augmentation methods is\nlargely missing. To study if (and when) is the LLM-based augmentation\nadvantageous, we compared the effects of recent LLM augmentation methods with\nestablished ones on 6 datasets, 3 classifiers and 2 fine-tuning methods. We\nalso varied the number of seeds and collected samples to better explore the\ndownstream model accuracy space. Finally, we performed a cost-benefit analysis\nand show that LLM-based methods are worthy of deployment only when very small\nnumber of seeds is used. Moreover, in many cases, established methods lead to\nsimilar or better model accuracies."
                },
                "authors": [
                    {
                        "name": "Jan Cegin"
                    },
                    {
                        "name": "Jakub Simko"
                    },
                    {
                        "name": "Peter Brusilovsky"
                    }
                ],
                "author_detail": {
                    "name": "Peter Brusilovsky"
                },
                "author": "Peter Brusilovsky",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16498v1",
                "updated": "2024-08-29T12:56:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    56,
                    6,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T12:56:06Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    56,
                    6,
                    3,
                    242,
                    0
                ],
                "title": "A Survey on Evaluating Large Language Models in Code Generation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Evaluating Large Language Models in Code Generation Tasks"
                },
                "summary": "This paper provides a comprehensive review of the current methods and metrics\nused to evaluate the performance of Large Language Models (LLMs) in code\ngeneration tasks. With the rapid growth in demand for automated software\ndevelopment, LLMs have demonstrated significant potential in the field of code\ngeneration. The paper begins by reviewing the historical development of LLMs\nand their applications in code generation. Next, it details various methods and\nmetrics for assessing the code generation capabilities of LLMs, including code\ncorrectness, efficiency, readability, and evaluation methods based on expert\nreview and user experience. The paper also evaluates the widely used benchmark\ndatasets, identifying their limitations and proposing directions for future\nimprovements. Specifically, the paper analyzes the performance of code\ngeneration models across different tasks by combining multiple evaluation\nmetrics, such as code compilation/interpretation success rates, unit test pass\nrates, and performance and efficiency metrics, to comprehensively assess the\npractical application of LLMs in code generation. Finally, the paper discusses\nthe challenges faced in evaluating LLMs in code generation, particularly how to\nensure the comprehensiveness and accuracy of evaluation methods and how to\nadapt to the evolving practices of software development. These analyses and\ndiscussions provide valuable insights for further optimizing and improving the\napplication of LLMs in code generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a comprehensive review of the current methods and metrics\nused to evaluate the performance of Large Language Models (LLMs) in code\ngeneration tasks. With the rapid growth in demand for automated software\ndevelopment, LLMs have demonstrated significant potential in the field of code\ngeneration. The paper begins by reviewing the historical development of LLMs\nand their applications in code generation. Next, it details various methods and\nmetrics for assessing the code generation capabilities of LLMs, including code\ncorrectness, efficiency, readability, and evaluation methods based on expert\nreview and user experience. The paper also evaluates the widely used benchmark\ndatasets, identifying their limitations and proposing directions for future\nimprovements. Specifically, the paper analyzes the performance of code\ngeneration models across different tasks by combining multiple evaluation\nmetrics, such as code compilation/interpretation success rates, unit test pass\nrates, and performance and efficiency metrics, to comprehensively assess the\npractical application of LLMs in code generation. Finally, the paper discusses\nthe challenges faced in evaluating LLMs in code generation, particularly how to\nensure the comprehensiveness and accuracy of evaluation methods and how to\nadapt to the evolving practices of software development. These analyses and\ndiscussions provide valuable insights for further optimizing and improving the\napplication of LLMs in code generation tasks."
                },
                "authors": [
                    {
                        "name": "Liguo Chen"
                    },
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Hongrui Jia"
                    },
                    {
                        "name": "Zhengran Zeng"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yijiang Xu"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Qing Gao"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Shikun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shikun Zhang"
                },
                "author": "Shikun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11512v2",
                "updated": "2024-08-29T12:25:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    25,
                    14,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-21T10:44:10Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    44,
                    10,
                    2,
                    234,
                    0
                ],
                "title": "IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine\n  Translation"
                },
                "summary": "This paper introduces two multilingual systems, IKUN and IKUN-C, developed\nfor the general machine translation task in WMT24. IKUN and IKUN-C represent an\nopen system and a constrained system, respectively, built on Llama-3-8b and\nMistral-7B-v0.3. Both systems are designed to handle all 11 language directions\nusing a single model. According to automatic evaluation metrics, IKUN-C\nachieved 6 first-place and 3 second-place finishes among all constrained\nsystems, while IKUN secured 1 first-place and 2 second-place finishes across\nboth open and constrained systems. These encouraging results suggest that large\nlanguage models (LLMs) are nearing the level of proficiency required for\neffective multilingual machine translation. The systems are based on a\ntwo-stage approach: first, continuous pre-training on monolingual data in 10\nlanguages, followed by fine-tuning on high-quality parallel data for 11\nlanguage directions. The primary difference between IKUN and IKUN-C lies in\ntheir monolingual pre-training strategy. IKUN-C is pre-trained using\nconstrained monolingual data, whereas IKUN leverages monolingual data from the\nOSCAR dataset. In the second phase, both systems are fine-tuned on parallel\ndata sourced from NTREX, Flores, and WMT16-23 for all 11 language pairs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces two multilingual systems, IKUN and IKUN-C, developed\nfor the general machine translation task in WMT24. IKUN and IKUN-C represent an\nopen system and a constrained system, respectively, built on Llama-3-8b and\nMistral-7B-v0.3. Both systems are designed to handle all 11 language directions\nusing a single model. According to automatic evaluation metrics, IKUN-C\nachieved 6 first-place and 3 second-place finishes among all constrained\nsystems, while IKUN secured 1 first-place and 2 second-place finishes across\nboth open and constrained systems. These encouraging results suggest that large\nlanguage models (LLMs) are nearing the level of proficiency required for\neffective multilingual machine translation. The systems are based on a\ntwo-stage approach: first, continuous pre-training on monolingual data in 10\nlanguages, followed by fine-tuning on high-quality parallel data for 11\nlanguage directions. The primary difference between IKUN and IKUN-C lies in\ntheir monolingual pre-training strategy. IKUN-C is pre-trained using\nconstrained monolingual data, whereas IKUN leverages monolingual data from the\nOSCAR dataset. In the second phase, both systems are fine-tuned on parallel\ndata sourced from NTREX, Flores, and WMT16-23 for all 11 language pairs."
                },
                "authors": [
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Shahram Khadivi"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "arxiv_comment": "typo: 120K -> 12K vocabulary size",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16482v1",
                "updated": "2024-08-29T12:18:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    18,
                    4,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T12:18:04Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    18,
                    4,
                    3,
                    242,
                    0
                ],
                "title": "Self-Alignment: Improving Alignment of Cultural Values in LLMs via\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Alignment: Improving Alignment of Cultural Values in LLMs via\n  In-Context Learning"
                },
                "summary": "Improving the alignment of Large Language Models (LLMs) with respect to the\ncultural values that they encode has become an increasingly important topic. In\nthis work, we study whether we can exploit existing knowledge about cultural\nvalues at inference time to adjust model responses to cultural value probes. We\npresent a simple and inexpensive method that uses a combination of in-context\nlearning (ICL) and human survey data, and show that we can improve the\nalignment to cultural values across 5 models that include both English-centric\nand multilingual LLMs. Importantly, we show that our method could prove useful\nin test languages other than English and can improve alignment to the cultural\nvalues that correspond to a range of culturally diverse countries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the alignment of Large Language Models (LLMs) with respect to the\ncultural values that they encode has become an increasingly important topic. In\nthis work, we study whether we can exploit existing knowledge about cultural\nvalues at inference time to adjust model responses to cultural value probes. We\npresent a simple and inexpensive method that uses a combination of in-context\nlearning (ICL) and human survey data, and show that we can improve the\nalignment to cultural values across 5 models that include both English-centric\nand multilingual LLMs. Importantly, we show that our method could prove useful\nin test languages other than English and can improve alignment to the cultural\nvalues that correspond to a range of culturally diverse countries."
                },
                "authors": [
                    {
                        "name": "Rochelle Choenni"
                    },
                    {
                        "name": "Ekaterina Shutova"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Shutova"
                },
                "author": "Ekaterina Shutova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17915v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17915v3",
                "updated": "2024-08-29T11:58:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    58,
                    46,
                    3,
                    242,
                    0
                ],
                "published": "2024-07-25T10:09:21Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    10,
                    9,
                    21,
                    3,
                    207,
                    0
                ],
                "title": "The Dark Side of Function Calling: Pathways to Jailbreaking Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Side of Function Calling: Pathways to Jailbreaking Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir power comes with significant security considerations. While extensive\nresearch has been conducted on the safety of LLMs in chat mode, the security\nimplications of their function calling feature have been largely overlooked.\nThis paper uncovers a critical vulnerability in the function calling process of\nLLMs, introducing a novel \"jailbreak function\" attack method that exploits\nalignment discrepancies, user coercion, and the absence of rigorous safety\nfilters. Our empirical study, conducted on six state-of-the-art LLMs including\nGPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average\nsuccess rate of over 90\\% for this attack. We provide a comprehensive analysis\nof why function calls are susceptible to such attacks and propose defensive\nstrategies, including the use of defensive prompts. Our findings highlight the\nurgent need for enhanced security measures in the function calling capabilities\nof LLMs, contributing to the field of AI safety by identifying a previously\nunexplored risk, designing an effective attack method, and suggesting practical\ndefensive measures. Our code is available at\nhttps://github.com/wooozihui/jailbreakfunction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir power comes with significant security considerations. While extensive\nresearch has been conducted on the safety of LLMs in chat mode, the security\nimplications of their function calling feature have been largely overlooked.\nThis paper uncovers a critical vulnerability in the function calling process of\nLLMs, introducing a novel \"jailbreak function\" attack method that exploits\nalignment discrepancies, user coercion, and the absence of rigorous safety\nfilters. Our empirical study, conducted on six state-of-the-art LLMs including\nGPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average\nsuccess rate of over 90\\% for this attack. We provide a comprehensive analysis\nof why function calls are susceptible to such attacks and propose defensive\nstrategies, including the use of defensive prompts. Our findings highlight the\nurgent need for enhanced security measures in the function calling capabilities\nof LLMs, contributing to the field of AI safety by identifying a previously\nunexplored risk, designing an effective attack method, and suggesting practical\ndefensive measures. Our code is available at\nhttps://github.com/wooozihui/jailbreakfunction."
                },
                "authors": [
                    {
                        "name": "Zihui Wu"
                    },
                    {
                        "name": "Haichang Gao"
                    },
                    {
                        "name": "Jianping He"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17915v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17915v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16465v1",
                "updated": "2024-08-29T11:54:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    54,
                    2,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T11:54:02Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    54,
                    2,
                    3,
                    242,
                    0
                ],
                "title": "Human and LLM-Based Voice Assistant Interaction: An Analytical Framework\n  for User Verbal and Nonverbal Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human and LLM-Based Voice Assistant Interaction: An Analytical Framework\n  for User Verbal and Nonverbal Behaviors"
                },
                "summary": "Recent progress in large language model (LLM) technology has significantly\nenhanced the interaction experience between humans and voice assistants (VAs).\nThis project aims to explore a user's continuous interaction with LLM-based VA\n(LLM-VA) during a complex task. We recruited 12 participants to interact with\nan LLM-VA during a cooking task, selected for its complexity and the\nrequirement for continuous interaction. We observed that users show both verbal\nand nonverbal behaviors, though they know that the LLM-VA can not capture those\nnonverbal signals. Despite the prevalence of nonverbal behavior in human-human\ncommunication, there is no established analytical methodology or framework for\nexploring it in human-VA interactions. After analyzing 3 hours and 39 minutes\nof video recordings, we developed an analytical framework with three\ndimensions: 1) behavior characteristics, including both verbal and nonverbal\nbehaviors, 2) interaction stages--exploration, conflict, and integration--that\nillustrate the progression of user interactions, and 3) stage transition\nthroughout the task. This analytical framework identifies key verbal and\nnonverbal behaviors that provide a foundation for future research and practical\napplications in optimizing human and LLM-VA interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language model (LLM) technology has significantly\nenhanced the interaction experience between humans and voice assistants (VAs).\nThis project aims to explore a user's continuous interaction with LLM-based VA\n(LLM-VA) during a complex task. We recruited 12 participants to interact with\nan LLM-VA during a cooking task, selected for its complexity and the\nrequirement for continuous interaction. We observed that users show both verbal\nand nonverbal behaviors, though they know that the LLM-VA can not capture those\nnonverbal signals. Despite the prevalence of nonverbal behavior in human-human\ncommunication, there is no established analytical methodology or framework for\nexploring it in human-VA interactions. After analyzing 3 hours and 39 minutes\nof video recordings, we developed an analytical framework with three\ndimensions: 1) behavior characteristics, including both verbal and nonverbal\nbehaviors, 2) interaction stages--exploration, conflict, and integration--that\nillustrate the progression of user interactions, and 3) stage transition\nthroughout the task. This analytical framework identifies key verbal and\nnonverbal behaviors that provide a foundation for future research and practical\napplications in optimizing human and LLM-VA interactions."
                },
                "authors": [
                    {
                        "name": "Szeyi Chan"
                    },
                    {
                        "name": "Shihan Fu"
                    },
                    {
                        "name": "Jiachen Li"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Smit Desai"
                    },
                    {
                        "name": "Mirjana Prpa"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16440v1",
                "updated": "2024-08-29T11:05:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    5,
                    54,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T11:05:54Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    5,
                    54,
                    3,
                    242,
                    0
                ],
                "title": "Instruction-tuned Large Language Models for Machine Translation in the\n  Medical Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuned Large Language Models for Machine Translation in the\n  Medical Domain"
                },
                "summary": "Large Language Models (LLMs) have shown promising results on machine\ntranslation for high resource language pairs and domains. However, in\nspecialised domains (e.g. medical) LLMs have shown lower performance compared\nto standard neural machine translation models. The consistency in the machine\ntranslation of terminology is crucial for users, researchers, and translators\nin specialised domains. In this study, we compare the performance between\nbaseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we\nintroduce terminology from specialised medical dictionaries into the\ninstruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs\nsignificantly outperform the baseline models with automatic metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promising results on machine\ntranslation for high resource language pairs and domains. However, in\nspecialised domains (e.g. medical) LLMs have shown lower performance compared\nto standard neural machine translation models. The consistency in the machine\ntranslation of terminology is crucial for users, researchers, and translators\nin specialised domains. In this study, we compare the performance between\nbaseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we\nintroduce terminology from specialised medical dictionaries into the\ninstruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs\nsignificantly outperform the baseline models with automatic metrics."
                },
                "authors": [
                    {
                        "name": "Miguel Rios"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Rios"
                },
                "author": "Miguel Rios",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16423v1",
                "updated": "2024-08-29T10:31:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    31,
                    52,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T10:31:52Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    31,
                    52,
                    3,
                    242,
                    0
                ],
                "title": "WHISMA: A Speech-LLM to Perform Zero-shot Spoken Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WHISMA: A Speech-LLM to Perform Zero-shot Spoken Language Understanding"
                },
                "summary": "Speech large language models (speech-LLMs) integrate speech and text-based\nfoundation models to provide a unified framework for handling a wide range of\ndownstream tasks. In this paper, we introduce WHISMA, a speech-LLM tailored for\nspoken language understanding (SLU) that demonstrates robust performance in\nvarious zero-shot settings. WHISMA combines the speech encoder from Whisper\nwith the Llama-3 LLM, and is fine-tuned in a parameter-efficient manner on a\ncomprehensive collection of SLU-related datasets. Our experiments show that\nWHISMA significantly improves the zero-shot slot filling performance on the\nSLURP benchmark, achieving a relative gain of 26.6% compared to the current\nstate-of-the-art model. Furthermore, to evaluate WHISMA's generalisation\ncapabilities to unseen domains, we develop a new task-agnostic benchmark named\nSLU-GLUE. The evaluation results indicate that WHISMA outperforms an existing\nspeech-LLM (Qwen-Audio) with a relative gain of 33.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech large language models (speech-LLMs) integrate speech and text-based\nfoundation models to provide a unified framework for handling a wide range of\ndownstream tasks. In this paper, we introduce WHISMA, a speech-LLM tailored for\nspoken language understanding (SLU) that demonstrates robust performance in\nvarious zero-shot settings. WHISMA combines the speech encoder from Whisper\nwith the Llama-3 LLM, and is fine-tuned in a parameter-efficient manner on a\ncomprehensive collection of SLU-related datasets. Our experiments show that\nWHISMA significantly improves the zero-shot slot filling performance on the\nSLURP benchmark, achieving a relative gain of 26.6% compared to the current\nstate-of-the-art model. Furthermore, to evaluate WHISMA's generalisation\ncapabilities to unseen domains, we develop a new task-agnostic benchmark named\nSLU-GLUE. The evaluation results indicate that WHISMA outperforms an existing\nspeech-LLM (Qwen-Audio) with a relative gain of 33.0%."
                },
                "authors": [
                    {
                        "name": "Mohan Li"
                    },
                    {
                        "name": "Cong-Thanh Do"
                    },
                    {
                        "name": "Simon Keizer"
                    },
                    {
                        "name": "Youmna Farag"
                    },
                    {
                        "name": "Svetlana Stoyanchev"
                    },
                    {
                        "name": "Rama Doddipatla"
                    }
                ],
                "author_detail": {
                    "name": "Rama Doddipatla"
                },
                "author": "Rama Doddipatla",
                "arxiv_comment": "accepted to SLT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16415v1",
                "updated": "2024-08-29T10:21:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    21,
                    2,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T10:21:02Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    21,
                    2,
                    3,
                    242,
                    0
                ],
                "title": "UAV's Rotor Micro-Doppler Feature Extraction Using Integrated Sensing\n  and Communication Signal: Algorithm Design and Testbed Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV's Rotor Micro-Doppler Feature Extraction Using Integrated Sensing\n  and Communication Signal: Algorithm Design and Testbed Evaluation"
                },
                "summary": "With the rapid application of unmanned aerial vehicles (UAVs) in urban areas,\nthe identification and tracking of hovering UAVs have become critical\nchallenges, significantly impacting the safety of aircraft take-off and landing\noperations. As a promising technology for 6G mobile systems, integrated sensing\nand communication (ISAC) can be used to detect high-mobility UAVs with a low\ndeployment cost. The micro-Doppler signals from UAV rotors can be leveraged to\naddress the detection of low-mobility and hovering UAVs using ISAC signals.\nHowever, determining whether the frame structure of the ISAC system can be used\nto identify UAVs, and how to accurately capture the weak rotor micro-Doppler\nsignals of UAVs in complex environments, remain two challenging problems. This\npaper first proposes a novel frame structure for UAV micro-Doppler extraction\nand the representation of UAV micro-Doppler signals within the channel state\ninformation (CSI). Furthermore, to address complex environments and the\ninterference caused by UAV body vibrations, the rotor micro-Doppler null space\npursuit (rmD-NSP) algorithm and the feature extraction algorithm\nsynchroextracting transform (SET) are designed to effectively separate UAV's\nrotor micro-Doppler signals and enhance their features in the spectrogram.\nFinally, both simulation and hardware testbed demonstrate that the proposed\nrmD-NSP algorithm enables the ISAC base station (BS) to accurately and\ncompletely extract UAV's rotor micro-Doppler signals. Within a 0.1s observation\nperiod, ISAC BS successfully captures eight rotations of the DJI M300 RTK UAV's\nrotor in urban environments. Compared to the existing AM-FM NSP and NSP signal\ndecomposition algorithms, the integrity of the rotor micro-Doppler features is\nimproved by 60%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid application of unmanned aerial vehicles (UAVs) in urban areas,\nthe identification and tracking of hovering UAVs have become critical\nchallenges, significantly impacting the safety of aircraft take-off and landing\noperations. As a promising technology for 6G mobile systems, integrated sensing\nand communication (ISAC) can be used to detect high-mobility UAVs with a low\ndeployment cost. The micro-Doppler signals from UAV rotors can be leveraged to\naddress the detection of low-mobility and hovering UAVs using ISAC signals.\nHowever, determining whether the frame structure of the ISAC system can be used\nto identify UAVs, and how to accurately capture the weak rotor micro-Doppler\nsignals of UAVs in complex environments, remain two challenging problems. This\npaper first proposes a novel frame structure for UAV micro-Doppler extraction\nand the representation of UAV micro-Doppler signals within the channel state\ninformation (CSI). Furthermore, to address complex environments and the\ninterference caused by UAV body vibrations, the rotor micro-Doppler null space\npursuit (rmD-NSP) algorithm and the feature extraction algorithm\nsynchroextracting transform (SET) are designed to effectively separate UAV's\nrotor micro-Doppler signals and enhance their features in the spectrogram.\nFinally, both simulation and hardware testbed demonstrate that the proposed\nrmD-NSP algorithm enables the ISAC base station (BS) to accurately and\ncompletely extract UAV's rotor micro-Doppler signals. Within a 0.1s observation\nperiod, ISAC BS successfully captures eight rotations of the DJI M300 RTK UAV's\nrotor in urban environments. Compared to the existing AM-FM NSP and NSP signal\ndecomposition algorithms, the integrity of the rotor micro-Doppler features is\nimproved by 60%."
                },
                "authors": [
                    {
                        "name": "Jiachen Wei"
                    },
                    {
                        "name": "Dingyou Ma"
                    },
                    {
                        "name": "Feiyang He"
                    },
                    {
                        "name": "Qixun Zhang"
                    },
                    {
                        "name": "Zhiyong Feng"
                    },
                    {
                        "name": "Zhengfeng Liu"
                    },
                    {
                        "name": "Taohong Liang"
                    }
                ],
                "author_detail": {
                    "name": "Taohong Liang"
                },
                "author": "Taohong Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11288v2",
                "updated": "2024-08-29T10:10:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    10,
                    55,
                    3,
                    242,
                    0
                ],
                "published": "2024-04-17T11:52:47Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    52,
                    47,
                    2,
                    108,
                    0
                ],
                "title": "A Preference-driven Paradigm for Enhanced Translation with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Preference-driven Paradigm for Enhanced Translation with Large\n  Language Models"
                },
                "summary": "Recent research has shown that large language models (LLMs) can achieve\nremarkable translation performance through supervised fine-tuning (SFT) using\nonly a small amount of parallel data. However, SFT simply instructs the model\nto imitate the reference translations at the token level, making it vulnerable\nto the noise present in the references. Hence, the assistance from SFT often\nreaches a plateau once the LLMs have achieved a certain level of translation\ncapability, and further increasing the size of parallel data does not provide\nadditional benefits. To overcome this plateau associated with imitation-based\nSFT, we propose a preference-based approach built upon the Plackett-Luce model.\nThe objective is to steer LLMs towards a more nuanced understanding of\ntranslation preferences from a holistic view, while also being more resilient\nin the absence of gold translations. We further build a dataset named MAPLE to\nverify the effectiveness of our approach, which includes multiple translations\nof varying quality for each source sentence. Extensive experiments demonstrate\nthe superiority of our approach in \"breaking the plateau\" across diverse LLMs\nand test settings. Our in-depth analysis underscores the pivotal role of\ndiverse translations and accurate preference scores in the success of our\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that large language models (LLMs) can achieve\nremarkable translation performance through supervised fine-tuning (SFT) using\nonly a small amount of parallel data. However, SFT simply instructs the model\nto imitate the reference translations at the token level, making it vulnerable\nto the noise present in the references. Hence, the assistance from SFT often\nreaches a plateau once the LLMs have achieved a certain level of translation\ncapability, and further increasing the size of parallel data does not provide\nadditional benefits. To overcome this plateau associated with imitation-based\nSFT, we propose a preference-based approach built upon the Plackett-Luce model.\nThe objective is to steer LLMs towards a more nuanced understanding of\ntranslation preferences from a holistic view, while also being more resilient\nin the absence of gold translations. We further build a dataset named MAPLE to\nverify the effectiveness of our approach, which includes multiple translations\nof varying quality for each source sentence. Extensive experiments demonstrate\nthe superiority of our approach in \"breaking the plateau\" across diverse LLMs\nand test settings. Our in-depth analysis underscores the pivotal role of\ndiverse translations and accurate preference scores in the success of our\napproach."
                },
                "authors": [
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Sony Trenous"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    },
                    {
                        "name": "Dietrich Klakow"
                    },
                    {
                        "name": "Bill Byrne"
                    },
                    {
                        "name": "Eva Hasler"
                    }
                ],
                "author_detail": {
                    "name": "Eva Hasler"
                },
                "author": "Eva Hasler",
                "arxiv_comment": "Accepted to NAACL 2024 (long, main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16400v1",
                "updated": "2024-08-29T10:00:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    0,
                    57,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T10:00:57Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    10,
                    0,
                    57,
                    3,
                    242,
                    0
                ],
                "title": "Outside the Comfort Zone: Analysing LLM Capabilities in Software\n  Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outside the Comfort Zone: Analysing LLM Capabilities in Software\n  Vulnerability Detection"
                },
                "summary": "The significant increase in software production driven by automation and\nfaster development lifecycles has resulted in a corresponding surge in software\nvulnerabilities. In parallel, the evolving landscape of software vulnerability\ndetection, highlighting the shift from traditional methods to machine learning\nand large language models (LLMs), provides massive opportunities at the cost of\nresource-demanding computations. This paper thoroughly analyses LLMs'\ncapabilities in detecting vulnerabilities within source code by testing models\nbeyond their usual applications to study their potential in cybersecurity\ntasks. We evaluate the performance of six open-source models that are\nspecifically trained for vulnerability detection against six general-purpose\nLLMs, three of which were further fine-tuned on a dataset that we compiled. Our\ndataset, alongside five state-of-the-art benchmark datasets, were used to\ncreate a pipeline to leverage a binary classification task, namely classifying\ncode into vulnerable and non-vulnerable. The findings highlight significant\nvariations in classification accuracy across benchmarks, revealing the critical\ninfluence of fine-tuning in enhancing the detection capabilities of small LLMs\nover their larger counterparts, yet only in the specific scenarios in which\nthey were trained. Further experiments and analysis also underscore the issues\nwith current benchmark datasets, particularly around mislabeling and their\nimpact on model training and performance, which raises concerns about the\ncurrent state of practice. We also discuss the road ahead in the field\nsuggesting strategies for improved model training and dataset curation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant increase in software production driven by automation and\nfaster development lifecycles has resulted in a corresponding surge in software\nvulnerabilities. In parallel, the evolving landscape of software vulnerability\ndetection, highlighting the shift from traditional methods to machine learning\nand large language models (LLMs), provides massive opportunities at the cost of\nresource-demanding computations. This paper thoroughly analyses LLMs'\ncapabilities in detecting vulnerabilities within source code by testing models\nbeyond their usual applications to study their potential in cybersecurity\ntasks. We evaluate the performance of six open-source models that are\nspecifically trained for vulnerability detection against six general-purpose\nLLMs, three of which were further fine-tuned on a dataset that we compiled. Our\ndataset, alongside five state-of-the-art benchmark datasets, were used to\ncreate a pipeline to leverage a binary classification task, namely classifying\ncode into vulnerable and non-vulnerable. The findings highlight significant\nvariations in classification accuracy across benchmarks, revealing the critical\ninfluence of fine-tuning in enhancing the detection capabilities of small LLMs\nover their larger counterparts, yet only in the specific scenarios in which\nthey were trained. Further experiments and analysis also underscore the issues\nwith current benchmark datasets, particularly around mislabeling and their\nimpact on model training and performance, which raises concerns about the\ncurrent state of practice. We also discuss the road ahead in the field\nsuggesting strategies for improved model training and dataset curation."
                },
                "authors": [
                    {
                        "name": "Yuejun Guo"
                    },
                    {
                        "name": "Constantinos Patsakis"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Qiang Tang"
                    },
                    {
                        "name": "Fran Casino"
                    }
                ],
                "author_detail": {
                    "name": "Fran Casino"
                },
                "author": "Fran Casino",
                "arxiv_comment": "Accepted to ESORICS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15256v2",
                "updated": "2024-08-29T09:34:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    9,
                    34,
                    48,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-09T19:21:14Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    19,
                    21,
                    14,
                    4,
                    222,
                    0
                ],
                "title": "Improving Ontology Requirements Engineering with OntoChat and\n  Participatory Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Ontology Requirements Engineering with OntoChat and\n  Participatory Prompting"
                },
                "summary": "Past ontology requirements engineering (ORE) has primarily relied on manual\nmethods, such as interviews and collaborative forums, to gather user\nrequirements from domain experts, especially in large projects. Current\nOntoChat offers a framework for ORE that utilises large language models (LLMs)\nto streamline the process through four key functions: user story creation,\ncompetency question (CQ) extraction, CQ filtration and analysis, and ontology\ntesting support. In OntoChat, users are expected to prompt the chatbot to\ngenerate user stories. However, preliminary evaluations revealed that they\nstruggle to do this effectively. To address this issue, we experimented with a\nresearch method called participatory prompting, which involves\nresearcher-mediated interactions to help users without deep knowledge of LLMs\nuse the chatbot more effectively. This participatory prompting user study\nproduces pre-defined prompt templates based on user queries, focusing on\ncreating and refining personas, goals, scenarios, sample data, and data\nresources for user stories. These refined user stories will subsequently be\nconverted into CQs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Past ontology requirements engineering (ORE) has primarily relied on manual\nmethods, such as interviews and collaborative forums, to gather user\nrequirements from domain experts, especially in large projects. Current\nOntoChat offers a framework for ORE that utilises large language models (LLMs)\nto streamline the process through four key functions: user story creation,\ncompetency question (CQ) extraction, CQ filtration and analysis, and ontology\ntesting support. In OntoChat, users are expected to prompt the chatbot to\ngenerate user stories. However, preliminary evaluations revealed that they\nstruggle to do this effectively. To address this issue, we experimented with a\nresearch method called participatory prompting, which involves\nresearcher-mediated interactions to help users without deep knowledge of LLMs\nuse the chatbot more effectively. This participatory prompting user study\nproduces pre-defined prompt templates based on user queries, focusing on\ncreating and refining personas, goals, scenarios, sample data, and data\nresources for user stories. These refined user stories will subsequently be\nconverted into CQs."
                },
                "authors": [
                    {
                        "name": "Yihang Zhao"
                    },
                    {
                        "name": "Bohui Zhang"
                    },
                    {
                        "name": "Xi Hu"
                    },
                    {
                        "name": "Shuyin Ouyang"
                    },
                    {
                        "name": "Jongmo Kim"
                    },
                    {
                        "name": "Nitisha Jain"
                    },
                    {
                        "name": "Jacopo de Berardinis"
                    },
                    {
                        "name": "Albert Meroño-Peñuela"
                    },
                    {
                        "name": "Elena Simperl"
                    }
                ],
                "author_detail": {
                    "name": "Elena Simperl"
                },
                "author": "Elena Simperl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01602v2",
                "updated": "2024-08-29T08:49:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    49,
                    14,
                    3,
                    242,
                    0
                ],
                "published": "2024-04-02T02:46:18Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    2,
                    46,
                    18,
                    1,
                    93,
                    0
                ],
                "title": "Helmsman of the Masses? Evaluate the Opinion Leadership of Large\n  Language Models in the Werewolf Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helmsman of the Masses? Evaluate the Opinion Leadership of Large\n  Language Models in the Werewolf Game"
                },
                "summary": "Large language models (LLMs) have exhibited memorable strategic behaviors in\nsocial deductive games. However, the significance of opinion leadership\nexhibited by LLM-based agents has been largely overlooked, which is crucial for\npractical applications in multi-agent and human-AI interaction settings.\nOpinion leaders are individuals who have a noticeable impact on the beliefs and\nbehaviors of others within a social group. In this work, we employ the Werewolf\ngame as a simulation platform to assess the opinion leadership of LLMs. The\ngame includes the role of the Sheriff, tasked with summarizing arguments and\nrecommending decision options, and therefore serves as a credible proxy for an\nopinion leader. We develop a framework integrating the Sheriff role and devise\ntwo novel metrics based on the critical characteristics of opinion leaders. The\nfirst metric measures the reliability of the opinion leader, and the second\nassesses the influence of the opinion leader on other players' decisions. We\nconduct extensive experiments to evaluate LLMs of different scales. In\naddition, we collect a Werewolf question-answering dataset (WWQA) to assess and\nenhance LLM's grasp of the game rules, and we also incorporate human\nparticipants for further analysis. The results suggest that the Werewolf game\nis a suitable test bed to evaluate the opinion leadership of LLMs, and few LLMs\npossess the capacity for opinion leadership.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited memorable strategic behaviors in\nsocial deductive games. However, the significance of opinion leadership\nexhibited by LLM-based agents has been largely overlooked, which is crucial for\npractical applications in multi-agent and human-AI interaction settings.\nOpinion leaders are individuals who have a noticeable impact on the beliefs and\nbehaviors of others within a social group. In this work, we employ the Werewolf\ngame as a simulation platform to assess the opinion leadership of LLMs. The\ngame includes the role of the Sheriff, tasked with summarizing arguments and\nrecommending decision options, and therefore serves as a credible proxy for an\nopinion leader. We develop a framework integrating the Sheriff role and devise\ntwo novel metrics based on the critical characteristics of opinion leaders. The\nfirst metric measures the reliability of the opinion leader, and the second\nassesses the influence of the opinion leader on other players' decisions. We\nconduct extensive experiments to evaluate LLMs of different scales. In\naddition, we collect a Werewolf question-answering dataset (WWQA) to assess and\nenhance LLM's grasp of the game rules, and we also incorporate human\nparticipants for further analysis. The results suggest that the Werewolf game\nis a suitable test bed to evaluate the opinion leadership of LLMs, and few LLMs\npossess the capacity for opinion leadership."
                },
                "authors": [
                    {
                        "name": "Silin Du"
                    },
                    {
                        "name": "Xiaowei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Zhang"
                },
                "author": "Xiaowei Zhang",
                "arxiv_comment": "Published as a conference paper at COLM 2024. 37 pages, 6 figures, 27\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15533v2",
                "updated": "2024-08-29T08:45:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    45,
                    30,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-28T04:44:43Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    4,
                    44,
                    43,
                    2,
                    241,
                    0
                ],
                "title": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via\n  Layer-wise Relevance Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via\n  Layer-wise Relevance Propagation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines."
                },
                "authors": [
                    {
                        "name": "Haichuan Hu"
                    },
                    {
                        "name": "Yuhan Sun"
                    },
                    {
                        "name": "Quanjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanjun Zhang"
                },
                "author": "Quanjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16345v1",
                "updated": "2024-08-29T08:30:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    30,
                    33,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T08:30:33Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    30,
                    33,
                    3,
                    242,
                    0
                ],
                "title": "The Unreasonable Ineffectiveness of Nucleus Sampling on Mitigating Text\n  Memorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Unreasonable Ineffectiveness of Nucleus Sampling on Mitigating Text\n  Memorization"
                },
                "summary": "This work analyses the text memorization behavior of large language models\n(LLMs) when subjected to nucleus sampling. Stochastic decoding methods like\nnucleus sampling are typically applied to overcome issues such as monotonous\nand repetitive text generation, which are often observed with\nmaximization-based decoding techniques. We hypothesize that nucleus sampling\nmight also reduce the occurrence of memorization patterns, because it could\nlead to the selection of tokens outside the memorized sequence. To test this\nhypothesis we create a diagnostic dataset with a known distribution of\nduplicates that gives us some control over the likelihood of memorization of\ncertain parts of the training data. Our analysis of two GPT-Neo models\nfine-tuned on this dataset interestingly shows that (i) an increase of the\nnucleus size reduces memorization only modestly, and (ii) even when models do\nnot engage in \"hard\" memorization -- a verbatim reproduction of training\nsamples -- they may still display \"soft\" memorization whereby they generate\noutputs that echo the training data but without a complete one-by-one\nresemblance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work analyses the text memorization behavior of large language models\n(LLMs) when subjected to nucleus sampling. Stochastic decoding methods like\nnucleus sampling are typically applied to overcome issues such as monotonous\nand repetitive text generation, which are often observed with\nmaximization-based decoding techniques. We hypothesize that nucleus sampling\nmight also reduce the occurrence of memorization patterns, because it could\nlead to the selection of tokens outside the memorized sequence. To test this\nhypothesis we create a diagnostic dataset with a known distribution of\nduplicates that gives us some control over the likelihood of memorization of\ncertain parts of the training data. Our analysis of two GPT-Neo models\nfine-tuned on this dataset interestingly shows that (i) an increase of the\nnucleus size reduces memorization only modestly, and (ii) even when models do\nnot engage in \"hard\" memorization -- a verbatim reproduction of training\nsamples -- they may still display \"soft\" memorization whereby they generate\noutputs that echo the training data but without a complete one-by-one\nresemblance."
                },
                "authors": [
                    {
                        "name": "Luka Borec"
                    },
                    {
                        "name": "Philipp Sadler"
                    },
                    {
                        "name": "David Schlangen"
                    }
                ],
                "author_detail": {
                    "name": "David Schlangen"
                },
                "author": "David Schlangen",
                "arxiv_comment": "9 pages, Accepted at INLG 2024 (International Natural Language\n  Generation Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12326v2",
                "updated": "2024-08-29T08:27:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    27,
                    27,
                    3,
                    242,
                    0
                ],
                "published": "2024-02-19T18:00:30Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    18,
                    0,
                    30,
                    0,
                    50,
                    0
                ],
                "title": "PsychoGAT: A Novel Psychological Measurement Paradigm through\n  Interactive Fiction Games with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsychoGAT: A Novel Psychological Measurement Paradigm through\n  Interactive Fiction Games with LLM Agents"
                },
                "summary": "Psychological measurement is essential for mental health, self-understanding,\nand personal development. Traditional methods, such as self-report scales and\npsychologist interviews, often face challenges with engagement and\naccessibility. While game-based and LLM-based tools have been explored to\nimprove user interest and automate assessment, they struggle to balance\nengagement with generalizability. In this work, we propose PsychoGAT\n(Psychological Game AgenTs) to achieve a generic gamification of psychological\nassessment. The main insight is that powerful LLMs can function both as adept\npsychologists and innovative game designers. By incorporating LLM agents into\ndesignated roles and carefully managing their interactions, PsychoGAT can\ntransform any standardized scales into personalized and engaging interactive\nfiction games. To validate the proposed method, we conduct psychometric\nevaluations to assess its effectiveness and employ human evaluators to examine\nthe generated content across various psychological constructs, including\ndepression, cognitive distortions, and personality traits. Results demonstrate\nthat PsychoGAT serves as an effective assessment tool, achieving statistically\nsignificant excellence in psychometric metrics such as reliability, convergent\nvalidity, and discriminant validity. Moreover, human evaluations confirm\nPsychoGAT's enhancements in content coherence, interactivity, interest,\nimmersion, and satisfaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psychological measurement is essential for mental health, self-understanding,\nand personal development. Traditional methods, such as self-report scales and\npsychologist interviews, often face challenges with engagement and\naccessibility. While game-based and LLM-based tools have been explored to\nimprove user interest and automate assessment, they struggle to balance\nengagement with generalizability. In this work, we propose PsychoGAT\n(Psychological Game AgenTs) to achieve a generic gamification of psychological\nassessment. The main insight is that powerful LLMs can function both as adept\npsychologists and innovative game designers. By incorporating LLM agents into\ndesignated roles and carefully managing their interactions, PsychoGAT can\ntransform any standardized scales into personalized and engaging interactive\nfiction games. To validate the proposed method, we conduct psychometric\nevaluations to assess its effectiveness and employ human evaluators to examine\nthe generated content across various psychological constructs, including\ndepression, cognitive distortions, and personality traits. Results demonstrate\nthat PsychoGAT serves as an effective assessment tool, achieving statistically\nsignificant excellence in psychometric metrics such as reliability, convergent\nvalidity, and discriminant validity. Moreover, human evaluations confirm\nPsychoGAT's enhancements in content coherence, interactivity, interest,\nimmersion, and satisfaction."
                },
                "authors": [
                    {
                        "name": "Qisen Yang"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Honghui Chen"
                    },
                    {
                        "name": "Shenzhi Wang"
                    },
                    {
                        "name": "Yifan Pu"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Shiji Song"
                    },
                    {
                        "name": "Gao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gao Huang"
                },
                "author": "Gao Huang",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14507v2",
                "updated": "2024-08-29T08:24:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    24,
                    42,
                    3,
                    242,
                    0
                ],
                "published": "2024-07-19T17:59:03Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    17,
                    59,
                    3,
                    4,
                    201,
                    0
                ],
                "title": "Internal Consistency and Self-Feedback in Large Language Models: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal Consistency and Self-Feedback in Large Language Models: A\n  Survey"
                },
                "summary": "Large language models (LLMs) often exhibit deficient reasoning or generate\nhallucinations. To address these, studies prefixed with \"Self-\" such as\nSelf-Consistency, Self-Improve, and Self-Refine have been initiated. They share\na commonality: involving LLMs evaluating and updating themselves. Nonetheless,\nthese efforts lack a unified perspective on summarization, as existing surveys\npredominantly focus on categorization.\n  In this paper, we summarize a theoretical framework, Internal Consistency,\noffering explanations for reasoning deficiencies and hallucinations. Internal\nConsistency refers to the consistency in expressions among LLMs' latent,\ndecoding, or response layers based on sampling methodologies. Then, we\nintroduce another effective theoretical framework capable of mining Internal\nConsistency, named Self-Feedback. This framework consists of two modules:\nSelf-Evaluation and Self-Update. The former captures Internal Consistency\nSignals, while the latter leverages the signals to enhance either the model's\nresponse or the model itself. This framework has been employed in numerous\nstudies.\n  We systematically classify these studies by tasks and lines of work;\nsummarize relevant evaluation methods and benchmarks; and delve into the\nconcern, \"Does Self-Feedback Really Work?\" We also propose several critical\nviewpoints, including the \"Hourglass Evolution of Internal Consistency\",\n\"Consistency Is (Almost) Correctness\" hypothesis, and \"The Paradox of Latent\nand Explicit Reasoning\". The relevant resources are open-sourced at\nhttps://github.com/IAAR-Shanghai/ICSFSurvey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit deficient reasoning or generate\nhallucinations. To address these, studies prefixed with \"Self-\" such as\nSelf-Consistency, Self-Improve, and Self-Refine have been initiated. They share\na commonality: involving LLMs evaluating and updating themselves. Nonetheless,\nthese efforts lack a unified perspective on summarization, as existing surveys\npredominantly focus on categorization.\n  In this paper, we summarize a theoretical framework, Internal Consistency,\noffering explanations for reasoning deficiencies and hallucinations. Internal\nConsistency refers to the consistency in expressions among LLMs' latent,\ndecoding, or response layers based on sampling methodologies. Then, we\nintroduce another effective theoretical framework capable of mining Internal\nConsistency, named Self-Feedback. This framework consists of two modules:\nSelf-Evaluation and Self-Update. The former captures Internal Consistency\nSignals, while the latter leverages the signals to enhance either the model's\nresponse or the model itself. This framework has been employed in numerous\nstudies.\n  We systematically classify these studies by tasks and lines of work;\nsummarize relevant evaluation methods and benchmarks; and delve into the\nconcern, \"Does Self-Feedback Really Work?\" We also propose several critical\nviewpoints, including the \"Hourglass Evolution of Internal Consistency\",\n\"Consistency Is (Almost) Correctness\" hypothesis, and \"The Paradox of Latent\nand Explicit Reasoning\". The relevant resources are open-sourced at\nhttps://github.com/IAAR-Shanghai/ICSFSurvey."
                },
                "authors": [
                    {
                        "name": "Xun Liang"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Zifan Zheng"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Qingchen Yu"
                    },
                    {
                        "name": "Xunkai Li"
                    },
                    {
                        "name": "Rong-Hua Li"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhonghao Wang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "24 pages, 9 figures, 7 tables, 14 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16326v1",
                "updated": "2024-08-29T08:02:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    2,
                    9,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T08:02:09Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    2,
                    9,
                    3,
                    242,
                    0
                ],
                "title": "Critic-CoT: Boosting the reasoning abilities of large language model via\n  Chain-of-thoughts Critic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critic-CoT: Boosting the reasoning abilities of large language model via\n  Chain-of-thoughts Critic"
                },
                "summary": "Self-critic has become an important mechanism for enhancing the reasoning\nperformance of LLMs. However, current approaches mainly involve basic prompts\nwithout further training, which tend to be over-simplified, leading to limited\naccuracy.Moreover, there is a lack of in-depth investigation of the\nrelationship between LLM's ability to criticism and its task-solving\nperformance.To address these issues, we propose Critic-CoT, a novel framework\nthat pushes LLMs toward System-2-like critic capability, via step-wise CoT\nreasoning format and distant-supervision data construction, without the need\nfor human annotation. Experiments on GSM8K and MATH show that via filtering out\ninvalid solutions or iterative refinement, our enhanced model boosts\ntask-solving performance, which demonstrates the effectiveness of our method.\nFurther, we find that training on critique and refinement alone improves the\ngeneration. We hope our work could shed light on future research on improving\nthe reasoning and critic ability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-critic has become an important mechanism for enhancing the reasoning\nperformance of LLMs. However, current approaches mainly involve basic prompts\nwithout further training, which tend to be over-simplified, leading to limited\naccuracy.Moreover, there is a lack of in-depth investigation of the\nrelationship between LLM's ability to criticism and its task-solving\nperformance.To address these issues, we propose Critic-CoT, a novel framework\nthat pushes LLMs toward System-2-like critic capability, via step-wise CoT\nreasoning format and distant-supervision data construction, without the need\nfor human annotation. Experiments on GSM8K and MATH show that via filtering out\ninvalid solutions or iterative refinement, our enhanced model boosts\ntask-solving performance, which demonstrates the effectiveness of our method.\nFurther, we find that training on critique and refinement alone improves the\ngeneration. We hope our work could shed light on future research on improving\nthe reasoning and critic ability of LLMs."
                },
                "authors": [
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Jie Lou"
                    },
                    {
                        "name": "Boxi Cao"
                    },
                    {
                        "name": "Xueru Wen"
                    },
                    {
                        "name": "Yuqiu Ji"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Debing Zhang"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10039v2",
                "updated": "2024-08-29T07:21:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    7,
                    21,
                    54,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-19T14:31:57Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    31,
                    57,
                    0,
                    232,
                    0
                ],
                "title": "MSDiagnosis: An EMR-based Dataset for Clinical Multi-Step Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSDiagnosis: An EMR-based Dataset for Clinical Multi-Step Diagnosis"
                },
                "summary": "Clinical diagnosis is critical in medical practice, typically requiring a\ncontinuous and evolving process that includes primary diagnosis, differential\ndiagnosis, and final diagnosis. However, most existing clinical diagnostic\ntasks are single-step processes, which does not align with the complex\nmulti-step diagnostic procedures found in real-world clinical settings. In this\npaper, we propose a multi-step diagnostic task and annotate a clinical\ndiagnostic dataset (MSDiagnosis). This dataset includes primary diagnosis,\ndifferential diagnosis, and final diagnosis questions. Additionally, we propose\na novel and effective framework. This framework combines forward inference,\nbackward inference, reflection, and refinement, enabling the LLM to\nself-evaluate and adjust its diagnostic results. To assess the effectiveness of\nour proposed method, we design and conduct extensive experiments. The\nexperimental results demonstrate the effectiveness of the proposed method. We\nalso provide a comprehensive experimental analysis and suggest future research\ndirections for this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical diagnosis is critical in medical practice, typically requiring a\ncontinuous and evolving process that includes primary diagnosis, differential\ndiagnosis, and final diagnosis. However, most existing clinical diagnostic\ntasks are single-step processes, which does not align with the complex\nmulti-step diagnostic procedures found in real-world clinical settings. In this\npaper, we propose a multi-step diagnostic task and annotate a clinical\ndiagnostic dataset (MSDiagnosis). This dataset includes primary diagnosis,\ndifferential diagnosis, and final diagnosis questions. Additionally, we propose\na novel and effective framework. This framework combines forward inference,\nbackward inference, reflection, and refinement, enabling the LLM to\nself-evaluate and adjust its diagnostic results. To assess the effectiveness of\nour proposed method, we design and conduct extensive experiments. The\nexperimental results demonstrate the effectiveness of the proposed method. We\nalso provide a comprehensive experimental analysis and suggest future research\ndirections for this task."
                },
                "authors": [
                    {
                        "name": "Ruihui Hou"
                    },
                    {
                        "name": "Shencheng Chen"
                    },
                    {
                        "name": "Yongqi Fan"
                    },
                    {
                        "name": "Lifeng Zhu"
                    },
                    {
                        "name": "Jing Sun"
                    },
                    {
                        "name": "Jingping Liu"
                    },
                    {
                        "name": "Tong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Tong Ruan"
                },
                "author": "Tong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16312v2",
                "updated": "2024-08-30T11:48:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    11,
                    48,
                    40,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-29T07:20:56Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    7,
                    20,
                    56,
                    3,
                    242,
                    0
                ],
                "title": "SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval"
                },
                "summary": "Large-scale test collections play a crucial role in Information Retrieval\n(IR) research. However, according to the Cranfield paradigm and the research\ninto publicly available datasets, the existing information retrieval research\nstudies are commonly developed on small-scale datasets that rely on human\nassessors for relevance judgments - a time-intensive and expensive process.\nRecent studies have shown the strong capability of Large Language Models (LLMs)\nin producing reliable relevance judgments with human accuracy but at a greatly\nreduced cost. In this paper, to address the missing large-scale ad-hoc document\nretrieval dataset, we extend the TREC Deep Learning Track (DL) test collection\nvia additional language model synthetic labels to enable researchers to test\nand evaluate their search systems at a large scale. Specifically, such a test\ncollection includes more than 1,900 test queries from the previous years of\ntracks. We compare system evaluation with past human labels from past years and\nfind that our synthetically created large-scale test collection can lead to\nhighly correlated system rankings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale test collections play a crucial role in Information Retrieval\n(IR) research. However, according to the Cranfield paradigm and the research\ninto publicly available datasets, the existing information retrieval research\nstudies are commonly developed on small-scale datasets that rely on human\nassessors for relevance judgments - a time-intensive and expensive process.\nRecent studies have shown the strong capability of Large Language Models (LLMs)\nin producing reliable relevance judgments with human accuracy but at a greatly\nreduced cost. In this paper, to address the missing large-scale ad-hoc document\nretrieval dataset, we extend the TREC Deep Learning Track (DL) test collection\nvia additional language model synthetic labels to enable researchers to test\nand evaluate their search systems at a large scale. Specifically, such a test\ncollection includes more than 1,900 test queries from the previous years of\ntracks. We compare system evaluation with past human labels from past years and\nfind that our synthetically created large-scale test collection can lead to\nhighly correlated system rankings."
                },
                "authors": [
                    {
                        "name": "Hossein A. Rahmani"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Emine Yilmaz"
                    },
                    {
                        "name": "Nick Craswell"
                    },
                    {
                        "name": "Bhaskar Mitra"
                    },
                    {
                        "name": "Paul Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Paul Thomas"
                },
                "author": "Paul Thomas",
                "arxiv_comment": "9 pages, resource paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16304v1",
                "updated": "2024-08-29T07:11:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    7,
                    11,
                    9,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T07:11:09Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    7,
                    11,
                    9,
                    3,
                    242,
                    0
                ],
                "title": "Understanding Privacy Norms through Web Forms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Privacy Norms through Web Forms"
                },
                "summary": "Web forms are one of the primary ways to collect personal information online,\nyet they are relatively under-studied. Unlike web tracking, data collection\nthrough web forms is explicit and contextualized. Users (i) are asked to input\nspecific personal information types, and (ii) know the specific context (i.e.,\non which website and for what purpose). For web forms to be trusted by users,\nthey must meet the common sense standards of appropriate data collection\npractices within a particular context (i.e., privacy norms). In this paper, we\nextract the privacy norms embedded within web forms through a measurement\nstudy. First, we build a specialized crawler to discover web forms on websites.\nWe run it on 11,500 popular websites, and we create a dataset of 293K web\nforms. Second, to process data of this scale, we develop a cost-efficient way\nto annotate web forms with form types and personal information types, using\ntext classifiers trained with assistance of large language models (LLMs).\nThird, by analyzing the annotated dataset, we reveal common patterns of data\ncollection practices. We find that (i) these patterns are explained by\nfunctional necessities and legal obligations, thus reflecting privacy norms,\nand that (ii) deviations from the observed norms often signal unnecessary data\ncollection. In addition, we analyze the privacy policies that accompany web\nforms. We show that, despite their wide adoption and use, there is a disconnect\nbetween privacy policy disclosures and the observed privacy norms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web forms are one of the primary ways to collect personal information online,\nyet they are relatively under-studied. Unlike web tracking, data collection\nthrough web forms is explicit and contextualized. Users (i) are asked to input\nspecific personal information types, and (ii) know the specific context (i.e.,\non which website and for what purpose). For web forms to be trusted by users,\nthey must meet the common sense standards of appropriate data collection\npractices within a particular context (i.e., privacy norms). In this paper, we\nextract the privacy norms embedded within web forms through a measurement\nstudy. First, we build a specialized crawler to discover web forms on websites.\nWe run it on 11,500 popular websites, and we create a dataset of 293K web\nforms. Second, to process data of this scale, we develop a cost-efficient way\nto annotate web forms with form types and personal information types, using\ntext classifiers trained with assistance of large language models (LLMs).\nThird, by analyzing the annotated dataset, we reveal common patterns of data\ncollection practices. We find that (i) these patterns are explained by\nfunctional necessities and legal obligations, thus reflecting privacy norms,\nand that (ii) deviations from the observed norms often signal unnecessary data\ncollection. In addition, we analyze the privacy policies that accompany web\nforms. We show that, despite their wide adoption and use, there is a disconnect\nbetween privacy policy disclosures and the observed privacy norms."
                },
                "authors": [
                    {
                        "name": "Hao Cui"
                    },
                    {
                        "name": "Rahmadi Trimananda"
                    },
                    {
                        "name": "Athina Markopoulou"
                    }
                ],
                "author_detail": {
                    "name": "Athina Markopoulou"
                },
                "author": "Athina Markopoulou",
                "arxiv_comment": "18 pages, 7 figures, to be published in the Proceedings on Privacy\n  Enhancing Technologies (PoPETs) 2025.1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16296v1",
                "updated": "2024-08-29T06:54:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    6,
                    54,
                    3,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T06:54:03Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    6,
                    54,
                    3,
                    3,
                    242,
                    0
                ],
                "title": "Rethinking Sparse Lexical Representations for Image Retrieval in the Age\n  of Rising Multi-Modal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Sparse Lexical Representations for Image Retrieval in the Age\n  of Rising Multi-Modal Large Language Models"
                },
                "summary": "In this paper, we rethink sparse lexical representations for image retrieval.\nBy utilizing multi-modal large language models (M-LLMs) that support visual\nprompting, we can extract image features and convert them into textual data,\nenabling us to utilize efficient sparse retrieval algorithms employed in\nnatural language processing for image retrieval tasks. To assist the LLM in\nextracting image features, we apply data augmentation techniques for key\nexpansion and analyze the impact with a metric for relevance between images and\ntextual data. We empirically show the superior precision and recall performance\nof our image retrieval method compared to conventional vision-language\nmodel-based methods on the MS-COCO, PASCAL VOC, and NUS-WIDE datasets in a\nkeyword-based image retrieval scenario, where keywords serve as search queries.\nWe also demonstrate that the retrieval performance can be improved by\niteratively incorporating keywords into search queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we rethink sparse lexical representations for image retrieval.\nBy utilizing multi-modal large language models (M-LLMs) that support visual\nprompting, we can extract image features and convert them into textual data,\nenabling us to utilize efficient sparse retrieval algorithms employed in\nnatural language processing for image retrieval tasks. To assist the LLM in\nextracting image features, we apply data augmentation techniques for key\nexpansion and analyze the impact with a metric for relevance between images and\ntextual data. We empirically show the superior precision and recall performance\nof our image retrieval method compared to conventional vision-language\nmodel-based methods on the MS-COCO, PASCAL VOC, and NUS-WIDE datasets in a\nkeyword-based image retrieval scenario, where keywords serve as search queries.\nWe also demonstrate that the retrieval performance can be improved by\niteratively incorporating keywords into search queries."
                },
                "authors": [
                    {
                        "name": "Kengo Nakata"
                    },
                    {
                        "name": "Daisuke Miyashita"
                    },
                    {
                        "name": "Youyang Ng"
                    },
                    {
                        "name": "Yasuto Hoshi"
                    },
                    {
                        "name": "Jun Deguchi"
                    }
                ],
                "author_detail": {
                    "name": "Jun Deguchi"
                },
                "author": "Jun Deguchi",
                "arxiv_comment": "Accepted to ECCV 2024 Workshops: 2nd Workshop on Traditional Computer\n  Vision in the Age of Deep Learning (TradiCV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16280v1",
                "updated": "2024-08-29T06:18:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    6,
                    18,
                    15,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T06:18:15Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    6,
                    18,
                    15,
                    3,
                    242,
                    0
                ],
                "title": "Double-decker: Productive Backscatter Communication Using a Single\n  Commodity Receiver",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Double-decker: Productive Backscatter Communication Using a Single\n  Commodity Receiver"
                },
                "summary": "Backscatter communication has attracted significant attention for\nInternet-of-Things applications due to its ultra-low-power consumption. The\nstate-of-the-art backscatter systems no longer require dedicated carrier\ngenerators and leverage ambient signals as carriers. However, there is an\nemerging challenge: most prior systems need dual receivers to capture the\noriginal and backscattered signals at the same time for tag data demodulation.\nThis is not conducive to the widespread deployment of backscatter\ncommunication. To address this problem, we present double-decker, a novel\nbackscatter system that only requires a single commercial device for\nbackscatter communication. The key technology of double-decker is to divide the\ncarrier OFDM symbols into two parts, which are pilot symbols and data symbols.\nPilot symbols can be used as reference signals for tag data demodulation, thus\ngetting rid of the dependence on the dual receiver structure. We have built an\nFPGA prototype and conducted extensive experiments. Empirical results show that\nwhen the excitation signal is 802.11g, double-decker achieves a tag data rate\nof 35.2kbps and a productive data rate of 38kbps, respectively. The\ncommunication range of double-decker is up to 28m in LOS deployment and 24m in\nNLOS deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backscatter communication has attracted significant attention for\nInternet-of-Things applications due to its ultra-low-power consumption. The\nstate-of-the-art backscatter systems no longer require dedicated carrier\ngenerators and leverage ambient signals as carriers. However, there is an\nemerging challenge: most prior systems need dual receivers to capture the\noriginal and backscattered signals at the same time for tag data demodulation.\nThis is not conducive to the widespread deployment of backscatter\ncommunication. To address this problem, we present double-decker, a novel\nbackscatter system that only requires a single commercial device for\nbackscatter communication. The key technology of double-decker is to divide the\ncarrier OFDM symbols into two parts, which are pilot symbols and data symbols.\nPilot symbols can be used as reference signals for tag data demodulation, thus\ngetting rid of the dependence on the dual receiver structure. We have built an\nFPGA prototype and conducted extensive experiments. Empirical results show that\nwhen the excitation signal is 802.11g, double-decker achieves a tag data rate\nof 35.2kbps and a productive data rate of 38kbps, respectively. The\ncommunication range of double-decker is up to 28m in LOS deployment and 24m in\nNLOS deployment."
                },
                "authors": [
                    {
                        "name": "Qiwei Wang"
                    },
                    {
                        "name": "Wei Gong"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gong"
                },
                "author": "Wei Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16276v1",
                "updated": "2024-08-29T05:47:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    5,
                    47,
                    14,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T05:47:14Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    5,
                    47,
                    14,
                    3,
                    242,
                    0
                ],
                "title": "Enhancing AI-Driven Psychological Consultation: Layered Prompts with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing AI-Driven Psychological Consultation: Layered Prompts with\n  Large Language Models"
                },
                "summary": "Psychological consultation is essential for improving mental health and\nwell-being, yet challenges such as the shortage of qualified professionals and\nscalability issues limit its accessibility. To address these challenges, we\nexplore the use of large language models (LLMs) like GPT-4 to augment\npsychological consultation services. Our approach introduces a novel layered\nprompting system that dynamically adapts to user input, enabling comprehensive\nand relevant information gathering. We also develop empathy-driven and\nscenario-based prompts to enhance the LLM's emotional intelligence and\ncontextual understanding in therapeutic settings. We validated our approach\nthrough experiments using a newly collected dataset of psychological\nconsultation dialogues, demonstrating significant improvements in response\nquality. The results highlight the potential of our prompt engineering\ntechniques to enhance AI-driven psychological consultation, offering a scalable\nand accessible solution to meet the growing demand for mental health support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psychological consultation is essential for improving mental health and\nwell-being, yet challenges such as the shortage of qualified professionals and\nscalability issues limit its accessibility. To address these challenges, we\nexplore the use of large language models (LLMs) like GPT-4 to augment\npsychological consultation services. Our approach introduces a novel layered\nprompting system that dynamically adapts to user input, enabling comprehensive\nand relevant information gathering. We also develop empathy-driven and\nscenario-based prompts to enhance the LLM's emotional intelligence and\ncontextual understanding in therapeutic settings. We validated our approach\nthrough experiments using a newly collected dataset of psychological\nconsultation dialogues, demonstrating significant improvements in response\nquality. The results highlight the potential of our prompt engineering\ntechniques to enhance AI-driven psychological consultation, offering a scalable\nand accessible solution to meet the growing demand for mental health support."
                },
                "authors": [
                    {
                        "name": "Rafael Souza"
                    },
                    {
                        "name": "Jia-Hao Lim"
                    },
                    {
                        "name": "Alexander Davis"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Davis"
                },
                "author": "Alexander Davis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.11911v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.11911v6",
                "updated": "2024-08-29T05:14:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    5,
                    14,
                    36,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-21T09:22:07Z",
                "published_parsed": [
                    2023,
                    9,
                    21,
                    9,
                    22,
                    7,
                    3,
                    264,
                    0
                ],
                "title": "InstructERC: Reforming Emotion Recognition in Conversation with\n  Multi-task Retrieval-Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructERC: Reforming Emotion Recognition in Conversation with\n  Multi-task Retrieval-Augmented Large Language Models"
                },
                "summary": "The field of emotion recognition of conversation (ERC) has been focusing on\nseparating sentence feature encoding and context modeling, lacking exploration\nin generative paradigms based on unified designs. In this study, we propose a\nnovel approach, InstructERC, to reformulate the ERC task from a discriminative\nframework to a generative framework based on Large Language Models (LLMs).\nInstructERC makes three significant contributions: (1) it introduces a simple\nyet effective retrieval template module, which helps the model explicitly\nintegrate multi-granularity dialogue supervision information. (2) We introduce\ntwo additional emotion alignment tasks, namely speaker identification and\nemotion prediction tasks, to implicitly model the dialogue role relationships\nand future emotional tendencies in conversations. (3) Pioneeringly, we unify\nemotion labels across benchmarks through the feeling wheel to fit real\napplication scenarios. InstructERC still perform impressively on this unified\ndataset. Our LLM-based plugin framework significantly outperforms all previous\nmodels and achieves comprehensive SOTA on three commonly used ERC datasets.\nExtensive analysis of parameter-efficient and data-scaling experiments provides\nempirical guidance for applying it in practical scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of emotion recognition of conversation (ERC) has been focusing on\nseparating sentence feature encoding and context modeling, lacking exploration\nin generative paradigms based on unified designs. In this study, we propose a\nnovel approach, InstructERC, to reformulate the ERC task from a discriminative\nframework to a generative framework based on Large Language Models (LLMs).\nInstructERC makes three significant contributions: (1) it introduces a simple\nyet effective retrieval template module, which helps the model explicitly\nintegrate multi-granularity dialogue supervision information. (2) We introduce\ntwo additional emotion alignment tasks, namely speaker identification and\nemotion prediction tasks, to implicitly model the dialogue role relationships\nand future emotional tendencies in conversations. (3) Pioneeringly, we unify\nemotion labels across benchmarks through the feeling wheel to fit real\napplication scenarios. InstructERC still perform impressively on this unified\ndataset. Our LLM-based plugin framework significantly outperforms all previous\nmodels and achieves comprehensive SOTA on three commonly used ERC datasets.\nExtensive analysis of parameter-efficient and data-scaling experiments provides\nempirical guidance for applying it in practical scenarios."
                },
                "authors": [
                    {
                        "name": "Shanglin Lei"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Xiaoping Wang"
                    },
                    {
                        "name": "Keheng Wang"
                    },
                    {
                        "name": "Runqi Qiao"
                    },
                    {
                        "name": "Sirui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sirui Wang"
                },
                "author": "Sirui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.11911v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.11911v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16264v1",
                "updated": "2024-08-29T05:02:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    5,
                    2,
                    52,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T05:02:52Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    5,
                    2,
                    52,
                    3,
                    242,
                    0
                ],
                "title": "LoraMap: Harnessing the Power of LoRA Connections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoraMap: Harnessing the Power of LoRA Connections"
                },
                "summary": "Large Language Models (LLMs) can benefit from mitigating hallucinations\nthrough fact-checking and overcoming substantial computational overhead with\nparameter-efficient techniques such as Low-Rank Adaptation (LoRA). While some\nstudies have explored the parallel integration of multiple LoRAs, these\napproaches need attention to the connections between them. This paper\ninvestigates methods to establish connections among multiple LoRAs. We create\nthree reasoning datasets tailored to fact-checking and fine-tune individual\nLoRAs, allowing them to view and reason from diverse perspectives. Then, we\nexplore strategies for allocating these reasoning LoRAs and introduce LoraMap,\nan approach to map connections between them. The results on the fact-checking\ntask demonstrate that the performance of LoraMap is superior to LoraHub, an\nexisting LoRA composition method. LoraMap also outperforms with significantly\nfewer parameters than LoraConcat, which concatenates LoRAs and further\nfine-tunes them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can benefit from mitigating hallucinations\nthrough fact-checking and overcoming substantial computational overhead with\nparameter-efficient techniques such as Low-Rank Adaptation (LoRA). While some\nstudies have explored the parallel integration of multiple LoRAs, these\napproaches need attention to the connections between them. This paper\ninvestigates methods to establish connections among multiple LoRAs. We create\nthree reasoning datasets tailored to fact-checking and fine-tune individual\nLoRAs, allowing them to view and reason from diverse perspectives. Then, we\nexplore strategies for allocating these reasoning LoRAs and introduce LoraMap,\nan approach to map connections between them. The results on the fact-checking\ntask demonstrate that the performance of LoraMap is superior to LoraHub, an\nexisting LoRA composition method. LoraMap also outperforms with significantly\nfewer parameters than LoraConcat, which concatenates LoRAs and further\nfine-tunes them."
                },
                "authors": [
                    {
                        "name": "Hyeryun Park"
                    },
                    {
                        "name": "Jeongwon Kwak"
                    },
                    {
                        "name": "Dongsuk Jang"
                    },
                    {
                        "name": "Sumin Park"
                    },
                    {
                        "name": "Jinwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinwook Choi"
                },
                "author": "Jinwook Choi",
                "arxiv_comment": "13 pages, 9 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13985v2",
                "updated": "2024-08-29T02:40:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    40,
                    12,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-26T02:35:37Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    35,
                    37,
                    0,
                    239,
                    0
                ],
                "title": "TF-Attack: Transferable and Fast Adversarial Attacks on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TF-Attack: Transferable and Fast Adversarial Attacks on Large Language\n  Models"
                },
                "summary": "With the great advancements in large language models (LLMs), adversarial\nattacks against LLMs have recently attracted increasing attention. We found\nthat pre-existing adversarial attack methodologies exhibit limited\ntransferability and are notably inefficient, particularly when applied to LLMs.\nIn this paper, we analyze the core mechanisms of previous predominant\nadversarial attack methods, revealing that 1) the distributions of importance\nscore differ markedly among victim models, restricting the transferability; 2)\nthe sequential attack processes induces substantial time overheads. Based on\nthe above two insights, we introduce a new scheme, named TF-Attack, for\nTransferable and Fast adversarial attacks on LLMs. TF-Attack employs an\nexternal LLM as a third-party overseer rather than the victim model to identify\ncritical units within sentences. Moreover, TF-Attack introduces the concept of\nImportance Level, which allows for parallel substitutions of attacks. We\nconduct extensive experiments on 6 widely adopted benchmarks, evaluating the\nproposed method through both automatic and human metrics. Results show that our\nmethod consistently surpasses previous methods in transferability and delivers\nsignificant speed improvements, up to 20 times faster than earlier attack\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the great advancements in large language models (LLMs), adversarial\nattacks against LLMs have recently attracted increasing attention. We found\nthat pre-existing adversarial attack methodologies exhibit limited\ntransferability and are notably inefficient, particularly when applied to LLMs.\nIn this paper, we analyze the core mechanisms of previous predominant\nadversarial attack methods, revealing that 1) the distributions of importance\nscore differ markedly among victim models, restricting the transferability; 2)\nthe sequential attack processes induces substantial time overheads. Based on\nthe above two insights, we introduce a new scheme, named TF-Attack, for\nTransferable and Fast adversarial attacks on LLMs. TF-Attack employs an\nexternal LLM as a third-party overseer rather than the victim model to identify\ncritical units within sentences. Moreover, TF-Attack introduces the concept of\nImportance Level, which allows for parallel substitutions of attacks. We\nconduct extensive experiments on 6 widely adopted benchmarks, evaluating the\nproposed method through both automatic and human metrics. Results show that our\nmethod consistently surpasses previous methods in transferability and delivers\nsignificant speed improvements, up to 20 times faster than earlier attack\nstrategies."
                },
                "authors": [
                    {
                        "name": "Zelin Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Mingming Yang"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10903v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10903v5",
                "updated": "2024-08-29T02:38:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    38,
                    5,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-20T14:47:38Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    47,
                    38,
                    1,
                    233,
                    0
                ],
                "title": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model"
                },
                "summary": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue."
                },
                "authors": [
                    {
                        "name": "Yeyong Yu"
                    },
                    {
                        "name": "Runsheng Yu"
                    },
                    {
                        "name": "Haojie Wei"
                    },
                    {
                        "name": "Zhanqiu Zhang"
                    },
                    {
                        "name": "Quan Qian"
                    }
                ],
                "author_detail": {
                    "name": "Quan Qian"
                },
                "author": "Quan Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10903v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10903v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16221v1",
                "updated": "2024-08-29T02:35:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    35,
                    53,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:35:53Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    35,
                    53,
                    3,
                    242,
                    0
                ],
                "title": "SSDM: Scalable Speech Dysfluency Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSDM: Scalable Speech Dysfluency Modeling"
                },
                "summary": "Speech dysfluency modeling is the core module for spoken language learning,\nand speech therapy. However, there are three challenges. First, current\nstate-of-the-art solutions suffer from poor scalability. Second, there is a\nlack of a large-scale dysfluency corpus. Third, there is not an effective\nlearning framework. In this paper, we propose \\textit{SSDM: Scalable Speech\nDysfluency Modeling}, which (1) adopts articulatory gestures as scalable forced\nalignment; (2) introduces connectionist subsequence aligner (CSA) to achieve\ndysfluency alignment; (3) introduces a large-scale simulated dysfluency corpus\ncalled Libri-Dys; and (4) develops an end-to-end system by leveraging the power\nof large language models (LLMs). We expect SSDM to serve as a standard in the\narea of dysfluency modeling. Demo is available at\n\\url{https://eureka235.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech dysfluency modeling is the core module for spoken language learning,\nand speech therapy. However, there are three challenges. First, current\nstate-of-the-art solutions suffer from poor scalability. Second, there is a\nlack of a large-scale dysfluency corpus. Third, there is not an effective\nlearning framework. In this paper, we propose \\textit{SSDM: Scalable Speech\nDysfluency Modeling}, which (1) adopts articulatory gestures as scalable forced\nalignment; (2) introduces connectionist subsequence aligner (CSA) to achieve\ndysfluency alignment; (3) introduces a large-scale simulated dysfluency corpus\ncalled Libri-Dys; and (4) develops an end-to-end system by leveraging the power\nof large language models (LLMs). We expect SSDM to serve as a standard in the\narea of dysfluency modeling. Demo is available at\n\\url{https://eureka235.github.io}."
                },
                "authors": [
                    {
                        "name": "Jiachen Lian"
                    },
                    {
                        "name": "Xuanru Zhou"
                    },
                    {
                        "name": "Zoe Ezzes"
                    },
                    {
                        "name": "Jet Vonk"
                    },
                    {
                        "name": "Brittany Morin"
                    },
                    {
                        "name": "David Baquirin"
                    },
                    {
                        "name": "Zachary Mille"
                    },
                    {
                        "name": "Maria Luisa Gorno Tempini"
                    },
                    {
                        "name": "Gopala Anumanchipalli"
                    }
                ],
                "author_detail": {
                    "name": "Gopala Anumanchipalli"
                },
                "author": "Gopala Anumanchipalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16219v1",
                "updated": "2024-08-29T02:25:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    25,
                    12,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:25:12Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    25,
                    12,
                    3,
                    242,
                    0
                ],
                "title": "Training-free Video Temporal Grounding using Large-scale Pre-trained\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free Video Temporal Grounding using Large-scale Pre-trained\n  Models"
                },
                "summary": "Video temporal grounding aims to identify video segments within untrimmed\nvideos that are most relevant to a given natural language query. Existing video\ntemporal localization models rely on specific datasets for training and have\nhigh data collection costs, but they exhibit poor generalization capability\nunder the across-dataset and out-of-distribution (OOD) settings. In this paper,\nwe propose a Training-Free Video Temporal Grounding (TFVTG) approach that\nleverages the ability of pre-trained large models. A naive baseline is to\nenumerate proposals in the video and use the pre-trained visual language models\n(VLMs) to select the best proposal according to the vision-language alignment.\nHowever, most existing VLMs are trained on image-text pairs or trimmed video\nclip-text pairs, making it struggle to (1) grasp the relationship and\ndistinguish the temporal boundaries of multiple events within the same video;\n(2) comprehend and be sensitive to the dynamic transition of events (the\ntransition from one event to another) in the video. To address these issues, we\npropose leveraging large language models (LLMs) to analyze multiple sub-events\ncontained in the query text and analyze the temporal order and relationships\nbetween these events. Secondly, we split a sub-event into dynamic transition\nand static status parts and propose the dynamic and static scoring functions\nusing VLMs to better evaluate the relevance between the event and the\ndescription. Finally, for each sub-event description, we use VLMs to locate the\ntop-k proposals and leverage the order and relationships between sub-events\nprovided by LLMs to filter and integrate these proposals. Our method achieves\nthe best performance on zero-shot video temporal grounding on Charades-STA and\nActivityNet Captions datasets without any training and demonstrates better\ngeneralization capabilities in cross-dataset and OOD settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video temporal grounding aims to identify video segments within untrimmed\nvideos that are most relevant to a given natural language query. Existing video\ntemporal localization models rely on specific datasets for training and have\nhigh data collection costs, but they exhibit poor generalization capability\nunder the across-dataset and out-of-distribution (OOD) settings. In this paper,\nwe propose a Training-Free Video Temporal Grounding (TFVTG) approach that\nleverages the ability of pre-trained large models. A naive baseline is to\nenumerate proposals in the video and use the pre-trained visual language models\n(VLMs) to select the best proposal according to the vision-language alignment.\nHowever, most existing VLMs are trained on image-text pairs or trimmed video\nclip-text pairs, making it struggle to (1) grasp the relationship and\ndistinguish the temporal boundaries of multiple events within the same video;\n(2) comprehend and be sensitive to the dynamic transition of events (the\ntransition from one event to another) in the video. To address these issues, we\npropose leveraging large language models (LLMs) to analyze multiple sub-events\ncontained in the query text and analyze the temporal order and relationships\nbetween these events. Secondly, we split a sub-event into dynamic transition\nand static status parts and propose the dynamic and static scoring functions\nusing VLMs to better evaluate the relevance between the event and the\ndescription. Finally, for each sub-event description, we use VLMs to locate the\ntop-k proposals and leverage the order and relationships between sub-events\nprovided by LLMs to filter and integrate these proposals. Our method achieves\nthe best performance on zero-shot video temporal grounding on Charades-STA and\nActivityNet Captions datasets without any training and demonstrates better\ngeneralization capabilities in cross-dataset and OOD settings."
                },
                "authors": [
                    {
                        "name": "Minghang Zheng"
                    },
                    {
                        "name": "Xinhao Cai"
                    },
                    {
                        "name": "Qingchao Chen"
                    },
                    {
                        "name": "Yuxin Peng"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "Accepted by ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13989v2",
                "updated": "2024-08-29T02:17:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    17,
                    51,
                    3,
                    242,
                    0
                ],
                "published": "2024-07-19T02:34:10Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    2,
                    34,
                    10,
                    4,
                    201,
                    0
                ],
                "title": "Enhancing Data-Limited Graph Neural Networks by Actively Distilling\n  Knowledge from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Data-Limited Graph Neural Networks by Actively Distilling\n  Knowledge from Large Language Models"
                },
                "summary": "Graphs are pervasive in the real-world, such as social network analysis,\nbioinformatics, and knowledge graphs. Graph neural networks (GNNs) have great\nability in node classification, a fundamental task on graphs. Unfortunately,\nconventional GNNs still face challenges in scenarios with few labeled nodes,\ndespite the prevalence of few-shot node classification tasks in real-world\napplications. To address this challenge, various approaches have been proposed,\nincluding graph meta-learning, transfer learning, and methods based on Large\nLanguage Models (LLMs). However, traditional meta-learning and transfer\nlearning methods often require prior knowledge from base classes or fail to\nexploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based\nmethods may overlook the zero-shot capabilities of LLMs and rely heavily on the\nquality of generated contexts. In this paper, we propose a novel approach that\nintegrates LLMs and GNNs, leveraging the zero-shot inference and reasoning\ncapabilities of LLMs and employing a Graph-LLM-based active learning paradigm\nto enhance GNNs' performance. Extensive experiments demonstrate the\neffectiveness of our model in improving node classification accuracy with\nconsiderably limited labeled data, surpassing state-of-the-art baselines by\nsignificant margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are pervasive in the real-world, such as social network analysis,\nbioinformatics, and knowledge graphs. Graph neural networks (GNNs) have great\nability in node classification, a fundamental task on graphs. Unfortunately,\nconventional GNNs still face challenges in scenarios with few labeled nodes,\ndespite the prevalence of few-shot node classification tasks in real-world\napplications. To address this challenge, various approaches have been proposed,\nincluding graph meta-learning, transfer learning, and methods based on Large\nLanguage Models (LLMs). However, traditional meta-learning and transfer\nlearning methods often require prior knowledge from base classes or fail to\nexploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based\nmethods may overlook the zero-shot capabilities of LLMs and rely heavily on the\nquality of generated contexts. In this paper, we propose a novel approach that\nintegrates LLMs and GNNs, leveraging the zero-shot inference and reasoning\ncapabilities of LLMs and employing a Graph-LLM-based active learning paradigm\nto enhance GNNs' performance. Extensive experiments demonstrate the\neffectiveness of our model in improving node classification accuracy with\nconsiderably limited labeled data, surpassing state-of-the-art baselines by\nsignificant margins."
                },
                "authors": [
                    {
                        "name": "Quan Li"
                    },
                    {
                        "name": "Tianxiang Zhao"
                    },
                    {
                        "name": "Lingwei Chen"
                    },
                    {
                        "name": "Junjie Xu"
                    },
                    {
                        "name": "Suhang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Suhang Wang"
                },
                "author": "Suhang Wang",
                "arxiv_comment": "10 pages, 3 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16213v1",
                "updated": "2024-08-29T02:12:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    12,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:12:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    12,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language\n  Models for Chest X-ray Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language\n  Models for Chest X-ray Interpretation"
                },
                "summary": "The rapid evolution of artificial intelligence, especially in large language\nmodels (LLMs), has significantly impacted various domains, including\nhealthcare. In chest X-ray (CXR) analysis, previous studies have employed LLMs,\nbut with limitations: either underutilizing the multi-tasking capabilities of\nLLMs or lacking clinical accuracy. This paper presents M4CXR, a multi-modal LLM\ndesigned to enhance CXR interpretation. The model is trained on a visual\ninstruction-following dataset that integrates various task-specific datasets in\na conversational format. As a result, the model supports multiple tasks such as\nmedical report generation (MRG), visual grounding, and visual question\nanswering (VQA). M4CXR achieves state-of-the-art clinical accuracy in MRG by\nemploying a chain-of-thought prompting strategy, in which it identifies\nfindings in CXR images and subsequently generates corresponding reports. The\nmodel is adaptable to various MRG scenarios depending on the available inputs,\nsuch as single-image, multi-image, and multi-study contexts. In addition to\nMRG, M4CXR performs visual grounding at a level comparable to specialized\nmodels and also demonstrates outstanding performance in VQA. Both quantitative\nand qualitative assessments reveal M4CXR's versatility in MRG, visual\ngrounding, and VQA, while consistently maintaining clinical accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of artificial intelligence, especially in large language\nmodels (LLMs), has significantly impacted various domains, including\nhealthcare. In chest X-ray (CXR) analysis, previous studies have employed LLMs,\nbut with limitations: either underutilizing the multi-tasking capabilities of\nLLMs or lacking clinical accuracy. This paper presents M4CXR, a multi-modal LLM\ndesigned to enhance CXR interpretation. The model is trained on a visual\ninstruction-following dataset that integrates various task-specific datasets in\na conversational format. As a result, the model supports multiple tasks such as\nmedical report generation (MRG), visual grounding, and visual question\nanswering (VQA). M4CXR achieves state-of-the-art clinical accuracy in MRG by\nemploying a chain-of-thought prompting strategy, in which it identifies\nfindings in CXR images and subsequently generates corresponding reports. The\nmodel is adaptable to various MRG scenarios depending on the available inputs,\nsuch as single-image, multi-image, and multi-study contexts. In addition to\nMRG, M4CXR performs visual grounding at a level comparable to specialized\nmodels and also demonstrates outstanding performance in VQA. Both quantitative\nand qualitative assessments reveal M4CXR's versatility in MRG, visual\ngrounding, and VQA, while consistently maintaining clinical accuracy."
                },
                "authors": [
                    {
                        "name": "Jonggwon Park"
                    },
                    {
                        "name": "Soobum Kim"
                    },
                    {
                        "name": "Byungmu Yoon"
                    },
                    {
                        "name": "Jihun Hyun"
                    },
                    {
                        "name": "Kyoyun Choi"
                    }
                ],
                "author_detail": {
                    "name": "Kyoyun Choi"
                },
                "author": "Kyoyun Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16208v1",
                "updated": "2024-08-29T02:03:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    3,
                    5,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:03:05Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    3,
                    5,
                    3,
                    242,
                    0
                ],
                "title": "ReXamine-Global: A Framework for Uncovering Inconsistencies in Radiology\n  Report Generation Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReXamine-Global: A Framework for Uncovering Inconsistencies in Radiology\n  Report Generation Metrics"
                },
                "summary": "Given the rapidly expanding capabilities of generative AI models for\nradiology, there is a need for robust metrics that can accurately measure the\nquality of AI-generated radiology reports across diverse hospitals. We develop\nReXamine-Global, a LLM-powered, multi-site framework that tests metrics across\ndifferent writing styles and patient populations, exposing gaps in their\ngeneralization. First, our method tests whether a metric is undesirably\nsensitive to reporting style, providing different scores depending on whether\nAI-generated reports are stylistically similar to ground-truth reports or not.\nSecond, our method measures whether a metric reliably agrees with experts, or\nwhether metric and expert scores of AI-generated report quality diverge for\nsome sites. Using 240 reports from 6 hospitals around the world, we apply\nReXamine-Global to 7 established report evaluation metrics and uncover serious\ngaps in their generalizability. Developers can apply ReXamine-Global when\ndesigning new report evaluation metrics, ensuring their robustness across\nsites. Additionally, our analysis of existing metrics can guide users of those\nmetrics towards evaluation procedures that work reliably at their sites of\ninterest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the rapidly expanding capabilities of generative AI models for\nradiology, there is a need for robust metrics that can accurately measure the\nquality of AI-generated radiology reports across diverse hospitals. We develop\nReXamine-Global, a LLM-powered, multi-site framework that tests metrics across\ndifferent writing styles and patient populations, exposing gaps in their\ngeneralization. First, our method tests whether a metric is undesirably\nsensitive to reporting style, providing different scores depending on whether\nAI-generated reports are stylistically similar to ground-truth reports or not.\nSecond, our method measures whether a metric reliably agrees with experts, or\nwhether metric and expert scores of AI-generated report quality diverge for\nsome sites. Using 240 reports from 6 hospitals around the world, we apply\nReXamine-Global to 7 established report evaluation metrics and uncover serious\ngaps in their generalizability. Developers can apply ReXamine-Global when\ndesigning new report evaluation metrics, ensuring their robustness across\nsites. Additionally, our analysis of existing metrics can guide users of those\nmetrics towards evaluation procedures that work reliably at their sites of\ninterest."
                },
                "authors": [
                    {
                        "name": "Oishi Banerjee"
                    },
                    {
                        "name": "Agustina Saenz"
                    },
                    {
                        "name": "Kay Wu"
                    },
                    {
                        "name": "Warren Clements"
                    },
                    {
                        "name": "Adil Zia"
                    },
                    {
                        "name": "Dominic Buensalido"
                    },
                    {
                        "name": "Helen Kavnoudias"
                    },
                    {
                        "name": "Alain S. Abi-Ghanem"
                    },
                    {
                        "name": "Nour El Ghawi"
                    },
                    {
                        "name": "Cibele Luna"
                    },
                    {
                        "name": "Patricia Castillo"
                    },
                    {
                        "name": "Khaled Al-Surimi"
                    },
                    {
                        "name": "Rayyan A. Daghistani"
                    },
                    {
                        "name": "Yuh-Min Chen"
                    },
                    {
                        "name": "Heng-sheng Chao"
                    },
                    {
                        "name": "Lars Heiliger"
                    },
                    {
                        "name": "Moon Kim"
                    },
                    {
                        "name": "Johannes Haubold"
                    },
                    {
                        "name": "Frederic Jonske"
                    },
                    {
                        "name": "Pranav Rajpurkar"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Rajpurkar"
                },
                "author": "Pranav Rajpurkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10064v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10064v3",
                "updated": "2024-08-29T01:44:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    1,
                    44,
                    8,
                    3,
                    242,
                    0
                ],
                "published": "2024-07-14T03:31:33Z",
                "published_parsed": [
                    2024,
                    7,
                    14,
                    3,
                    31,
                    33,
                    6,
                    196,
                    0
                ],
                "title": "Revolutionizing Bridge Operation and maintenance with LLM-based Agents:\n  An Overview of Applications and Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Bridge Operation and maintenance with LLM-based Agents:\n  An Overview of Applications and Insights"
                },
                "summary": "In various industrial fields of human social development, people have been\nexploring methods aimed at freeing human labor. Constructing LLM-based agents\nis considered to be one of the most effective tools to achieve this goal.\nAgent, as a kind of human-like intelligent entity with the ability of\nperception, planning, decision-making, and action, has created great production\nvalue in many fields. However, the bridge O\\&M field shows a relatively low\nlevel of intelligence compared to other industries. Nevertheless, the bridge\nO\\&M field has developed numerous intelligent inspection devices, machine\nlearning algorithms, and autonomous evaluation and decision-making methods,\nwhich provide a feasible basis for breakthroughs in artificial intelligence in\nthis field. The aim of this study is to explore the impact of AI bodies based\non large-scale language models on the field of bridge O\\&M and to analyze the\npotential challenges and opportunities it brings to the core tasks of bridge\nO\\&M. Through in-depth research and analysis, this paper expects to provide a\nmore comprehensive perspective for understanding the application of\nintelligentsia in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In various industrial fields of human social development, people have been\nexploring methods aimed at freeing human labor. Constructing LLM-based agents\nis considered to be one of the most effective tools to achieve this goal.\nAgent, as a kind of human-like intelligent entity with the ability of\nperception, planning, decision-making, and action, has created great production\nvalue in many fields. However, the bridge O\\&M field shows a relatively low\nlevel of intelligence compared to other industries. Nevertheless, the bridge\nO\\&M field has developed numerous intelligent inspection devices, machine\nlearning algorithms, and autonomous evaluation and decision-making methods,\nwhich provide a feasible basis for breakthroughs in artificial intelligence in\nthis field. The aim of this study is to explore the impact of AI bodies based\non large-scale language models on the field of bridge O\\&M and to analyze the\npotential challenges and opportunities it brings to the core tasks of bridge\nO\\&M. Through in-depth research and analysis, this paper expects to provide a\nmore comprehensive perspective for understanding the application of\nintelligentsia in this field."
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Lianzhen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lianzhen Zhang"
                },
                "author": "Lianzhen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10064v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10064v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16200v1",
                "updated": "2024-08-29T01:42:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    1,
                    42,
                    38,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T01:42:38Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    1,
                    42,
                    38,
                    3,
                    242,
                    0
                ],
                "title": "PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object\n  Detection in Bird's-Eye-View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object\n  Detection in Bird's-Eye-View"
                },
                "summary": "Recently, LSS-based multi-view 3D object detection provides an economical and\ndeployment-friendly solution for autonomous driving. However, all the existing\nLSS-based methods transform multi-view image features into a Cartesian\nBird's-Eye-View(BEV) representation, which does not take into account the\nnon-uniform image information distribution and hardly exploits the view\nsymmetry. In this paper, in order to adapt the image information distribution\nand preserve the view symmetry by regular convolution, we propose to employ the\npolar BEV representation to substitute the Cartesian BEV representation. To\nachieve this, we elaborately tailor three modules: a polar view transformer to\ngenerate the polar BEV representation, a polar temporal fusion module for\nfusing historical polar BEV features and a polar detection head to predict the\npolar-parameterized representation of the object. In addition, we design a 2D\nauxiliary detection head and a spatial attention enhancement module to improve\nthe quality of feature extraction in perspective view and BEV, respectively.\nFinally, we integrate the above improvements into a novel multi-view 3D object\ndetector, PolarBEVDet. Experiments on nuScenes show that PolarBEVDet achieves\nthe superior performance. The code is available at\nhttps://github.com/Yzichen/PolarBEVDet.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, LSS-based multi-view 3D object detection provides an economical and\ndeployment-friendly solution for autonomous driving. However, all the existing\nLSS-based methods transform multi-view image features into a Cartesian\nBird's-Eye-View(BEV) representation, which does not take into account the\nnon-uniform image information distribution and hardly exploits the view\nsymmetry. In this paper, in order to adapt the image information distribution\nand preserve the view symmetry by regular convolution, we propose to employ the\npolar BEV representation to substitute the Cartesian BEV representation. To\nachieve this, we elaborately tailor three modules: a polar view transformer to\ngenerate the polar BEV representation, a polar temporal fusion module for\nfusing historical polar BEV features and a polar detection head to predict the\npolar-parameterized representation of the object. In addition, we design a 2D\nauxiliary detection head and a spatial attention enhancement module to improve\nthe quality of feature extraction in perspective view and BEV, respectively.\nFinally, we integrate the above improvements into a novel multi-view 3D object\ndetector, PolarBEVDet. Experiments on nuScenes show that PolarBEVDet achieves\nthe superior performance. The code is available at\nhttps://github.com/Yzichen/PolarBEVDet.git."
                },
                "authors": [
                    {
                        "name": "Zichen Yu"
                    },
                    {
                        "name": "Quanli Liu"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Liyong Zhang"
                    },
                    {
                        "name": "Xiaoguang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoguang Zhao"
                },
                "author": "Xiaoguang Zhao",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16198v1",
                "updated": "2024-08-29T01:32:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    1,
                    32,
                    49,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T01:32:49Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    1,
                    32,
                    49,
                    3,
                    242,
                    0
                ],
                "title": "Chain-of-Experts (CoE): Reverse Engineering Software Bills of Materials\n  for JavaScript Application Bundles through Code Clone Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Experts (CoE): Reverse Engineering Software Bills of Materials\n  for JavaScript Application Bundles through Code Clone Search"
                },
                "summary": "A Software Bill of Materials (SBoM) is a detailed inventory of all\ncomponents, libraries, and modules in a software artifact, providing\ntraceability throughout the software supply chain. With the increasing\npopularity of JavaScript in software engineering due to its dynamic syntax and\nseamless supply chain integration, the exposure to vulnerabilities and attacks\nhas risen significantly. A JavaScript application bundle, which is a\nconsolidated, symbol-stripped, and optimized assembly of code for deployment\npurpose. Generating a SBoM from a JavaScript application bundle through a\nreverse-engineering process ensures the integrity, security, and compliance of\nthe supplier's software release, even without access to the original dependency\ngraphs.\n  This paper presents the first study on SBoM generation for JavaScript\napplication bundles. We identify three key challenges for this task, i.e.,\nnested code scopes, extremely long sequences, and large retrieval spaces. To\naddress these challenges, we introduce Chain-of-Experts (CoE), a multi-task\ndeep learning model designed to generate SBoMs through three tasks: code\nsegmentation, code classification, and code clone retrieval. We evaluate CoE\nagainst individual task-specific solutions on 500 web application bundles with\nover 66,000 dependencies. Our experimental results demonstrate that CoE offers\ncompetitive outcomes with less training and inference time when compared with\ncombined individual task-specific solutions. Consequently, CoE provides the\nfirst scalable, efficient, and end-to-end solution for the SBoM generation of\nreal-world JavaScript application bundles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Software Bill of Materials (SBoM) is a detailed inventory of all\ncomponents, libraries, and modules in a software artifact, providing\ntraceability throughout the software supply chain. With the increasing\npopularity of JavaScript in software engineering due to its dynamic syntax and\nseamless supply chain integration, the exposure to vulnerabilities and attacks\nhas risen significantly. A JavaScript application bundle, which is a\nconsolidated, symbol-stripped, and optimized assembly of code for deployment\npurpose. Generating a SBoM from a JavaScript application bundle through a\nreverse-engineering process ensures the integrity, security, and compliance of\nthe supplier's software release, even without access to the original dependency\ngraphs.\n  This paper presents the first study on SBoM generation for JavaScript\napplication bundles. We identify three key challenges for this task, i.e.,\nnested code scopes, extremely long sequences, and large retrieval spaces. To\naddress these challenges, we introduce Chain-of-Experts (CoE), a multi-task\ndeep learning model designed to generate SBoMs through three tasks: code\nsegmentation, code classification, and code clone retrieval. We evaluate CoE\nagainst individual task-specific solutions on 500 web application bundles with\nover 66,000 dependencies. Our experimental results demonstrate that CoE offers\ncompetitive outcomes with less training and inference time when compared with\ncombined individual task-specific solutions. Consequently, CoE provides the\nfirst scalable, efficient, and end-to-end solution for the SBoM generation of\nreal-world JavaScript application bundles."
                },
                "authors": [
                    {
                        "name": "Leo Song"
                    },
                    {
                        "name": "Steven H. H. Ding"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Li Tao Li"
                    },
                    {
                        "name": "Philippe Charland"
                    },
                    {
                        "name": "Andrew Walenstein"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Walenstein"
                },
                "author": "Andrew Walenstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13467v2",
                "updated": "2024-08-29T00:54:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    0,
                    54,
                    27,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-24T05:03:08Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    5,
                    3,
                    8,
                    5,
                    237,
                    0
                ],
                "title": "LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to\n  Small-Scale Local LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to\n  Small-Scale Local LLMs"
                },
                "summary": "The widespread adoption of cloud-based proprietary large language models\n(LLMs) has introduced significant challenges, including operational\ndependencies, privacy concerns, and the necessity of continuous internet\nconnectivity. In this work, we introduce an LLMOps pipeline, \"LlamaDuo\", for\nthe seamless migration of knowledge and abilities from service-oriented LLMs to\nsmaller, locally manageable models. This pipeline is crucial for ensuring\nservice continuity in the presence of operational failures, strict privacy\npolicies, or offline requirements. Our LlamaDuo involves fine-tuning a small\nlanguage model against the service LLM using a synthetic dataset generated by\nthe latter. If the performance of the fine-tuned model falls short of\nexpectations, it is enhanced by further fine-tuning with additional similar\ndata created by the service LLM. This iterative process guarantees that the\nsmaller model can eventually match or even surpass the service LLM's\ncapabilities in specific downstream tasks, offering a practical and scalable\nsolution for managing AI deployments in constrained environments. Extensive\nexperiments with leading edge LLMs are conducted to demonstrate the\neffectiveness, adaptability, and affordability of LlamaDuo across various\ndownstream tasks. Our pipeline implementation is available at\nhttps://github.com/deep-diver/llamaduo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of cloud-based proprietary large language models\n(LLMs) has introduced significant challenges, including operational\ndependencies, privacy concerns, and the necessity of continuous internet\nconnectivity. In this work, we introduce an LLMOps pipeline, \"LlamaDuo\", for\nthe seamless migration of knowledge and abilities from service-oriented LLMs to\nsmaller, locally manageable models. This pipeline is crucial for ensuring\nservice continuity in the presence of operational failures, strict privacy\npolicies, or offline requirements. Our LlamaDuo involves fine-tuning a small\nlanguage model against the service LLM using a synthetic dataset generated by\nthe latter. If the performance of the fine-tuned model falls short of\nexpectations, it is enhanced by further fine-tuning with additional similar\ndata created by the service LLM. This iterative process guarantees that the\nsmaller model can eventually match or even surpass the service LLM's\ncapabilities in specific downstream tasks, offering a practical and scalable\nsolution for managing AI deployments in constrained environments. Extensive\nexperiments with leading edge LLMs are conducted to demonstrate the\neffectiveness, adaptability, and affordability of LlamaDuo across various\ndownstream tasks. Our pipeline implementation is available at\nhttps://github.com/deep-diver/llamaduo."
                },
                "authors": [
                    {
                        "name": "Chansung Park"
                    },
                    {
                        "name": "Juyong Jiang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Sayak Paul"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "arxiv_comment": "28 pages, 18 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.15793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.15793v2",
                "updated": "2024-08-29T00:32:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    0,
                    32,
                    15,
                    3,
                    242,
                    0
                ],
                "published": "2023-07-28T20:25:11Z",
                "published_parsed": [
                    2023,
                    7,
                    28,
                    20,
                    25,
                    11,
                    4,
                    209,
                    0
                ],
                "title": "Summaries, Highlights, and Action items: Design, implementation and\n  evaluation of an LLM-powered meeting recap system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Summaries, Highlights, and Action items: Design, implementation and\n  evaluation of an LLM-powered meeting recap system"
                },
                "summary": "Meetings play a critical infrastructural role in the coordination of work. In\nrecent years, due to shift to hybrid and remote work, more meetings are moving\nto online Computer Mediated Spaces. This has led to new problems (e.g. more\ntime spent in less engaging meetings) and new opportunities (e.g. automated\ntranscription/captioning and recap support). Recent advances in large language\nmodels (LLMs) for dialog summarization have the potential to improve the\nexperience of meetings by reducing individuals' meeting load and increasing the\nclarity and alignment of meeting outputs. Despite this potential, they face\ntechnological limitation due to long transcripts and inability to capture\ndiverse recap needs based on user's context. To address these gaps, we design,\nimplement and evaluate in-context a meeting recap system. We first\nconceptualize two salient recap representations -- important highlights, and a\nstructured, hierarchical minutes view. We develop a system to operationalize\nthe representations with dialogue summarization as its building blocks.\nFinally, we evaluate the effectiveness of the system with seven users in the\ncontext of their work meetings. Our findings show promise in using LLM-based\ndialogue summarization for meeting recap and the need for both representations\nin different contexts. However, we find that LLM-based recap still lacks an\nunderstanding of whats personally relevant to participants, can miss important\ndetails, and mis-attributions can be detrimental to group dynamics. We identify\ncollaboration opportunities such as a shared recap document that a high quality\nrecap enables. We report on implications for designing AI systems to partner\nwith users to learn and improve from natural interactions to overcome the\nlimitations related to personal relevance and summarization quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meetings play a critical infrastructural role in the coordination of work. In\nrecent years, due to shift to hybrid and remote work, more meetings are moving\nto online Computer Mediated Spaces. This has led to new problems (e.g. more\ntime spent in less engaging meetings) and new opportunities (e.g. automated\ntranscription/captioning and recap support). Recent advances in large language\nmodels (LLMs) for dialog summarization have the potential to improve the\nexperience of meetings by reducing individuals' meeting load and increasing the\nclarity and alignment of meeting outputs. Despite this potential, they face\ntechnological limitation due to long transcripts and inability to capture\ndiverse recap needs based on user's context. To address these gaps, we design,\nimplement and evaluate in-context a meeting recap system. We first\nconceptualize two salient recap representations -- important highlights, and a\nstructured, hierarchical minutes view. We develop a system to operationalize\nthe representations with dialogue summarization as its building blocks.\nFinally, we evaluate the effectiveness of the system with seven users in the\ncontext of their work meetings. Our findings show promise in using LLM-based\ndialogue summarization for meeting recap and the need for both representations\nin different contexts. However, we find that LLM-based recap still lacks an\nunderstanding of whats personally relevant to participants, can miss important\ndetails, and mis-attributions can be detrimental to group dynamics. We identify\ncollaboration opportunities such as a shared recap document that a high quality\nrecap enables. We report on implications for designing AI systems to partner\nwith users to learn and improve from natural interactions to overcome the\nlimitations related to personal relevance and summarization quality."
                },
                "authors": [
                    {
                        "name": "Sumit Asthana"
                    },
                    {
                        "name": "Sagih Hilleli"
                    },
                    {
                        "name": "Pengcheng He"
                    },
                    {
                        "name": "Aaron Halfaker"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Halfaker"
                },
                "author": "Aaron Halfaker",
                "arxiv_comment": "in review for CSCW 24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.15793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.15793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16180v1",
                "updated": "2024-08-29T00:18:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    0,
                    18,
                    12,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T00:18:12Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    0,
                    18,
                    12,
                    3,
                    242,
                    0
                ],
                "title": "Benchmarking Japanese Speech Recognition on ASR-LLM Setups with\n  Multi-Pass Augmented Generative Error Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Japanese Speech Recognition on ASR-LLM Setups with\n  Multi-Pass Augmented Generative Error Correction"
                },
                "summary": "With the strong representational power of large language models (LLMs),\ngenerative error correction (GER) for automatic speech recognition (ASR) aims\nto provide semantic and phonetic refinements to address ASR errors. This work\nexplores how LLM-based GER can enhance and expand the capabilities of Japanese\nlanguage processing, presenting the first GER benchmark for Japanese ASR with\n0.9-2.6k text utterances. We also introduce a new multi-pass augmented\ngenerative error correction (MPA GER) by integrating multiple system hypotheses\non the input side with corrections from multiple LLMs on the output side and\nthen merging them. To the best of our knowledge, this is the first\ninvestigation of the use of LLMs for Japanese GER, which involves second-pass\nlanguage modeling on the output transcriptions generated by the ASR system\n(e.g., N-best hypotheses). Our experiments demonstrated performance improvement\nin the proposed methods of ASR quality and generalization both in SPREDS-U1-ja\nand CSJ data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the strong representational power of large language models (LLMs),\ngenerative error correction (GER) for automatic speech recognition (ASR) aims\nto provide semantic and phonetic refinements to address ASR errors. This work\nexplores how LLM-based GER can enhance and expand the capabilities of Japanese\nlanguage processing, presenting the first GER benchmark for Japanese ASR with\n0.9-2.6k text utterances. We also introduce a new multi-pass augmented\ngenerative error correction (MPA GER) by integrating multiple system hypotheses\non the input side with corrections from multiple LLMs on the output side and\nthen merging them. To the best of our knowledge, this is the first\ninvestigation of the use of LLMs for Japanese GER, which involves second-pass\nlanguage modeling on the output transcriptions generated by the ASR system\n(e.g., N-best hypotheses). Our experiments demonstrated performance improvement\nin the proposed methods of ASR quality and generalization both in SPREDS-U1-ja\nand CSJ data."
                },
                "authors": [
                    {
                        "name": "Yuka Ko"
                    },
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Chao-Han Huck Yang"
                    },
                    {
                        "name": "Tatsuya Kawahara"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuya Kawahara"
                },
                "author": "Tatsuya Kawahara",
                "arxiv_comment": "submitted to SLT2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16173v1",
                "updated": "2024-08-28T23:39:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    23,
                    39,
                    50,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T23:39:50Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    23,
                    39,
                    50,
                    2,
                    241,
                    0
                ],
                "title": "LLM-assisted Labeling Function Generation for Semantic Type Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-assisted Labeling Function Generation for Semantic Type Detection"
                },
                "summary": "Detecting semantic types of columns in data lake tables is an important\napplication. A key bottleneck in semantic type detection is the availability of\nhuman annotation due to the inherent complexity of data lakes. In this paper,\nwe propose using programmatic weak supervision to assist in annotating the\ntraining data for semantic type detection by leveraging labeling functions. One\nchallenge in this process is the difficulty of manually writing labeling\nfunctions due to the large volume and low quality of the data lake table\ndatasets. To address this issue, we explore employing Large Language Models\n(LLMs) for labeling function generation and introduce several prompt\nengineering strategies for this purpose. We conduct experiments on real-world\nweb table datasets. Based on the initial results, we perform extensive analysis\nand provide empirical insights and future directions for researchers in this\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting semantic types of columns in data lake tables is an important\napplication. A key bottleneck in semantic type detection is the availability of\nhuman annotation due to the inherent complexity of data lakes. In this paper,\nwe propose using programmatic weak supervision to assist in annotating the\ntraining data for semantic type detection by leveraging labeling functions. One\nchallenge in this process is the difficulty of manually writing labeling\nfunctions due to the large volume and low quality of the data lake table\ndatasets. To address this issue, we explore employing Large Language Models\n(LLMs) for labeling function generation and introduce several prompt\nengineering strategies for this purpose. We conduct experiments on real-world\nweb table datasets. Based on the initial results, we perform extensive analysis\nand provide empirical insights and future directions for researchers in this\nfield."
                },
                "authors": [
                    {
                        "name": "Chenjie Li"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Jin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jin Wang"
                },
                "author": "Jin Wang",
                "arxiv_comment": "VLDB'24-DATAI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02325v2",
                "updated": "2024-08-28T22:54:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    22,
                    54,
                    15,
                    2,
                    241,
                    0
                ],
                "published": "2024-04-02T21:51:39Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    21,
                    51,
                    39,
                    1,
                    93,
                    0
                ],
                "title": "Heat Death of Generative Models in Closed-Loop Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heat Death of Generative Models in Closed-Loop Learning"
                },
                "summary": "Improvement and adoption of generative machine learning models is rapidly\naccelerating, as exemplified by the popularity of LLMs (Large Language Models)\nfor text, and diffusion models for image generation. As generative models\nbecome widespread, data they generate is incorporated into shared content\nthrough the public web. This opens the question of what happens when data\ngenerated by a model is fed back to the model in subsequent training campaigns.\nThis is a question about the stability of the training process, whether the\ndistribution of publicly accessible content, which we refer to as \"knowledge\",\nremains stable or collapses.\n  Small scale empirical experiments reported in the literature show that this\nclosed-loop training process is prone to degenerating. Models may start\nproducing gibberish data, or sample from only a small subset of the desired\ndata distribution (a phenomenon referred to as mode collapse). So far there has\nbeen only limited theoretical understanding of this process, in part due to the\ncomplexity of the deep networks underlying these generative models.\n  The aim of this paper is to provide insights into this process (that we refer\nto as \"generative closed-loop learning\") by studying the learning dynamics of\ngenerative models that are fed back their own produced content in addition to\ntheir original training dataset. The sampling of many of these models can be\ncontrolled via a \"temperature\" parameter. Using dynamical systems tools, we\nshow that, unless a sufficient amount of external data is introduced at each\niteration, any non-trivial temperature leads the model to asymptotically\ndegenerate. In fact, either the generative distribution collapses to a small\nset of outputs or becomes uniform over a large set of outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improvement and adoption of generative machine learning models is rapidly\naccelerating, as exemplified by the popularity of LLMs (Large Language Models)\nfor text, and diffusion models for image generation. As generative models\nbecome widespread, data they generate is incorporated into shared content\nthrough the public web. This opens the question of what happens when data\ngenerated by a model is fed back to the model in subsequent training campaigns.\nThis is a question about the stability of the training process, whether the\ndistribution of publicly accessible content, which we refer to as \"knowledge\",\nremains stable or collapses.\n  Small scale empirical experiments reported in the literature show that this\nclosed-loop training process is prone to degenerating. Models may start\nproducing gibberish data, or sample from only a small subset of the desired\ndata distribution (a phenomenon referred to as mode collapse). So far there has\nbeen only limited theoretical understanding of this process, in part due to the\ncomplexity of the deep networks underlying these generative models.\n  The aim of this paper is to provide insights into this process (that we refer\nto as \"generative closed-loop learning\") by studying the learning dynamics of\ngenerative models that are fed back their own produced content in addition to\ntheir original training dataset. The sampling of many of these models can be\ncontrolled via a \"temperature\" parameter. Using dynamical systems tools, we\nshow that, unless a sufficient amount of external data is introduced at each\niteration, any non-trivial temperature leads the model to asymptotically\ndegenerate. In fact, either the generative distribution collapses to a small\nset of outputs or becomes uniform over a large set of outputs."
                },
                "authors": [
                    {
                        "name": "Matteo Marchi"
                    },
                    {
                        "name": "Stefano Soatto"
                    },
                    {
                        "name": "Pratik Chaudhari"
                    },
                    {
                        "name": "Paulo Tabuada"
                    }
                ],
                "author_detail": {
                    "name": "Paulo Tabuada"
                },
                "author": "Paulo Tabuada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16163v1",
                "updated": "2024-08-28T22:51:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    22,
                    51,
                    29,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T22:51:29Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    22,
                    51,
                    29,
                    2,
                    241,
                    0
                ],
                "title": "FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational\n  Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational\n  Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench"
                },
                "summary": "This paper introduces FRACTURED-SORRY-Bench, a framework for evaluating the\nsafety of Large Language Models (LLMs) against multi-turn conversational\nattacks. Building upon the SORRY-Bench dataset, we propose a simple yet\neffective method for generating adversarial prompts by breaking down harmful\nqueries into seemingly innocuous sub-questions. Our approach achieves a maximum\nincrease of +46.22\\% in Attack Success Rates (ASRs) across GPT-4, GPT-4o,\nGPT-4o-mini, and GPT-3.5-Turbo models compared to baseline methods. We\ndemonstrate that this technique poses a challenge to current LLM safety\nmeasures and highlights the need for more robust defenses against subtle,\nmulti-turn attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces FRACTURED-SORRY-Bench, a framework for evaluating the\nsafety of Large Language Models (LLMs) against multi-turn conversational\nattacks. Building upon the SORRY-Bench dataset, we propose a simple yet\neffective method for generating adversarial prompts by breaking down harmful\nqueries into seemingly innocuous sub-questions. Our approach achieves a maximum\nincrease of +46.22\\% in Attack Success Rates (ASRs) across GPT-4, GPT-4o,\nGPT-4o-mini, and GPT-3.5-Turbo models compared to baseline methods. We\ndemonstrate that this technique poses a challenge to current LLM safety\nmeasures and highlights the need for more robust defenses against subtle,\nmulti-turn attacks."
                },
                "authors": [
                    {
                        "name": "Aman Priyanshu"
                    },
                    {
                        "name": "Supriti Vijay"
                    }
                ],
                "author_detail": {
                    "name": "Supriti Vijay"
                },
                "author": "Supriti Vijay",
                "arxiv_comment": "4 pages, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.10994v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.10994v2",
                "updated": "2024-08-28T22:22:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    22,
                    22,
                    29,
                    2,
                    241,
                    0
                ],
                "published": "2023-05-18T14:14:42Z",
                "published_parsed": [
                    2023,
                    5,
                    18,
                    14,
                    14,
                    42,
                    3,
                    138,
                    0
                ],
                "title": "Graphical vs. Deep Generative Models: Measuring the Impact of\n  Differentially Private Mechanisms and Budgets on Utility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical vs. Deep Generative Models: Measuring the Impact of\n  Differentially Private Mechanisms and Budgets on Utility"
                },
                "summary": "Generative models trained with Differential Privacy (DP) can produce\nsynthetic data while reducing privacy risks. However, navigating their\nprivacy-utility tradeoffs makes finding the best models for specific\nsettings/tasks challenging. This paper bridges this gap by profiling how DP\ngenerative models for tabular data distribute privacy budgets across rows and\ncolumns, which is one of the primary sources of utility degradation. We compare\ngraphical and deep generative models, focusing on the key factors contributing\nto how privacy budgets are spent, i.e., underlying modeling techniques, DP\nmechanisms, and data dimensionality.\n  Through our measurement study, we shed light on the characteristics that make\ndifferent models suitable for various settings and tasks. For instance, we find\nthat graphical models distribute privacy budgets horizontally and thus cannot\nhandle relatively wide datasets for a fixed training time; also, the\nperformance on the task they were optimized for monotonically increases with\nmore data but could also overfit. Deep generative models spend their budgets\nper iteration, so their behavior is less predictable with varying dataset\ndimensions, but are more flexible as they could perform better if trained on\nmore features. Moreover, low levels of privacy ($\\epsilon\\geq100$) could help\nsome models generalize, achieving better results than without applying DP. We\nbelieve our work will aid the deployment of DP synthetic data techniques by\nnavigating through the best candidate models vis-a-vis the dataset features,\ndesired privacy levels, and downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models trained with Differential Privacy (DP) can produce\nsynthetic data while reducing privacy risks. However, navigating their\nprivacy-utility tradeoffs makes finding the best models for specific\nsettings/tasks challenging. This paper bridges this gap by profiling how DP\ngenerative models for tabular data distribute privacy budgets across rows and\ncolumns, which is one of the primary sources of utility degradation. We compare\ngraphical and deep generative models, focusing on the key factors contributing\nto how privacy budgets are spent, i.e., underlying modeling techniques, DP\nmechanisms, and data dimensionality.\n  Through our measurement study, we shed light on the characteristics that make\ndifferent models suitable for various settings and tasks. For instance, we find\nthat graphical models distribute privacy budgets horizontally and thus cannot\nhandle relatively wide datasets for a fixed training time; also, the\nperformance on the task they were optimized for monotonically increases with\nmore data but could also overfit. Deep generative models spend their budgets\nper iteration, so their behavior is less predictable with varying dataset\ndimensions, but are more flexible as they could perform better if trained on\nmore features. Moreover, low levels of privacy ($\\epsilon\\geq100$) could help\nsome models generalize, achieving better results than without applying DP. We\nbelieve our work will aid the deployment of DP synthetic data techniques by\nnavigating through the best candidate models vis-a-vis the dataset features,\ndesired privacy levels, and downstream tasks."
                },
                "authors": [
                    {
                        "name": "Georgi Ganev"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Emiliano De Cristofaro"
                    }
                ],
                "author_detail": {
                    "name": "Emiliano De Cristofaro"
                },
                "author": "Emiliano De Cristofaro",
                "arxiv_comment": "A shorter version of this paper appears in the Proceedings of the\n  31st ACM Conference on Computer and Communications Security (ACM CCS 2024).\n  This is the full version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.10994v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.10994v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16151v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16151v2",
                "updated": "2024-08-30T14:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    14,
                    17,
                    46,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-28T22:03:54Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    22,
                    3,
                    54,
                    2,
                    241,
                    0
                ],
                "title": "Automatic Library Migration Using Large Language Models: First Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Library Migration Using Large Language Models: First Results"
                },
                "summary": "Despite being introduced only a few years ago, Large Language Models (LLMs)\nare already widely used by developers for code generation. However, their\napplication in automating other Software Engineering activities remains largely\nunexplored. Thus, in this paper, we report the first results of a study in\nwhich we are exploring the use of ChatGPT to support API migration tasks, an\nimportant problem that demands manual effort and attention from developers.\nSpecifically, in the paper, we share our initial results involving the use of\nChatGPT to migrate a client application to use a newer version of SQLAlchemy,\nan ORM (Object Relational Mapping) library widely used in Python. We evaluate\nthe use of three types of prompts (Zero-Shot, One-Shot, and Chain Of Thoughts)\nand show that the best results are achieved by the One-Shot prompt, followed by\nthe Chain Of Thoughts. Particularly, with the One-Shot prompt we were able to\nsuccessfully migrate all columns of our target application and upgrade its code\nto use new functionalities enabled by SQLAlchemy's latest version, such as\nPython's asyncio and typing modules, while preserving the original code\nbehavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite being introduced only a few years ago, Large Language Models (LLMs)\nare already widely used by developers for code generation. However, their\napplication in automating other Software Engineering activities remains largely\nunexplored. Thus, in this paper, we report the first results of a study in\nwhich we are exploring the use of ChatGPT to support API migration tasks, an\nimportant problem that demands manual effort and attention from developers.\nSpecifically, in the paper, we share our initial results involving the use of\nChatGPT to migrate a client application to use a newer version of SQLAlchemy,\nan ORM (Object Relational Mapping) library widely used in Python. We evaluate\nthe use of three types of prompts (Zero-Shot, One-Shot, and Chain Of Thoughts)\nand show that the best results are achieved by the One-Shot prompt, followed by\nthe Chain Of Thoughts. Particularly, with the One-Shot prompt we were able to\nsuccessfully migrate all columns of our target application and upgrade its code\nto use new functionalities enabled by SQLAlchemy's latest version, such as\nPython's asyncio and typing modules, while preserving the original code\nbehavior."
                },
                "authors": [
                    {
                        "name": "Aylton Almeida"
                    },
                    {
                        "name": "Laerte Xavier"
                    },
                    {
                        "name": "Marco Tulio Valente"
                    }
                ],
                "author_detail": {
                    "name": "Marco Tulio Valente"
                },
                "author": "Marco Tulio Valente",
                "arxiv_doi": "10.1145/3674805.3690746",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3674805.3690746",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.16151v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16151v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16119v1",
                "updated": "2024-08-28T20:12:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    20,
                    12,
                    17,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T20:12:17Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    20,
                    12,
                    17,
                    2,
                    241,
                    0
                ],
                "title": "Data Formulator 2: Iteratively Creating Rich Visualizations with AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Formulator 2: Iteratively Creating Rich Visualizations with AI"
                },
                "summary": "To create rich visualizations, data analysts often need to iterate back and\nforth among data processing and chart specification to achieve their goals. To\nachieve this, analysts need not only proficiency in data transformation and\nvisualization tools but also efforts to manage the branching history consisting\nof many different versions of data and charts. Recent LLM-powered AI systems\nhave greatly improved visualization authoring experiences, for example by\nmitigating manual data transformation barriers via LLMs' code generation\nability. However, these systems do not work well for iterative visualization\nauthoring, because they often require analysts to provide, in a single turn, a\ntext-only prompt that fully describes the complex visualization task to be\nperformed, which is unrealistic to both users and models in many cases. In this\npaper, we present Data Formulator 2, an LLM-powered visualization system to\naddress these challenges. With Data Formulator 2, users describe their\nvisualization intent with blended UI and natural language inputs, and data\ntransformation are delegated to AI. To support iteration, Data Formulator 2\nlets users navigate their iteration history and reuse previous designs towards\nnew ones so that they don't need to start from scratch every time. In a user\nstudy with eight participants, we observed that Data Formulator 2 allows\nparticipants to develop their own iteration strategies to complete challenging\ndata exploration sessions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To create rich visualizations, data analysts often need to iterate back and\nforth among data processing and chart specification to achieve their goals. To\nachieve this, analysts need not only proficiency in data transformation and\nvisualization tools but also efforts to manage the branching history consisting\nof many different versions of data and charts. Recent LLM-powered AI systems\nhave greatly improved visualization authoring experiences, for example by\nmitigating manual data transformation barriers via LLMs' code generation\nability. However, these systems do not work well for iterative visualization\nauthoring, because they often require analysts to provide, in a single turn, a\ntext-only prompt that fully describes the complex visualization task to be\nperformed, which is unrealistic to both users and models in many cases. In this\npaper, we present Data Formulator 2, an LLM-powered visualization system to\naddress these challenges. With Data Formulator 2, users describe their\nvisualization intent with blended UI and natural language inputs, and data\ntransformation are delegated to AI. To support iteration, Data Formulator 2\nlets users navigate their iteration history and reuse previous designs towards\nnew ones so that they don't need to start from scratch every time. In a user\nstudy with eight participants, we observed that Data Formulator 2 allows\nparticipants to develop their own iteration strategies to complete challenging\ndata exploration sessions."
                },
                "authors": [
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Bongshin Lee"
                    },
                    {
                        "name": "Steven Drucker"
                    },
                    {
                        "name": "Dan Marshall"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11939v2",
                "updated": "2024-08-28T19:51:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    19,
                    51,
                    4,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-21T18:44:21Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    18,
                    44,
                    21,
                    2,
                    234,
                    0
                ],
                "title": "Matmul or No Matmul in the Era of 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matmul or No Matmul in the Era of 1-bit LLMs"
                },
                "summary": "The advent of 1-bit large language models (LLMs) has attracted considerable\nattention and opened up new research opportunities. However, 1-bit LLMs only\nimprove a fraction of models by applying extreme quantization to the projection\nlayers while leaving attention heads unchanged. Therefore, to avoid\nfundamentally wrong choices of goals in future research, it is crucial to\nunderstand the actual improvements in computation and memory usage that 1-bit\nLLMs can deliver. In this work, we present an adaptation of Amdahl's Law\ntailored for the 1-bit LLM context, which illustrates how partial improvements\nin 1-bit LLMs impact overall model performance. Through extensive experiments,\nwe uncover key nuances across different model architectures and hardware\nconfigurations, offering a roadmap for future research in the era of 1-bit\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of 1-bit large language models (LLMs) has attracted considerable\nattention and opened up new research opportunities. However, 1-bit LLMs only\nimprove a fraction of models by applying extreme quantization to the projection\nlayers while leaving attention heads unchanged. Therefore, to avoid\nfundamentally wrong choices of goals in future research, it is crucial to\nunderstand the actual improvements in computation and memory usage that 1-bit\nLLMs can deliver. In this work, we present an adaptation of Amdahl's Law\ntailored for the 1-bit LLM context, which illustrates how partial improvements\nin 1-bit LLMs impact overall model performance. Through extensive experiments,\nwe uncover key nuances across different model architectures and hardware\nconfigurations, offering a roadmap for future research in the era of 1-bit\nLLMs."
                },
                "authors": [
                    {
                        "name": "Jinendra Malekar"
                    },
                    {
                        "name": "Mohammed E. Elbtity"
                    },
                    {
                        "name": "Ramtin Zand"
                    }
                ],
                "author_detail": {
                    "name": "Ramtin Zand"
                },
                "author": "Ramtin Zand",
                "arxiv_comment": "Fixed Typo in title, Fixed typo in author name, fixed typo in\n  amdhal's law para",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16100v1",
                "updated": "2024-08-28T19:07:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    19,
                    7,
                    8,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T19:07:08Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    19,
                    7,
                    8,
                    2,
                    241,
                    0
                ],
                "title": "LLMSecCode: Evaluating Large Language Models for Secure Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMSecCode: Evaluating Large Language Models for Secure Coding"
                },
                "summary": "The rapid deployment of Large Language Models (LLMs) requires careful\nconsideration of their effect on cybersecurity. Our work aims to improve the\nselection process of LLMs that are suitable for facilitating Secure Coding\n(SC). This raises challenging research questions, such as (RQ1) Which\nfunctionality can streamline the LLM evaluation? (RQ2) What should the\nevaluation measure? (RQ3) How to attest that the evaluation process is\nimpartial? To address these questions, we introduce LLMSecCode, an open-source\nevaluation framework designed to assess LLM SC capabilities objectively.\n  We validate the LLMSecCode implementation through experiments. When varying\nparameters and prompts, we find a 10% and 9% difference in performance,\nrespectively. We also compare some results to reliable external actors, where\nour results show a 5% difference.\n  We strive to ensure the ease of use of our open-source framework and\nencourage further development by external actors. With LLMSecCode, we hope to\nencourage the standardization and benchmarking of LLMs' capabilities in\nsecurity-oriented code and tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid deployment of Large Language Models (LLMs) requires careful\nconsideration of their effect on cybersecurity. Our work aims to improve the\nselection process of LLMs that are suitable for facilitating Secure Coding\n(SC). This raises challenging research questions, such as (RQ1) Which\nfunctionality can streamline the LLM evaluation? (RQ2) What should the\nevaluation measure? (RQ3) How to attest that the evaluation process is\nimpartial? To address these questions, we introduce LLMSecCode, an open-source\nevaluation framework designed to assess LLM SC capabilities objectively.\n  We validate the LLMSecCode implementation through experiments. When varying\nparameters and prompts, we find a 10% and 9% difference in performance,\nrespectively. We also compare some results to reliable external actors, where\nour results show a 5% difference.\n  We strive to ensure the ease of use of our open-source framework and\nencourage further development by external actors. With LLMSecCode, we hope to\nencourage the standardization and benchmarking of LLMs' capabilities in\nsecurity-oriented code and tasks."
                },
                "authors": [
                    {
                        "name": "Anton Rydén"
                    },
                    {
                        "name": "Erik Näslund"
                    },
                    {
                        "name": "Elad Michael Schiller"
                    },
                    {
                        "name": "Magnus Almgren"
                    }
                ],
                "author_detail": {
                    "name": "Magnus Almgren"
                },
                "author": "Magnus Almgren",
                "arxiv_comment": "This manuscript serves as a complementary technical report to the\n  proceedings version, which will be presented at the International Symposium\n  on Cyber Security, Cryptography, and Machine Learning (CSCML) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16098v1",
                "updated": "2024-08-28T19:03:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    19,
                    3,
                    41,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T19:03:41Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    19,
                    3,
                    41,
                    2,
                    241,
                    0
                ],
                "title": "Structured Event Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Event Reasoning with Large Language Models"
                },
                "summary": "Reasoning about real-life events is a unifying challenge in AI and NLP that\nhas profound utility in a variety of domains, while fallacy in high-stake\napplications could be catastrophic. Able to work with diverse text in these\ndomains, large language models (LLMs) have proven capable of answering\nquestions and solving problems. However, I show that end-to-end LLMs still\nsystematically fail to reason about complex events, and they lack\ninterpretability due to their black-box nature. To address these issues, I\npropose three general approaches to use LLMs in conjunction with a structured\nrepresentation of events. The first is a language-based representation\ninvolving relations of sub-events that can be learned by LLMs via fine-tuning.\nThe second is a semi-symbolic representation involving states of entities that\ncan be predicted and leveraged by LLMs via few-shot prompting. The third is a\nfully symbolic representation that can be predicted by LLMs trained with\nstructured data and be executed by symbolic solvers. On a suite of event\nreasoning tasks spanning common-sense inference and planning, I show that each\napproach greatly outperforms end-to-end LLMs with more interpretability. These\nresults suggest manners of synergy between LLMs and structured representations\nfor event reasoning and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning about real-life events is a unifying challenge in AI and NLP that\nhas profound utility in a variety of domains, while fallacy in high-stake\napplications could be catastrophic. Able to work with diverse text in these\ndomains, large language models (LLMs) have proven capable of answering\nquestions and solving problems. However, I show that end-to-end LLMs still\nsystematically fail to reason about complex events, and they lack\ninterpretability due to their black-box nature. To address these issues, I\npropose three general approaches to use LLMs in conjunction with a structured\nrepresentation of events. The first is a language-based representation\ninvolving relations of sub-events that can be learned by LLMs via fine-tuning.\nThe second is a semi-symbolic representation involving states of entities that\ncan be predicted and leveraged by LLMs via few-shot prompting. The third is a\nfully symbolic representation that can be predicted by LLMs trained with\nstructured data and be executed by symbolic solvers. On a suite of event\nreasoning tasks spanning common-sense inference and planning, I show that each\napproach greatly outperforms end-to-end LLMs with more interpretability. These\nresults suggest manners of synergy between LLMs and structured representations\nfor event reasoning and beyond."
                },
                "authors": [
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "arxiv_comment": "PhD thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07877v2",
                "updated": "2024-08-28T19:01:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    19,
                    1,
                    23,
                    2,
                    241,
                    0
                ],
                "published": "2024-02-12T18:41:55Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    18,
                    41,
                    55,
                    0,
                    43,
                    0
                ],
                "title": "WildfireGPT: Tailored Large Language Model for Wildfire Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildfireGPT: Tailored Large Language Model for Wildfire Analysis"
                },
                "summary": "Recent advancement of large language models (LLMs) represents a\ntransformational capability at the frontier of artificial intelligence.\nHowever, LLMs are generalized models, trained on extensive text corpus, and\noften struggle to provide context-specific information, particularly in areas\nrequiring specialized knowledge, such as wildfire details within the broader\ncontext of climate change. For decision-makers focused on wildfire resilience\nand adaptation, it is crucial to obtain responses that are not only precise but\nalso domain-specific. To that end, we developed WildfireGPT, a prototype LLM\nagent designed to transform user queries into actionable insights on wildfire\nrisks. We enrich WildfireGPT by providing additional context, such as climate\nprojections and scientific literature, to ensure its information is current,\nrelevant, and scientifically accurate. This enables WildfireGPT to be an\neffective tool for delivering detailed, user-specific insights on wildfire\nrisks to support a diverse set of end users, including but not limited to\nresearchers and engineers, for making positive impact and decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancement of large language models (LLMs) represents a\ntransformational capability at the frontier of artificial intelligence.\nHowever, LLMs are generalized models, trained on extensive text corpus, and\noften struggle to provide context-specific information, particularly in areas\nrequiring specialized knowledge, such as wildfire details within the broader\ncontext of climate change. For decision-makers focused on wildfire resilience\nand adaptation, it is crucial to obtain responses that are not only precise but\nalso domain-specific. To that end, we developed WildfireGPT, a prototype LLM\nagent designed to transform user queries into actionable insights on wildfire\nrisks. We enrich WildfireGPT by providing additional context, such as climate\nprojections and scientific literature, to ensure its information is current,\nrelevant, and scientifically accurate. This enables WildfireGPT to be an\neffective tool for delivering detailed, user-specific insights on wildfire\nrisks to support a diverse set of end users, including but not limited to\nresearchers and engineers, for making positive impact and decision making."
                },
                "authors": [
                    {
                        "name": "Yangxinyu Xie"
                    },
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Tanwi Mallick"
                    },
                    {
                        "name": "Joshua David Bergerson"
                    },
                    {
                        "name": "John K. Hutchison"
                    },
                    {
                        "name": "Duane R. Verner"
                    },
                    {
                        "name": "Jordan Branham"
                    },
                    {
                        "name": "M. Ross Alexander"
                    },
                    {
                        "name": "Robert B. Ross"
                    },
                    {
                        "name": "Yan Feng"
                    },
                    {
                        "name": "Leslie-Anne Levy"
                    },
                    {
                        "name": "Weijie Su"
                    },
                    {
                        "name": "Camillo J. Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Camillo J. Taylor"
                },
                "author": "Camillo J. Taylor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16090v1",
                "updated": "2024-08-28T18:44:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    18,
                    44,
                    2,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T18:44:02Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    18,
                    44,
                    2,
                    2,
                    241,
                    0
                ],
                "title": "EPO: Hierarchical LLM Agents with Environment Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPO: Hierarchical LLM Agents with Environment Preference Optimization"
                },
                "summary": "Long-horizon decision-making tasks present significant challenges for\nLLM-based agents due to the need for extensive planning over multiple steps. In\nthis paper, we propose a hierarchical framework that decomposes complex tasks\ninto manageable subgoals, utilizing separate LLMs for subgoal prediction and\nlow-level action generation. To address the challenge of creating training\nsignals for unannotated datasets, we develop a reward model that leverages\nmultimodal environment feedback to automatically generate reward signals. We\nintroduce Environment Preference Optimization (EPO), a novel method that\ngenerates preference signals from the environment's feedback and uses them to\ntrain LLM-based agents. Extensive experiments on ALFRED demonstrate the\nstate-of-the-art performance of our framework, achieving first place on the\nALFRED public leaderboard and showcasing its potential to improve long-horizon\ndecision-making in diverse environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-horizon decision-making tasks present significant challenges for\nLLM-based agents due to the need for extensive planning over multiple steps. In\nthis paper, we propose a hierarchical framework that decomposes complex tasks\ninto manageable subgoals, utilizing separate LLMs for subgoal prediction and\nlow-level action generation. To address the challenge of creating training\nsignals for unannotated datasets, we develop a reward model that leverages\nmultimodal environment feedback to automatically generate reward signals. We\nintroduce Environment Preference Optimization (EPO), a novel method that\ngenerates preference signals from the environment's feedback and uses them to\ntrain LLM-based agents. Extensive experiments on ALFRED demonstrate the\nstate-of-the-art performance of our framework, achieving first place on the\nALFRED public leaderboard and showcasing its potential to improve long-horizon\ndecision-making in diverse environments."
                },
                "authors": [
                    {
                        "name": "Qi Zhao"
                    },
                    {
                        "name": "Haotian Fu"
                    },
                    {
                        "name": "Chen Sun"
                    },
                    {
                        "name": "George Konidaris"
                    }
                ],
                "author_detail": {
                    "name": "George Konidaris"
                },
                "author": "George Konidaris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16081v1",
                "updated": "2024-08-28T18:25:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    18,
                    25,
                    35,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T18:25:35Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    18,
                    25,
                    35,
                    2,
                    241,
                    0
                ],
                "title": "Logic-Enhanced Language Model Agents for Trustworthy Social Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic-Enhanced Language Model Agents for Trustworthy Social Simulations"
                },
                "summary": "We introduce the Logic-Enhanced Language Model Agents (LELMA) framework, a\nnovel approach to enhance the trustworthiness of social simulations that\nutilize large language models (LLMs). While LLMs have gained attention as\nagents for simulating human behaviour, their applicability in this role is\nlimited by issues such as inherent hallucinations and logical inconsistencies.\nLELMA addresses these challenges by integrating LLMs with symbolic AI, enabling\nlogical verification of the reasoning generated by LLMs. This verification\nprocess provides corrective feedback, refining the reasoning output. The\nframework consists of three main components: an LLM-Reasoner for producing\nstrategic reasoning, an LLM-Translator for mapping natural language reasoning\nto logic queries, and a Solver for evaluating these queries. This study focuses\non decision-making in game-theoretic scenarios as a model of human interaction.\nExperiments involving the Hawk-Dove game, Prisoner's Dilemma, and Stag Hunt\nhighlight the limitations of state-of-the-art LLMs, GPT-4 Omni and Gemini 1.0\nPro, in producing correct reasoning in these contexts. LELMA demonstrates high\naccuracy in error detection and improves the reasoning correctness of LLMs via\nself-refinement, particularly in GPT-4 Omni.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Logic-Enhanced Language Model Agents (LELMA) framework, a\nnovel approach to enhance the trustworthiness of social simulations that\nutilize large language models (LLMs). While LLMs have gained attention as\nagents for simulating human behaviour, their applicability in this role is\nlimited by issues such as inherent hallucinations and logical inconsistencies.\nLELMA addresses these challenges by integrating LLMs with symbolic AI, enabling\nlogical verification of the reasoning generated by LLMs. This verification\nprocess provides corrective feedback, refining the reasoning output. The\nframework consists of three main components: an LLM-Reasoner for producing\nstrategic reasoning, an LLM-Translator for mapping natural language reasoning\nto logic queries, and a Solver for evaluating these queries. This study focuses\non decision-making in game-theoretic scenarios as a model of human interaction.\nExperiments involving the Hawk-Dove game, Prisoner's Dilemma, and Stag Hunt\nhighlight the limitations of state-of-the-art LLMs, GPT-4 Omni and Gemini 1.0\nPro, in producing correct reasoning in these contexts. LELMA demonstrates high\naccuracy in error detection and improves the reasoning correctness of LLMs via\nself-refinement, particularly in GPT-4 Omni."
                },
                "authors": [
                    {
                        "name": "Agnieszka Mensfelt"
                    },
                    {
                        "name": "Kostas Stathis"
                    },
                    {
                        "name": "Vince Trencsenyi"
                    }
                ],
                "author_detail": {
                    "name": "Vince Trencsenyi"
                },
                "author": "Vince Trencsenyi",
                "arxiv_comment": "Source code: https://github.com/dicelab-rhul/LELMA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16073v1",
                "updated": "2024-08-28T18:14:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    18,
                    14,
                    39,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T18:14:39Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    18,
                    14,
                    39,
                    2,
                    241,
                    0
                ],
                "title": "Using Large Language Models to Create AI Personas for Replication and\n  Prediction of Media Effects: An Empirical Test of 133 Published Experimental\n  Research Findings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models to Create AI Personas for Replication and\n  Prediction of Media Effects: An Empirical Test of 133 Published Experimental\n  Research Findings"
                },
                "summary": "This report analyzes the potential for large language models (LLMs) to\nexpedite accurate replication of published message effects studies. We tested\nLLM-powered participants (personas) by replicating 133 experimental findings\nfrom 14 papers containing 45 recent studies in the Journal of Marketing\n(January 2023-May 2024). We used a new software tool, Viewpoints AI\n(https://viewpoints.ai/), that takes study designs, stimuli, and measures as\ninput, automatically generates prompts for LLMs to act as a specified sample of\nunique personas, and collects their responses to produce a final output in the\nform of a complete dataset and statistical analysis. The underlying LLM used\nwas Anthropic's Claude Sonnet 3.5. We generated 19,447 AI personas to replicate\nthese studies with the exact same sample attributes, study designs, stimuli,\nand measures reported in the original human research. Our LLM replications\nsuccessfully reproduced 76% of the original main effects (84 out of 111),\ndemonstrating strong potential for AI-assisted replication of studies in which\npeople respond to media stimuli. When including interaction effects, the\noverall replication rate was 68% (90 out of 133). The use of LLMs to replicate\nand accelerate marketing research on media effects is discussed with respect to\nthe replication crisis in social science, potential solutions to\ngeneralizability problems in sampling subjects and experimental conditions, and\nthe ability to rapidly test consumer responses to various media stimuli. We\nalso address the limitations of this approach, particularly in replicating\ncomplex interaction effects in media response studies, and suggest areas for\nfuture research and improvement in AI-assisted experimental replication of\nmedia effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report analyzes the potential for large language models (LLMs) to\nexpedite accurate replication of published message effects studies. We tested\nLLM-powered participants (personas) by replicating 133 experimental findings\nfrom 14 papers containing 45 recent studies in the Journal of Marketing\n(January 2023-May 2024). We used a new software tool, Viewpoints AI\n(https://viewpoints.ai/), that takes study designs, stimuli, and measures as\ninput, automatically generates prompts for LLMs to act as a specified sample of\nunique personas, and collects their responses to produce a final output in the\nform of a complete dataset and statistical analysis. The underlying LLM used\nwas Anthropic's Claude Sonnet 3.5. We generated 19,447 AI personas to replicate\nthese studies with the exact same sample attributes, study designs, stimuli,\nand measures reported in the original human research. Our LLM replications\nsuccessfully reproduced 76% of the original main effects (84 out of 111),\ndemonstrating strong potential for AI-assisted replication of studies in which\npeople respond to media stimuli. When including interaction effects, the\noverall replication rate was 68% (90 out of 133). The use of LLMs to replicate\nand accelerate marketing research on media effects is discussed with respect to\nthe replication crisis in social science, potential solutions to\ngeneralizability problems in sampling subjects and experimental conditions, and\nthe ability to rapidly test consumer responses to various media stimuli. We\nalso address the limitations of this approach, particularly in replicating\ncomplex interaction effects in media response studies, and suggest areas for\nfuture research and improvement in AI-assisted experimental replication of\nmedia effects."
                },
                "authors": [
                    {
                        "name": "Leo Yeykelis"
                    },
                    {
                        "name": "Kaavya Pichai"
                    },
                    {
                        "name": "James J. Cummings"
                    },
                    {
                        "name": "Byron Reeves"
                    }
                ],
                "author_detail": {
                    "name": "Byron Reeves"
                },
                "author": "Byron Reeves",
                "arxiv_comment": "24 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15998v1",
                "updated": "2024-08-28T17:59:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    59,
                    31,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:59:31Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    59,
                    31,
                    2,
                    241,
                    0
                ],
                "title": "Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of\n  Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of\n  Encoders"
                },
                "summary": "The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.\nModels and code: https://github.com/NVlabs/Eagle",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.\nModels and code: https://github.com/NVlabs/Eagle"
                },
                "authors": [
                    {
                        "name": "Min Shi"
                    },
                    {
                        "name": "Fuxiao Liu"
                    },
                    {
                        "name": "Shihao Wang"
                    },
                    {
                        "name": "Shijia Liao"
                    },
                    {
                        "name": "Subhashree Radhakrishnan"
                    },
                    {
                        "name": "De-An Huang"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Karan Sapra"
                    },
                    {
                        "name": "Yaser Yacoob"
                    },
                    {
                        "name": "Humphrey Shi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Andrew Tao"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Guilin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Guilin Liu"
                },
                "author": "Guilin Liu",
                "arxiv_comment": "Github: https://github.com/NVlabs/Eagle, HuggingFace:\n  https://huggingface.co/NVEagle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15978v1",
                "updated": "2024-08-28T17:49:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    49,
                    29,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:49:29Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    49,
                    29,
                    2,
                    241,
                    0
                ],
                "title": "WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task\n  Execution with Strategic Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task\n  Execution with Strategic Exploration"
                },
                "summary": "LLM-based autonomous agents often fail to execute complex web tasks that\nrequire dynamic interaction due to the inherent uncertainty and complexity of\nthese environments. Existing LLM-based web agents typically rely on rigid,\nexpert-designed policies specific to certain states and actions, which lack the\nflexibility and generalizability needed to adapt to unseen tasks. In contrast,\nhumans excel by exploring unknowns, continuously adapting strategies, and\nresolving ambiguities through exploration. To emulate human-like adaptability,\nweb agents need strategic exploration and complex decision-making. Monte Carlo\nTree Search (MCTS) is well-suited for this, but classical MCTS struggles with\nvast action spaces, unpredictable state transitions, and incomplete information\nin web tasks. In light of this, we develop WebPilot, a multi-agent system with\na dual optimization strategy that improves MCTS to better handle complex web\nenvironments. Specifically, the Global Optimization phase involves generating a\nhigh-level plan by breaking down tasks into manageable subtasks and\ncontinuously refining this plan, thereby focusing the search process and\nmitigating the challenges posed by vast action spaces in classical MCTS.\nSubsequently, the Local Optimization phase executes each subtask using a\ntailored MCTS designed for complex environments, effectively addressing\nuncertainties and managing incomplete information. Experimental results on\nWebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, on\nWebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93%\nrelative increase in success rate over the concurrent tree search-based method.\nWebPilot marks a significant advancement in general autonomous agent\ncapabilities, paving the way for more advanced and reliable decision-making in\npractical environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based autonomous agents often fail to execute complex web tasks that\nrequire dynamic interaction due to the inherent uncertainty and complexity of\nthese environments. Existing LLM-based web agents typically rely on rigid,\nexpert-designed policies specific to certain states and actions, which lack the\nflexibility and generalizability needed to adapt to unseen tasks. In contrast,\nhumans excel by exploring unknowns, continuously adapting strategies, and\nresolving ambiguities through exploration. To emulate human-like adaptability,\nweb agents need strategic exploration and complex decision-making. Monte Carlo\nTree Search (MCTS) is well-suited for this, but classical MCTS struggles with\nvast action spaces, unpredictable state transitions, and incomplete information\nin web tasks. In light of this, we develop WebPilot, a multi-agent system with\na dual optimization strategy that improves MCTS to better handle complex web\nenvironments. Specifically, the Global Optimization phase involves generating a\nhigh-level plan by breaking down tasks into manageable subtasks and\ncontinuously refining this plan, thereby focusing the search process and\nmitigating the challenges posed by vast action spaces in classical MCTS.\nSubsequently, the Local Optimization phase executes each subtask using a\ntailored MCTS designed for complex environments, effectively addressing\nuncertainties and managing incomplete information. Experimental results on\nWebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, on\nWebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93%\nrelative increase in success rate over the concurrent tree search-based method.\nWebPilot marks a significant advancement in general autonomous agent\ncapabilities, paving the way for more advanced and reliable decision-making in\npractical environments."
                },
                "authors": [
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Zijian Ma"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Zhen Han"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Volker Tresp"
                    }
                ],
                "author_detail": {
                    "name": "Volker Tresp"
                },
                "author": "Volker Tresp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15971v1",
                "updated": "2024-08-28T17:43:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    43,
                    55,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:43:55Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    43,
                    55,
                    2,
                    241,
                    0
                ],
                "title": "BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition\n  Capabilities of Language Models in Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition\n  Capabilities of Language Models in Multi-Agent Systems"
                },
                "summary": "Large Language Models (LLMs) are becoming increasingly powerful and capable\nof handling complex tasks, e.g., building single agents and multi-agent\nsystems. Compared to single agents, multi-agent systems have higher\nrequirements for the collaboration capabilities of language models. Many\nbenchmarks are proposed to evaluate their collaborative abilities. However,\nthese benchmarks lack fine-grained evaluations of LLM collaborative\ncapabilities. Additionally, multi-agent collaborative and competitive scenarios\nare ignored in existing works. To address these two problems, we propose a\nbenchmark, called BattleAgentBench, which defines seven sub-stages of three\nvarying difficulty levels and conducts a fine-grained evaluation of language\nmodels in terms of single-agent scenario navigation capabilities, paired-agent\ntask execution abilities, and multi-agent collaboration and competition\ncapabilities. We conducted extensive evaluations on leading four closed-source\nand seven open-source models. Experimental results indicate that API-based\nmodels perform excellently on simple tasks but open-source small models\nstruggle with simple tasks. Regarding difficult tasks that require\ncollaborative and competitive abilities, although API-based models have\ndemonstrated some collaborative capabilities, there is still enormous room for\nimprovement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming increasingly powerful and capable\nof handling complex tasks, e.g., building single agents and multi-agent\nsystems. Compared to single agents, multi-agent systems have higher\nrequirements for the collaboration capabilities of language models. Many\nbenchmarks are proposed to evaluate their collaborative abilities. However,\nthese benchmarks lack fine-grained evaluations of LLM collaborative\ncapabilities. Additionally, multi-agent collaborative and competitive scenarios\nare ignored in existing works. To address these two problems, we propose a\nbenchmark, called BattleAgentBench, which defines seven sub-stages of three\nvarying difficulty levels and conducts a fine-grained evaluation of language\nmodels in terms of single-agent scenario navigation capabilities, paired-agent\ntask execution abilities, and multi-agent collaboration and competition\ncapabilities. We conducted extensive evaluations on leading four closed-source\nand seven open-source models. Experimental results indicate that API-based\nmodels perform excellently on simple tasks but open-source small models\nstruggle with simple tasks. Regarding difficult tasks that require\ncollaborative and competitive abilities, although API-based models have\ndemonstrated some collaborative capabilities, there is still enormous room for\nimprovement."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Tao Feng"
                    },
                    {
                        "name": "Boyan Wang"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15970v1",
                "updated": "2024-08-28T17:43:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    43,
                    21,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:43:21Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    43,
                    21,
                    2,
                    241,
                    0
                ],
                "title": "Ain't How You Deploy: An Analysis of BGP Security Policies Performance\n  Against Various Attack Scenarios with Differing Deployment Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ain't How You Deploy: An Analysis of BGP Security Policies Performance\n  Against Various Attack Scenarios with Differing Deployment Strategies"
                },
                "summary": "This paper investigates the performance of various Border Gateway Protocol\n(BGP) security policies against multiple attack scenarios using different\ndeployment strategies. Through extensive simulations, we evaluate the\neffectiveness of defensive mechanisms such as Root Origin Validation (ROV),\nAutonomous System Provider Authorization (ASPA), and PeerROV across distinct AS\ndeployment types. Our findings reveal critical insights into the strengths and\nlimitations of current BGP security measures, providing guidance for future\npolicy development and implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the performance of various Border Gateway Protocol\n(BGP) security policies against multiple attack scenarios using different\ndeployment strategies. Through extensive simulations, we evaluate the\neffectiveness of defensive mechanisms such as Root Origin Validation (ROV),\nAutonomous System Provider Authorization (ASPA), and PeerROV across distinct AS\ndeployment types. Our findings reveal critical insights into the strengths and\nlimitations of current BGP security measures, providing guidance for future\npolicy development and implementation."
                },
                "authors": [
                    {
                        "name": "Seth Barrett"
                    },
                    {
                        "name": "Calvin Idom"
                    },
                    {
                        "name": "German Zavala Villafuerte"
                    },
                    {
                        "name": "Andrew Byers"
                    },
                    {
                        "name": "Berk Gulmezoglu"
                    }
                ],
                "author_detail": {
                    "name": "Berk Gulmezoglu"
                },
                "author": "Berk Gulmezoglu",
                "arxiv_comment": "8 pages, 1 table, 8 figures, submitted to and accepted by IEEE\n  ISNCC'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15966v1",
                "updated": "2024-08-28T17:38:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    38,
                    44,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:38:44Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    38,
                    44,
                    2,
                    241,
                    0
                ],
                "title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding"
                },
                "summary": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM."
                },
                "authors": [
                    {
                        "name": "Yuan Tang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Xianzhi Li"
                    },
                    {
                        "name": "Qiao Yu"
                    },
                    {
                        "name": "Jinfeng Xu"
                    },
                    {
                        "name": "Yixue Hao"
                    },
                    {
                        "name": "Long Hu"
                    },
                    {
                        "name": "Min Chen"
                    }
                ],
                "author_detail": {
                    "name": "Min Chen"
                },
                "author": "Min Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10260v2",
                "updated": "2024-08-28T17:26:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    26,
                    3,
                    2,
                    241,
                    0
                ],
                "published": "2024-06-11T01:16:10Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    1,
                    16,
                    10,
                    1,
                    163,
                    0
                ],
                "title": "Flextron: Many-in-One Flexible Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flextron: Many-in-One Flexible Large Language Model"
                },
                "summary": "Training modern LLMs is extremely resource intensive, and customizing them\nfor various deployment scenarios characterized by limited compute and memory\nresources through repeated training is impractical. In this paper, we introduce\nFlextron, a network architecture and post-training model optimization framework\nsupporting flexible model deployment. The Flextron architecture utilizes a\nnested elastic structure to rapidly adapt to specific user-defined latency and\naccuracy targets during inference with no additional fine-tuning required. It\nis also input-adaptive, and can automatically route tokens through its\nsub-networks for improved performance and efficiency. We present a\nsample-efficient training method and associated routing algorithms for\nsystematically transforming an existing trained LLM into a Flextron model. We\nevaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate\nsuperior performance over multiple end-to-end trained variants and other\nstate-of-the-art elastic networks, all with a single pretraining run that\nconsumes a mere 7.63% tokens compared to original pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training modern LLMs is extremely resource intensive, and customizing them\nfor various deployment scenarios characterized by limited compute and memory\nresources through repeated training is impractical. In this paper, we introduce\nFlextron, a network architecture and post-training model optimization framework\nsupporting flexible model deployment. The Flextron architecture utilizes a\nnested elastic structure to rapidly adapt to specific user-defined latency and\naccuracy targets during inference with no additional fine-tuning required. It\nis also input-adaptive, and can automatically route tokens through its\nsub-networks for improved performance and efficiency. We present a\nsample-efficient training method and associated routing algorithms for\nsystematically transforming an existing trained LLM into a Flextron model. We\nevaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate\nsuperior performance over multiple end-to-end trained variants and other\nstate-of-the-art elastic networks, all with a single pretraining run that\nconsumes a mere 7.63% tokens compared to original pretraining."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15950v1",
                "updated": "2024-08-28T17:08:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    8,
                    56,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:08:56Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    8,
                    56,
                    2,
                    241,
                    0
                ],
                "title": "Atari-GPT: Investigating the Capabilities of Multimodal Large Language\n  Models as Low-Level Policies for Atari Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atari-GPT: Investigating the Capabilities of Multimodal Large Language\n  Models as Low-Level Policies for Atari Games"
                },
                "summary": "Recent advancements in large language models (LLMs) have expanded their\ncapabilities beyond traditional text-based tasks to multimodal domains,\nintegrating visual, auditory, and textual data. While multimodal LLMs have been\nextensively explored for high-level planning in domains like robotics and\ngames, their potential as low-level controllers remains largely untapped. This\npaper explores the application of multimodal LLMs as low-level controllers in\nthe domain of Atari video games, introducing Atari game performance as a new\nbenchmark for evaluating the ability of multimodal LLMs to perform low-level\ncontrol tasks. Unlike traditional reinforcement learning (RL) and imitation\nlearning (IL) methods that require extensive computational resources as well as\nreward function specification, these LLMs utilize pre-existing multimodal\nknowledge to directly engage with game environments. Our study assesses\nmultiple multimodal LLMs performance against traditional RL agents, human\nplayers, and random agents, focusing on their ability to understand and\ninteract with complex visual scenes and formulate strategic responses.\nAdditionally, we examine the impact of In-Context Learning (ICL) by\nincorporating human-demonstrated game-play trajectories to enhance the models\ncontextual understanding. Through this investigation, we aim to determine the\nextent to which multimodal LLMs can leverage their extensive training to\neffectively function as low-level controllers, thereby redefining potential\napplications in dynamic and visually complex environments. Additional results\nand videos are available at our project webpage:\nhttps://sites.google.com/view/atari-gpt/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have expanded their\ncapabilities beyond traditional text-based tasks to multimodal domains,\nintegrating visual, auditory, and textual data. While multimodal LLMs have been\nextensively explored for high-level planning in domains like robotics and\ngames, their potential as low-level controllers remains largely untapped. This\npaper explores the application of multimodal LLMs as low-level controllers in\nthe domain of Atari video games, introducing Atari game performance as a new\nbenchmark for evaluating the ability of multimodal LLMs to perform low-level\ncontrol tasks. Unlike traditional reinforcement learning (RL) and imitation\nlearning (IL) methods that require extensive computational resources as well as\nreward function specification, these LLMs utilize pre-existing multimodal\nknowledge to directly engage with game environments. Our study assesses\nmultiple multimodal LLMs performance against traditional RL agents, human\nplayers, and random agents, focusing on their ability to understand and\ninteract with complex visual scenes and formulate strategic responses.\nAdditionally, we examine the impact of In-Context Learning (ICL) by\nincorporating human-demonstrated game-play trajectories to enhance the models\ncontextual understanding. Through this investigation, we aim to determine the\nextent to which multimodal LLMs can leverage their extensive training to\neffectively function as low-level controllers, thereby redefining potential\napplications in dynamic and visually complex environments. Additional results\nand videos are available at our project webpage:\nhttps://sites.google.com/view/atari-gpt/."
                },
                "authors": [
                    {
                        "name": "Nicholas R. Waytowich"
                    },
                    {
                        "name": "Devin White"
                    },
                    {
                        "name": "MD Sunbeam"
                    },
                    {
                        "name": "Vinicius G. Goecks"
                    }
                ],
                "author_detail": {
                    "name": "Vinicius G. Goecks"
                },
                "author": "Vinicius G. Goecks",
                "arxiv_comment": "Currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15915v1",
                "updated": "2024-08-28T16:28:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    28,
                    7,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T16:28:07Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    28,
                    7,
                    2,
                    241,
                    0
                ],
                "title": "Leveraging Open Knowledge for Advancing Task Expertise in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Open Knowledge for Advancing Task Expertise in Large Language\n  Models"
                },
                "summary": "The cultivation of expertise for large language models (LLMs) to solve tasks\nof specific areas often requires special-purpose tuning with calibrated\nbehaviors on the expected stable outputs. To avoid huge cost brought by manual\npreparation of instruction datasets and training resources up to hundreds of\nhours, the exploitation of open knowledge including a wealth of low rank\nadaptation (LoRA) models and instruction datasets serves as a good starting\npoint. However, existing methods on model and data selection focus on the\nperformance of general-purpose capabilities while neglecting the knowledge gap\nexposed in domain-specific deployment. In the present study, we propose to\nbridge such gap by introducing few human-annotated samples (i.e., K-shot) for\nadvancing task expertise of LLMs with open knowledge. Specifically, we develop\nan efficient and scalable pipeline to cost-efficiently produce task experts\nwhere K-shot data intervene in selecting the most promising expert candidates\nand the task-relevant instructions. A mixture-of-expert (MoE) system is built\nto make the best use of individual-yet-complementary knowledge between multiple\nexperts. We unveil the two keys to the success of a MoE system, 1) the abidance\nby K-shot, and 2) the insistence on diversity. For the former, we ensure that\nmodels that truly possess problem-solving abilities on K-shot are selected\nrather than those blind guessers. Besides, during data selection, instructions\nthat share task-relevant contexts with K-shot are prioritized. For the latter,\nwe highlight the diversity of constituting experts and that of the fine-tuning\ninstructions throughout the model and data selection process. Extensive\nexperimental results confirm the superiority of our approach over existing\nmethods on utilization of open knowledge across various tasks. Codes and models\nwill be released later.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cultivation of expertise for large language models (LLMs) to solve tasks\nof specific areas often requires special-purpose tuning with calibrated\nbehaviors on the expected stable outputs. To avoid huge cost brought by manual\npreparation of instruction datasets and training resources up to hundreds of\nhours, the exploitation of open knowledge including a wealth of low rank\nadaptation (LoRA) models and instruction datasets serves as a good starting\npoint. However, existing methods on model and data selection focus on the\nperformance of general-purpose capabilities while neglecting the knowledge gap\nexposed in domain-specific deployment. In the present study, we propose to\nbridge such gap by introducing few human-annotated samples (i.e., K-shot) for\nadvancing task expertise of LLMs with open knowledge. Specifically, we develop\nan efficient and scalable pipeline to cost-efficiently produce task experts\nwhere K-shot data intervene in selecting the most promising expert candidates\nand the task-relevant instructions. A mixture-of-expert (MoE) system is built\nto make the best use of individual-yet-complementary knowledge between multiple\nexperts. We unveil the two keys to the success of a MoE system, 1) the abidance\nby K-shot, and 2) the insistence on diversity. For the former, we ensure that\nmodels that truly possess problem-solving abilities on K-shot are selected\nrather than those blind guessers. Besides, during data selection, instructions\nthat share task-relevant contexts with K-shot are prioritized. For the latter,\nwe highlight the diversity of constituting experts and that of the fine-tuning\ninstructions throughout the model and data selection process. Extensive\nexperimental results confirm the superiority of our approach over existing\nmethods on utilization of open knowledge across various tasks. Codes and models\nwill be released later."
                },
                "authors": [
                    {
                        "name": "Yuncheng Yang"
                    },
                    {
                        "name": "Yulei Qin"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Zihan Xu"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Pengcheng Guo"
                    },
                    {
                        "name": "Hang Shao"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Yun Gu"
                    }
                ],
                "author_detail": {
                    "name": "Yun Gu"
                },
                "author": "Yun Gu",
                "arxiv_comment": "28 pages, 12 tables, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.11844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.11844v3",
                "updated": "2024-08-28T16:26:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    26,
                    16,
                    2,
                    241,
                    0
                ],
                "published": "2023-11-20T15:34:45Z",
                "published_parsed": [
                    2023,
                    11,
                    20,
                    15,
                    34,
                    45,
                    0,
                    324,
                    0
                ],
                "title": "Towards Human-Level Text Coding with LLMs: The Case of Fatherhood Roles\n  in Public Policy Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Human-Level Text Coding with LLMs: The Case of Fatherhood Roles\n  in Public Policy Documents"
                },
                "summary": "Recent advances in large language models (LLMs) like GPT-3.5 and GPT-4\npromise automation with better results and less programming, opening up new\nopportunities for text analysis in political science. In this study, we\nevaluate LLMs on three original coding tasks involving typical complexities\nencountered in political science settings: a non-English language, legal and\npolitical jargon, and complex labels based on abstract constructs. Along the\npaper, we propose a practical workflow to optimize the choice of the model and\nthe prompt. We find that the best prompting strategy consists of providing the\nLLMs with a detailed codebook, as the one provided to human coders. In this\nsetting, an LLM can be as good as or possibly better than a human annotator\nwhile being much faster, considerably cheaper, and much easier to scale to\nlarge amounts of text. We also provide a comparison of GPT and popular\nopen-source LLMs, discussing the trade-offs in the model's choice. Our software\nallows LLMs to be easily used as annotators and is publicly available:\nhttps://github.com/lorelupo/pappa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) like GPT-3.5 and GPT-4\npromise automation with better results and less programming, opening up new\nopportunities for text analysis in political science. In this study, we\nevaluate LLMs on three original coding tasks involving typical complexities\nencountered in political science settings: a non-English language, legal and\npolitical jargon, and complex labels based on abstract constructs. Along the\npaper, we propose a practical workflow to optimize the choice of the model and\nthe prompt. We find that the best prompting strategy consists of providing the\nLLMs with a detailed codebook, as the one provided to human coders. In this\nsetting, an LLM can be as good as or possibly better than a human annotator\nwhile being much faster, considerably cheaper, and much easier to scale to\nlarge amounts of text. We also provide a comparison of GPT and popular\nopen-source LLMs, discussing the trade-offs in the model's choice. Our software\nallows LLMs to be easily used as annotators and is publicly available:\nhttps://github.com/lorelupo/pappa."
                },
                "authors": [
                    {
                        "name": "Lorenzo Lupo"
                    },
                    {
                        "name": "Oscar Magnusson"
                    },
                    {
                        "name": "Dirk Hovy"
                    },
                    {
                        "name": "Elin Naurin"
                    },
                    {
                        "name": "Lena Wängnerud"
                    }
                ],
                "author_detail": {
                    "name": "Lena Wängnerud"
                },
                "author": "Lena Wängnerud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.11844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.11844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15907v1",
                "updated": "2024-08-28T16:20:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    20,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T16:20:45Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    20,
                    45,
                    2,
                    241,
                    0
                ],
                "title": "Decentralized LLM Inference over Edge Networks with Energy Harvesting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized LLM Inference over Edge Networks with Energy Harvesting"
                },
                "summary": "Large language models have significantly transformed multiple fields with\ntheir exceptional performance in natural language tasks, but their deployment\nin resource-constrained environments like edge networks presents an ongoing\nchallenge. Decentralized techniques for inference have emerged, distributing\nthe model blocks among multiple devices to improve flexibility and cost\neffectiveness. However, energy limitations remain a significant concern for\nedge devices. We propose a sustainable model for collaborative inference on\ninterconnected, battery-powered edge devices with energy harvesting. A\nsemi-Markov model is developed to describe the states of the devices,\nconsidering processing parameters and average green energy arrivals. This\ninforms the design of scheduling algorithms that aim to minimize device\ndowntimes and maximize network throughput. Through empirical evaluations and\nsimulated runs, we validate the effectiveness of our approach, paving the way\nfor energy-efficient decentralized inference over edge networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have significantly transformed multiple fields with\ntheir exceptional performance in natural language tasks, but their deployment\nin resource-constrained environments like edge networks presents an ongoing\nchallenge. Decentralized techniques for inference have emerged, distributing\nthe model blocks among multiple devices to improve flexibility and cost\neffectiveness. However, energy limitations remain a significant concern for\nedge devices. We propose a sustainable model for collaborative inference on\ninterconnected, battery-powered edge devices with energy harvesting. A\nsemi-Markov model is developed to describe the states of the devices,\nconsidering processing parameters and average green energy arrivals. This\ninforms the design of scheduling algorithms that aim to minimize device\ndowntimes and maximize network throughput. Through empirical evaluations and\nsimulated runs, we validate the effectiveness of our approach, paving the way\nfor energy-efficient decentralized inference over edge networks."
                },
                "authors": [
                    {
                        "name": "Aria Khoshsirat"
                    },
                    {
                        "name": "Giovanni Perin"
                    },
                    {
                        "name": "Michele Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Michele Rossi"
                },
                "author": "Michele Rossi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15903v1",
                "updated": "2024-08-28T16:15:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    15,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T16:15:45Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    15,
                    45,
                    2,
                    241,
                    0
                ],
                "title": "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration\n  in Evolving Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration\n  in Evolving Environments"
                },
                "summary": "The rapid obsolescence of information in Large Language Models (LLMs) has\ndriven the development of various techniques to incorporate new facts. However,\nexisting methods for knowledge editing still face difficulties with multi-hop\nquestions that require accurate fact identification and sequential logical\nreasoning, particularly among numerous fact updates. To tackle these\nchallenges, this paper introduces Graph Memory-based Editing for Large Language\nModels (GMeLLo), a straitforward and effective method that merges the explicit\nknowledge representation of Knowledge Graphs (KGs) with the linguistic\nflexibility of LLMs. Beyond merely leveraging LLMs for question answering,\nGMeLLo employs these models to convert free-form language into structured\nqueries and fact triples, facilitating seamless interaction with KGs for rapid\nupdates and precise multi-hop reasoning. Our results show that GMeLLo\nsignificantly surpasses current state-of-the-art knowledge editing methods in\nthe multi-hop question answering benchmark, MQuAKE, especially in scenarios\nwith extensive knowledge edits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid obsolescence of information in Large Language Models (LLMs) has\ndriven the development of various techniques to incorporate new facts. However,\nexisting methods for knowledge editing still face difficulties with multi-hop\nquestions that require accurate fact identification and sequential logical\nreasoning, particularly among numerous fact updates. To tackle these\nchallenges, this paper introduces Graph Memory-based Editing for Large Language\nModels (GMeLLo), a straitforward and effective method that merges the explicit\nknowledge representation of Knowledge Graphs (KGs) with the linguistic\nflexibility of LLMs. Beyond merely leveraging LLMs for question answering,\nGMeLLo employs these models to convert free-form language into structured\nqueries and fact triples, facilitating seamless interaction with KGs for rapid\nupdates and precise multi-hop reasoning. Our results show that GMeLLo\nsignificantly surpasses current state-of-the-art knowledge editing methods in\nthe multi-hop question answering benchmark, MQuAKE, especially in scenarios\nwith extensive knowledge edits."
                },
                "authors": [
                    {
                        "name": "Ruirui Chen"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Ishaan Singh Rawal"
                    },
                    {
                        "name": "Cheston Tan"
                    },
                    {
                        "name": "Dongkyu Choi"
                    },
                    {
                        "name": "Bo Xiong"
                    },
                    {
                        "name": "Bo Ai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Ai"
                },
                "author": "Bo Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15895v1",
                "updated": "2024-08-28T16:05:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    5,
                    20,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T16:05:20Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    5,
                    20,
                    2,
                    241,
                    0
                ],
                "title": "Bias in LLMs as Annotators: The Effect of Party Cues on Labelling\n  Decision by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in LLMs as Annotators: The Effect of Party Cues on Labelling\n  Decision by Large Language Models"
                },
                "summary": "Human coders are biased. We test similar biases in Large Language Models\n(LLMs) as annotators. By replicating an experiment run by Ennser-Jedenastik and\nMeyer (2018), we find evidence that LLMs use political information, and\nspecifically party cues, to judge political statements. Not only do LLMs use\nrelevant information to contextualize whether a statement is positive,\nnegative, or neutral based on the party cue, they also reflect the biases of\nthe human-generated data upon which they have been trained. We also find that\nunlike humans, who are only biased when faced with statements from extreme\nparties, LLMs exhibit significant bias even when prompted with statements from\ncenter-left and center-right parties. The implications of our findings are\ndiscussed in the conclusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human coders are biased. We test similar biases in Large Language Models\n(LLMs) as annotators. By replicating an experiment run by Ennser-Jedenastik and\nMeyer (2018), we find evidence that LLMs use political information, and\nspecifically party cues, to judge political statements. Not only do LLMs use\nrelevant information to contextualize whether a statement is positive,\nnegative, or neutral based on the party cue, they also reflect the biases of\nthe human-generated data upon which they have been trained. We also find that\nunlike humans, who are only biased when faced with statements from extreme\nparties, LLMs exhibit significant bias even when prompted with statements from\ncenter-left and center-right parties. The implications of our findings are\ndiscussed in the conclusion."
                },
                "authors": [
                    {
                        "name": "Sebastian Vallejo Vera"
                    },
                    {
                        "name": "Hunter Driggers"
                    }
                ],
                "author_detail": {
                    "name": "Hunter Driggers"
                },
                "author": "Hunter Driggers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07510v2",
                "updated": "2024-08-28T15:53:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    53,
                    4,
                    2,
                    241,
                    0
                ],
                "published": "2024-02-12T09:31:21Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    9,
                    31,
                    21,
                    0,
                    43,
                    0
                ],
                "title": "Secret Collusion among Generative AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secret Collusion among Generative AI Agents"
                },
                "summary": "Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models."
                },
                "authors": [
                    {
                        "name": "Sumeet Ramesh Motwani"
                    },
                    {
                        "name": "Mikhail Baranchuk"
                    },
                    {
                        "name": "Martin Strohmeier"
                    },
                    {
                        "name": "Vijay Bolina"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Lewis Hammond"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schroeder de Witt"
                },
                "author": "Christian Schroeder de Witt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15879v1",
                "updated": "2024-08-28T15:50:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    50,
                    41,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T15:50:41Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    50,
                    41,
                    2,
                    241,
                    0
                ],
                "title": "Persuasion Games using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persuasion Games using Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have emerged as formidable instruments capable\nof comprehending and producing human-like text. This paper explores the\npotential of LLMs, to shape human perspectives and subsequently influence their\ndecisions on particular tasks. This capability finds applications in diverse\ndomains such as Investment, Credit cards and Insurance, wherein they assist\nusers in selecting appropriate insurance policies, investment plans, Credit\ncards, Retail, as well as in Behavioral Change Support Systems (BCSS).\n  We present a sophisticated multi-agent framework wherein a consortium of\nagents operate in collaborative manner. The primary agent engages directly with\nusers through persuasive dialogue, while the auxiliary agents perform tasks\nsuch as information retrieval, response analysis, development of persuasion\nstrategies, and validation of facts. Empirical evidence from our experiments\ndemonstrates that this collaborative methodology significantly enhances the\npersuasive efficacy of the LLM. We analyze user resistance to persuasive\nefforts continuously and counteract it by employing a combination of rule-based\nand LLM-based resistance-persuasion mapping techniques.\n  We employ simulated personas and generate conversations in insurance,\nbanking, and retail domains to evaluate the proficiency of large language\nmodels (LLMs) in recognizing, adjusting to, and influencing various personality\ntypes. Concurrently, we examine the resistance mechanisms employed by LLM\nsimulated personas. Persuasion is quantified via measurable surveys before and\nafter interaction, LLM-generated scores on conversation, and user decisions\n(purchase or non-purchase).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as formidable instruments capable\nof comprehending and producing human-like text. This paper explores the\npotential of LLMs, to shape human perspectives and subsequently influence their\ndecisions on particular tasks. This capability finds applications in diverse\ndomains such as Investment, Credit cards and Insurance, wherein they assist\nusers in selecting appropriate insurance policies, investment plans, Credit\ncards, Retail, as well as in Behavioral Change Support Systems (BCSS).\n  We present a sophisticated multi-agent framework wherein a consortium of\nagents operate in collaborative manner. The primary agent engages directly with\nusers through persuasive dialogue, while the auxiliary agents perform tasks\nsuch as information retrieval, response analysis, development of persuasion\nstrategies, and validation of facts. Empirical evidence from our experiments\ndemonstrates that this collaborative methodology significantly enhances the\npersuasive efficacy of the LLM. We analyze user resistance to persuasive\nefforts continuously and counteract it by employing a combination of rule-based\nand LLM-based resistance-persuasion mapping techniques.\n  We employ simulated personas and generate conversations in insurance,\nbanking, and retail domains to evaluate the proficiency of large language\nmodels (LLMs) in recognizing, adjusting to, and influencing various personality\ntypes. Concurrently, we examine the resistance mechanisms employed by LLM\nsimulated personas. Persuasion is quantified via measurable surveys before and\nafter interaction, LLM-generated scores on conversation, and user decisions\n(purchase or non-purchase)."
                },
                "authors": [
                    {
                        "name": "Ganesh Prasath Ramani"
                    },
                    {
                        "name": "Shirish Karande"
                    },
                    {
                        "name": "Santhosh V"
                    },
                    {
                        "name": "Yash Bhatia"
                    }
                ],
                "author_detail": {
                    "name": "Yash Bhatia"
                },
                "author": "Yash Bhatia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15867v1",
                "updated": "2024-08-28T15:35:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    35,
                    5,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T15:35:05Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    35,
                    5,
                    2,
                    241,
                    0
                ],
                "title": "Practical Challenges for Reliable RIS Deployment in Heterogeneous\n  Multi-Operator Multi-Band Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Challenges for Reliable RIS Deployment in Heterogeneous\n  Multi-Operator Multi-Band Networks"
                },
                "summary": "Reconfigurable intelligent surfaces (RISs) have been introduced as arrays of\nnearly passive elements with software-tunable electromagnetic properties to\ndynamically manipulate the reflection/transmission of radio signals. Research\nworks in this area are focused on two applications, namely {\\it user-assist}\nRIS aiming at tuning the RIS to enhance the quality-of-service (QoS) of target\nusers, and the {\\it malicious} RIS aiming for an attacker to degrade the QoS at\nvictim receivers through generating {\\it intended} destructive interference.\nWhile both user-assist and malicious RIS applications have been explored\nextensively, the impact of RIS deployments on imposing {\\it unintended}\ninterference on various wireless user-equipments (EUs) remains underexplored.\nThis paper investigates the challenges of integrating RISs into multi-carrier,\nmulti-user, and multi-operator networks. We discuss how RIS deployments\nintended to benefit specific users can negatively impact other users served at\nvarious carrier frequencies through different network operators. While not an\nideal solution, we discuss how ultra-narrowband metasurfaces can be\nincorporated into the manufacturing of RISs to mitigate some challenges of RIS\ndeployment in wireless networks. We also present a simulation scenario to\nilluminate some practical challenges associated with the deployment of RISs in\nshared public environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surfaces (RISs) have been introduced as arrays of\nnearly passive elements with software-tunable electromagnetic properties to\ndynamically manipulate the reflection/transmission of radio signals. Research\nworks in this area are focused on two applications, namely {\\it user-assist}\nRIS aiming at tuning the RIS to enhance the quality-of-service (QoS) of target\nusers, and the {\\it malicious} RIS aiming for an attacker to degrade the QoS at\nvictim receivers through generating {\\it intended} destructive interference.\nWhile both user-assist and malicious RIS applications have been explored\nextensively, the impact of RIS deployments on imposing {\\it unintended}\ninterference on various wireless user-equipments (EUs) remains underexplored.\nThis paper investigates the challenges of integrating RISs into multi-carrier,\nmulti-user, and multi-operator networks. We discuss how RIS deployments\nintended to benefit specific users can negatively impact other users served at\nvarious carrier frequencies through different network operators. While not an\nideal solution, we discuss how ultra-narrowband metasurfaces can be\nincorporated into the manufacturing of RISs to mitigate some challenges of RIS\ndeployment in wireless networks. We also present a simulation scenario to\nilluminate some practical challenges associated with the deployment of RISs in\nshared public environments."
                },
                "authors": [
                    {
                        "name": "Mehdi Monemi"
                    },
                    {
                        "name": "Mehdi Rasti"
                    },
                    {
                        "name": "Arthur S. de Sena"
                    },
                    {
                        "name": "Mohammad Amir Fallah"
                    },
                    {
                        "name": "Matti Latva-Aho"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15857v1",
                "updated": "2024-08-28T15:18:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    18,
                    46,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T15:18:46Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    18,
                    46,
                    2,
                    241,
                    0
                ],
                "title": "What is YOLOv8: An In-Depth Exploration of the Internal Features of the\n  Next-Generation Object Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is YOLOv8: An In-Depth Exploration of the Internal Features of the\n  Next-Generation Object Detector"
                },
                "summary": "This study presents a detailed analysis of the YOLOv8 object detection model,\nfocusing on its architecture, training techniques, and performance improvements\nover previous iterations like YOLOv5. Key innovations, including the CSPNet\nbackbone for enhanced feature extraction, the FPN+PAN neck for superior\nmulti-scale object detection, and the transition to an anchor-free approach,\nare thoroughly examined. The paper reviews YOLOv8's performance across\nbenchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy\nand real-time capabilities across diverse hardware platforms. Additionally, the\nstudy explores YOLOv8's developer-friendly enhancements, such as its unified\nPython package and CLI, which streamline model training and deployment.\nOverall, this research positions YOLOv8 as a state-of-the-art solution in the\nevolving object detection field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a detailed analysis of the YOLOv8 object detection model,\nfocusing on its architecture, training techniques, and performance improvements\nover previous iterations like YOLOv5. Key innovations, including the CSPNet\nbackbone for enhanced feature extraction, the FPN+PAN neck for superior\nmulti-scale object detection, and the transition to an anchor-free approach,\nare thoroughly examined. The paper reviews YOLOv8's performance across\nbenchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy\nand real-time capabilities across diverse hardware platforms. Additionally, the\nstudy explores YOLOv8's developer-friendly enhancements, such as its unified\nPython package and CLI, which streamline model training and deployment.\nOverall, this research positions YOLOv8 as a state-of-the-art solution in the\nevolving object detection field."
                },
                "authors": [
                    {
                        "name": "Muhammad Yaseen"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Yaseen"
                },
                "author": "Muhammad Yaseen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16536v1",
                "updated": "2024-08-28T15:15:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    15,
                    30,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T15:15:30Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    15,
                    30,
                    2,
                    241,
                    0
                ],
                "title": "Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data\n  Generation Toolkit for Auditing 3D Human Pose Estimators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data\n  Generation Toolkit for Auditing 3D Human Pose Estimators"
                },
                "summary": "The estimation of 3D human poses from images has progressed tremendously over\nthe last few years as measured on standard benchmarks. However, performance in\nthe open world remains underexplored, as current benchmarks cannot capture its\nfull extent. Especially in safety-critical systems, it is crucial that 3D pose\nestimators are audited before deployment, and their sensitivity towards single\nfactors or attributes occurring in the operational domain is thoroughly\nexamined. Nevertheless, we currently lack a benchmark that would enable such\nfine-grained analysis. We thus present STAGE, a GenAI data toolkit for auditing\n3D human pose estimators. We enable a text-to-image model to control the 3D\nhuman body pose in the generated image. This allows us to create customized\nannotated data covering a wide range of open-world attributes. We leverage\nSTAGE and generate a series of benchmarks to audit the sensitivity of popular\npose estimators towards attributes such as gender, ethnicity, age, clothing,\nlocation, and weather. Our results show that the presence of such naturally\noccurring attributes can cause severe degradation in the performance of pose\nestimators and leads us to question if they are ready for open-world\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The estimation of 3D human poses from images has progressed tremendously over\nthe last few years as measured on standard benchmarks. However, performance in\nthe open world remains underexplored, as current benchmarks cannot capture its\nfull extent. Especially in safety-critical systems, it is crucial that 3D pose\nestimators are audited before deployment, and their sensitivity towards single\nfactors or attributes occurring in the operational domain is thoroughly\nexamined. Nevertheless, we currently lack a benchmark that would enable such\nfine-grained analysis. We thus present STAGE, a GenAI data toolkit for auditing\n3D human pose estimators. We enable a text-to-image model to control the 3D\nhuman body pose in the generated image. This allows us to create customized\nannotated data covering a wide range of open-world attributes. We leverage\nSTAGE and generate a series of benchmarks to audit the sensitivity of popular\npose estimators towards attributes such as gender, ethnicity, age, clothing,\nlocation, and weather. Our results show that the presence of such naturally\noccurring attributes can cause severe degradation in the performance of pose\nestimators and leads us to question if they are ready for open-world\ndeployment."
                },
                "authors": [
                    {
                        "name": "Nikita Kister"
                    },
                    {
                        "name": "István Sárándi"
                    },
                    {
                        "name": "Anna Khoreva"
                    },
                    {
                        "name": "Gerard Pons-Moll"
                    }
                ],
                "author_detail": {
                    "name": "Gerard Pons-Moll"
                },
                "author": "Gerard Pons-Moll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01245v2",
                "updated": "2024-08-28T15:01:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    1,
                    4,
                    2,
                    241,
                    0
                ],
                "published": "2024-04-01T17:03:41Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    17,
                    3,
                    41,
                    0,
                    92,
                    0
                ],
                "title": "A Statistical Framework of Watermarks for Large Language Models: Pivot,\n  Detection Efficiency and Optimal Rules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Statistical Framework of Watermarks for Large Language Models: Pivot,\n  Detection Efficiency and Optimal Rules"
                },
                "summary": "Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Feng Ruan"
                    },
                    {
                        "name": "Huiyuan Wang"
                    },
                    {
                        "name": "Qi Long"
                    },
                    {
                        "name": "Weijie J. Su"
                    }
                ],
                "author_detail": {
                    "name": "Weijie J. Su"
                },
                "author": "Weijie J. Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00612v2",
                "updated": "2024-08-28T14:59:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    59,
                    31,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-01T14:52:04Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    14,
                    52,
                    4,
                    3,
                    214,
                    0
                ],
                "title": "Downstream bias mitigation is all you need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Downstream bias mitigation is all you need"
                },
                "summary": "The advent of transformer-based architectures and large language models\n(LLMs) have significantly advanced the performance of natural language\nprocessing (NLP) models. Since these LLMs are trained on huge corpuses of data\nfrom the web and other sources, there has been a major concern about harmful\nprejudices that may potentially be transferred from the data. In many\napplications, these pre-trained LLMs are fine-tuned on task specific datasets,\nwhich can further contribute to biases. This paper studies the extent of biases\nabsorbed by LLMs during pre-training as well as task-specific behaviour after\nfine-tuning. We found that controlled interventions on pre-trained LLMs, prior\nto fine-tuning, have minimal effect on lowering biases in classifiers. However,\nthe biases present in domain-specific datasets play a much bigger role, and\nhence mitigating them at this stage has a bigger impact. While pre-training\ndoes matter, but after the model has been pre-trained, even slight changes to\nco-occurrence rates in the fine-tuning dataset has a significant effect on the\nbias of the model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of transformer-based architectures and large language models\n(LLMs) have significantly advanced the performance of natural language\nprocessing (NLP) models. Since these LLMs are trained on huge corpuses of data\nfrom the web and other sources, there has been a major concern about harmful\nprejudices that may potentially be transferred from the data. In many\napplications, these pre-trained LLMs are fine-tuned on task specific datasets,\nwhich can further contribute to biases. This paper studies the extent of biases\nabsorbed by LLMs during pre-training as well as task-specific behaviour after\nfine-tuning. We found that controlled interventions on pre-trained LLMs, prior\nto fine-tuning, have minimal effect on lowering biases in classifiers. However,\nthe biases present in domain-specific datasets play a much bigger role, and\nhence mitigating them at this stage has a bigger impact. While pre-training\ndoes matter, but after the model has been pre-trained, even slight changes to\nco-occurrence rates in the fine-tuning dataset has a significant effect on the\nbias of the model."
                },
                "authors": [
                    {
                        "name": "Arkadeep Baksi"
                    },
                    {
                        "name": "Rahul Singh"
                    },
                    {
                        "name": "Tarun Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Tarun Joshi"
                },
                "author": "Tarun Joshi",
                "arxiv_comment": "arXiv admin note: This work has been withdrawn by arXiv\n  administrators due to inappropriate text reuse from external sources",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16696v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16696v3",
                "updated": "2024-08-28T14:54:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    54,
                    11,
                    2,
                    241,
                    0
                ],
                "published": "2024-02-26T16:11:03Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    16,
                    11,
                    3,
                    0,
                    57,
                    0
                ],
                "title": "Look Before You Leap: Towards Decision-Aware and Generalizable\n  Tool-Usage for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Look Before You Leap: Towards Decision-Aware and Generalizable\n  Tool-Usage for Large Language Models"
                },
                "summary": "Tool-augmented large language models (LLMs) are attracting widespread\nattention when accessing up-to-date knowledge and alleviating hallucination\nissues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated\nsurprising tool-usage capabilities through prompting and in-context learning\ntechniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in\nmanipulating tools, current efforts focus on either template-driven or\ntoken-triggered tool-usage. However, the former hampers LLMs' flexibility to\naddress diverse user's queries due to constrained tool interactions, while the\nlatter limits the generalizability when engaging with new tools, since\ntool-usage learning is based on task- and tool-specific datasets. To alleviate\nthese concerns, in this paper, we propose a decision-aware and generalizable\ntool-usage framework (DEER). Specifically, we first construct the tool-usage\nsamples with multiple decision branches via an automatic generation pipeline,\nthereby inspiring the decision-making awareness of LLMs under diverse\nscenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the\ngeneralizability of LLMs over unseen tools. Extensive experiments demonstrate\nthat our proposed DEER is effective and significantly outperforms baselines\nacross various datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-augmented large language models (LLMs) are attracting widespread\nattention when accessing up-to-date knowledge and alleviating hallucination\nissues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated\nsurprising tool-usage capabilities through prompting and in-context learning\ntechniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in\nmanipulating tools, current efforts focus on either template-driven or\ntoken-triggered tool-usage. However, the former hampers LLMs' flexibility to\naddress diverse user's queries due to constrained tool interactions, while the\nlatter limits the generalizability when engaging with new tools, since\ntool-usage learning is based on task- and tool-specific datasets. To alleviate\nthese concerns, in this paper, we propose a decision-aware and generalizable\ntool-usage framework (DEER). Specifically, we first construct the tool-usage\nsamples with multiple decision branches via an automatic generation pipeline,\nthereby inspiring the decision-making awareness of LLMs under diverse\nscenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the\ngeneralizability of LLMs over unseen tools. Extensive experiments demonstrate\nthat our proposed DEER is effective and significantly outperforms baselines\nacross various datasets."
                },
                "authors": [
                    {
                        "name": "Anchun Gui"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Dai"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Han Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Han Xiao"
                },
                "author": "Han Xiao",
                "arxiv_comment": "20 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16696v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16696v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15836v1",
                "updated": "2024-08-28T14:48:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    48,
                    37,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T14:48:37Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    48,
                    37,
                    2,
                    241,
                    0
                ],
                "title": "Knowledge Navigator: LLM-guided Browsing Framework for Exploratory\n  Search in Scientific Literature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Navigator: LLM-guided Browsing Framework for Exploratory\n  Search in Scientific Literature"
                },
                "summary": "The exponential growth of scientific literature necessitates advanced tools\nfor effective knowledge exploration. We present Knowledge Navigator, a system\ndesigned to enhance exploratory search abilities by organizing and structuring\nthe retrieved documents from broad topical queries into a navigable, two-level\nhierarchy of named and descriptive scientific topics and subtopics. This\nstructured organization provides an overall view of the research themes in a\ndomain, while also enabling iterative search and deeper knowledge discovery\nwithin specific subtopics by allowing users to refine their focus and retrieve\nadditional relevant documents. Knowledge Navigator combines LLM capabilities\nwith cluster-based methods to enable an effective browsing method. We\ndemonstrate our approach's effectiveness through automatic and manual\nevaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code,\nprompts, and benchmarks are made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of scientific literature necessitates advanced tools\nfor effective knowledge exploration. We present Knowledge Navigator, a system\ndesigned to enhance exploratory search abilities by organizing and structuring\nthe retrieved documents from broad topical queries into a navigable, two-level\nhierarchy of named and descriptive scientific topics and subtopics. This\nstructured organization provides an overall view of the research themes in a\ndomain, while also enabling iterative search and deeper knowledge discovery\nwithin specific subtopics by allowing users to refine their focus and retrieve\nadditional relevant documents. Knowledge Navigator combines LLM capabilities\nwith cluster-based methods to enable an effective browsing method. We\ndemonstrate our approach's effectiveness through automatic and manual\nevaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code,\nprompts, and benchmarks are made publicly available."
                },
                "authors": [
                    {
                        "name": "Uri Katz"
                    },
                    {
                        "name": "Mosh Levy"
                    },
                    {
                        "name": "Yoav Goldberg"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Goldberg"
                },
                "author": "Yoav Goldberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10155v2",
                "updated": "2024-08-28T14:38:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    38,
                    51,
                    2,
                    241,
                    0
                ],
                "published": "2024-04-15T22:02:58Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    22,
                    2,
                    58,
                    0,
                    106,
                    0
                ],
                "title": "The Fault in our Stars: Quality Assessment of Code Generation Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fault in our Stars: Quality Assessment of Code Generation Benchmarks"
                },
                "summary": "Large Language Models (LLMs) are gaining popularity among software engineers.\nA crucial aspect of developing effective code generation LLMs is to evaluate\nthese models using a robust benchmark. Evaluation benchmarks with quality\nissues can provide a false sense of performance. In this work, we conduct the\nfirst-of-its-kind study of the quality of prompts within benchmarks used to\ncompare the performance of different code generation models. To conduct this\nstudy, we analyzed 3,566 prompts from 9 code generation benchmarks to identify\nquality issues in them. We also investigated whether fixing the identified\nquality issues in the benchmarks' prompts affects a model's performance. We\nalso studied memorization issues of the evaluation dataset, which can put into\nquestion a benchmark's trustworthiness. We found that code generation\nevaluation benchmarks mainly focused on Python and coding exercises and had\nvery limited contextual dependencies to challenge the model. These datasets and\nthe developers' prompts suffer from quality issues like spelling and\ngrammatical errors, unclear sentences to express developers' intent, and not\nusing proper documentation style. Fixing all these issues in the benchmarks can\nlead to a better performance for Python code generation, but not a significant\nimprovement was observed for Java code generation. We also found evidence that\nGPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are gaining popularity among software engineers.\nA crucial aspect of developing effective code generation LLMs is to evaluate\nthese models using a robust benchmark. Evaluation benchmarks with quality\nissues can provide a false sense of performance. In this work, we conduct the\nfirst-of-its-kind study of the quality of prompts within benchmarks used to\ncompare the performance of different code generation models. To conduct this\nstudy, we analyzed 3,566 prompts from 9 code generation benchmarks to identify\nquality issues in them. We also investigated whether fixing the identified\nquality issues in the benchmarks' prompts affects a model's performance. We\nalso studied memorization issues of the evaluation dataset, which can put into\nquestion a benchmark's trustworthiness. We found that code generation\nevaluation benchmarks mainly focused on Python and coding exercises and had\nvery limited contextual dependencies to challenge the model. These datasets and\nthe developers' prompts suffer from quality issues like spelling and\ngrammatical errors, unclear sentences to express developers' intent, and not\nusing proper documentation style. Fixing all these issues in the benchmarks can\nlead to a better performance for Python code generation, but not a significant\nimprovement was observed for Java code generation. We also found evidence that\nGPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues."
                },
                "authors": [
                    {
                        "name": "Mohammed Latif Siddiq"
                    },
                    {
                        "name": "Simantika Dristi"
                    },
                    {
                        "name": "Joy Saha"
                    },
                    {
                        "name": "Joanna C. S. Santos"
                    }
                ],
                "author_detail": {
                    "name": "Joanna C. S. Santos"
                },
                "author": "Joanna C. S. Santos",
                "arxiv_comment": "Accepted at the 24th IEEE International Conference on Source Code\n  Analysis and Manipulation(SCAM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15815v1",
                "updated": "2024-08-28T14:24:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    24,
                    48,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T14:24:48Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    24,
                    48,
                    2,
                    241,
                    0
                ],
                "title": "MR-Adopt: Automatic Deduction of Input Transformation Function for\n  Metamorphic Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MR-Adopt: Automatic Deduction of Input Transformation Function for\n  Metamorphic Testing"
                },
                "summary": "While a recent study reveals that many developer-written test cases can\nencode a reusable Metamorphic Relation (MR), over 70% of them directly\nhard-code the source input and follow-up input in the encoded relation. Such\nencoded MRs, which do not contain an explicit input transformation to transform\nthe source inputs to corresponding follow-up inputs, cannot be reused with new\nsource inputs to enhance test adequacy.\n  In this paper, we propose MR-Adopt (Automatic Deduction Of inPut\nTransformation) to automatically deduce the input transformation from the\nhard-coded source and follow-up inputs, aiming to enable the encoded MRs to be\nreused with new source inputs. With typically only one pair of source and\nfollow-up inputs available in an MR-encoded test case as the example, we\nleveraged LLMs to understand the intention of the test case and generate\nadditional examples of source-followup input pairs. This helps to guide the\ngeneration of input transformations generalizable to multiple source inputs.\nBesides, to mitigate the issue that LLMs generate erroneous code, we refine\nLLM-generated transformations by removing MR- irrelevant code elements with\ndata-flow analysis. Finally, we assess candidate transformations based on\nencoded output relations and select the best transformation as the result.\nEvaluation results show that MR-Adopt can generate input transformations\napplicable to all experimental source inputs for 72.00% of encoded MRs, which\nis 33.33% more than using vanilla GPT-3.5. By incorporating MR- Adopt-generated\ninput transformations, encoded MR-based test cases can effectively enhance the\ntest adequacy, increasing the line coverage and mutation score by 10.62% and\n18.91%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While a recent study reveals that many developer-written test cases can\nencode a reusable Metamorphic Relation (MR), over 70% of them directly\nhard-code the source input and follow-up input in the encoded relation. Such\nencoded MRs, which do not contain an explicit input transformation to transform\nthe source inputs to corresponding follow-up inputs, cannot be reused with new\nsource inputs to enhance test adequacy.\n  In this paper, we propose MR-Adopt (Automatic Deduction Of inPut\nTransformation) to automatically deduce the input transformation from the\nhard-coded source and follow-up inputs, aiming to enable the encoded MRs to be\nreused with new source inputs. With typically only one pair of source and\nfollow-up inputs available in an MR-encoded test case as the example, we\nleveraged LLMs to understand the intention of the test case and generate\nadditional examples of source-followup input pairs. This helps to guide the\ngeneration of input transformations generalizable to multiple source inputs.\nBesides, to mitigate the issue that LLMs generate erroneous code, we refine\nLLM-generated transformations by removing MR- irrelevant code elements with\ndata-flow analysis. Finally, we assess candidate transformations based on\nencoded output relations and select the best transformation as the result.\nEvaluation results show that MR-Adopt can generate input transformations\napplicable to all experimental source inputs for 72.00% of encoded MRs, which\nis 33.33% more than using vanilla GPT-3.5. By incorporating MR- Adopt-generated\ninput transformations, encoded MR-based test cases can effectively enhance the\ntest adequacy, increasing the line coverage and mutation score by 10.62% and\n18.91%, respectively."
                },
                "authors": [
                    {
                        "name": "Congying Xu"
                    },
                    {
                        "name": "Songqiang Chen"
                    },
                    {
                        "name": "Jiarong Wu"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    },
                    {
                        "name": "Valerio Terragni"
                    },
                    {
                        "name": "Hengcheng Zhu"
                    },
                    {
                        "name": "Jialun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jialun Cao"
                },
                "author": "Jialun Cao",
                "arxiv_comment": "This paper is accepted to ASE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14511v2",
                "updated": "2024-08-28T14:13:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    13,
                    41,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-25T04:07:18Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    4,
                    7,
                    18,
                    6,
                    238,
                    0
                ],
                "title": "Unveiling the Statistical Foundations of Chain-of-Thought Prompting\n  Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Statistical Foundations of Chain-of-Thought Prompting\n  Methods"
                },
                "summary": "Chain-of-Thought (CoT) prompting and its variants have gained popularity as\neffective methods for solving multi-step reasoning problems using pretrained\nlarge language models (LLMs). In this work, we analyze CoT prompting from a\nstatistical estimation perspective, providing a comprehensive characterization\nof its sample complexity. To this end, we introduce a multi-step latent\nvariable model that encapsulates the reasoning process, where the latent\nvariable encodes the task information. Under this framework, we demonstrate\nthat when the pretraining dataset is sufficiently large, the estimator formed\nby CoT prompting is equivalent to a Bayesian estimator. This estimator\neffectively solves the multi-step reasoning problem by aggregating a posterior\ndistribution inferred from the demonstration examples in the prompt. Moreover,\nwe prove that the statistical error of the CoT estimator can be decomposed into\ntwo main components: (i) a prompting error, which arises from inferring the\ntrue task using CoT prompts, and (ii) the statistical error of the pretrained\nLLM. We establish that, under appropriate assumptions, the prompting error\ndecays exponentially to zero as the number of demonstrations increases.\nAdditionally, we explicitly characterize the approximation and generalization\nerrors of the pretrained LLM. Notably, we construct a transformer model that\napproximates the target distribution of the multi-step reasoning problem with\nan error that decreases exponentially in the number of transformer blocks. Our\nanalysis extends to other variants of CoT, including Self-Consistent CoT,\nTree-of-Thought, and Selection-Inference, offering a broad perspective on the\nefficacy of these methods. We also provide numerical experiments to validate\nthe theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting and its variants have gained popularity as\neffective methods for solving multi-step reasoning problems using pretrained\nlarge language models (LLMs). In this work, we analyze CoT prompting from a\nstatistical estimation perspective, providing a comprehensive characterization\nof its sample complexity. To this end, we introduce a multi-step latent\nvariable model that encapsulates the reasoning process, where the latent\nvariable encodes the task information. Under this framework, we demonstrate\nthat when the pretraining dataset is sufficiently large, the estimator formed\nby CoT prompting is equivalent to a Bayesian estimator. This estimator\neffectively solves the multi-step reasoning problem by aggregating a posterior\ndistribution inferred from the demonstration examples in the prompt. Moreover,\nwe prove that the statistical error of the CoT estimator can be decomposed into\ntwo main components: (i) a prompting error, which arises from inferring the\ntrue task using CoT prompts, and (ii) the statistical error of the pretrained\nLLM. We establish that, under appropriate assumptions, the prompting error\ndecays exponentially to zero as the number of demonstrations increases.\nAdditionally, we explicitly characterize the approximation and generalization\nerrors of the pretrained LLM. Notably, we construct a transformer model that\napproximates the target distribution of the multi-step reasoning problem with\nan error that decreases exponentially in the number of transformer blocks. Our\nanalysis extends to other variants of CoT, including Self-Consistent CoT,\nTree-of-Thought, and Selection-Inference, offering a broad perspective on the\nefficacy of these methods. We also provide numerical experiments to validate\nthe theoretical findings."
                },
                "authors": [
                    {
                        "name": "Xinyang Hu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Zhuoran Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhuoran Yang"
                },
                "author": "Zhuoran Yang",
                "arxiv_comment": "150 pages, 18 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]