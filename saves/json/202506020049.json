[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.23666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23666v1",
                "updated": "2025-05-29T17:12:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "LoLA: Low-Rank Linear Attention With Sparse Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLA: Low-Rank Linear Attention With Sparse Caching"
                },
                "summary": "Transformer-based large language models suffer from quadratic complexity at\ninference on long sequences. Linear attention methods are efficient\nalternatives, however, they fail to provide an accurate approximation of\nsoftmax attention. By additionally incorporating sliding window attention into\neach linear attention head, this gap can be closed for short context-length\ntasks. Unfortunately, these approaches cannot recall important information from\nlong contexts due to \"memory collisions\". In this paper , we propose LoLA:\nLow-rank Linear Attention with sparse caching. LoLA separately stores\nadditional key-value pairs that would otherwise interfere with past associative\nmemories. Moreover, LoLA further closes the gap between linear attention models\nand transformers by distributing past key-value pairs into three forms of\nmemory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. As an inference-only strategy, LoLA enables\npass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks\nfrom RULER. It boosts the accuracy of the base subquadratic model from 0.6% to\n97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1\n8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning\ntasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an\nextremely lightweight approach: Nearly all of our results can be reproduced on\na single consumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models suffer from quadratic complexity at\ninference on long sequences. Linear attention methods are efficient\nalternatives, however, they fail to provide an accurate approximation of\nsoftmax attention. By additionally incorporating sliding window attention into\neach linear attention head, this gap can be closed for short context-length\ntasks. Unfortunately, these approaches cannot recall important information from\nlong contexts due to \"memory collisions\". In this paper , we propose LoLA:\nLow-rank Linear Attention with sparse caching. LoLA separately stores\nadditional key-value pairs that would otherwise interfere with past associative\nmemories. Moreover, LoLA further closes the gap between linear attention models\nand transformers by distributing past key-value pairs into three forms of\nmemory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. As an inference-only strategy, LoLA enables\npass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks\nfrom RULER. It boosts the accuracy of the base subquadratic model from 0.6% to\n97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1\n8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning\ntasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an\nextremely lightweight approach: Nearly all of our results can be reproduced on\na single consumer GPU."
                },
                "authors": [
                    {
                        "name": "Luke McDermott"
                    },
                    {
                        "name": "Robert W. Heath Jr."
                    },
                    {
                        "name": "Rahul Parhi"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Parhi"
                },
                "author": "Rahul Parhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23520v1",
                "updated": "2025-05-29T14:59:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    59,
                    6,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:59:06Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    59,
                    6,
                    3,
                    149,
                    0
                ],
                "title": "AnchorAttention: Difference-Aware Sparse Attention with Stripe\n  Granularity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnchorAttention: Difference-Aware Sparse Attention with Stripe\n  Granularity"
                },
                "summary": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Dong Guo"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Guoliang Zhu"
                    },
                    {
                        "name": "Dian Ding"
                    },
                    {
                        "name": "Yiming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Zhang"
                },
                "author": "Yiming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23416v1",
                "updated": "2025-05-29T13:05:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T13:05:47Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction"
                },
                "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios."
                },
                "authors": [
                    {
                        "name": "Jang-Hyun Kim"
                    },
                    {
                        "name": "Jinuk Kim"
                    },
                    {
                        "name": "Sangwoo Kwon"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Hyun Oh Song"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Oh Song"
                },
                "author": "Hyun Oh Song",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21889v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21889v2",
                "updated": "2025-05-29T12:59:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    12,
                    59,
                    26,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-28T02:07:03Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    2,
                    7,
                    3,
                    2,
                    148,
                    0
                ],
                "title": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV\n  Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV\n  Cache Reuse"
                },
                "summary": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability. EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability. EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Hande Dong"
                    },
                    {
                        "name": "Yichong Leng"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Cheater Lin"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Xianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xianwei Zhang"
                },
                "author": "Xianwei Zhang",
                "arxiv_comment": "31st International European Conference on Parallel and Distributed\n  Computing (Euro-Par 2025 Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21889v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21889v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23351v1",
                "updated": "2025-05-29T11:16:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    11,
                    16,
                    18,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T11:16:18Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    11,
                    16,
                    18,
                    3,
                    149,
                    0
                ],
                "title": "Energy-Efficient QoS-Aware Scheduling for S-NUCA Many-Cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient QoS-Aware Scheduling for S-NUCA Many-Cores"
                },
                "summary": "Optimizing performance and energy efficiency in many-core processors,\nespecially within Non-Uniform Cache Access (NUCA) architectures, remains a\ncritical challenge. The performance heterogeneity inherent in S-NUCA systems\ncomplicates task scheduling due to varying cache access latencies across cores.\nThis paper introduces a novel QoS management policy to maintain application\nexecution within predefined Quality of Service (QoS) targets, measured using\nthe Application Heartbeats framework. QoS metrics like Heartbeats ensure\npredictable application performance in dynamic computing environments. The\nproposed policy dynamically controls QoS by orchestrating task migrations\nwithin the S-NUCA many-core system and adjusting the clock frequency of cores.\nAfter satisfying the QoS objectives, the policy optimizes energy efficiency,\nreducing overall system energy consumption without compromising performance\nconstraints. Our work leverages the state-of-the-art multi-/many-core simulator\n{\\em HotSniper}. We have extended it with two key components: an integrated\nheartbeat framework for precise, application-specific performance monitoring,\nand our QoS management policy that maintains application QoS requirements while\nminimizing the system's energy consumption. Experimental evaluations\ndemonstrate that our approach effectively maintains desired QoS levels and\nachieves 18.7\\% energy savings compared to state-of-the-art scheduling methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing performance and energy efficiency in many-core processors,\nespecially within Non-Uniform Cache Access (NUCA) architectures, remains a\ncritical challenge. The performance heterogeneity inherent in S-NUCA systems\ncomplicates task scheduling due to varying cache access latencies across cores.\nThis paper introduces a novel QoS management policy to maintain application\nexecution within predefined Quality of Service (QoS) targets, measured using\nthe Application Heartbeats framework. QoS metrics like Heartbeats ensure\npredictable application performance in dynamic computing environments. The\nproposed policy dynamically controls QoS by orchestrating task migrations\nwithin the S-NUCA many-core system and adjusting the clock frequency of cores.\nAfter satisfying the QoS objectives, the policy optimizes energy efficiency,\nreducing overall system energy consumption without compromising performance\nconstraints. Our work leverages the state-of-the-art multi-/many-core simulator\n{\\em HotSniper}. We have extended it with two key components: an integrated\nheartbeat framework for precise, application-specific performance monitoring,\nand our QoS management policy that maintains application QoS requirements while\nminimizing the system's energy consumption. Experimental evaluations\ndemonstrate that our approach effectively maintains desired QoS levels and\nachieves 18.7\\% energy savings compared to state-of-the-art scheduling methods."
                },
                "authors": [
                    {
                        "name": "Sudam M. Wasala"
                    },
                    {
                        "name": "Jurre Wolff"
                    },
                    {
                        "name": "Yixian Shen"
                    },
                    {
                        "name": "Anuj Pathania"
                    },
                    {
                        "name": "Clemens Grelck"
                    },
                    {
                        "name": "Andy D. Pimentel"
                    }
                ],
                "author_detail": {
                    "name": "Andy D. Pimentel"
                },
                "author": "Andy D. Pimentel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23275v1",
                "updated": "2025-05-29T09:23:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    23,
                    11,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T09:23:11Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    23,
                    11,
                    3,
                    149,
                    0
                ],
                "title": "Wireless Agentic AI with Retrieval-Augmented Multimodal Semantic\n  Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Agentic AI with Retrieval-Augmented Multimodal Semantic\n  Perception"
                },
                "summary": "The rapid development of multimodal AI and Large Language Models (LLMs) has\ngreatly enhanced real-time interaction, decision-making, and collaborative\ntasks. However, in wireless multi-agent scenarios, limited bandwidth poses\nsignificant challenges to exchanging semantically rich multimodal information\nefficiently. Traditional semantic communication methods, though effective,\nstruggle with redundancy and loss of crucial details. To overcome these\nchallenges, we propose a Retrieval-Augmented Multimodal Semantic Communication\n(RAMSemCom) framework. RAMSemCom incorporates iterative, retrieval-driven\nsemantic refinement tailored for distributed multi-agent environments, enabling\nefficient exchange of critical multimodal elements through local caching and\nselective transmission. Our approach dynamically optimizes retrieval using deep\nreinforcement learning (DRL) to balance semantic fidelity with bandwidth\nconstraints. A comprehensive case study on multi-agent autonomous driving\ndemonstrates that our DRL-based retrieval strategy significantly improves task\ncompletion efficiency and reduces communication overhead compared to baseline\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of multimodal AI and Large Language Models (LLMs) has\ngreatly enhanced real-time interaction, decision-making, and collaborative\ntasks. However, in wireless multi-agent scenarios, limited bandwidth poses\nsignificant challenges to exchanging semantically rich multimodal information\nefficiently. Traditional semantic communication methods, though effective,\nstruggle with redundancy and loss of crucial details. To overcome these\nchallenges, we propose a Retrieval-Augmented Multimodal Semantic Communication\n(RAMSemCom) framework. RAMSemCom incorporates iterative, retrieval-driven\nsemantic refinement tailored for distributed multi-agent environments, enabling\nefficient exchange of critical multimodal elements through local caching and\nselective transmission. Our approach dynamically optimizes retrieval using deep\nreinforcement learning (DRL) to balance semantic fidelity with bandwidth\nconstraints. A comprehensive case study on multi-agent autonomous driving\ndemonstrates that our DRL-based retrieval strategy significantly improves task\ncompletion efficiency and reduces communication overhead compared to baseline\nmethods."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Jamalipour"
                },
                "author": "Abbas Jamalipour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11501v2",
                "updated": "2025-05-29T09:18:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    18,
                    35,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-17T07:05:36Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "title": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?"
                },
                "summary": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods."
                },
                "authors": [
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23258v1",
                "updated": "2025-05-29T09:06:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    6,
                    1,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T09:06:01Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    6,
                    1,
                    3,
                    149,
                    0
                ],
                "title": "SealOS+: A Sealos-based Approach for Adaptive Resource Optimization\n  Under Dynamic Workloads for Securities Trading System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SealOS+: A Sealos-based Approach for Adaptive Resource Optimization\n  Under Dynamic Workloads for Securities Trading System"
                },
                "summary": "As securities trading systems transition to a microservices architecture,\noptimizing system performance presents challenges such as inefficient resource\nscheduling and high service response delays. Existing container orchestration\nplatforms lack tailored performance optimization mechanisms for trading\nscenarios, making it difficult to meet the stringent 50ms response time\nrequirement imposed by exchanges. This paper introduces SealOS+, a Sealos-based\nperformance optimization approach for securities trading, incorporating an\nadaptive resource scheduling algorithm leveraging deep reinforcement learning,\na three-level caching mechanism for trading operations, and a Long Short-Term\nMemory (LSTM) based load prediction model. Real-world deployment at a\nsecurities exchange demonstrates that the optimized system achieves an average\nCPU utilization of 78\\%, reduces transaction response time to 105ms, and\nreaches a peak processing capacity of 15,000 transactions per second,\neffectively meeting the rigorous performance and reliability demands of\nsecurities trading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As securities trading systems transition to a microservices architecture,\noptimizing system performance presents challenges such as inefficient resource\nscheduling and high service response delays. Existing container orchestration\nplatforms lack tailored performance optimization mechanisms for trading\nscenarios, making it difficult to meet the stringent 50ms response time\nrequirement imposed by exchanges. This paper introduces SealOS+, a Sealos-based\nperformance optimization approach for securities trading, incorporating an\nadaptive resource scheduling algorithm leveraging deep reinforcement learning,\na three-level caching mechanism for trading operations, and a Long Short-Term\nMemory (LSTM) based load prediction model. Real-world deployment at a\nsecurities exchange demonstrates that the optimized system achieves an average\nCPU utilization of 78\\%, reduces transaction response time to 105ms, and\nreaches a peak processing capacity of 15,000 transactions per second,\neffectively meeting the rigorous performance and reliability demands of\nsecurities trading."
                },
                "authors": [
                    {
                        "name": "Haojie Jia"
                    },
                    {
                        "name": "Zhenhao Li"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_comment": "9 pages, In Proceedings of IEEE ICCCN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v4",
                "updated": "2025-05-29T09:01:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    1,
                    23,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor Product Attention Transformer,(T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines, including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at the decoding stage enable processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor Product Attention Transformer,(T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines, including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at the decoding stage enable processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew C Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew C Yao"
                },
                "author": "Andrew C Yao",
                "arxiv_comment": "52 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v2",
                "updated": "2025-05-29T03:11:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    3,
                    11,
                    10,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21070v2",
                "updated": "2025-05-29T01:34:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    1,
                    34,
                    8,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-27T11:55:22Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    11,
                    55,
                    22,
                    1,
                    147,
                    0
                ],
                "title": "Minute-Long Videos with Dual Parallelisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minute-Long Videos with Dual Parallelisms"
                },
                "summary": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs."
                },
                "authors": [
                    {
                        "name": "Zeqing Wang"
                    },
                    {
                        "name": "Bowen Zheng"
                    },
                    {
                        "name": "Xingyi Yang"
                    },
                    {
                        "name": "Zhenxiong Tan"
                    },
                    {
                        "name": "Yuecong Xu"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "The code is available at\n  https://github.com/DualParal-Project/DualParal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22927v1",
                "updated": "2025-05-28T22:59:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    59,
                    24,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T22:59:24Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    59,
                    24,
                    2,
                    148,
                    0
                ],
                "title": "Wideband Glide-Symmetric Slow-Wave Structure for Millimeter-Wave Sheet\n  Beam TWTs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wideband Glide-Symmetric Slow-Wave Structure for Millimeter-Wave Sheet\n  Beam TWTs"
                },
                "summary": "We introduce a slow-wave structure (SWS) for a millimeter-wave sheet-beam\ntraveling-wave tube (TWT) with wide bandwidth. The wideband and stable\noperation is enabled through the topological properties associated with\nglide-symmetry that close the bandgap at the $3\\pi$-point and also make the\non-axis interaction impedance negligible for the backward wave. This space\nharmonic structure is designed to operate in the $V$-band over 55-68 GHz with\nsynchronism to a 5.2 kV, 11 mA sheet electron beam that will be produced by a\ndiamond field-emitter array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a slow-wave structure (SWS) for a millimeter-wave sheet-beam\ntraveling-wave tube (TWT) with wide bandwidth. The wideband and stable\noperation is enabled through the topological properties associated with\nglide-symmetry that close the bandgap at the $3\\pi$-point and also make the\non-axis interaction impedance negligible for the backward wave. This space\nharmonic structure is designed to operate in the $V$-band over 55-68 GHz with\nsynchronism to a 5.2 kV, 11 mA sheet electron beam that will be produced by a\ndiamond field-emitter array."
                },
                "authors": [
                    {
                        "name": "Robert Marosi"
                    },
                    {
                        "name": "Muhammed Zuboraj"
                    },
                    {
                        "name": "Filippo Capolino"
                    }
                ],
                "author_detail": {
                    "name": "Filippo Capolino"
                },
                "author": "Filippo Capolino",
                "arxiv_comment": "8 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22913v1",
                "updated": "2025-05-28T22:32:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    32,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T22:32:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    32,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM\n  Inference"
                },
                "summary": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar."
                },
                "authors": [
                    {
                        "name": "Donghyeon Joo"
                    },
                    {
                        "name": "Helya Hosseini"
                    },
                    {
                        "name": "Ramyad Hadidi"
                    },
                    {
                        "name": "Bahar Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Bahar Asgari"
                },
                "author": "Bahar Asgari",
                "arxiv_comment": "19 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v6",
                "updated": "2025-05-28T18:58:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    18,
                    58,
                    29,
                    2,
                    148,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v1",
                "updated": "2025-05-28T17:39:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22425v1",
                "updated": "2025-05-28T14:52:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:52:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Scaling Reasoning without Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Reasoning without Attention"
                },
                "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning."
                },
                "authors": [
                    {
                        "name": "Xueliang Zhao"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v4",
                "updated": "2025-05-28T12:07:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    7,
                    57,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility\n  and Speedup",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility\n  and Speedup"
                },
                "summary": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22156v1",
                "updated": "2025-05-28T09:20:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T09:20:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing"
                },
                "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."
                },
                "authors": [
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21919v1",
                "updated": "2025-05-28T03:05:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    3,
                    5,
                    55,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T03:05:55Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    3,
                    5,
                    55,
                    2,
                    148,
                    0
                ],
                "title": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in\n  LLM Inference"
                },
                "summary": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference."
                },
                "authors": [
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Zhuoran Liu"
                    },
                    {
                        "name": "Eun Kyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Eun Kyung Lee"
                },
                "author": "Eun Kyung Lee",
                "arxiv_comment": "This paper has been accepted at IEEE Cloud 2025 as WIP paper. The\n  final version will appear in IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v2",
                "updated": "2025-05-28T01:38:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    1,
                    38,
                    7,
                    2,
                    148,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07872v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07872v3",
                "updated": "2025-05-28T00:43:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    0,
                    43,
                    47,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-09T21:05:20Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "title": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions"
                },
                "summary": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07872v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07872v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21669v1",
                "updated": "2025-05-27T18:47:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    47,
                    34,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T18:47:34Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    47,
                    34,
                    1,
                    147,
                    0
                ],
                "title": "Improved Prefetching Techniques for Linked Data Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Prefetching Techniques for Linked Data Structures"
                },
                "summary": "With ever-increasing main memory stall times, we need novel techniques to\nreduce effective memory access latencies. Prefetching has been shown to be an\neffective solution, especially with contiguous data structures that follow the\ntraditional principles of spatial and temporal locality. However, on linked\ndata structures$-$made up of many nodes linked together with pointers$-$typical\nprefetchers struggle, failing to predict accesses as elements are arbitrarily\nscattered throughout memory and access patters are arbitrarily complex and\nhence difficult to predict. To remedy these issues, we introduce\n$\\textit{Linkey}$, a novel prefetcher that utilizes hints from the\nprogrammer/compiler to cache layout information and accurately prefetch linked\ndata structures. $\\textit{Linkey}$ obtains substantial performance improvements\nover a striding baseline. We achieve a geomean 13% reduction in miss rate with\na maximum improvement of 58.8%, and a 65.4% geomean increase in accuracy, with\nmany benchmarks improving from 0%. On benchmarks where $\\textit{Linkey}$ is\napplicable, we observe a geomean IPC improvement of 1.40%, up to 12.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With ever-increasing main memory stall times, we need novel techniques to\nreduce effective memory access latencies. Prefetching has been shown to be an\neffective solution, especially with contiguous data structures that follow the\ntraditional principles of spatial and temporal locality. However, on linked\ndata structures$-$made up of many nodes linked together with pointers$-$typical\nprefetchers struggle, failing to predict accesses as elements are arbitrarily\nscattered throughout memory and access patters are arbitrarily complex and\nhence difficult to predict. To remedy these issues, we introduce\n$\\textit{Linkey}$, a novel prefetcher that utilizes hints from the\nprogrammer/compiler to cache layout information and accurately prefetch linked\ndata structures. $\\textit{Linkey}$ obtains substantial performance improvements\nover a striding baseline. We achieve a geomean 13% reduction in miss rate with\na maximum improvement of 58.8%, and a 65.4% geomean increase in accuracy, with\nmany benchmarks improving from 0%. On benchmarks where $\\textit{Linkey}$ is\napplicable, we observe a geomean IPC improvement of 1.40%, up to 12.1%."
                },
                "authors": [
                    {
                        "name": "Nikola Vuk Maruszewski"
                    }
                ],
                "author_detail": {
                    "name": "Nikola Vuk Maruszewski"
                },
                "author": "Nikola Vuk Maruszewski",
                "arxiv_comment": "73 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.5.3; E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21487v1",
                "updated": "2025-05-27T17:54:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:54:07Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "title": "Hardware-Efficient Attention for Fast Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Efficient Attention for Fast Decoding"
                },
                "summary": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$."
                },
                "authors": [
                    {
                        "name": "Ted Zadouri"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "37 pages, 15 figures, 45 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21467v1",
                "updated": "2025-05-27T17:39:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:39:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion"
                },
                "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains."
                },
                "authors": [
                    {
                        "name": "Zhanqiu Hu"
                    },
                    {
                        "name": "Jian Meng"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Jae-sun Seo"
                    },
                    {
                        "name": "Zhiru Zhang"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21259v1",
                "updated": "2025-05-27T14:39:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    39,
                    28,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:39:28Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    39,
                    28,
                    1,
                    147,
                    0
                ],
                "title": "Stochastic Geometry-Based Performance Evaluation for LEO\n  Satellite-Assisted Space Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Geometry-Based Performance Evaluation for LEO\n  Satellite-Assisted Space Caching"
                },
                "summary": "To achieve the Internet of Things (IoT) vision,Mobile Edge Computing (MEC) is\na promising technology aimed at providing low-latency computing services to\nuser equipment (UE). However, terrestrial MEC network struggles to provide\nservice to UEs in remote and maritime region. Low Earth Orbit (LEO) satellite\nnetworks have the potential to overcome geographical restrictions and provide\nseamless global coverage for UEs. In this paper, we provide the first attempt\nto use stochastic geometry to investigate the performance of implementing space\ncaching with LEO satellites (SATs) in the MEC network. We study a LEO\nsatellite-assisted space caching MEC network, and LEO SATs can be equipped with\nservers to enable space caching, with the advantage of seamless coverage to\nassist terrestrial CSs for serving UEs in remote or maritime reigon. Using\nstochastic geometry and queuing theory, we establish an analytical framework\nfor this MEC network. Meanwhile, we develop association strategies for UEs to\nconnect with LEO SATs or CSs and utilize stochastic geometry to derive uplink\nand downlink coverage probabilities, considering the diversity of task and\nservice types. On this basis, we employ the queuing theory to calculate the\naverage delay to evaluate the system performance. Through Monte Carlo\nsimulations and numerical results, the system performance is evaluated. The\nresults show the potential of SAT spatial caching in improving the performance\nof the MEC network. Additionally, our results reveal useful insights such as\nthe significant impact of the altitude and number of LEO SATs on the average\ndelay of the network, providing helpful system-level recommendations for the\ndesign and configuration of the space-caching MEC network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve the Internet of Things (IoT) vision,Mobile Edge Computing (MEC) is\na promising technology aimed at providing low-latency computing services to\nuser equipment (UE). However, terrestrial MEC network struggles to provide\nservice to UEs in remote and maritime region. Low Earth Orbit (LEO) satellite\nnetworks have the potential to overcome geographical restrictions and provide\nseamless global coverage for UEs. In this paper, we provide the first attempt\nto use stochastic geometry to investigate the performance of implementing space\ncaching with LEO satellites (SATs) in the MEC network. We study a LEO\nsatellite-assisted space caching MEC network, and LEO SATs can be equipped with\nservers to enable space caching, with the advantage of seamless coverage to\nassist terrestrial CSs for serving UEs in remote or maritime reigon. Using\nstochastic geometry and queuing theory, we establish an analytical framework\nfor this MEC network. Meanwhile, we develop association strategies for UEs to\nconnect with LEO SATs or CSs and utilize stochastic geometry to derive uplink\nand downlink coverage probabilities, considering the diversity of task and\nservice types. On this basis, we employ the queuing theory to calculate the\naverage delay to evaluate the system performance. Through Monte Carlo\nsimulations and numerical results, the system performance is evaluated. The\nresults show the potential of SAT spatial caching in improving the performance\nof the MEC network. Additionally, our results reveal useful insights such as\nthe significant impact of the altitude and number of LEO SATs on the average\ndelay of the network, providing helpful system-level recommendations for the\ndesign and configuration of the space-caching MEC network."
                },
                "authors": [
                    {
                        "name": "Chunyi Ma"
                    },
                    {
                        "name": "Jiajie Xu"
                    },
                    {
                        "name": "Jianhua Yang"
                    },
                    {
                        "name": "Mustafa A. Kishk"
                    }
                ],
                "author_detail": {
                    "name": "Mustafa A. Kishk"
                },
                "author": "Mustafa A. Kishk",
                "arxiv_doi": "10.1109/JIOT.2025.3574814",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JIOT.2025.3574814",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.21259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 12 figures, be accepted by IEEE IoTJ",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v3",
                "updated": "2025-05-27T12:05:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    5,
                    4,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but are slow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but are slow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "SEA-2025 version. 18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v3",
                "updated": "2025-05-27T09:24:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    9,
                    24,
                    50,
                    1,
                    147,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Caching for Serving Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Caching for Serving Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) show great capabilities in a wide range of\napplications, but serving them efficiently becomes increasingly challenging as\nrequests (prompts) become more complex. Context caching improves serving\nperformance by reusing Key-Value (KV) vectors, the intermediate representations\nof tokens that are repeated across requests. However, existing context caching\nrequires exact prefix matches across requests, limiting reuse cases in settings\nsuch as few-shot learning and retrieval-augmented generation, where immutable\ncontent (e.g., documents) remains unchanged across requests but is preceded by\nvarying prefixes. Position-Independent Caching (PIC) addresses this issue by\nenabling modular reuse of the KV vectors regardless of prefixes. We formalize\nPIC and advance prior work by introducing EPIC, a serving system incorporating\nour new LegoLink algorithm, which mitigates the inappropriate \"attention sink\"\neffect at every document beginning, to maintain accuracy with minimal\ncomputation. Experiments show that EPIC achieves up to 8x improvements in\nTime-To-First-Token (TTFT) and 7x throughput gains over existing systems, with\nnegligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show great capabilities in a wide range of\napplications, but serving them efficiently becomes increasingly challenging as\nrequests (prompts) become more complex. Context caching improves serving\nperformance by reusing Key-Value (KV) vectors, the intermediate representations\nof tokens that are repeated across requests. However, existing context caching\nrequires exact prefix matches across requests, limiting reuse cases in settings\nsuch as few-shot learning and retrieval-augmented generation, where immutable\ncontent (e.g., documents) remains unchanged across requests but is preceded by\nvarying prefixes. Position-Independent Caching (PIC) addresses this issue by\nenabling modular reuse of the KV vectors regardless of prefixes. We formalize\nPIC and advance prior work by introducing EPIC, a serving system incorporating\nour new LegoLink algorithm, which mitigates the inappropriate \"attention sink\"\neffect at every document beginning, to maintain accuracy with minimal\ncomputation. Experiments show that EPIC achieves up to 8x improvements in\nTime-To-First-Token (TTFT) and 7x throughput gains over existing systems, with\nnegligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v1",
                "updated": "2025-05-27T06:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "arxiv_comment": "8 pages, 3 figures. Under review at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v3",
                "updated": "2025-05-27T04:15:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    4,
                    15,
                    22,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "vCache: Verified Semantic Prompt Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vCache: Verified Semantic Prompt Caching"
                },
                "summary": "Semantic caches return cached LLM-generated responses for semantically\nsimilar prompts to reduce inference latency and cost. They embed cached prompts\nand store them alongside their response in a vector database. Embedding\nsimilarity metrics assign a numerical score to quantify the similarity between\na request and its nearest neighbor prompt from the cache. Existing systems use\nthe same static similarity threshold across all requests to determine whether\ntwo prompts can share similar responses. However, we observe that static\nthresholds do not give formal correctness guarantees, can result in unexpected\nerror rates, and lead to suboptimal cache hit rates. This paper proposes\nvCache, the first verified semantic cache with user-defined error rate\nguarantees. It employs an online learning algorithm to estimate an optimal\nthreshold for each cached prompt, enabling reliable cache responses without\nadditional training. Our experiments show that vCache consistently meets the\nspecified error bounds while outperforming state-of-the-art static-threshold\nand fine-tuned embedding baselines. We release the vCache implementation and\nbenchmarks to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caches return cached LLM-generated responses for semantically\nsimilar prompts to reduce inference latency and cost. They embed cached prompts\nand store them alongside their response in a vector database. Embedding\nsimilarity metrics assign a numerical score to quantify the similarity between\na request and its nearest neighbor prompt from the cache. Existing systems use\nthe same static similarity threshold across all requests to determine whether\ntwo prompts can share similar responses. However, we observe that static\nthresholds do not give formal correctness guarantees, can result in unexpected\nerror rates, and lead to suboptimal cache hit rates. This paper proposes\nvCache, the first verified semantic cache with user-defined error rate\nguarantees. It employs an online learning algorithm to estimate an optimal\nthreshold for each cached prompt, enabling reliable cache responses without\nadditional training. Our experiments show that vCache consistently meets the\nspecified error bounds while outperforming state-of-the-art static-threshold\nand fine-tuned embedding baselines. We release the vCache implementation and\nbenchmarks to support future research."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Kyle Chu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18458v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18458v2",
                "updated": "2025-05-27T03:57:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    57,
                    47,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-24T01:57:12Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    1,
                    57,
                    12,
                    5,
                    144,
                    0
                ],
                "title": "A Survey of LLM $\\times$ DATA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM $\\times$ DATA"
                },
                "summary": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration."
                },
                "authors": [
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Junxuan He"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Haodong Chen"
                    },
                    {
                        "name": "Zirui Tang"
                    },
                    {
                        "name": "Haoyu Zhao"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Zhaojun Sun"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "arxiv_comment": "Please refer to the paper list at:\n  https://github.com/weAIDB/awesome-data-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18458v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18458v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19586v2",
                "updated": "2025-05-27T03:16:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    16,
                    32,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-26T07:00:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    0,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV\n  Cache Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV\n  Cache Optimization"
                },
                "summary": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding."
                },
                "authors": [
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Bowen Shen"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v4",
                "updated": "2025-05-27T03:08:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    8,
                    57,
                    1,
                    147,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20600v1",
                "updated": "2025-05-27T00:36:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    0,
                    36,
                    56,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T00:36:56Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    0,
                    36,
                    56,
                    1,
                    147,
                    0
                ],
                "title": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware\n  Caching and Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware\n  Caching and Scheduling"
                },
                "summary": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality."
                },
                "authors": [
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Tianyu Feng"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Guoxuan Zhu"
                    },
                    {
                        "name": "Xiu Lin"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20438v1",
                "updated": "2025-05-26T18:34:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    34,
                    7,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T18:34:07Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    34,
                    7,
                    0,
                    146,
                    0
                ],
                "title": "HAMburger: Accelerating LLM Inference via Token Smashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAMburger: Accelerating LLM Inference via Token Smashing"
                },
                "summary": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design."
                },
                "authors": [
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Ce Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ce Zhang"
                },
                "author": "Ce Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17644v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17644v5",
                "updated": "2025-05-26T16:16:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    16,
                    43,
                    0,
                    146,
                    0
                ],
                "published": "2024-01-31T07:52:48Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    7,
                    52,
                    48,
                    2,
                    31,
                    0
                ],
                "title": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems"
                },
                "summary": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry."
                },
                "authors": [
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xueze Kang"
                    },
                    {
                        "name": "Yuchu Fang"
                    },
                    {
                        "name": "Yeju Zhou"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Rui Guo"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17644v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17644v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17138v2",
                "updated": "2025-05-26T13:20:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    13,
                    20,
                    45,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-22T06:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    12,
                    42,
                    3,
                    142,
                    0
                ],
                "title": "RAP: Runtime-Adaptive Pruning for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAP: Runtime-Adaptive Pruning for LLM Inference"
                },
                "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
                },
                "authors": [
                    {
                        "name": "Huanrong Liu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xuyang Wei"
                    },
                    {
                        "name": "Jiaheng Dai"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Tianqi Wei"
                    },
                    {
                        "name": "Qingbiao Li"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v1",
                "updated": "2025-05-26T12:06:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_comment": "Accepted for publication in 2025 IEEE 18th International Conference\n  on Cloud Computing (CLOUD)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19849v1",
                "updated": "2025-05-26T11:35:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T11:35:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems"
                },
                "summary": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://anonymous.4open.science/r/HIT_model-5C23.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://anonymous.4open.science/r/HIT_model-5C23."
                },
                "authors": [
                    {
                        "name": "Haoqiang Yang"
                    },
                    {
                        "name": "Congde Yuan"
                    },
                    {
                        "name": "Kun Bai"
                    },
                    {
                        "name": "Mengzhuo Guo"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Chao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhou"
                },
                "author": "Chao Zhou",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16582v2",
                "updated": "2025-05-26T10:07:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    10,
                    7,
                    5,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-22T12:17:13Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    12,
                    17,
                    13,
                    3,
                    142,
                    0
                ],
                "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering"
                },
                "summary": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones."
                },
                "authors": [
                    {
                        "name": "Jianbiao Mei"
                    },
                    {
                        "name": "Tao Hu"
                    },
                    {
                        "name": "Daocheng Fu"
                    },
                    {
                        "name": "Licheng Wen"
                    },
                    {
                        "name": "Xuemeng Yang"
                    },
                    {
                        "name": "Rong Wu"
                    },
                    {
                        "name": "Pinlong Cai"
                    },
                    {
                        "name": "Xinyu Cai"
                    },
                    {
                        "name": "Xing Gao"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Chengjun Xie"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Yu Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Qiao"
                },
                "author": "Yu Qiao",
                "arxiv_comment": "25 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17062v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17062v3",
                "updated": "2025-05-26T08:39:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    8,
                    39,
                    26,
                    0,
                    146,
                    0
                ],
                "published": "2024-05-27T11:31:58Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    11,
                    31,
                    58,
                    0,
                    148,
                    0
                ],
                "title": "UniICL: An Efficient Unified Framework Unifying Compression, Selection,\n  and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniICL: An Efficient Unified Framework Unifying Compression, Selection,\n  and Generation"
                },
                "summary": "In-context learning (ICL) enhances the reasoning abilities of Large Language\nModels (LLMs) by prepending a few demonstrations. It motivates researchers to\nintroduce more examples to provide additional contextual information for the\ngeneration. However, existing methods show a significant limitation due to the\nproblem of excessive growth in context length, which causes a large hardware\nburden. In addition, shallow-relevant examples selected by off-the-shelf tools\nhinder LLMs from capturing useful contextual information for generation. In\nthis paper, we propose \\textbf{UniICL}, a novel \\textbf{Uni}fied \\textbf{ICL}\nframework that unifies demonstration compression, demonstration selection, and\nfinal response generation. Furthermore, to boost inference efficiency, we\ndesign a tailored compression strategy that allows UniICL to cache compression\nresults into \\textbf{Demonstration Bank} (\\textbf{DB}), which avoids repeated\ncompression of the same demonstration. Extensive out-of-domain evaluations\nprove the advantages of UniICL in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enhances the reasoning abilities of Large Language\nModels (LLMs) by prepending a few demonstrations. It motivates researchers to\nintroduce more examples to provide additional contextual information for the\ngeneration. However, existing methods show a significant limitation due to the\nproblem of excessive growth in context length, which causes a large hardware\nburden. In addition, shallow-relevant examples selected by off-the-shelf tools\nhinder LLMs from capturing useful contextual information for generation. In\nthis paper, we propose \\textbf{UniICL}, a novel \\textbf{Uni}fied \\textbf{ICL}\nframework that unifies demonstration compression, demonstration selection, and\nfinal response generation. Furthermore, to boost inference efficiency, we\ndesign a tailored compression strategy that allows UniICL to cache compression\nresults into \\textbf{Demonstration Bank} (\\textbf{DB}), which avoids repeated\ncompression of the same demonstration. Extensive out-of-domain evaluations\nprove the advantages of UniICL in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Jun Gao"
                    },
                    {
                        "name": "Qi Lv"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Tianxiang Wu"
                    },
                    {
                        "name": "Ziqiang Cao"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "arxiv_comment": "ACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17062v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17062v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v2",
                "updated": "2025-05-26T07:30:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    30,
                    17,
                    0,
                    146,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are typically served from clusters of GPUs/NPUs\nthat consist of large number of devices. Unfortunately, communication between\nthese devices incurs significant overhead, increasing the inference latency and\ncost while limiting the scalability. Prior work addressed this issue by\noverlapping communication with compute, but has severe limitations due to the\ndata dependencies between these operations. In this paper, we propose PRESERVE,\na novel framework that prefetches model weights and KV-cache from off-chip HBM\nmemory to the on-chip cache of AI accelerators during the communication\noperations, which offers various advantages and performance improvements\ncompared to prior methods.\n  Through extensive experiments conducted on commercial AI accelerators, we\ndemonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source\nLLMs. Additionally, we perform a design space exploration that identifies the\noptimal hardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically served from clusters of GPUs/NPUs\nthat consist of large number of devices. Unfortunately, communication between\nthese devices incurs significant overhead, increasing the inference latency and\ncost while limiting the scalability. Prior work addressed this issue by\noverlapping communication with compute, but has severe limitations due to the\ndata dependencies between these operations. In this paper, we propose PRESERVE,\na novel framework that prefetches model weights and KV-cache from off-chip HBM\nmemory to the on-chip cache of AI accelerators during the communication\noperations, which offers various advantages and performance improvements\ncompared to prior methods.\n  Through extensive experiments conducted on commercial AI accelerators, we\ndemonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source\nLLMs. Additionally, we perform a design space exploration that identifies the\noptimal hardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19602v1",
                "updated": "2025-05-26T07:11:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    11,
                    42,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T07:11:42Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    11,
                    42,
                    0,
                    146,
                    0
                ],
                "title": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression"
                },
                "summary": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity."
                },
                "authors": [
                    {
                        "name": "Kunjun Li"
                    },
                    {
                        "name": "Zigeng Chen"
                    },
                    {
                        "name": "Cheng-Yen Yang"
                    },
                    {
                        "name": "Jenq-Neng Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Jenq-Neng Hwang"
                },
                "author": "Jenq-Neng Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v1",
                "updated": "2025-05-26T05:58:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v2",
                "updated": "2025-05-26T05:56:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    56,
                    51,
                    0,
                    146,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12392v2",
                "updated": "2025-05-26T05:28:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    28,
                    49,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-18T12:37:56Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    37,
                    56,
                    6,
                    138,
                    0
                ],
                "title": "SLOT: Sample-specific Language Model Optimization at Test-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLOT: Sample-specific Language Model Optimization at Test-time"
                },
                "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT."
                },
                "authors": [
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Xingyu Zhang"
                    },
                    {
                        "name": "Xueji Fang"
                    },
                    {
                        "name": "Zhiyang Chen"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huatian Zhang"
                    },
                    {
                        "name": "Guojun Qi"
                    }
                ],
                "author_detail": {
                    "name": "Guojun Qi"
                },
                "author": "Guojun Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12731v2",
                "updated": "2025-05-25T13:03:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    13,
                    3,
                    54,
                    6,
                    145,
                    0
                ],
                "published": "2025-05-19T05:39:38Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    39,
                    38,
                    0,
                    139,
                    0
                ],
                "title": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps"
                },
                "summary": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality."
                },
                "authors": [
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Jinyu Guo"
                    },
                    {
                        "name": "Shuaihong Jiang"
                    },
                    {
                        "name": "Zhaokun Wang"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Shunyu Yao"
                    },
                    {
                        "name": "Wenhong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Wenhong Tian"
                },
                "author": "Wenhong Tian",
                "arxiv_comment": "Accepted at Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19089v1",
                "updated": "2025-05-25T10:57:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    10,
                    57,
                    35,
                    6,
                    145,
                    0
                ],
                "published": "2025-05-25T10:57:35Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    10,
                    57,
                    35,
                    6,
                    145,
                    0
                ],
                "title": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation"
                },
                "summary": "Masked generative models (MGMs) have emerged as a powerful framework for\nimage synthesis, combining parallel decoding with strong bidirectional context\nmodeling. However, generating high-quality samples typically requires many\niterative decoding steps, resulting in high inference costs. A straightforward\nway to speed up generation is by decoding more tokens in each step, thereby\nreducing the total number of steps. However, when many tokens are decoded\nsimultaneously, the model can only estimate the univariate marginal\ndistributions independently, failing to capture the dependency among them. As a\nresult, reducing the number of steps significantly compromises generation\nfidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a\nplug-and-play module that accelerates inference in MGMs by constructing\nlow-cost steps via reusing feature embeddings from previously decoded context\ntokens. ReCAP interleaves standard full evaluations with lightweight steps that\ncache and reuse context features, substantially reducing computation while\npreserving the benefits of fine-grained, iterative generation. We demonstrate\nits effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),\nincluding both discrete and continuous token spaces and covering diverse\narchitectural designs. In particular, on ImageNet256 class-conditional\ngeneration, ReCAP achieves up to 2.4x faster inference than the base model with\nminimal performance drop, and consistently delivers better efficiency-fidelity\ntrade-offs under various generation settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked generative models (MGMs) have emerged as a powerful framework for\nimage synthesis, combining parallel decoding with strong bidirectional context\nmodeling. However, generating high-quality samples typically requires many\niterative decoding steps, resulting in high inference costs. A straightforward\nway to speed up generation is by decoding more tokens in each step, thereby\nreducing the total number of steps. However, when many tokens are decoded\nsimultaneously, the model can only estimate the univariate marginal\ndistributions independently, failing to capture the dependency among them. As a\nresult, reducing the number of steps significantly compromises generation\nfidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a\nplug-and-play module that accelerates inference in MGMs by constructing\nlow-cost steps via reusing feature embeddings from previously decoded context\ntokens. ReCAP interleaves standard full evaluations with lightweight steps that\ncache and reuse context features, substantially reducing computation while\npreserving the benefits of fine-grained, iterative generation. We demonstrate\nits effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),\nincluding both discrete and continuous token spaces and covering diverse\narchitectural designs. In particular, on ImageNet256 class-conditional\ngeneration, ReCAP achieves up to 2.4x faster inference than the base model with\nminimal performance drop, and consistently delivers better efficiency-fidelity\ntrade-offs under various generation settings."
                },
                "authors": [
                    {
                        "name": "Xuejie Liu"
                    },
                    {
                        "name": "Anji Liu"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00776v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00776v4",
                "updated": "2025-05-25T05:26:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    5,
                    26,
                    2,
                    6,
                    145,
                    0
                ],
                "published": "2024-12-01T11:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    43,
                    46,
                    6,
                    336,
                    0
                ],
                "title": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning"
                },
                "summary": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation."
                },
                "authors": [
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00776v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00776v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18809v1",
                "updated": "2025-05-24T17:46:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T17:46:47Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VORTA: Efficient Video Diffusion via Routing Sparse Attention"
                },
                "summary": "Video Diffusion Transformers (VDiTs) have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nattention acceleration methods leverage the sparsity of attention patterns to\nimprove efficiency; however, they often overlook inefficiencies of redundant\nlong-range interactions. To address this problem, we propose \\textbf{VORTA}, an\nacceleration framework with two novel components: 1) a sparse attention\nmechanism that efficiently captures long-range dependencies, and 2) a routing\nstrategy that adaptively replaces full 3D attention with specialized sparse\nattention variants throughout the sampling process. It achieves a $1.76\\times$\nend-to-end speedup without quality loss on VBench. Furthermore, VORTA can\nseamlessly integrate with various other acceleration methods, such as caching\nand step distillation, reaching up to $14.41\\times$ speedup with negligible\nperformance degradation. VORTA demonstrates its efficiency and enhances the\npracticality of VDiTs in real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (VDiTs) have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nattention acceleration methods leverage the sparsity of attention patterns to\nimprove efficiency; however, they often overlook inefficiencies of redundant\nlong-range interactions. To address this problem, we propose \\textbf{VORTA}, an\nacceleration framework with two novel components: 1) a sparse attention\nmechanism that efficiently captures long-range dependencies, and 2) a routing\nstrategy that adaptively replaces full 3D attention with specialized sparse\nattention variants throughout the sampling process. It achieves a $1.76\\times$\nend-to-end speedup without quality loss on VBench. Furthermore, VORTA can\nseamlessly integrate with various other acceleration methods, such as caching\nand step distillation, reaching up to $14.41\\times$ speedup with negligible\nperformance degradation. VORTA demonstrates its efficiency and enhances the\npracticality of VDiTs in real-world settings."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "19 pages, 15 figures. The code is available at\n  https://github.com/wenhao728/VORTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v3",
                "updated": "2025-05-24T17:39:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    39,
                    32,
                    5,
                    144,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "18 pages, 14 figures. Accepted by ICML 2025. The code is available at\n  https://github.com/wenhao728/AsymRnR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16366v2",
                "updated": "2025-05-24T17:04:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    4,
                    26,
                    5,
                    144,
                    0
                ],
                "published": "2024-04-25T07:09:05Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    7,
                    9,
                    5,
                    3,
                    116,
                    0
                ],
                "title": "Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection"
                },
                "summary": "Unsupervised graph anomaly detection aims at identifying rare patterns that\ndeviate from the majority in a graph without the aid of labels, which is\nimportant for a variety of real-world applications. Recent advances have\nutilized Graph Neural Networks (GNNs) to learn effective node representations\nby aggregating information from neighborhoods. This is motivated by the\nhypothesis that nodes in the graph tend to exhibit consistent behaviors with\ntheir neighborhoods. However, such consistency can be disrupted by graph\nanomalies in multiple ways. Most existing methods directly employ GNNs to learn\nrepresentations, disregarding the negative impact of graph anomalies on GNNs,\nresulting in sub-optimal node representations and anomaly detection\nperformance. While a few recent approaches have redesigned GNNs for graph\nanomaly detection under semi-supervised label guidance, how to address the\nadverse effects of graph anomalies on GNNs in unsupervised scenarios and learn\neffective representations for anomaly detection are still under-explored. To\nbridge this gap, in this paper, we propose a simple yet effective framework for\nGuarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD).\nSpecifically, G3AD first introduces two auxiliary networks along with\ncorrelation constraints to guard the GNNs against inconsistent information\nencoding. Furthermore, G3AD introduces an adaptive caching module to guard the\nGNNs from directly reconstructing the observed graph data that contains\nanomalies. Extensive experiments demonstrate that our G3AD can outperform\ntwenty state-of-the-art methods on both synthetic and real-world graph anomaly\ndatasets, with flexible generalization ability in different GNN backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised graph anomaly detection aims at identifying rare patterns that\ndeviate from the majority in a graph without the aid of labels, which is\nimportant for a variety of real-world applications. Recent advances have\nutilized Graph Neural Networks (GNNs) to learn effective node representations\nby aggregating information from neighborhoods. This is motivated by the\nhypothesis that nodes in the graph tend to exhibit consistent behaviors with\ntheir neighborhoods. However, such consistency can be disrupted by graph\nanomalies in multiple ways. Most existing methods directly employ GNNs to learn\nrepresentations, disregarding the negative impact of graph anomalies on GNNs,\nresulting in sub-optimal node representations and anomaly detection\nperformance. While a few recent approaches have redesigned GNNs for graph\nanomaly detection under semi-supervised label guidance, how to address the\nadverse effects of graph anomalies on GNNs in unsupervised scenarios and learn\neffective representations for anomaly detection are still under-explored. To\nbridge this gap, in this paper, we propose a simple yet effective framework for\nGuarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD).\nSpecifically, G3AD first introduces two auxiliary networks along with\ncorrelation constraints to guard the GNNs against inconsistent information\nencoding. Furthermore, G3AD introduces an adaptive caching module to guard the\nGNNs from directly reconstructing the observed graph data that contains\nanomalies. Extensive experiments demonstrate that our G3AD can outperform\ntwenty state-of-the-art methods on both synthetic and real-world graph anomaly\ndatasets, with flexible generalization ability in different GNN backbones."
                },
                "authors": [
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Jinke Shi"
                    },
                    {
                        "name": "Yao Ma"
                    },
                    {
                        "name": "Haishuai Wang"
                    },
                    {
                        "name": "Jiajun Bu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Bu"
                },
                "author": "Jiajun Bu",
                "arxiv_comment": "Accepted by IEEE TNNLS (14 pages, 10 figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20334v1",
                "updated": "2025-05-24T10:34:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    10,
                    34,
                    38,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T10:34:38Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    10,
                    34,
                    38,
                    5,
                    144,
                    0
                ],
                "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via\n  Pseudo Query",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via\n  Pseudo Query"
                },
                "summary": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements."
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05130v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05130v2",
                "updated": "2025-05-24T09:33:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    33,
                    35,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-08T11:07:35Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "title": "CacheFL: Privacy-Preserving and Efficient Federated Cache Model\n  Fine-Tuning for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFL: Privacy-Preserving and Efficient Federated Cache Model\n  Fine-Tuning for Vision-Language Models"
                },
                "summary": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Mengjun Yi"
                    },
                    {
                        "name": "Hanwen Zhang"
                    },
                    {
                        "name": "Hui Dou"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Furao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Furao Shen"
                },
                "author": "Furao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05130v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05130v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18610v1",
                "updated": "2025-05-24T09:18:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    18,
                    11,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T09:18:11Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    18,
                    11,
                    5,
                    144,
                    0
                ],
                "title": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT\n  LLMs"
                },
                "summary": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ."
                },
                "authors": [
                    {
                        "name": "Tengxuan Liu"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Jiayi Yang"
                    },
                    {
                        "name": "Tianchen Zhao"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18577v1",
                "updated": "2025-05-24T07:57:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    7,
                    57,
                    57,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T07:57:57Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    7,
                    57,
                    57,
                    5,
                    144,
                    0
                ],
                "title": "CXL Topology-Aware and Expander-Driven Prefetching: Unlocking SSD\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL Topology-Aware and Expander-Driven Prefetching: Unlocking SSD\n  Performance"
                },
                "summary": "Integrating compute express link (CXL) with SSDs allows scalable access to\nlarge memory but has slower speeds than DRAMs. We present ExPAND, an\nexpander-driven CXL prefetcher that offloads last-level cache (LLC) prefetching\nfrom host CPU to CXL-SSDs. ExPAND uses a heterogeneous prediction algorithm for\nprefetching and ensures data consistency with CXL.mem's back-invalidation. We\nexamine prefetch timeliness for accurate latency estimation. ExPAND, being\naware of CXL multi-tiered switching, provides end-to-end latency for each\nCXL-SSD and precise prefetch timeliness estimations. Our method reduces CXL-SSD\nreliance and enables direct host cache access for most data. ExPAND enhances\ngraph application performance and SPEC CPU's performance by 9.0$\\times$ and\n14.7$\\times$, respectively, surpassing CXL-SSD pools with diverse prefetching\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating compute express link (CXL) with SSDs allows scalable access to\nlarge memory but has slower speeds than DRAMs. We present ExPAND, an\nexpander-driven CXL prefetcher that offloads last-level cache (LLC) prefetching\nfrom host CPU to CXL-SSDs. ExPAND uses a heterogeneous prediction algorithm for\nprefetching and ensures data consistency with CXL.mem's back-invalidation. We\nexamine prefetch timeliness for accurate latency estimation. ExPAND, being\naware of CXL multi-tiered switching, provides end-to-end latency for each\nCXL-SSD and precise prefetch timeliness estimations. Our method reduces CXL-SSD\nreliance and enables direct host cache access for most data. ExPAND enhances\ngraph application performance and SPEC CPU's performance by 9.0$\\times$ and\n14.7$\\times$, respectively, surpassing CXL-SSD pools with diverse prefetching\nstrategies."
                },
                "authors": [
                    {
                        "name": "Dongsuk Oh"
                    },
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Jiseon Kim"
                    },
                    {
                        "name": "Eunjee Na"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Hyunkyu Choi"
                    },
                    {
                        "name": "Seonghyeon Jang"
                    },
                    {
                        "name": "Hanjin Choi"
                    },
                    {
                        "name": "Hongjoo Jung"
                    },
                    {
                        "name": "Sangwon Lee"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18554v1",
                "updated": "2025-05-24T06:45:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    45,
                    16,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T06:45:16Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    45,
                    16,
                    5,
                    144,
                    0
                ],
                "title": "Garibaldi: A Pairwise Instruction-Data Management for Enhancing Shared\n  Last-Level Cache Performance in Server Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Garibaldi: A Pairwise Instruction-Data Management for Enhancing Shared\n  Last-Level Cache Performance in Server Workloads"
                },
                "summary": "Modern CPUs suffer from the frontend bottleneck because the instruction\nfootprint of server workloads exceeds the private cache capacity. Prior works\nhave examined the CPU components or private cache to improve the instruction\nhit rate. The large footprint leads to significant cache misses not only in the\ncore and faster-level cache but also in the last-level cache (LLC). We observe\nthat even with an advanced branch predictor and instruction prefetching\ntechniques, a considerable amount of instruction accesses descend to the LLC.\nHowever, state-of-the-art LLC designs with elaborate data management overlook\nhandling the instruction misses that precede corresponding data accesses.\nSpecifically, when an instruction requiring numerous data accesses is missed,\nthe frontend of a CPU should wait for the instruction fetch, regardless of how\nmuch data are present in the LLC.\n  To preserve hot instructions in the LLC, we propose Garibaldi, a novel\npairwise instruction-data management scheme. Garibaldi tracks the hotness of\ninstruction accesses by coupling it with that of data accesses and adopts\nmanagement techniques. On the one hand, this scheme includes a selective\nprotection mechanism that prevents the cache evictions of high-cost instruction\ncachelines. On the other hand, in the case of unprotected instruction line\nmisses, Garibaldi conservatively issues prefetch requests of the paired data\nlines while handling those misses. In our experiments, we evaluate Garibaldi\nwith 16 server workloads on a 40-core machine. We also implement Garibaldi on\ntop of a modern LLC design, including Mockingjay. Garibaldi improves 13.2% and\n6.1% of CPU performance on baseline LLC design and Mockingjay, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern CPUs suffer from the frontend bottleneck because the instruction\nfootprint of server workloads exceeds the private cache capacity. Prior works\nhave examined the CPU components or private cache to improve the instruction\nhit rate. The large footprint leads to significant cache misses not only in the\ncore and faster-level cache but also in the last-level cache (LLC). We observe\nthat even with an advanced branch predictor and instruction prefetching\ntechniques, a considerable amount of instruction accesses descend to the LLC.\nHowever, state-of-the-art LLC designs with elaborate data management overlook\nhandling the instruction misses that precede corresponding data accesses.\nSpecifically, when an instruction requiring numerous data accesses is missed,\nthe frontend of a CPU should wait for the instruction fetch, regardless of how\nmuch data are present in the LLC.\n  To preserve hot instructions in the LLC, we propose Garibaldi, a novel\npairwise instruction-data management scheme. Garibaldi tracks the hotness of\ninstruction accesses by coupling it with that of data accesses and adopts\nmanagement techniques. On the one hand, this scheme includes a selective\nprotection mechanism that prevents the cache evictions of high-cost instruction\ncachelines. On the other hand, in the case of unprotected instruction line\nmisses, Garibaldi conservatively issues prefetch requests of the paired data\nlines while handling those misses. In our experiments, we evaluate Garibaldi\nwith 16 server workloads on a 40-core machine. We also implement Garibaldi on\ntop of a modern LLC design, including Mockingjay. Garibaldi improves 13.2% and\n6.1% of CPU performance on baseline LLC design and Mockingjay, respectively."
                },
                "authors": [
                    {
                        "name": "Jaewon Kwon"
                    },
                    {
                        "name": "Yongju Lee"
                    },
                    {
                        "name": "Jiwan Kim"
                    },
                    {
                        "name": "Enhyeok Jang"
                    },
                    {
                        "name": "Hongju Kal"
                    },
                    {
                        "name": "Won Woo Ro"
                    }
                ],
                "author_detail": {
                    "name": "Won Woo Ro"
                },
                "arxiv_affiliation": "Yonsei University, Seoul, Republic of Korea",
                "author": "Won Woo Ro",
                "arxiv_comment": "Accepted to ISCA '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v2",
                "updated": "2025-05-24T04:37:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    4,
                    37,
                    34,
                    5,
                    144,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "User profile embedded in the prompt template of personalized recommendation\nagents play a crucial role in shaping their decision-making process.\nHigh-quality user profiles are essential for aligning agent behavior with real\nuser interests. Typically, these profiles are constructed by leveraging LLMs\nfor user profile modeling (LLM-UM). However, this process faces several\nchallenges: (1) LLMs struggle with long user behaviors due to context length\nlimitations and performance degradation. (2) Existing methods often extract\nonly partial segments from full historical behavior sequence, inevitably\ndiscarding diverse user interests embedded in the omitted content, leading to\nincomplete modeling and suboptimal profiling. (3) User profiling is often\ntightly coupled with the inference context, requiring online processing, which\nintroduces significant latency overhead. In this paper, we propose PersonaX, an\nagent-agnostic LLM-UM framework to address these challenges. It augments\ndownstream recommendation agents to achieve better recommendation performance\nand inference efficiency. PersonaX (a) segments complete historical behaviors\ninto clustered groups, (b) selects multiple sub behavior sequences (SBS) with a\nbalance of prototypicality and diversity to form a high quality core set, (c)\nperforms offline multi-persona profiling to capture diverse user interests and\ngenerate fine grained, cached textual personas, and (d) decouples user\nprofiling from online inference, enabling profile retrieval instead of real\ntime generation. Extensive experiments demonstrate its effectiveness: using\nonly 30 to 50% of behavioral data (sequence length 480), PersonaX enhances\nAgentCF by 3 to 11% and Agent4Rec by 10 to 50%. As a scalable and\nmodel-agnostic LLM-UM solution, PersonaX sets a new benchmark in scalable user\nmodeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User profile embedded in the prompt template of personalized recommendation\nagents play a crucial role in shaping their decision-making process.\nHigh-quality user profiles are essential for aligning agent behavior with real\nuser interests. Typically, these profiles are constructed by leveraging LLMs\nfor user profile modeling (LLM-UM). However, this process faces several\nchallenges: (1) LLMs struggle with long user behaviors due to context length\nlimitations and performance degradation. (2) Existing methods often extract\nonly partial segments from full historical behavior sequence, inevitably\ndiscarding diverse user interests embedded in the omitted content, leading to\nincomplete modeling and suboptimal profiling. (3) User profiling is often\ntightly coupled with the inference context, requiring online processing, which\nintroduces significant latency overhead. In this paper, we propose PersonaX, an\nagent-agnostic LLM-UM framework to address these challenges. It augments\ndownstream recommendation agents to achieve better recommendation performance\nand inference efficiency. PersonaX (a) segments complete historical behaviors\ninto clustered groups, (b) selects multiple sub behavior sequences (SBS) with a\nbalance of prototypicality and diversity to form a high quality core set, (c)\nperforms offline multi-persona profiling to capture diverse user interests and\ngenerate fine grained, cached textual personas, and (d) decouples user\nprofiling from online inference, enabling profile retrieval instead of real\ntime generation. Extensive experiments demonstrate its effectiveness: using\nonly 30 to 50% of behavioral data (sequence length 480), PersonaX enhances\nAgentCF by 3 to 11% and Agent4Rec by 10 to 50%. As a scalable and\nmodel-agnostic LLM-UM solution, PersonaX sets a new benchmark in scalable user\nmodeling."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "2025 ACL Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v1",
                "updated": "2025-05-23T18:46:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20325v1",
                "updated": "2025-05-23T18:19:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    19,
                    9,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T18:19:09Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    19,
                    9,
                    4,
                    143,
                    0
                ],
                "title": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic\n  Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic\n  Confidence"
                },
                "summary": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)\nreasoning often incur substantial computational costs, primarily due to\nextensive reliance on external Process Reward Models (PRMs) or sampling methods\nlike Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient\nself-guided TTS framework that achieves PRM-level performance without costly\nexternal verifier models. Our method employs a lightweight tree search guided\nsolely by intrinsic LLM signals, token-level confidence and step novelty. One\ncritical innovation is improving the reliability of internal confidence\nestimates via a targeted reinforcement learning fine-tuning phase. Empirical\nevaluations on challenging mathematical reasoning benchmarks demonstrate that\nGG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching\nor surpassing significantly larger models (e.g., 32B-70B parameters), while\nreducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG\nachieves comparable accuracy with 8x faster inference speeds and 4-5x lower\nmemory usage. Additionally, GG reduces KV cache memory usage by approximately\n50% compared to the BoN strategy, facilitating more efficient and practical\ndeployment of TTS techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)\nreasoning often incur substantial computational costs, primarily due to\nextensive reliance on external Process Reward Models (PRMs) or sampling methods\nlike Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient\nself-guided TTS framework that achieves PRM-level performance without costly\nexternal verifier models. Our method employs a lightweight tree search guided\nsolely by intrinsic LLM signals, token-level confidence and step novelty. One\ncritical innovation is improving the reliability of internal confidence\nestimates via a targeted reinforcement learning fine-tuning phase. Empirical\nevaluations on challenging mathematical reasoning benchmarks demonstrate that\nGG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching\nor surpassing significantly larger models (e.g., 32B-70B parameters), while\nreducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG\nachieves comparable accuracy with 8x faster inference speeds and 4-5x lower\nmemory usage. Additionally, GG reduces KV cache memory usage by approximately\n50% compared to the BoN strategy, facilitating more efficient and practical\ndeployment of TTS techniques."
                },
                "authors": [
                    {
                        "name": "Amirhosein Ghasemabadi"
                    },
                    {
                        "name": "Keith G. Mills"
                    },
                    {
                        "name": "Baochun Li"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v3",
                "updated": "2025-05-23T17:02:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    17,
                    2,
                    5,
                    4,
                    143,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. We include a codebase implementing aLoRA in the supplementary\nmaterial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. We include a codebase implementing aLoRA in the supplementary\nmaterial."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v3",
                "updated": "2025-05-23T16:36:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    16,
                    36,
                    12,
                    4,
                    143,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the LLM instances to come up with their own collaboration\nstrategy for the problem at hand, all the while \"seeing\" each other's memory in\nthe concurrent KV cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\nmemory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE)\nto avoid recomputation while improving parallel hardware utilization. We find\nthat modern reasoning-capable LLMs can perform inference with shared Key-Value\ncache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the LLM instances to come up with their own collaboration\nstrategy for the problem at hand, all the while \"seeing\" each other's memory in\nthe concurrent KV cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\nmemory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE)\nto avoid recomputation while improving parallel hardware utilization. We find\nthat modern reasoning-capable LLMs can perform inference with shared Key-Value\ncache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Erik Schultheis"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18013v1",
                "updated": "2025-05-23T15:15:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    15,
                    15,
                    21,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T15:15:21Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    15,
                    15,
                    21,
                    4,
                    143,
                    0
                ],
                "title": "DiFache: Efficient and Scalable Caching on Disaggregated Memory using\n  Decentralized Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiFache: Efficient and Scalable Caching on Disaggregated Memory using\n  Decentralized Coherence"
                },
                "summary": "The disaggregated memory (DM) architecture offers high resource elasticity at\nthe cost of data access performance. While caching frequently accessed data in\ncompute nodes (CNs) reduces access overhead, it requires costly centralized\nmaintenance of cache coherence across CNs. This paper presents DiFache, an\nefficient, scalable, and coherent CN-side caching framework for DM\napplications. Observing that DM applications already serialize conflicting\nremote data access internally rather than relying on the cache layer, DiFache\nintroduces decentralized coherence that aligns its consistency model with\nmemory nodes instead of CPU caches, thereby eliminating the need for\ncentralized management. DiFache features a decentralized invalidation mechanism\nto independently invalidate caches on remote CNs and a fine-grained adaptive\nscheme to cache objects with varying read-write ratios. Evaluations using 54\nreal-world traces from Twitter show that DiFache outperforms existing\napproaches by up to 10.83$\\times$ (5.53$\\times$ on average). By integrating\nDiFache, the peak throughput of two real-world DM applications increases by\n7.94$\\times$ and 2.19$\\times$, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The disaggregated memory (DM) architecture offers high resource elasticity at\nthe cost of data access performance. While caching frequently accessed data in\ncompute nodes (CNs) reduces access overhead, it requires costly centralized\nmaintenance of cache coherence across CNs. This paper presents DiFache, an\nefficient, scalable, and coherent CN-side caching framework for DM\napplications. Observing that DM applications already serialize conflicting\nremote data access internally rather than relying on the cache layer, DiFache\nintroduces decentralized coherence that aligns its consistency model with\nmemory nodes instead of CPU caches, thereby eliminating the need for\ncentralized management. DiFache features a decentralized invalidation mechanism\nto independently invalidate caches on remote CNs and a fine-grained adaptive\nscheme to cache objects with varying read-write ratios. Evaluations using 54\nreal-world traces from Twitter show that DiFache outperforms existing\napproaches by up to 10.83$\\times$ (5.53$\\times$ on average). By integrating\nDiFache, the peak throughput of two real-world DM applications increases by\n7.94$\\times$ and 2.19$\\times$, respectively."
                },
                "authors": [
                    {
                        "name": "Hanze Zhang"
                    },
                    {
                        "name": "Kaiming Wang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17934v1",
                "updated": "2025-05-23T14:12:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    14,
                    12,
                    5,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T14:12:05Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    14,
                    12,
                    5,
                    4,
                    143,
                    0
                ],
                "title": "Evaluating the impact of the L3 cache size of AMD EPYC CPUs on the\n  performance of CFD applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the impact of the L3 cache size of AMD EPYC CPUs on the\n  performance of CFD applications"
                },
                "summary": "In this work, the authors focus on assessing the impact of the AMD EPYC\nprocessor architecture on the performance of CFD applications. Several\ngenerations of architectures were analyzed, such as Rome, Milan, Milan X,\nGenoa, Genoa X and Bergamo, characterized by a different number of cores\n(64-128), L3 cache size (256 - 1152 MB) and RAM type (8-channel DDR4 or\n12-channel DDR5). The research was conducted based on the OpenFOAM application\nusing two memory-bound models: motorBike and Urban Air Pollution. In order to\ncompare the performance of applications on different architectures, the FVOPS\n(Finite VOlumes solved Per Second) metric was introduced, which allows a direct\ncomparison of the performance on the different architectures. It was noticed\nthat local maximum performance occurs in the grid sizes assigned to the\nprocessing process, which is related to individual processor attributes.\nAdditionally, the behavior of the models was analyzed in detail using the\nsoftware profiling analysis tool AMD uProf to reveal the applications'\ninteraction with the hardware. It enabled fine-tuned monitoring of the CPU's\nbehaviours and identified potential inefficiencies in AMD EPYC CPUs. Particular\nattention was paid to the effective use of L2 and L3 cache memory in the\ncontext of their capacity and the bandwidth of memory channels, which are a key\nfactor in memory-bound applications. Processor features were analyzed from a\ncross-platform perspective, which allowed for the determination of metrics of\nparticular importance in terms of their impact on the performance achieved by\nCFD applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, the authors focus on assessing the impact of the AMD EPYC\nprocessor architecture on the performance of CFD applications. Several\ngenerations of architectures were analyzed, such as Rome, Milan, Milan X,\nGenoa, Genoa X and Bergamo, characterized by a different number of cores\n(64-128), L3 cache size (256 - 1152 MB) and RAM type (8-channel DDR4 or\n12-channel DDR5). The research was conducted based on the OpenFOAM application\nusing two memory-bound models: motorBike and Urban Air Pollution. In order to\ncompare the performance of applications on different architectures, the FVOPS\n(Finite VOlumes solved Per Second) metric was introduced, which allows a direct\ncomparison of the performance on the different architectures. It was noticed\nthat local maximum performance occurs in the grid sizes assigned to the\nprocessing process, which is related to individual processor attributes.\nAdditionally, the behavior of the models was analyzed in detail using the\nsoftware profiling analysis tool AMD uProf to reveal the applications'\ninteraction with the hardware. It enabled fine-tuned monitoring of the CPU's\nbehaviours and identified potential inefficiencies in AMD EPYC CPUs. Particular\nattention was paid to the effective use of L2 and L3 cache memory in the\ncontext of their capacity and the bandwidth of memory channels, which are a key\nfactor in memory-bound applications. Processor features were analyzed from a\ncross-platform perspective, which allowed for the determination of metrics of\nparticular importance in terms of their impact on the performance achieved by\nCFD applications."
                },
                "authors": [
                    {
                        "name": "Marcin Lawenda"
                    },
                    {
                        "name": "Łukasz Szustak"
                    },
                    {
                        "name": "László Környei"
                    },
                    {
                        "name": "Flavio Cesar Cunha Galeazzo"
                    },
                    {
                        "name": "Paweł Bratek"
                    }
                ],
                "author_detail": {
                    "name": "Paweł Bratek"
                },
                "author": "Paweł Bratek",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18231v1",
                "updated": "2025-05-23T12:40:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    40,
                    7,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T12:40:07Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    40,
                    7,
                    4,
                    143,
                    0
                ],
                "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit\n  Vector Quantization of KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit\n  Vector Quantization of KV Cache"
                },
                "summary": "Large Language Model (LLM) inference is typically memory-intensive,\nespecially when processing large batch sizes and long sequences, due to the\nlarge size of key-value (KV) cache. Vector Quantization (VQ) is recently\nadopted to alleviate this issue, but we find that the existing approach is\nsusceptible to distribution shift due to its reliance on calibration datasets.\nTo address this limitation, we introduce NSNQuant, a calibration-free Vector\nQuantization (VQ) technique designed for low-bit compression of the KV cache.\nBy applying a three-step transformation-1) a token-wise normalization\n(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise\nnormalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns\nthe token distribution with the standard normal distribution. This alignment\nenables robust, calibration-free vector quantization using a single reusable\ncodebook. Extensive experiments show that NSNQuant consistently outperforms\nprior methods in both 1-bit and 2-bit settings, offering strong generalization\nand up to 3$\\times$ throughput gain over full-precision baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is typically memory-intensive,\nespecially when processing large batch sizes and long sequences, due to the\nlarge size of key-value (KV) cache. Vector Quantization (VQ) is recently\nadopted to alleviate this issue, but we find that the existing approach is\nsusceptible to distribution shift due to its reliance on calibration datasets.\nTo address this limitation, we introduce NSNQuant, a calibration-free Vector\nQuantization (VQ) technique designed for low-bit compression of the KV cache.\nBy applying a three-step transformation-1) a token-wise normalization\n(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise\nnormalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns\nthe token distribution with the standard normal distribution. This alignment\nenables robust, calibration-free vector quantization using a single reusable\ncodebook. Extensive experiments show that NSNQuant consistently outperforms\nprior methods in both 1-bit and 2-bit settings, offering strong generalization\nand up to 3$\\times$ throughput gain over full-precision baselines."
                },
                "authors": [
                    {
                        "name": "Donghyun Son"
                    },
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17787v1",
                "updated": "2025-05-23T12:00:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    0,
                    9,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T12:00:09Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    0,
                    9,
                    4,
                    143,
                    0
                ],
                "title": "Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM\n  Acceleration"
                },
                "summary": "Large language models (LLMs) have gained great success in various domains.\nExisting systems cache Key and Value within the attention block to avoid\nredundant computations. However, the size of key-value cache (KV cache) is\nunpredictable and can even be tens of times larger than the weights in the long\ncontext length scenario. In this work, we propose Titanus, a software-hardware\nco-design to efficiently compress the KV cache on-the-fly. We first propose the\ncascade pruning-quantization (CPQ) method to reduce the KV cache movement. The\nhierarchical quantization extension strategy is introduced to tackle the\nnon-independent per-channel quantization issue. To further reduce KV cache\nmovement, we transfer only the non-zero KV cache between the accelerator and\noff-chip memory. Moreover, we customize a two-stage design space exploration\nframework for the CPQ method. A novel pipeline and parallelism dataflow is\ndesigned to reduce the first token generation time. Experiments show that\nTitanus achieves 159.9x (49.6x) and 34.8x (29.2x) energy efficiency\n(throughput) compared to Nvidia A100 GPU and FlightLLM respectively. The code\nfor Titanus is available at\nhttps://github.com/peilin-chen/Titanus-for-LLM-acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained great success in various domains.\nExisting systems cache Key and Value within the attention block to avoid\nredundant computations. However, the size of key-value cache (KV cache) is\nunpredictable and can even be tens of times larger than the weights in the long\ncontext length scenario. In this work, we propose Titanus, a software-hardware\nco-design to efficiently compress the KV cache on-the-fly. We first propose the\ncascade pruning-quantization (CPQ) method to reduce the KV cache movement. The\nhierarchical quantization extension strategy is introduced to tackle the\nnon-independent per-channel quantization issue. To further reduce KV cache\nmovement, we transfer only the non-zero KV cache between the accelerator and\noff-chip memory. Moreover, we customize a two-stage design space exploration\nframework for the CPQ method. A novel pipeline and parallelism dataflow is\ndesigned to reduce the first token generation time. Experiments show that\nTitanus achieves 159.9x (49.6x) and 34.8x (29.2x) energy efficiency\n(throughput) compared to Nvidia A100 GPU and FlightLLM respectively. The code\nfor Titanus is available at\nhttps://github.com/peilin-chen/Titanus-for-LLM-acceleration."
                },
                "authors": [
                    {
                        "name": "Peilin Chen"
                    },
                    {
                        "name": "Xiaoxuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxuan Yang"
                },
                "author": "Xiaoxuan Yang",
                "arxiv_comment": "Accepted to GLSVLSI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15684v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15684v2",
                "updated": "2025-05-23T11:59:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    11,
                    59,
                    22,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-21T15:58:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy"
                },
                "summary": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption."
                },
                "authors": [
                    {
                        "name": "Gengyang Li"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15684v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15684v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v4",
                "updated": "2025-05-23T10:45:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    45,
                    9,
                    4,
                    143,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "ARCANE: Adaptive Routing with Caching and Aware Network Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive Routing with Caching and Aware Network Exploration"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose ARCANE, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. ARCANE adapts to network conditions by caching\ngood-performing paths. In case of a network failure, ARCANE re-routes traffic\naway from it in less than 100 microseconds. ARCANE is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nARCANE in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose ARCANE, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. ARCANE adapts to network conditions by caching\ngood-performing paths. In case of a network failure, ARCANE re-routes traffic\naway from it in less than 100 microseconds. ARCANE is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nARCANE in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17694v1",
                "updated": "2025-05-23T10:03:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    3,
                    28,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T10:03:28Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    3,
                    28,
                    4,
                    143,
                    0
                ],
                "title": "FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding"
                },
                "summary": "Prefix-sharing among multiple prompts presents opportunities to combine the\noperations of the shared prefix, while attention computation in the decode\nstage, which becomes a critical bottleneck with increasing context lengths, is\na memory-intensive process requiring heavy memory access on the key-value (KV)\ncache of the prefixes. Therefore, in this paper, we explore the potential of\nprefix-sharing in the attention computation of the decode stage. However, the\ntree structure of the prefix-sharing mechanism presents significant challenges\nfor attention computation in efficiently processing shared KV cache access\npatterns while managing complex dependencies and balancing irregular workloads.\nTo address the above challenges, we propose a dedicated attention kernel to\ncombine the memory access of shared prefixes in the decoding stage, namely\nFlashForge. FlashForge delivers two key innovations: a novel shared-prefix\nattention kernel that optimizes memory hierarchy and exploits both intra-block\nand inter-block parallelism, and a comprehensive workload balancing mechanism\nthat efficiently estimates cost, divides tasks, and schedules execution.\nExperimental results show that FlashForge achieves an average 1.9x speedup and\n120.9x memory access reduction compared to the state-of-the-art FlashDecoding\nkernel regarding attention computation in the decode stage and 3.8x end-to-end\ntime per output token compared to the vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix-sharing among multiple prompts presents opportunities to combine the\noperations of the shared prefix, while attention computation in the decode\nstage, which becomes a critical bottleneck with increasing context lengths, is\na memory-intensive process requiring heavy memory access on the key-value (KV)\ncache of the prefixes. Therefore, in this paper, we explore the potential of\nprefix-sharing in the attention computation of the decode stage. However, the\ntree structure of the prefix-sharing mechanism presents significant challenges\nfor attention computation in efficiently processing shared KV cache access\npatterns while managing complex dependencies and balancing irregular workloads.\nTo address the above challenges, we propose a dedicated attention kernel to\ncombine the memory access of shared prefixes in the decoding stage, namely\nFlashForge. FlashForge delivers two key innovations: a novel shared-prefix\nattention kernel that optimizes memory hierarchy and exploits both intra-block\nand inter-block parallelism, and a comprehensive workload balancing mechanism\nthat efficiently estimates cost, divides tasks, and schedules execution.\nExperimental results show that FlashForge achieves an average 1.9x speedup and\n120.9x memory access reduction compared to the state-of-the-art FlashDecoding\nkernel regarding attention computation in the decode stage and 3.8x end-to-end\ntime per output token compared to the vLLM."
                },
                "authors": [
                    {
                        "name": "Zhibin Wang"
                    },
                    {
                        "name": "Rui Ning"
                    },
                    {
                        "name": "Chao Fang"
                    },
                    {
                        "name": "Zhonghui Zhang"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Shaobo Ma"
                    },
                    {
                        "name": "Mo Zhou"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    },
                    {
                        "name": "Chengying Huan"
                    },
                    {
                        "name": "Rong Gu"
                    },
                    {
                        "name": "Kun Yang"
                    },
                    {
                        "name": "Guihai Chen"
                    },
                    {
                        "name": "Sheng Zhong"
                    },
                    {
                        "name": "Chen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Tian"
                },
                "author": "Chen Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02536v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v3",
                "updated": "2025-05-23T10:01:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    1,
                    57,
                    4,
                    143,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v3",
                "updated": "2025-05-23T09:33:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    9,
                    33,
                    32,
                    4,
                    143,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "ACL25 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11820v2",
                "updated": "2025-05-23T08:12:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    8,
                    12,
                    10,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-17T04:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    4,
                    6,
                    12,
                    5,
                    137,
                    0
                ],
                "title": "Chain-of-Model Learning for Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Model Learning for Language Model"
                },
                "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM."
                },
                "authors": [
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Cen LU"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Zifan Song"
                    },
                    {
                        "name": "Caihua Shan"
                    },
                    {
                        "name": "Yansen Wang"
                    },
                    {
                        "name": "Kan Ren"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Tao Qin"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16839v2",
                "updated": "2025-05-23T07:07:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    7,
                    7,
                    29,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-22T16:07:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding"
                },
                "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Akash Gokul"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "25 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v2",
                "updated": "2025-05-23T04:58:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    4,
                    58,
                    47,
                    4,
                    143,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Quantize What Counts: Bit Allocation Insights Informed by Spectral Gaps\n  in Keys and Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantize What Counts: Bit Allocation Insights Informed by Spectral Gaps\n  in Keys and Values"
                },
                "summary": "Large Language Models (LLMs) have introduced significant advancements to the\ncapabilities of Natural Language Processing (NLP) in recent years. However, as\nthese models continue to scale in size, memory constraints pose substantial\nchallenge. Key and Value cache (KV cache) quantization has been well-documented\nas a promising solution to this limitation. In this work, we provide two novel\ntheorems aimed at enhancing KV quantization methods. Our first theorem, termed\nKey-Value Norm Disparity, states that the key weight matrices by nature carry\nricher information compared to the value weight matrices, as evidenced by\nhigher spectral and Frobenius norms across most of the layers. Our second\ntheorem, Key-Driven Quantization, posits that prioritizing the quantization\nprecision of keys over values induces significant improvements to the overall\nquantization performance. In particular, assigning greater precision to the\nkeys compared to the values achieves a higher degree of precision reduction\nwith minimal impact on model accuracy. We validate these theorems through\ntheory and extensive experiments on several state-of-the-art LLM architectures\nand benchmarks. These findings offer valuable guidelines for improving KV cache\nquantization strategies, facilitating more efficient memory utilization without\ncompromising model performance across diverse NLP tasks. Source code is\navailable at https://github.com/mohsenhariri/spectral-kv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have introduced significant advancements to the\ncapabilities of Natural Language Processing (NLP) in recent years. However, as\nthese models continue to scale in size, memory constraints pose substantial\nchallenge. Key and Value cache (KV cache) quantization has been well-documented\nas a promising solution to this limitation. In this work, we provide two novel\ntheorems aimed at enhancing KV quantization methods. Our first theorem, termed\nKey-Value Norm Disparity, states that the key weight matrices by nature carry\nricher information compared to the value weight matrices, as evidenced by\nhigher spectral and Frobenius norms across most of the layers. Our second\ntheorem, Key-Driven Quantization, posits that prioritizing the quantization\nprecision of keys over values induces significant improvements to the overall\nquantization performance. In particular, assigning greater precision to the\nkeys compared to the values achieves a higher degree of precision reduction\nwith minimal impact on model accuracy. We validate these theorems through\ntheory and extensive experiments on several state-of-the-art LLM architectures\nand benchmarks. These findings offer valuable guidelines for improving KV cache\nquantization strategies, facilitating more efficient memory utilization without\ncompromising model performance across diverse NLP tasks. Source code is\navailable at https://github.com/mohsenhariri/spectral-kv."
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Alan Luo"
                    },
                    {
                        "name": "Mohammadreza Nemati"
                    },
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17331v1",
                "updated": "2025-05-22T22:54:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T22:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training"
                },
                "summary": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Maryam Dialameh"
                    },
                    {
                        "name": "Rezaul Karim"
                    },
                    {
                        "name": "Hossein Rajabzadeh"
                    },
                    {
                        "name": "Omar Mohamed Awad"
                    },
                    {
                        "name": "Hyock Ju Kwon"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Walid Ahmed"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17272v1",
                "updated": "2025-05-22T20:39:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    39,
                    57,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T20:39:57Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    39,
                    57,
                    3,
                    142,
                    0
                ],
                "title": "Zebra-Llama: Towards Extremely Efficient Hybrid Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zebra-Llama: Towards Extremely Efficient Hybrid Models"
                },
                "summary": "With the growing demand for deploying large language models (LLMs) across\ndiverse applications, improving their inference efficiency is crucial for\nsustainable and democratized access. However, retraining LLMs to meet new\nuser-specific requirements is prohibitively expensive and environmentally\nunsustainable. In this work, we propose a practical and scalable alternative:\ncomposing efficient hybrid language models from existing pre-trained models.\nOur approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models\nby combining State Space Models (SSMs) and Multi-head Latent Attention (MLA)\nlayers, using a refined initialization and post-training pipeline to\nefficiently transfer knowledge from pre-trained Transformers. Zebra-Llama\nachieves Transformer-level accuracy with near-SSM efficiency using only 7-11B\ntraining tokens (compared to trillions of tokens required for pre-training) and\nan 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down\nto 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants,\nrespectively-while preserving 100%, 100%, and >97% of average zero-shot\nperformance on LM Harness tasks. Compared to models like MambaInLLaMA,\nX-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive\nor superior accuracy while using significantly fewer tokens, smaller teachers,\nand vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses\nMinitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens,\nover 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves\n2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context\nlength. We will release code and model checkpoints upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for deploying large language models (LLMs) across\ndiverse applications, improving their inference efficiency is crucial for\nsustainable and democratized access. However, retraining LLMs to meet new\nuser-specific requirements is prohibitively expensive and environmentally\nunsustainable. In this work, we propose a practical and scalable alternative:\ncomposing efficient hybrid language models from existing pre-trained models.\nOur approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models\nby combining State Space Models (SSMs) and Multi-head Latent Attention (MLA)\nlayers, using a refined initialization and post-training pipeline to\nefficiently transfer knowledge from pre-trained Transformers. Zebra-Llama\nachieves Transformer-level accuracy with near-SSM efficiency using only 7-11B\ntraining tokens (compared to trillions of tokens required for pre-training) and\nan 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down\nto 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants,\nrespectively-while preserving 100%, 100%, and >97% of average zero-shot\nperformance on LM Harness tasks. Compared to models like MambaInLLaMA,\nX-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive\nor superior accuracy while using significantly fewer tokens, smaller teachers,\nand vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses\nMinitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens,\nover 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves\n2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context\nlength. We will release code and model checkpoints upon acceptance."
                },
                "authors": [
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v2",
                "updated": "2025-05-22T20:10:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    10,
                    16,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16986v1",
                "updated": "2025-05-22T17:54:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios."
                },
                "authors": [
                    {
                        "name": "Amartya Chakraborty"
                    },
                    {
                        "name": "Paresh Dashore"
                    },
                    {
                        "name": "Nadia Bathaee"
                    },
                    {
                        "name": "Anmol Jain"
                    },
                    {
                        "name": "Anirban Das"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Sambit Sahu"
                    },
                    {
                        "name": "Milind Naphade"
                    },
                    {
                        "name": "Genta Indra Winata"
                    }
                ],
                "author_detail": {
                    "name": "Genta Indra Winata"
                },
                "author": "Genta Indra Winata",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v1",
                "updated": "2025-05-22T17:33:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning"
                },
                "summary": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Zhongwei Yu"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15431v2",
                "updated": "2025-05-22T06:44:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    44,
                    25,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T12:11:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    11,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
                },
                "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Tencent Hunyuan Team"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Botong Zhou"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "ChenChen Zhang"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Guanwei Zhang"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Lan Jiang"
                    },
                    {
                        "name": "Lixin Liu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Peiqi Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Qianbiao Xiang"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Richard Guo"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Tian Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Weijin Zhou"
                    },
                    {
                        "name": "Weikang Wang"
                    },
                    {
                        "name": "Wesleye Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Yang Zhen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Yulong Wang"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "ZhenXiang Yan"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Alex Yan"
                    },
                    {
                        "name": "Ande Liang"
                    },
                    {
                        "name": "Baitong Liu"
                    },
                    {
                        "name": "Beiping Pan"
                    },
                    {
                        "name": "Bin Xing"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Bingxin Qu"
                    },
                    {
                        "name": "Bolin Ni"
                    },
                    {
                        "name": "Boyu Wu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chengjun Liu"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Chiyu Wang"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Daisy Yi"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Fanyang Lu"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Guanghua Yu"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Guohua Wang"
                    },
                    {
                        "name": "Haisheng Lin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Haoqing Jiang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Huangjin Dai"
                    },
                    {
                        "name": "Huankui Chen"
                    },
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Huihui Cai"
                    },
                    {
                        "name": "Huxin Peng"
                    },
                    {
                        "name": "Jackson Lv"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Jiangtao Guan"
                    },
                    {
                        "name": "Jianing Xu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jieneng Yang"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jin lv"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Jinxing Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Juntao Guo"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Liya Zhan"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Long Xu"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Nanli Chen"
                    },
                    {
                        "name": "Peirui Chen"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Pengju Pan"
                    },
                    {
                        "name": "Pengzhi Wei"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Ruixu Zhou"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shuaishuai Chang"
                    },
                    {
                        "name": "Shulin Liu"
                    },
                    {
                        "name": "SiQi Wang"
                    },
                    {
                        "name": "Songjia Feng"
                    },
                    {
                        "name": "Songling Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Tongkai Li"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Weigang Zhang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Wenzhi Sun"
                    },
                    {
                        "name": "Wenzhuo Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Xiangyu He"
                    },
                    {
                        "name": "Xianshun Ren"
                    },
                    {
                        "name": "XiaoYing Zhu"
                    },
                    {
                        "name": "Xiaolong Guo"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Xican Lu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xudong Gao"
                    },
                    {
                        "name": "Xun Luo"
                    },
                    {
                        "name": "Xuxiang Qi"
                    },
                    {
                        "name": "Yangkun Chen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yanling Xiao"
                    },
                    {
                        "name": "Yantao Mai"
                    },
                    {
                        "name": "Yanze Chen"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Yeting Yang"
                    },
                    {
                        "name": "YiFan Song"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yijiao Zhu"
                    },
                    {
                        "name": "Yinhe Wu"
                    },
                    {
                        "name": "Yixian Liu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuanjun Cai"
                    },
                    {
                        "name": "Yuanlin Tu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yuhui Hu"
                    },
                    {
                        "name": "Yujin Lin"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Zhaoliang Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhiguang Liu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zhiqing Kui"
                    },
                    {
                        "name": "Zhiyin Zeng"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Zhuo Han"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Zigang Geng"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Ziyan Tang"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Zonglei Zhu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Xu"
                },
                "author": "Zhijiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16271v1",
                "updated": "2025-05-22T06:04:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    4,
                    20,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T06:04:20Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    4,
                    20,
                    3,
                    142,
                    0
                ],
                "title": "Variations of the Near-Surface Electric field measured at Aragats during\n  Geomagnetic Storms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variations of the Near-Surface Electric field measured at Aragats during\n  Geomagnetic Storms"
                },
                "summary": "At least two mechanisms effectively transfer interplanetary magnetic field\n(IMF) disturbances into the atmosphere. First, the inflow of solar wind into\nthe ionosphere at low latitudes significantly enhances the total vertical\nelectron content, increasing atmospheric conductivity. Second, Forbush\ndecreases (FD) reduce the cosmic ray flux by a few percent, lowering ionization\nlevels at middle latitudes and decreasing conductivity. Changes in atmospheric\nconductivity affect the global electric circuit and atmospheric electric field\n(AEF). However, to study the response of AEF to geomagnetic storms (GMS), it is\nnecessary to carefully monitor atmospheric conditions before and during storms,\nas meteorological influences can be much stronger than those of GMS. Charged\nclouds above detectors, lightning flashes, and abrupt weather changes\nsignificantly impact near-surface electric field (NSEF) variations, which serve\nas a proxy for AEF measured at the Earth's surface. The facilities at Aragats\nstation monitor all environmental parameters on a one-minute timescale. We\nanalyze four GMS events described in previous studies, detailing the\ncorresponding weather conditions to isolate the genuine influence of GMS on\nNSEF. The GMS of June 22, 2015, and September 8, 2017, occurred under\nfair-weather conditions, providing clear evidence of GMS influence on NSEF.\nThese events were long-lasting, positive, and modest, ranging from 0.2 to 0.3\nkV/m, and coincided with the depletion phase of FD. The sky was clear, no rain\nwas detected, and lightning flashes from previous thunderstorms were more than\n20 km from the station. The other two events did not meet favorable weather\ncriteria, and their occurrence during GMS seemed incidental. We identify a\nfeature that may indicate the solar (FD) origin of NSEF enhancement: a dip in\nthe enhanced NSEF during the daytime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At least two mechanisms effectively transfer interplanetary magnetic field\n(IMF) disturbances into the atmosphere. First, the inflow of solar wind into\nthe ionosphere at low latitudes significantly enhances the total vertical\nelectron content, increasing atmospheric conductivity. Second, Forbush\ndecreases (FD) reduce the cosmic ray flux by a few percent, lowering ionization\nlevels at middle latitudes and decreasing conductivity. Changes in atmospheric\nconductivity affect the global electric circuit and atmospheric electric field\n(AEF). However, to study the response of AEF to geomagnetic storms (GMS), it is\nnecessary to carefully monitor atmospheric conditions before and during storms,\nas meteorological influences can be much stronger than those of GMS. Charged\nclouds above detectors, lightning flashes, and abrupt weather changes\nsignificantly impact near-surface electric field (NSEF) variations, which serve\nas a proxy for AEF measured at the Earth's surface. The facilities at Aragats\nstation monitor all environmental parameters on a one-minute timescale. We\nanalyze four GMS events described in previous studies, detailing the\ncorresponding weather conditions to isolate the genuine influence of GMS on\nNSEF. The GMS of June 22, 2015, and September 8, 2017, occurred under\nfair-weather conditions, providing clear evidence of GMS influence on NSEF.\nThese events were long-lasting, positive, and modest, ranging from 0.2 to 0.3\nkV/m, and coincided with the depletion phase of FD. The sky was clear, no rain\nwas detected, and lightning flashes from previous thunderstorms were more than\n20 km from the station. The other two events did not meet favorable weather\ncriteria, and their occurrence during GMS seemed incidental. We identify a\nfeature that may indicate the solar (FD) origin of NSEF enhancement: a dip in\nthe enhanced NSEF during the daytime."
                },
                "authors": [
                    {
                        "name": "A. Chilingarian"
                    }
                ],
                "author_detail": {
                    "name": "A. Chilingarian"
                },
                "author": "A. Chilingarian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15793v2",
                "updated": "2025-05-22T04:48:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    48,
                    12,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T17:47:24Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    47,
                    24,
                    2,
                    141,
                    0
                ],
                "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for\n  Autonomous Driving"
                },
                "summary": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations. Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations. Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios."
                },
                "authors": [
                    {
                        "name": "Zhiwen Chen"
                    },
                    {
                        "name": "Bo Leng"
                    },
                    {
                        "name": "Zhuoren Li"
                    },
                    {
                        "name": "Hanming Deng"
                    },
                    {
                        "name": "Guizhe Jin"
                    },
                    {
                        "name": "Ran Yu"
                    },
                    {
                        "name": "Huanxi Wen"
                    }
                ],
                "author_detail": {
                    "name": "Huanxi Wen"
                },
                "author": "Huanxi Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16210v1",
                "updated": "2025-05-22T04:23:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    23,
                    19,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T04:23:19Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    23,
                    19,
                    3,
                    142,
                    0
                ],
                "title": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution\n  Characteristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution\n  Characteristics"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used."
                },
                "authors": [
                    {
                        "name": "Zhihang Cai"
                    },
                    {
                        "name": "Xingjun Zhang"
                    },
                    {
                        "name": "Zhendong Tan"
                    },
                    {
                        "name": "Zheng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wei"
                },
                "author": "Zheng Wei",
                "arxiv_comment": "11 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16175v1",
                "updated": "2025-05-22T03:26:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    26,
                    50,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T03:26:50Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    26,
                    50,
                    3,
                    142,
                    0
                ],
                "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design"
                },
                "summary": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice."
                },
                "authors": [
                    {
                        "name": "Benjamin Schneider"
                    },
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "19 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17132v1",
                "updated": "2025-05-22T03:00:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    0,
                    39,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T03:00:39Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    0,
                    39,
                    3,
                    142,
                    0
                ],
                "title": "Robustifying Vision-Language Models via Dynamic Token Reweighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Vision-Language Models via Dynamic Token Reweighting"
                },
                "summary": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. The code for replicating DTR is available:\nhttps://anonymous.4open.science/r/DTR-2755 (warning: this paper contains\npotentially harmful content generated by VLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. The code for replicating DTR is available:\nhttps://anonymous.4open.science/r/DTR-2755 (warning: this paper contains\npotentially harmful content generated by VLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Rongyi Zhu"
                    },
                    {
                        "name": "Jiawei Zhou"
                    },
                    {
                        "name": "Fenglong Ma"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16076v1",
                "updated": "2025-05-21T23:23:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    23,
                    23,
                    37,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T23:23:37Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    23,
                    23,
                    37,
                    2,
                    141,
                    0
                ],
                "title": "AudioMorphix: Training-free audio editing with diffusion probabilistic\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioMorphix: Training-free audio editing with diffusion probabilistic\n  models"
                },
                "summary": "Editing sound with precision is a crucial yet underexplored challenge in\naudio content creation. While existing works can manipulate sounds by text\ninstructions or audio exemplar pairs, they often struggled to modify audio\ncontent precisely while preserving fidelity to the original recording. In this\nwork, we introduce a novel editing approach that enables localized\nmodifications to specific time-frequency regions while keeping the remaining of\nthe audio intact by operating on spectrograms directly. To achieve this, we\npropose AudioMorphix, a training-free audio editor that manipulates a target\nregion on the spectrogram by referring to another recording. Inspired by\nmorphing theory, we conceptualize audio mixing as a process where different\nsounds blend seamlessly through morphing and can be decomposed back into\nindividual components via demorphing. Our AudioMorphix optimizes the noised\nlatent conditioned on raw input and reference audio while rectifying the guided\ndiffusion process through a series of energy functions. Additionally, we\nenhance self-attention layers with a cache mechanism to preserve detailed\ncharacteristics from the original recordings. To advance audio editing\nresearch, we devise a new evaluation benchmark, which includes a curated\ndataset with a variety of editing instructions. Extensive experiments\ndemonstrate that AudioMorphix yields promising performance on various audio\nediting tasks, including addition, removal, time shifting and stretching, and\npitch shifting, achieving high fidelity and precision. Demo and code are\navailable at this url.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Editing sound with precision is a crucial yet underexplored challenge in\naudio content creation. While existing works can manipulate sounds by text\ninstructions or audio exemplar pairs, they often struggled to modify audio\ncontent precisely while preserving fidelity to the original recording. In this\nwork, we introduce a novel editing approach that enables localized\nmodifications to specific time-frequency regions while keeping the remaining of\nthe audio intact by operating on spectrograms directly. To achieve this, we\npropose AudioMorphix, a training-free audio editor that manipulates a target\nregion on the spectrogram by referring to another recording. Inspired by\nmorphing theory, we conceptualize audio mixing as a process where different\nsounds blend seamlessly through morphing and can be decomposed back into\nindividual components via demorphing. Our AudioMorphix optimizes the noised\nlatent conditioned on raw input and reference audio while rectifying the guided\ndiffusion process through a series of energy functions. Additionally, we\nenhance self-attention layers with a cache mechanism to preserve detailed\ncharacteristics from the original recordings. To advance audio editing\nresearch, we devise a new evaluation benchmark, which includes a curated\ndataset with a variety of editing instructions. Extensive experiments\ndemonstrate that AudioMorphix yields promising performance on various audio\nediting tasks, including addition, removal, time shifting and stretching, and\npitch shifting, achieving high fidelity and precision. Demo and code are\navailable at this url."
                },
                "authors": [
                    {
                        "name": "Jinhua Liang"
                    },
                    {
                        "name": "Yuanzhe Chen"
                    },
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Dongya Jia"
                    },
                    {
                        "name": "Xiaobin Zhuang"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Wang"
                },
                "author": "Yuxuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16056v1",
                "updated": "2025-05-21T22:13:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T22:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models"
                },
                "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."
                },
                "authors": [
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Miren Tian"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v2",
                "updated": "2025-05-21T20:52:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    20,
                    52,
                    59,
                    2,
                    141,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache. Accepted\n  at CVPR eLVM workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v2",
                "updated": "2025-05-21T20:42:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    20,
                    42,
                    8,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15781v1",
                "updated": "2025-05-21T17:32:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    32,
                    10,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:32:10Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    32,
                    10,
                    2,
                    141,
                    0
                ],
                "title": "dKV-Cache: The Cache for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dKV-Cache: The Cache for Diffusion Language Models"
                },
                "summary": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs."
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Runpeng Yu"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "The code is available at https://github.com/horseee/dKV-Cache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v1",
                "updated": "2025-05-21T15:58:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "A Federated Splitting Framework for LLMs: Security, Efficiency, and\n  Adaptability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Federated Splitting Framework for LLMs: Security, Efficiency, and\n  Adaptability"
                },
                "summary": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Jiaying Zheng"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Jin Dong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15558v1",
                "updated": "2025-05-21T14:17:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    17,
                    6,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:17:06Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    17,
                    6,
                    2,
                    141,
                    0
                ],
                "title": "Robo-DM: Data Management For Large Robot Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robo-DM: Data Management For Large Robot Datasets"
                },
                "summary": "Recent results suggest that very large datasets of teleoperated robot\ndemonstrations can be used to train transformer-based models that have the\npotential to generalize to new scenes, robots, and tasks. However, curating,\ndistributing, and loading large datasets of robot trajectories, which typically\nconsist of video, textual, and numerical modalities - including streams from\nmultiple cameras - remains challenging. We propose Robo-DM, an efficient\nopen-source cloud-based data management toolkit for collecting, sharing, and\nlearning with robot data. With Robo-DM, robot datasets are stored in a\nself-contained format with Extensible Binary Meta Language (EBML). Robo-DM can\nsignificantly reduce the size of robot trajectory data, transfer costs, and\ndata load time during training. Compared to the RLDS format used in OXE\ndatasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x\n(lossless). Robo-DM also accelerates data retrieval by load-balancing video\ndecoding with memory-mapped decoding caches. Compared to LeRobot, a framework\nthat also uses lossy video compression, Robo-DM is up to 50x faster when\ndecoding sequentially. We physically evaluate a model trained by Robo-DM with\nlossy compression, a pick-and-place task, and In-Context Robot Transformer.\nRobo-DM uses 75x compression of the original dataset and does not suffer\nreduction in downstream task accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent results suggest that very large datasets of teleoperated robot\ndemonstrations can be used to train transformer-based models that have the\npotential to generalize to new scenes, robots, and tasks. However, curating,\ndistributing, and loading large datasets of robot trajectories, which typically\nconsist of video, textual, and numerical modalities - including streams from\nmultiple cameras - remains challenging. We propose Robo-DM, an efficient\nopen-source cloud-based data management toolkit for collecting, sharing, and\nlearning with robot data. With Robo-DM, robot datasets are stored in a\nself-contained format with Extensible Binary Meta Language (EBML). Robo-DM can\nsignificantly reduce the size of robot trajectory data, transfer costs, and\ndata load time during training. Compared to the RLDS format used in OXE\ndatasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x\n(lossless). Robo-DM also accelerates data retrieval by load-balancing video\ndecoding with memory-mapped decoding caches. Compared to LeRobot, a framework\nthat also uses lossy video compression, Robo-DM is up to 50x faster when\ndecoding sequentially. We physically evaluate a model trained by Robo-DM with\nlossy compression, a pick-and-place task, and In-Context Robot Transformer.\nRobo-DM uses 75x compression of the original dataset and does not suffer\nreduction in downstream task accuracy."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "Letian Fu"
                    },
                    {
                        "name": "David Huang"
                    },
                    {
                        "name": "Yanxiang Zhang"
                    },
                    {
                        "name": "Lawrence Yunliang Chen"
                    },
                    {
                        "name": "Huang Huang"
                    },
                    {
                        "name": "Kush Hari"
                    },
                    {
                        "name": "Ashwin Balakrishna"
                    },
                    {
                        "name": "Ted Xiao"
                    },
                    {
                        "name": "Pannag R Sanketi"
                    },
                    {
                        "name": "John Kubiatowicz"
                    },
                    {
                        "name": "Ken Goldberg"
                    }
                ],
                "author_detail": {
                    "name": "Ken Goldberg"
                },
                "author": "Ken Goldberg",
                "arxiv_comment": "Best paper finalist of IEEE ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15535v1",
                "updated": "2025-05-21T13:56:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    56,
                    16,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T13:56:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    56,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "Matrix-Free Methods for Finite-Strain Elasticity: Automatic Code\n  Generation with No Performance Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-Free Methods for Finite-Strain Elasticity: Automatic Code\n  Generation with No Performance Overhead"
                },
                "summary": "This study explores matrix-free tangent evaluations in finite-strain\nelasticity with the use of automatically-generated code for the\nquadrature-point level calculations. The code generation is done via automatic\ndifferentiation (AD) with AceGen. We compare hand-written and AD-generated\ncodes under two computing strategies: on-the-fly evaluation and caching\nintermediate results. The comparison reveals that the AD-generated code\nachieves superior performance in matrix-free computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores matrix-free tangent evaluations in finite-strain\nelasticity with the use of automatically-generated code for the\nquadrature-point level calculations. The code generation is done via automatic\ndifferentiation (AD) with AceGen. We compare hand-written and AD-generated\ncodes under two computing strategies: on-the-fly evaluation and caching\nintermediate results. The comparison reveals that the AD-generated code\nachieves superior performance in matrix-free computations."
                },
                "authors": [
                    {
                        "name": "Michał Wichrowski"
                    },
                    {
                        "name": "Mohsen Rezaee-Hajidehi"
                    },
                    {
                        "name": "Jože Korelc"
                    },
                    {
                        "name": "Martin Kronbichler"
                    },
                    {
                        "name": "Stanisław Stupkiewicz"
                    }
                ],
                "author_detail": {
                    "name": "Stanisław Stupkiewicz"
                },
                "author": "Stanisław Stupkiewicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65M60, 74B20, 74S05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15531v1",
                "updated": "2025-05-21T13:52:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    52,
                    45,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T13:52:45Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    52,
                    45,
                    2,
                    141,
                    0
                ],
                "title": "Modeling and Optimizing Latency for Delayed Hit Caching with Stochastic\n  Miss Latency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Optimizing Latency for Delayed Hit Caching with Stochastic\n  Miss Latency"
                },
                "summary": "Caching is crucial for system performance, but the delayed hit phenomenon,\nwhere requests queue during lengthy fetches after a cache miss, significantly\ndegrades user-perceived latency in modern high-throughput systems. While prior\nworks address delayed hits by estimating aggregate delay, they universally\nassume deterministic fetch latencies. This paper tackles the more realistic,\nyet unexplored, scenario where fetch latencies are stochastic. We present, to\nour knowledge, the first theoretical analysis of delayed hits under this\ncondition, deriving analytical expressions for both the mean and variance of\nthe aggregate delay assuming exponentially distributed fetch latency.\nLeveraging these insights, we develop a novel variance-aware ranking function\ntailored for this stochastic setting to guide cache eviction decisions more\neffectively. The simulations on synthetic and real-world datasets demonstrate\nthat our proposed algorithm significantly reduces overall latency compared to\nstate-of-the-art delayed-hit strategies, achieving a $3\\%-30\\%$ reduction on\nsynthetic datasets and approximately $1\\%-7\\%$ reduction on real-world traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is crucial for system performance, but the delayed hit phenomenon,\nwhere requests queue during lengthy fetches after a cache miss, significantly\ndegrades user-perceived latency in modern high-throughput systems. While prior\nworks address delayed hits by estimating aggregate delay, they universally\nassume deterministic fetch latencies. This paper tackles the more realistic,\nyet unexplored, scenario where fetch latencies are stochastic. We present, to\nour knowledge, the first theoretical analysis of delayed hits under this\ncondition, deriving analytical expressions for both the mean and variance of\nthe aggregate delay assuming exponentially distributed fetch latency.\nLeveraging these insights, we develop a novel variance-aware ranking function\ntailored for this stochastic setting to guide cache eviction decisions more\neffectively. The simulations on synthetic and real-world datasets demonstrate\nthat our proposed algorithm significantly reduces overall latency compared to\nstate-of-the-art delayed-hit strategies, achieving a $3\\%-30\\%$ reduction on\nsynthetic datasets and approximately $1\\%-7\\%$ reduction on real-world traces."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chaofan Ma"
                },
                "author": "Chaofan Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v2",
                "updated": "2025-05-21T10:38:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    38,
                    37,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "41 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v2",
                "updated": "2025-05-21T10:38:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    38,
                    1,
                    2,
                    141,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available:\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available:\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Accepted by ICML 2025. Code is available:\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01941v2",
                "updated": "2025-05-21T10:37:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    37,
                    50,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-04T02:23:06Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?"
                },
                "summary": "This paper investigates an underexplored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. Although existing methods achieve impressive compression ratios\non long-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive benchmark KVFundaBench to\nsystematically evaluate the effects of KV cache compression across diverse\nfundamental LLM capabilities, spanning world knowledge, commonsense reasoning,\narithmetic reasoning, code generation, safety, and long-context understanding\nand generation.Our analysis reveals serval key findings: (1)\n\\textit{Task-Dependent Degradation}; (2) \\textit{Model-Type Robustness} (3)\n\\textit{Prompt Length Vulnerability}; (4) \\textit{Chunk-Level Superiority}; (5)\n\\textit{Prompt-Gain Sensitivity}; (6) \\textit{Long-Context Generation\nSensitivity}. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates an underexplored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. Although existing methods achieve impressive compression ratios\non long-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive benchmark KVFundaBench to\nsystematically evaluate the effects of KV cache compression across diverse\nfundamental LLM capabilities, spanning world knowledge, commonsense reasoning,\narithmetic reasoning, code generation, safety, and long-context understanding\nand generation.Our analysis reveals serval key findings: (1)\n\\textit{Task-Dependent Degradation}; (2) \\textit{Model-Type Robustness} (3)\n\\textit{Prompt Length Vulnerability}; (4) \\textit{Chunk-Level Superiority}; (5)\n\\textit{Prompt-Gain Sensitivity}; (6) \\textit{Long-Context Generation\nSensitivity}. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xiuze Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15347v1",
                "updated": "2025-05-21T10:20:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    20,
                    46,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T10:20:46Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    20,
                    46,
                    2,
                    141,
                    0
                ],
                "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15269v1",
                "updated": "2025-05-21T08:47:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    8,
                    47,
                    15,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T08:47:15Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    8,
                    47,
                    15,
                    2,
                    141,
                    0
                ],
                "title": "LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV\n  Cache and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV\n  Cache and Retrieval"
                },
                "summary": "Recent developments in Video Large Language Models (Video LLMs) have enabled\nmodels to process long video sequences and demonstrate remarkable performance.\nNonetheless, studies predominantly focus on offline video question answering,\nneglecting memory usage and response speed that are essential in various\nreal-world applications, such as Deepseek services, autonomous driving, and\nrobotics. To mitigate these challenges, we propose $\\textbf{LiveVLM}$, a\ntraining-free framework specifically designed for streaming, online video\nunderstanding and real-time interaction. Unlike existing works that process\nvideos only after one question is posed, LiveVLM constructs an innovative\nstreaming-oriented KV cache to process video streams in real-time, retain\nlong-term video details and eliminate redundant KVs, ensuring prompt responses\nto user queries. For continuous video streams, LiveVLM generates and compresses\nvideo key-value tensors (video KVs) to reserve visual information while\nimproving memory efficiency. Furthermore, when a new question is proposed,\nLiveVLM incorporates an online question-answering process that efficiently\nfetches both short-term and long-term visual information, while minimizing\ninterference from redundant context. Extensive experiments demonstrate that\nLiveVLM enables the foundation LLaVA-OneVision model to process 44$\\times$\nnumber of frames on the same device, and achieves up to 5$\\times$ speedup in\nresponse speed compared with SoTA online methods at an input of 256 frames,\nwhile maintaining the same or better model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in Video Large Language Models (Video LLMs) have enabled\nmodels to process long video sequences and demonstrate remarkable performance.\nNonetheless, studies predominantly focus on offline video question answering,\nneglecting memory usage and response speed that are essential in various\nreal-world applications, such as Deepseek services, autonomous driving, and\nrobotics. To mitigate these challenges, we propose $\\textbf{LiveVLM}$, a\ntraining-free framework specifically designed for streaming, online video\nunderstanding and real-time interaction. Unlike existing works that process\nvideos only after one question is posed, LiveVLM constructs an innovative\nstreaming-oriented KV cache to process video streams in real-time, retain\nlong-term video details and eliminate redundant KVs, ensuring prompt responses\nto user queries. For continuous video streams, LiveVLM generates and compresses\nvideo key-value tensors (video KVs) to reserve visual information while\nimproving memory efficiency. Furthermore, when a new question is proposed,\nLiveVLM incorporates an online question-answering process that efficiently\nfetches both short-term and long-term visual information, while minimizing\ninterference from redundant context. Extensive experiments demonstrate that\nLiveVLM enables the foundation LLaVA-OneVision model to process 44$\\times$\nnumber of frames on the same device, and achieves up to 5$\\times$ speedup in\nresponse speed compared with SoTA online methods at an input of 256 frames,\nwhile maintaining the same or better model performance."
                },
                "authors": [
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Qihao Jin"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v2",
                "updated": "2025-05-21T06:45:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    6,
                    45,
                    58,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to reduce latency for\nlong-context inference. FastKV improves processing speed while preserving\naccuracy by adopting Token-Selective Propagation (TSP). This approach preserves\nfull-context information in early layers of LLMs and selectively propagates\nonly a portion of this information in later layers. This design enables FastKV\nto minimize redundant computation without sacrificing contextual fidelity. Our\nexperimental results show that FastKV achieves up to 1.97$\\times$ and\n4.82$\\times$ improvements in time-to-first-token (TTFT) and throughput,\nrespectively, compared to baseline without KV cache compression. Moreover,\nFastKV successfully maintains accuracy within 1\\% of the baseline on\nlong-context benchmarks. Our code is available at\nhttps://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to reduce latency for\nlong-context inference. FastKV improves processing speed while preserving\naccuracy by adopting Token-Selective Propagation (TSP). This approach preserves\nfull-context information in early layers of LLMs and selectively propagates\nonly a portion of this information in later layers. This design enables FastKV\nto minimize redundant computation without sacrificing contextual fidelity. Our\nexperimental results show that FastKV achieves up to 1.97$\\times$ and\n4.82$\\times$ improvements in time-to-first-token (TTFT) and throughput,\nrespectively, compared to baseline without KV cache compression. Moreover,\nFastKV successfully maintains accuracy within 1\\% of the baseline on\nlong-context benchmarks. Our code is available at\nhttps://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15859v1",
                "updated": "2025-05-21T04:32:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    4,
                    32,
                    35,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T04:32:35Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    4,
                    32,
                    35,
                    2,
                    141,
                    0
                ],
                "title": "AutoData: A Multi-Agent System for Open Web Data Collection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoData: A Multi-Agent System for Open Web Data Collection"
                },
                "summary": "The exponential growth of data-driven systems and AI technologies has\nintensified the demand for high-quality web-sourced datasets. While existing\ndatasets have proven valuable, conventional web data collection approaches face\nsignificant limitations in terms of human effort and scalability. Current\ndata-collecting solutions fall into two categories: wrapper-based methods that\nstruggle with adaptability and reproducibility, and large language model\n(LLM)-based approaches that incur substantial computational and financial\ncosts. To address these challenges, we propose AutoData, a novel multi-agent\nsystem for Automated web Data collection, that requires minimal human\nintervention, i.e., only necessitating a natural language instruction\nspecifying the desired dataset. In addition, AutoData is designed with a robust\nmulti-agent architecture, featuring a novel oriented message hypergraph\ncoordinated by a central task manager, to efficiently organize agents across\nresearch and development squads. Besides, we introduce a novel hypergraph cache\nsystem to advance the multi-agent collaboration process that enables efficient\nautomated data collection and mitigates the token cost issues prevalent in\nexisting LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark\ndataset supporting live data collection from web sources across three domains:\nacademic, finance, and sports. Comprehensive evaluations over Instruct2DS and\nthree existing benchmark datasets demonstrate AutoData's superior performance\ncompared to baseline methods. Case studies on challenging tasks such as picture\nbook collection and paper extraction from surveys further validate its\napplicability. Our source code and dataset are available at\nhttps://github.com/GraphResearcher/AutoData.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-driven systems and AI technologies has\nintensified the demand for high-quality web-sourced datasets. While existing\ndatasets have proven valuable, conventional web data collection approaches face\nsignificant limitations in terms of human effort and scalability. Current\ndata-collecting solutions fall into two categories: wrapper-based methods that\nstruggle with adaptability and reproducibility, and large language model\n(LLM)-based approaches that incur substantial computational and financial\ncosts. To address these challenges, we propose AutoData, a novel multi-agent\nsystem for Automated web Data collection, that requires minimal human\nintervention, i.e., only necessitating a natural language instruction\nspecifying the desired dataset. In addition, AutoData is designed with a robust\nmulti-agent architecture, featuring a novel oriented message hypergraph\ncoordinated by a central task manager, to efficiently organize agents across\nresearch and development squads. Besides, we introduce a novel hypergraph cache\nsystem to advance the multi-agent collaboration process that enables efficient\nautomated data collection and mitigates the token cost issues prevalent in\nexisting LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark\ndataset supporting live data collection from web sources across three domains:\nacademic, finance, and sports. Comprehensive evaluations over Instruct2DS and\nthree existing benchmark datasets demonstrate AutoData's superior performance\ncompared to baseline methods. Case studies on challenging tasks such as picture\nbook collection and paper extraction from surveys further validate its\napplicability. Our source code and dataset are available at\nhttps://github.com/GraphResearcher/AutoData."
                },
                "authors": [
                    {
                        "name": "Tianyi Ma"
                    },
                    {
                        "name": "Yiyue Qian"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Zehong Wang"
                    },
                    {
                        "name": "Xiaoye Qian"
                    },
                    {
                        "name": "Feifan Bai"
                    },
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Xuwei Luo"
                    },
                    {
                        "name": "Shinan Zhang"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Chuxu Zhang"
                    },
                    {
                        "name": "Yanfang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yanfang Ye"
                },
                "author": "Yanfang Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.09623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09623v2",
                "updated": "2025-05-29T17:59:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    55,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-13T18:59:50Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    59,
                    50,
                    3,
                    44,
                    0
                ],
                "title": "Weight Space Representation Learning on Diverse NeRF Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weight Space Representation Learning on Diverse NeRF Architectures"
                },
                "summary": "Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for\nrepresenting 3D objects and scenes by encoding shape and appearance information\ninto the weights of a neural network. Recent studies have demonstrated that\nthese weights can be used as input for frameworks designed to address deep\nlearning tasks; however, such frameworks require NeRFs to adhere to a specific,\npredefined architecture. In this paper, we introduce the first framework\ncapable of processing NeRFs with diverse architectures and performing inference\non architectures unseen at training time. We achieve this by training a Graph\nMeta-Network within an unsupervised representation learning framework, and show\nthat a contrastive objective is conducive to obtaining an architecture-agnostic\nlatent space. In experiments conducted across 13 NeRF architectures belonging\nto three families (MLPs, tri-planes, and, for the first time, hash tables), our\napproach demonstrates robust performance in classification and retrieval tasks\ninvolving multiple architectures, even unseen at training time, while also\nexceeding the results of existing frameworks limited to single architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for\nrepresenting 3D objects and scenes by encoding shape and appearance information\ninto the weights of a neural network. Recent studies have demonstrated that\nthese weights can be used as input for frameworks designed to address deep\nlearning tasks; however, such frameworks require NeRFs to adhere to a specific,\npredefined architecture. In this paper, we introduce the first framework\ncapable of processing NeRFs with diverse architectures and performing inference\non architectures unseen at training time. We achieve this by training a Graph\nMeta-Network within an unsupervised representation learning framework, and show\nthat a contrastive objective is conducive to obtaining an architecture-agnostic\nlatent space. In experiments conducted across 13 NeRF architectures belonging\nto three families (MLPs, tri-planes, and, for the first time, hash tables), our\napproach demonstrates robust performance in classification and retrieval tasks\ninvolving multiple architectures, even unseen at training time, while also\nexceeding the results of existing frameworks limited to single architectures."
                },
                "authors": [
                    {
                        "name": "Francesco Ballerini"
                    },
                    {
                        "name": "Pierluigi Zama Ramirez"
                    },
                    {
                        "name": "Samuele Salti"
                    },
                    {
                        "name": "Luigi Di Stefano"
                    }
                ],
                "author_detail": {
                    "name": "Luigi Di Stefano"
                },
                "author": "Luigi Di Stefano",
                "arxiv_comment": "v2: added third NeRF architecture. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23765v1",
                "updated": "2025-05-29T17:59:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    55,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:59:55Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    55,
                    3,
                    149,
                    0
                ],
                "title": "From Chat Logs to Collective Insights: Aggregative Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Chat Logs to Collective Insights: Aggregative Question Answering"
                },
                "summary": "Conversational agents powered by large language models (LLMs) are rapidly\nbecoming integral to our daily interactions, generating unprecedented amounts\nof conversational data. Such datasets offer a powerful lens into societal\ninterests, trending topics, and collective concerns. Yet, existing approaches\ntypically treat these interactions as independent and miss critical insights\nthat could emerge from aggregating and reasoning across large-scale\nconversation logs. In this paper, we introduce Aggregative Question Answering,\na novel task requiring models to reason explicitly over thousands of\nuser-chatbot interactions to answer aggregative queries, such as identifying\nemerging concerns among specific demographics. To enable research in this\ndirection, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative\nquestions derived from 182,330 real-world chatbot conversations. Experiments\nshow that existing methods either struggle to reason effectively or incur\nprohibitive computational costs, underscoring the need for new approaches\ncapable of extracting collective insights from large-scale conversational data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational agents powered by large language models (LLMs) are rapidly\nbecoming integral to our daily interactions, generating unprecedented amounts\nof conversational data. Such datasets offer a powerful lens into societal\ninterests, trending topics, and collective concerns. Yet, existing approaches\ntypically treat these interactions as independent and miss critical insights\nthat could emerge from aggregating and reasoning across large-scale\nconversation logs. In this paper, we introduce Aggregative Question Answering,\na novel task requiring models to reason explicitly over thousands of\nuser-chatbot interactions to answer aggregative queries, such as identifying\nemerging concerns among specific demographics. To enable research in this\ndirection, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative\nquestions derived from 182,330 real-world chatbot conversations. Experiments\nshow that existing methods either struggle to reason effectively or incur\nprohibitive computational costs, underscoring the need for new approaches\ncapable of extracting collective insights from large-scale conversational data."
                },
                "authors": [
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Woojeong Kim"
                    },
                    {
                        "name": "Yuntian Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yuntian Deng"
                },
                "author": "Yuntian Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23763v1",
                "updated": "2025-05-29T17:59:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    51,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:59:51Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    51,
                    3,
                    149,
                    0
                ],
                "title": "Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch"
                },
                "summary": "As sketch research has collectively matured over time, its adaptation for\nat-mass commercialisation emerges on the immediate horizon. Despite an already\nmature research endeavour for photos, there is no research on the efficient\ninference specifically designed for sketch data. In this paper, we first\ndemonstrate existing state-of-the-art efficient light-weight models designed\nfor photos do not work on sketches. We then propose two sketch-specific\ncomponents which work in a plug-n-play manner on any photo efficient network to\nadapt them to work on sketch data. We specifically chose fine-grained\nsketch-based image retrieval (FG-SBIR) as a demonstrator as the most recognised\nsketch problem with immediate commercial value. Technically speaking, we first\npropose a cross-modal knowledge distillation network to transfer existing photo\nefficient networks to be compatible with sketch, which brings down number of\nFLOPs and model parameters by 97.96% percent and 84.89% respectively. We then\nexploit the abstract trait of sketch to introduce a RL-based canvas selector\nthat dynamically adjusts to the abstraction level which further cuts down\nnumber of FLOPs by two thirds. The end result is an overall reduction of 99.37%\nof FLOPs (from 40.18G to 0.254G) when compared with a full network, while\nretaining the accuracy (33.03% vs 32.77%) -- finally making an efficient\nnetwork for the sparse sketch data that exhibit even fewer FLOPs than the best\nphoto counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As sketch research has collectively matured over time, its adaptation for\nat-mass commercialisation emerges on the immediate horizon. Despite an already\nmature research endeavour for photos, there is no research on the efficient\ninference specifically designed for sketch data. In this paper, we first\ndemonstrate existing state-of-the-art efficient light-weight models designed\nfor photos do not work on sketches. We then propose two sketch-specific\ncomponents which work in a plug-n-play manner on any photo efficient network to\nadapt them to work on sketch data. We specifically chose fine-grained\nsketch-based image retrieval (FG-SBIR) as a demonstrator as the most recognised\nsketch problem with immediate commercial value. Technically speaking, we first\npropose a cross-modal knowledge distillation network to transfer existing photo\nefficient networks to be compatible with sketch, which brings down number of\nFLOPs and model parameters by 97.96% percent and 84.89% respectively. We then\nexploit the abstract trait of sketch to introduce a RL-based canvas selector\nthat dynamically adjusts to the abstraction level which further cuts down\nnumber of FLOPs by two thirds. The end result is an overall reduction of 99.37%\nof FLOPs (from 40.18G to 0.254G) when compared with a full network, while\nretaining the accuracy (33.03% vs 32.77%) -- finally making an efficient\nnetwork for the sparse sketch data that exhibit even fewer FLOPs than the best\nphoto counterpart."
                },
                "authors": [
                    {
                        "name": "Aneeshan Sain"
                    },
                    {
                        "name": "Subhajit Maity"
                    },
                    {
                        "name": "Pinaki Nath Chowdhury"
                    },
                    {
                        "name": "Subhadeep Koley"
                    },
                    {
                        "name": "Ayan Kumar Bhunia"
                    },
                    {
                        "name": "Yi-Zhe Song"
                    }
                ],
                "author_detail": {
                    "name": "Yi-Zhe Song"
                },
                "author": "Yi-Zhe Song",
                "arxiv_comment": "Accepted at CVPR 2025, Project Page:\n  https://subhajitmaity.me/SketchDownTheFLOPs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23754v1",
                "updated": "2025-05-29T17:59:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    39,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:59:39Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    39,
                    3,
                    149,
                    0
                ],
                "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural\n  Language and Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural\n  Language and Reinforcement Learning"
                },
                "summary": "Theorem proving serves as a major testbed for evaluating complex reasoning\nabilities in large language models (LLMs). However, traditional automated\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\npoorly align with LLMs' strength derived from informal, natural language\nknowledge acquired during pre-training. In this work, we propose DeepTheorem, a\ncomprehensive informal theorem-proving framework exploiting natural language to\nenhance LLM mathematical reasoning. DeepTheorem includes a large-scale\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\nand proofs spanning diverse mathematical domains, rigorously annotated for\ncorrectness, difficulty, and topic categories, accompanied by systematically\nconstructed verifiable theorem variants. We devise a novel reinforcement\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\nleveraging the verified theorem variants to incentivize robust mathematical\ninference. Additionally, we propose comprehensive outcome and process\nevaluation metrics examining proof correctness and the quality of reasoning\nsteps. Extensive experimental analyses demonstrate DeepTheorem significantly\nimproves LLM theorem-proving performance compared to existing datasets and\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy and\nreasoning quality. Our findings highlight DeepTheorem's potential to\nfundamentally advance automated informal theorem proving and mathematical\nexploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theorem proving serves as a major testbed for evaluating complex reasoning\nabilities in large language models (LLMs). However, traditional automated\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\npoorly align with LLMs' strength derived from informal, natural language\nknowledge acquired during pre-training. In this work, we propose DeepTheorem, a\ncomprehensive informal theorem-proving framework exploiting natural language to\nenhance LLM mathematical reasoning. DeepTheorem includes a large-scale\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\nand proofs spanning diverse mathematical domains, rigorously annotated for\ncorrectness, difficulty, and topic categories, accompanied by systematically\nconstructed verifiable theorem variants. We devise a novel reinforcement\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\nleveraging the verified theorem variants to incentivize robust mathematical\ninference. Additionally, we propose comprehensive outcome and process\nevaluation metrics examining proof correctness and the quality of reasoning\nsteps. Extensive experimental analyses demonstrate DeepTheorem significantly\nimproves LLM theorem-proving performance compared to existing datasets and\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy and\nreasoning quality. Our findings highlight DeepTheorem's potential to\nfundamentally advance automated informal theorem proving and mathematical\nexploration."
                },
                "authors": [
                    {
                        "name": "Ziyin Zhang"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Yansi Li"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Zhengwen Liang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23752v1",
                "updated": "2025-05-29T17:59:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    38,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:59:38Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    38,
                    3,
                    149,
                    0
                ],
                "title": "ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks"
                },
                "summary": "Recent progress in large language models (LLMs) has enabled tool-augmented\nagents capable of solving complex real-world tasks through step-by-step\nreasoning. However, existing evaluations often focus on general-purpose or\nmultimodal scenarios, leaving a gap in domain-specific benchmarks that assess\ntool-use capabilities in complex remote sensing use cases. We present ThinkGeo,\nan agentic benchmark designed to evaluate LLM-driven agents on remote sensing\ntasks via structured tool use and multi-step planning. Inspired by\ntool-interaction paradigms, ThinkGeo includes human-curated queries spanning a\nwide range of real-world applications such as urban planning, disaster\nassessment and change analysis, environmental monitoring, transportation\nanalysis, aviation monitoring, recreational infrastructure, and industrial site\nanalysis. Each query is grounded in satellite or aerial imagery and requires\nagents to reason through a diverse toolset. We implement a ReAct-style\ninteraction loop and evaluate both open and closed-source LLMs (e.g., GPT-4o,\nQwen2.5) on 436 structured agentic tasks. The benchmark reports both step-wise\nexecution metrics and final answer correctness. Our analysis reveals notable\ndisparities in tool accuracy and planning consistency across models. ThinkGeo\nprovides the first extensive testbed for evaluating how tool-enabled LLMs\nhandle spatial reasoning in remote sensing. Our code and dataset are publicly\navailable",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has enabled tool-augmented\nagents capable of solving complex real-world tasks through step-by-step\nreasoning. However, existing evaluations often focus on general-purpose or\nmultimodal scenarios, leaving a gap in domain-specific benchmarks that assess\ntool-use capabilities in complex remote sensing use cases. We present ThinkGeo,\nan agentic benchmark designed to evaluate LLM-driven agents on remote sensing\ntasks via structured tool use and multi-step planning. Inspired by\ntool-interaction paradigms, ThinkGeo includes human-curated queries spanning a\nwide range of real-world applications such as urban planning, disaster\nassessment and change analysis, environmental monitoring, transportation\nanalysis, aviation monitoring, recreational infrastructure, and industrial site\nanalysis. Each query is grounded in satellite or aerial imagery and requires\nagents to reason through a diverse toolset. We implement a ReAct-style\ninteraction loop and evaluate both open and closed-source LLMs (e.g., GPT-4o,\nQwen2.5) on 436 structured agentic tasks. The benchmark reports both step-wise\nexecution metrics and final answer correctness. Our analysis reveals notable\ndisparities in tool accuracy and planning consistency across models. ThinkGeo\nprovides the first extensive testbed for evaluating how tool-enabled LLMs\nhandle spatial reasoning in remote sensing. Our code and dataset are publicly\navailable"
                },
                "authors": [
                    {
                        "name": "Akashah Shabbir"
                    },
                    {
                        "name": "Muhammad Akhtar Munir"
                    },
                    {
                        "name": "Akshay Dudhane"
                    },
                    {
                        "name": "Muhammad Umer Sheikh"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    },
                    {
                        "name": "Paolo Fraccaro"
                    },
                    {
                        "name": "Juan Bernabe Moreno"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    },
                    {
                        "name": "Salman Khan"
                    }
                ],
                "author_detail": {
                    "name": "Salman Khan"
                },
                "author": "Salman Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23746v1",
                "updated": "2025-05-29T17:59:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    4,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:59:04Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    4,
                    3,
                    149,
                    0
                ],
                "title": "Comparative of Genetic Fuzzy regression techniques for aeroacoustic\n  phenomenons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative of Genetic Fuzzy regression techniques for aeroacoustic\n  phenomenons"
                },
                "summary": "This study investigates the application of Genetic Fuzzy Systems (GFS) to\nmodel the self-noise generated by airfoils, a key issue in aeroaccoustics with\nsignificant implications for aerospace, automotive and drone applications.\nUsing the publicly available Airfoil Self Noise dataset, various Fuzzy\nregression strategies are explored and compared. The paper evaluates a brute\nforce Takagi Sugeno Kang (TSK) fuzzy system with high rule density, a cascading\nGeneti Fuzzy Tree (GFT) architecture and a novel clustered approach based on\nFuzzy C-means (FCM) to reduce the model's complexity. This highlights the\nviability of clustering assisted fuzzy inference as an effective regression\ntool for complex aero accoustic phenomena. Keywords : Fuzzy logic, Regression,\nCascading systems, Clustering and AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the application of Genetic Fuzzy Systems (GFS) to\nmodel the self-noise generated by airfoils, a key issue in aeroaccoustics with\nsignificant implications for aerospace, automotive and drone applications.\nUsing the publicly available Airfoil Self Noise dataset, various Fuzzy\nregression strategies are explored and compared. The paper evaluates a brute\nforce Takagi Sugeno Kang (TSK) fuzzy system with high rule density, a cascading\nGeneti Fuzzy Tree (GFT) architecture and a novel clustered approach based on\nFuzzy C-means (FCM) to reduce the model's complexity. This highlights the\nviability of clustering assisted fuzzy inference as an effective regression\ntool for complex aero accoustic phenomena. Keywords : Fuzzy logic, Regression,\nCascading systems, Clustering and AI."
                },
                "authors": [
                    {
                        "name": "Hugo Henry"
                    },
                    {
                        "name": "Kelly Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Kelly Cohen"
                },
                "author": "Kelly Cohen",
                "arxiv_comment": "11 pages and 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23747v1",
                "updated": "2025-05-29T17:59:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    4,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:59:04Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    4,
                    3,
                    149,
                    0
                ],
                "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial\n  Intelligence"
                },
                "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/."
                },
                "authors": [
                    {
                        "name": "Diankun Wu"
                    },
                    {
                        "name": "Fangfu Liu"
                    },
                    {
                        "name": "Yi-Hsin Hung"
                    },
                    {
                        "name": "Yueqi Duan"
                    }
                ],
                "author_detail": {
                    "name": "Yueqi Duan"
                },
                "author": "Yueqi Duan",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23742v1",
                "updated": "2025-05-29T17:58:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    58,
                    15,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:58:15Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    58,
                    15,
                    3,
                    149,
                    0
                ],
                "title": "MAGREF: Masked Guidance for Any-Reference Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGREF: Masked Guidance for Any-Reference Video Generation"
                },
                "summary": "Video generation has made substantial strides with the emergence of deep\ngenerative models, especially diffusion-based approaches. However, video\ngeneration based on multiple reference subjects still faces significant\nchallenges in maintaining multi-subject consistency and ensuring high\ngeneration quality. In this paper, we propose MAGREF, a unified framework for\nany-reference video generation that introduces masked guidance to enable\ncoherent multi-subject video synthesis conditioned on diverse reference images\nand a textual prompt. Specifically, we propose (1) a region-aware dynamic\nmasking mechanism that enables a single model to flexibly handle various\nsubject inference, including humans, objects, and backgrounds, without\narchitectural changes, and (2) a pixel-wise channel concatenation mechanism\nthat operates on the channel dimension to better preserve appearance features.\nOur model delivers state-of-the-art video generation quality, generalizing from\nsingle-subject training to complex multi-subject scenarios with coherent\nsynthesis and precise control over individual subjects, outperforming existing\nopen-source and commercial baselines. To facilitate evaluation, we also\nintroduce a comprehensive multi-subject video benchmark. Extensive experiments\ndemonstrate the effectiveness of our approach, paving the way for scalable,\ncontrollable, and high-fidelity multi-subject video synthesis. Code and model\ncan be found at: https://github.com/MAGREF-Video/MAGREF",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation has made substantial strides with the emergence of deep\ngenerative models, especially diffusion-based approaches. However, video\ngeneration based on multiple reference subjects still faces significant\nchallenges in maintaining multi-subject consistency and ensuring high\ngeneration quality. In this paper, we propose MAGREF, a unified framework for\nany-reference video generation that introduces masked guidance to enable\ncoherent multi-subject video synthesis conditioned on diverse reference images\nand a textual prompt. Specifically, we propose (1) a region-aware dynamic\nmasking mechanism that enables a single model to flexibly handle various\nsubject inference, including humans, objects, and backgrounds, without\narchitectural changes, and (2) a pixel-wise channel concatenation mechanism\nthat operates on the channel dimension to better preserve appearance features.\nOur model delivers state-of-the-art video generation quality, generalizing from\nsingle-subject training to complex multi-subject scenarios with coherent\nsynthesis and precise control over individual subjects, outperforming existing\nopen-source and commercial baselines. To facilitate evaluation, we also\nintroduce a comprehensive multi-subject video benchmark. Extensive experiments\ndemonstrate the effectiveness of our approach, paving the way for scalable,\ncontrollable, and high-fidelity multi-subject video synthesis. Code and model\ncan be found at: https://github.com/MAGREF-Video/MAGREF"
                },
                "authors": [
                    {
                        "name": "Yufan Deng"
                    },
                    {
                        "name": "Xun Guo"
                    },
                    {
                        "name": "Yuanyang Yin"
                    },
                    {
                        "name": "Jacob Zhiyuan Fang"
                    },
                    {
                        "name": "Yiding Yang"
                    },
                    {
                        "name": "Yizhi Wang"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Angtian Wang"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Haibin Huang"
                    },
                    {
                        "name": "Chongyang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chongyang Ma"
                },
                "author": "Chongyang Ma",
                "arxiv_comment": "Project website: https://magref-video.github.io/magref.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17690v2",
                "updated": "2025-05-29T17:58:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    58,
                    2,
                    3,
                    149,
                    0
                ],
                "published": "2024-11-26T18:57:29Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    57,
                    29,
                    1,
                    331,
                    0
                ],
                "title": "Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis"
                },
                "summary": "The rapid progress of foundation models and large language models (LLMs) has\nfueled significantly improvement in the capabilities of machine learning\nsystems that benefit from mutlimodal input data. However, existing multimodal\nmodels are predominantly built on top of pre-trained LLMs, which can limit\naccurate modeling of temporal dependencies across other modalities and thus\nlimit the model's ability to jointly process and leverage multimodal inputs. To\nspecifically investigate the alignment of text, video, and speech modalities in\nLLM-style (decoder-only) models, we consider a simplified multimodal generation\ntask, Video-Text to Speech (VTTS): speech generation conditioned on both its\ncorresponding text and video of talking people. The ultimate goal is to\ngenerate speech that not only follows the text but also aligns temporally with\nthe video and is consistent with the facial expressions. In this paper, we\nfirst introduce Visatronic, a unified multimodal decoder-only transformer model\nthat adopts an LLM-style architecture to embed visual, textual, and speech\ninputs into a shared subspace, treating all modalities as temporally aligned\ntoken streams. Next, we carefully explore different token mixing strategies to\nunderstand the best way to propagate information from the steps where video and\ntext conditioning is input to the steps where the audio is generated. We\nextensively evaluate Visatronic on the challenging VoxCeleb2 dataset and\ndemonstrate zero-shot generalization to LRS3, where Visatronic, trained on\nVoxCeleb2, achieves a 4.5% WER, outperforming prior SOTA methods trained only\non LRS3, which report a 21.4% WER. Additionally, we propose a new objective\nmetric, TimeSync, specifically designed to measure phoneme-level temporal\nalignment between generated and reference speech, further ensuring\nsynchronization quality. Demo: https://apple.github.io/visatronic-demo/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of foundation models and large language models (LLMs) has\nfueled significantly improvement in the capabilities of machine learning\nsystems that benefit from mutlimodal input data. However, existing multimodal\nmodels are predominantly built on top of pre-trained LLMs, which can limit\naccurate modeling of temporal dependencies across other modalities and thus\nlimit the model's ability to jointly process and leverage multimodal inputs. To\nspecifically investigate the alignment of text, video, and speech modalities in\nLLM-style (decoder-only) models, we consider a simplified multimodal generation\ntask, Video-Text to Speech (VTTS): speech generation conditioned on both its\ncorresponding text and video of talking people. The ultimate goal is to\ngenerate speech that not only follows the text but also aligns temporally with\nthe video and is consistent with the facial expressions. In this paper, we\nfirst introduce Visatronic, a unified multimodal decoder-only transformer model\nthat adopts an LLM-style architecture to embed visual, textual, and speech\ninputs into a shared subspace, treating all modalities as temporally aligned\ntoken streams. Next, we carefully explore different token mixing strategies to\nunderstand the best way to propagate information from the steps where video and\ntext conditioning is input to the steps where the audio is generated. We\nextensively evaluate Visatronic on the challenging VoxCeleb2 dataset and\ndemonstrate zero-shot generalization to LRS3, where Visatronic, trained on\nVoxCeleb2, achieves a 4.5% WER, outperforming prior SOTA methods trained only\non LRS3, which report a 21.4% WER. Additionally, we propose a new objective\nmetric, TimeSync, specifically designed to measure phoneme-level temporal\nalignment between generated and reference speech, further ensuring\nsynchronization quality. Demo: https://apple.github.io/visatronic-demo/"
                },
                "authors": [
                    {
                        "name": "Akshita Gupta"
                    },
                    {
                        "name": "Tatiana Likhomanenko"
                    },
                    {
                        "name": "Karren Dai Yang"
                    },
                    {
                        "name": "Richard He Bai"
                    },
                    {
                        "name": "Zakaria Aldeneh"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    }
                ],
                "author_detail": {
                    "name": "Navdeep Jaitly"
                },
                "author": "Navdeep Jaitly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23734v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23734v2",
                "updated": "2025-05-30T06:57:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    6,
                    57,
                    49,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-29T17:57:04Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    57,
                    4,
                    3,
                    149,
                    0
                ],
                "title": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS"
                },
                "summary": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a\npromising solution for novel view synthesis, enabling one-pass inference\nwithout the need for per-scene 3DGS optimization. However, their scalability is\nfundamentally constrained by the limited capacity of their encoders, leading to\ndegraded performance or excessive memory consumption as the number of input\nviews increases. In this work, we analyze feed-forward 3DGS frameworks through\nthe lens of the Information Bottleneck principle and introduce ZPressor, a\nlightweight architecture-agnostic module that enables efficient compression of\nmulti-view inputs into a compact latent state $Z$ that retains essential scene\ninformation while discarding redundancy. Concretely, ZPressor enables existing\nfeed-forward 3DGS models to scale to over 100 input views at 480P resolution on\nan 80GB GPU, by partitioning the views into anchor and support sets and using\ncross attention to compress the information from the support views into anchor\nviews, forming the compressed latent state $Z$. We show that integrating\nZPressor into several state-of-the-art feed-forward 3DGS models consistently\nimproves performance under moderate input views and enhances robustness under\ndense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K.\nThe video results, code and trained models are available on our project page:\nhttps://lhmd.top/zpressor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a\npromising solution for novel view synthesis, enabling one-pass inference\nwithout the need for per-scene 3DGS optimization. However, their scalability is\nfundamentally constrained by the limited capacity of their encoders, leading to\ndegraded performance or excessive memory consumption as the number of input\nviews increases. In this work, we analyze feed-forward 3DGS frameworks through\nthe lens of the Information Bottleneck principle and introduce ZPressor, a\nlightweight architecture-agnostic module that enables efficient compression of\nmulti-view inputs into a compact latent state $Z$ that retains essential scene\ninformation while discarding redundancy. Concretely, ZPressor enables existing\nfeed-forward 3DGS models to scale to over 100 input views at 480P resolution on\nan 80GB GPU, by partitioning the views into anchor and support sets and using\ncross attention to compress the information from the support views into anchor\nviews, forming the compressed latent state $Z$. We show that integrating\nZPressor into several state-of-the-art feed-forward 3DGS models consistently\nimproves performance under moderate input views and enhances robustness under\ndense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K.\nThe video results, code and trained models are available on our project page:\nhttps://lhmd.top/zpressor."
                },
                "authors": [
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Donny Y. Chen"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Duochao Shi"
                    },
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Project Page: https://lhmd.top/zpressor, Code:\n  https://github.com/ziplab/ZPressor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23734v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23734v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23729v1",
                "updated": "2025-05-29T17:56:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    56,
                    5,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:56:05Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    56,
                    5,
                    3,
                    149,
                    0
                ],
                "title": "Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time"
                },
                "summary": "Aligning large language models with humans is challenging due to the\ninherently multifaceted nature of preference feedback. While existing\napproaches typically frame this as a multi-objective optimization problem, they\noften overlook how humans actually make decisions. Research on bounded\nrationality suggests that human decision making follows satisficing\nstrategies-optimizing primary objectives while ensuring others meet acceptable\nthresholds. To bridge this gap and operationalize the notion of satisficing\nalignment, we propose SITAlign: an inference time framework that addresses the\nmultifaceted nature of alignment by maximizing a primary objective while\nsatisfying threshold-based constraints on secondary criteria. We provide\ntheoretical insights by deriving sub-optimality bounds of our satisficing based\ninference alignment approach. We empirically validate SITAlign's performance\nthrough extensive experimentation on multiple benchmarks. For instance, on the\nPKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while\nensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art\nmulti objective decoding strategy by a margin of 22.3% in terms of GPT-4\nwin-tie rate for helpfulness reward while adhering to the threshold on\nharmlessness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models with humans is challenging due to the\ninherently multifaceted nature of preference feedback. While existing\napproaches typically frame this as a multi-objective optimization problem, they\noften overlook how humans actually make decisions. Research on bounded\nrationality suggests that human decision making follows satisficing\nstrategies-optimizing primary objectives while ensuring others meet acceptable\nthresholds. To bridge this gap and operationalize the notion of satisficing\nalignment, we propose SITAlign: an inference time framework that addresses the\nmultifaceted nature of alignment by maximizing a primary objective while\nsatisfying threshold-based constraints on secondary criteria. We provide\ntheoretical insights by deriving sub-optimality bounds of our satisficing based\ninference alignment approach. We empirically validate SITAlign's performance\nthrough extensive experimentation on multiple benchmarks. For instance, on the\nPKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while\nensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art\nmulti objective decoding strategy by a margin of 22.3% in terms of GPT-4\nwin-tie rate for helpfulness reward while adhering to the threshold on\nharmlessness."
                },
                "authors": [
                    {
                        "name": "Mohamad Chehade"
                    },
                    {
                        "name": "Soumya Suvra Ghosal"
                    },
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Avinash Reddy"
                    },
                    {
                        "name": "Dinesh Manocha"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    }
                ],
                "author_detail": {
                    "name": "Amrit Singh Bedi"
                },
                "author": "Amrit Singh Bedi",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23725v1",
                "updated": "2025-05-29T17:55:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    55,
                    37,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:55:37Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    55,
                    37,
                    3,
                    149,
                    0
                ],
                "title": "MuLoCo: Muon is a practical inner optimizer for DiLoCo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuLoCo: Muon is a practical inner optimizer for DiLoCo"
                },
                "summary": "DiLoCo is a powerful framework for training large language models (LLMs)\nunder networking constraints with advantages for increasing parallelism and\naccelerator utilization in data center settings. Despite significantly reducing\ncommunication frequency, however, DiLoCo's communication steps still involve\nall-reducing a complete copy of the model's parameters. While existing works\nhave explored ways to reduce communication in DiLoCo, the role of error\nfeedback accumulators and the effect of the inner-optimizer on compressibility\nremain under-explored. In this work, we investigate the effectiveness of\nstandard compression methods including Top-k sparsification and quantization\nfor reducing the communication overhead of DiLoCo when paired with two local\noptimizers (AdamW and Muon). Our experiments pre-training decoder-only\ntransformer language models (LMs) reveal that leveraging Muon as the inner\noptimizer for DiLoCo along with an error-feedback accumulator allows to\naggressively compress the communicated delta to 2-bits with next to no\nperformance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo)\nsignificantly outperforms DiLoCo while communicating 8X less and having\nidentical memory complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiLoCo is a powerful framework for training large language models (LLMs)\nunder networking constraints with advantages for increasing parallelism and\naccelerator utilization in data center settings. Despite significantly reducing\ncommunication frequency, however, DiLoCo's communication steps still involve\nall-reducing a complete copy of the model's parameters. While existing works\nhave explored ways to reduce communication in DiLoCo, the role of error\nfeedback accumulators and the effect of the inner-optimizer on compressibility\nremain under-explored. In this work, we investigate the effectiveness of\nstandard compression methods including Top-k sparsification and quantization\nfor reducing the communication overhead of DiLoCo when paired with two local\noptimizers (AdamW and Muon). Our experiments pre-training decoder-only\ntransformer language models (LMs) reveal that leveraging Muon as the inner\noptimizer for DiLoCo along with an error-feedback accumulator allows to\naggressively compress the communicated delta to 2-bits with next to no\nperformance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo)\nsignificantly outperforms DiLoCo while communicating 8X less and having\nidentical memory complexity."
                },
                "authors": [
                    {
                        "name": "Benjamin Thérien"
                    },
                    {
                        "name": "Xiaolong Huang"
                    },
                    {
                        "name": "Irina Rish"
                    },
                    {
                        "name": "Eugene Belilovsky"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Belilovsky"
                },
                "author": "Eugene Belilovsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23724v1",
                "updated": "2025-05-29T17:55:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    55,
                    21,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:55:21Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    55,
                    21,
                    3,
                    149,
                    0
                ],
                "title": "SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via\n  Subspace-Constrained LoRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via\n  Subspace-Constrained LoRA"
                },
                "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), are indispensable for efficiently customizing Large Language\nModels (LLMs). However, vanilla LoRA suffers from slow convergence speed and\nknowledge forgetting problems. Recent studies have leveraged the power of\ndesigned LoRA initialization, to enhance the fine-tuning efficiency, or to\npreserve knowledge in the pre-trained LLM. However, none of these works can\naddress the two cases at the same time. To this end, we introduce\nSubspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework\nengineered to navigate the trade-off between efficient fine-tuning and\nknowledge preservation. We achieve this by constraining the output of trainable\nLoRA adapters in a low-rank subspace, where the context information of\nfine-tuning data is most preserved while the context information of preserved\nknowledge is least retained, in a balanced way. Such constraint enables the\ntrainable weights to primarily focus on the main features of fine-tuning data\nwhile avoiding damaging the preserved knowledge features. We provide\ntheoretical analysis on our method, and conduct extensive experiments including\nsafety preservation and world knowledge preservation, on various downstream\ntasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning\nperformance while markedly diminishing knowledge forgetting, surpassing\ncontemporary LoRA initialization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), are indispensable for efficiently customizing Large Language\nModels (LLMs). However, vanilla LoRA suffers from slow convergence speed and\nknowledge forgetting problems. Recent studies have leveraged the power of\ndesigned LoRA initialization, to enhance the fine-tuning efficiency, or to\npreserve knowledge in the pre-trained LLM. However, none of these works can\naddress the two cases at the same time. To this end, we introduce\nSubspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework\nengineered to navigate the trade-off between efficient fine-tuning and\nknowledge preservation. We achieve this by constraining the output of trainable\nLoRA adapters in a low-rank subspace, where the context information of\nfine-tuning data is most preserved while the context information of preserved\nknowledge is least retained, in a balanced way. Such constraint enables the\ntrainable weights to primarily focus on the main features of fine-tuning data\nwhile avoiding damaging the preserved knowledge features. We provide\ntheoretical analysis on our method, and conduct extensive experiments including\nsafety preservation and world knowledge preservation, on various downstream\ntasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning\nperformance while markedly diminishing knowledge forgetting, surpassing\ncontemporary LoRA initialization methods."
                },
                "authors": [
                    {
                        "name": "Minrui Luo"
                    },
                    {
                        "name": "Fuhang Kuang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Tianxing He"
                    }
                ],
                "author_detail": {
                    "name": "Tianxing He"
                },
                "author": "Tianxing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23723v1",
                "updated": "2025-05-29T17:54:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    54,
                    44,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:54:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    54,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning\n  Engineering"
                },
                "summary": "The emergence of large language model (LLM)-based agents has significantly\nadvanced the development of autonomous machine learning (ML) engineering.\nHowever, most existing approaches rely heavily on manual prompt engineering,\nfailing to adapt and optimize based on diverse experimental experiences.\nFocusing on this, for the first time, we explore the paradigm of learning-based\nagentic ML, where an LLM agent learns through interactive experimentation on ML\ntasks using online reinforcement learning (RL). To realize this, we propose a\nnovel agentic ML training framework with three key components: (1)\nexploration-enriched fine-tuning, which enables LLM agents to generate diverse\nactions for enhanced RL exploration; (2) step-wise RL, which enables training\non a single action step, accelerating experience collection and improving\ntraining efficiency; (3) an agentic ML-specific reward module, which unifies\nvaried ML feedback signals into consistent rewards for RL optimization.\nLeveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM\nfor autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our\n7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it\nachieves continuous performance improvements and demonstrates exceptional\ncross-task generalization capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language model (LLM)-based agents has significantly\nadvanced the development of autonomous machine learning (ML) engineering.\nHowever, most existing approaches rely heavily on manual prompt engineering,\nfailing to adapt and optimize based on diverse experimental experiences.\nFocusing on this, for the first time, we explore the paradigm of learning-based\nagentic ML, where an LLM agent learns through interactive experimentation on ML\ntasks using online reinforcement learning (RL). To realize this, we propose a\nnovel agentic ML training framework with three key components: (1)\nexploration-enriched fine-tuning, which enables LLM agents to generate diverse\nactions for enhanced RL exploration; (2) step-wise RL, which enables training\non a single action step, accelerating experience collection and improving\ntraining efficiency; (3) an agentic ML-specific reward module, which unifies\nvaried ML feedback signals into consistent rewards for RL optimization.\nLeveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM\nfor autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our\n7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it\nachieves continuous performance improvements and demonstrates exceptional\ncross-task generalization capabilities."
                },
                "authors": [
                    {
                        "name": "Zexi Liu"
                    },
                    {
                        "name": "Jingyi Chai"
                    },
                    {
                        "name": "Xinyu Zhu"
                    },
                    {
                        "name": "Shuo Tang"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23722v1",
                "updated": "2025-05-29T17:54:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    54,
                    32,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    54,
                    32,
                    3,
                    149,
                    0
                ],
                "title": "Label-Guided In-Context Learning for Named Entity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Label-Guided In-Context Learning for Named Entity Recognition"
                },
                "summary": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks using only a few demonstrations. In Named Entity Recognition (NER),\ndemonstrations are typically selected based on semantic similarity to the test\ninstance, ignoring training labels and resulting in suboptimal performance. We\nintroduce DEER, a new method that leverages training labels through token-level\nstatistics to improve ICL performance. DEER first enhances example selection\nwith a label-guided, token-based retriever that prioritizes tokens most\ninformative for entity recognition. It then prompts the LLM to revisit\nerror-prone tokens, which are also identified using label statistics, and make\ntargeted corrections. Evaluated on five NER datasets using four different LLMs,\nDEER consistently outperforms existing ICL methods and approaches the\nperformance of supervised fine-tuning. Further analysis shows its effectiveness\non both seen and unseen entities and its robustness in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks using only a few demonstrations. In Named Entity Recognition (NER),\ndemonstrations are typically selected based on semantic similarity to the test\ninstance, ignoring training labels and resulting in suboptimal performance. We\nintroduce DEER, a new method that leverages training labels through token-level\nstatistics to improve ICL performance. DEER first enhances example selection\nwith a label-guided, token-based retriever that prioritizes tokens most\ninformative for entity recognition. It then prompts the LLM to revisit\nerror-prone tokens, which are also identified using label statistics, and make\ntargeted corrections. Evaluated on five NER datasets using four different LLMs,\nDEER consistently outperforms existing ICL methods and approaches the\nperformance of supervised fine-tuning. Further analysis shows its effectiveness\non both seen and unseen entities and its robustness in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Fan Bai"
                    },
                    {
                        "name": "Hamid Hassanzadeh"
                    },
                    {
                        "name": "Ardavan Saeedi"
                    },
                    {
                        "name": "Mark Dredze"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dredze"
                },
                "author": "Mark Dredze",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14643v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14643v2",
                "updated": "2025-05-29T17:52:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    52,
                    30,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-20T15:30:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    30,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Length-Controlled Margin-Based Preference Optimization without Reference\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Length-Controlled Margin-Based Preference Optimization without Reference\n  Model"
                },
                "summary": "Direct Preference Optimization (DPO) is a widely adopted offline algorithm\nfor preference-based reinforcement learning from human feedback (RLHF),\ndesigned to improve training simplicity and stability by redefining reward\nfunctions. However, DPO is hindered by several limitations, including length\nbias, memory inefficiency, and probability degradation. To address these\nchallenges, we propose Length-Controlled Margin-Based Preference Optimization\n(LMPO), a more efficient and robust alternative. LMPO introduces a uniform\nreference model as an upper bound for the DPO loss, enabling a more accurate\napproximation of the original optimization objective. Additionally, an average\nlog-probability optimization strategy is employed to minimize discrepancies\nbetween training and inference phases. A key innovation of LMPO lies in its\nLength-Controlled Margin-Based loss function, integrated within the\nBradley-Terry framework. This loss function regulates response length while\nsimultaneously widening the margin between preferred and rejected outputs. By\ndoing so, it mitigates probability degradation for both accepted and discarded\nresponses, addressing a significant limitation of existing methods. We evaluate\nLMPO against state-of-the-art preference optimization techniques on two\nopen-ended large language models, Mistral and LLaMA3, across six conditional\nbenchmarks. Our experimental results demonstrate that LMPO effectively controls\nresponse length, reduces probability degradation, and outperforms existing\napproaches. The code is available at https://github.com/gengxuli/LMPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) is a widely adopted offline algorithm\nfor preference-based reinforcement learning from human feedback (RLHF),\ndesigned to improve training simplicity and stability by redefining reward\nfunctions. However, DPO is hindered by several limitations, including length\nbias, memory inefficiency, and probability degradation. To address these\nchallenges, we propose Length-Controlled Margin-Based Preference Optimization\n(LMPO), a more efficient and robust alternative. LMPO introduces a uniform\nreference model as an upper bound for the DPO loss, enabling a more accurate\napproximation of the original optimization objective. Additionally, an average\nlog-probability optimization strategy is employed to minimize discrepancies\nbetween training and inference phases. A key innovation of LMPO lies in its\nLength-Controlled Margin-Based loss function, integrated within the\nBradley-Terry framework. This loss function regulates response length while\nsimultaneously widening the margin between preferred and rejected outputs. By\ndoing so, it mitigates probability degradation for both accepted and discarded\nresponses, addressing a significant limitation of existing methods. We evaluate\nLMPO against state-of-the-art preference optimization techniques on two\nopen-ended large language models, Mistral and LLaMA3, across six conditional\nbenchmarks. Our experimental results demonstrate that LMPO effectively controls\nresponse length, reduces probability degradation, and outperforms existing\napproaches. The code is available at https://github.com/gengxuli/LMPO."
                },
                "authors": [
                    {
                        "name": "Gengxu Li"
                    },
                    {
                        "name": "Tingyu Xia"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "18 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14643v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14643v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23715v1",
                "updated": "2025-05-29T17:49:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    49,
                    44,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:49:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    49,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "Don't Take the Premise for Granted: Evaluating the Premise Critique\n  Ability of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Take the Premise for Granted: Evaluating the Premise Critique\n  Ability of Large Language Models"
                },
                "summary": "Large language models (LLMs) have witnessed rapid advancements, demonstrating\nremarkable capabilities. However, a notable vulnerability persists: LLMs often\nuncritically accept flawed or contradictory premises, leading to inefficient\nreasoning and unreliable outputs. This emphasizes the significance of\npossessing the \\textbf{Premise Critique Ability} for LLMs, defined as the\ncapacity to proactively identify and articulate errors in input premises. Most\nexisting studies assess LLMs' reasoning ability in ideal settings, largely\nignoring their vulnerabilities when faced with flawed premises. Thus, we\nintroduce the \\textbf{Premise Critique Bench (PCBench)}, designed by\nincorporating four error types across three difficulty levels, paired with\nmulti-faceted evaluation metrics. We conducted systematic evaluations of 15\nrepresentative LLMs. Our findings reveal: (1) Most models rely heavily on\nexplicit prompts to detect errors, with limited autonomous critique; (2)\nPremise critique ability depends on question difficulty and error type, with\ndirect contradictions being easier to detect than complex or procedural errors;\n(3) Reasoning ability does not consistently correlate with the premise critique\nability; (4) Flawed premises trigger overthinking in reasoning models, markedly\nlengthening responses due to repeated attempts at resolving conflicts. These\ninsights underscore the urgent need to enhance LLMs' proactive evaluation of\ninput validity, positioning premise critique as a foundational capability for\ndeveloping reliable, human-centric systems. The code is available at\nhttps://github.com/MLGroupJLU/Premise_Critique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have witnessed rapid advancements, demonstrating\nremarkable capabilities. However, a notable vulnerability persists: LLMs often\nuncritically accept flawed or contradictory premises, leading to inefficient\nreasoning and unreliable outputs. This emphasizes the significance of\npossessing the \\textbf{Premise Critique Ability} for LLMs, defined as the\ncapacity to proactively identify and articulate errors in input premises. Most\nexisting studies assess LLMs' reasoning ability in ideal settings, largely\nignoring their vulnerabilities when faced with flawed premises. Thus, we\nintroduce the \\textbf{Premise Critique Bench (PCBench)}, designed by\nincorporating four error types across three difficulty levels, paired with\nmulti-faceted evaluation metrics. We conducted systematic evaluations of 15\nrepresentative LLMs. Our findings reveal: (1) Most models rely heavily on\nexplicit prompts to detect errors, with limited autonomous critique; (2)\nPremise critique ability depends on question difficulty and error type, with\ndirect contradictions being easier to detect than complex or procedural errors;\n(3) Reasoning ability does not consistently correlate with the premise critique\nability; (4) Flawed premises trigger overthinking in reasoning models, markedly\nlengthening responses due to repeated attempts at resolving conflicts. These\ninsights underscore the urgent need to enhance LLMs' proactive evaluation of\ninput validity, positioning premise critique as a foundational capability for\ndeveloping reliable, human-centric systems. The code is available at\nhttps://github.com/MLGroupJLU/Premise_Critique."
                },
                "authors": [
                    {
                        "name": "Jinzhe Li"
                    },
                    {
                        "name": "Gengxu Li"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "31 pages,13 figures,15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23713v1",
                "updated": "2025-05-29T17:47:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    47,
                    36,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:47:36Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    47,
                    36,
                    3,
                    149,
                    0
                ],
                "title": "SocialMaze: A Benchmark for Evaluating Social Reasoning in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SocialMaze: A Benchmark for Evaluating Social Reasoning in Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) are increasingly applied to socially grounded\ntasks, such as online community moderation, media content analysis, and social\nreasoning games. Success in these contexts depends on a model's social\nreasoning ability - the capacity to interpret social contexts, infer others'\nmental states, and assess the truthfulness of presented information. However,\nthere is currently no systematic evaluation framework that comprehensively\nassesses the social reasoning capabilities of LLMs. Existing efforts often\noversimplify real-world scenarios and consist of tasks that are too basic to\nchallenge advanced models. To address this gap, we introduce SocialMaze, a new\nbenchmark specifically designed to evaluate social reasoning. SocialMaze\nsystematically incorporates three core challenges: deep reasoning, dynamic\ninteraction, and information uncertainty. It provides six diverse tasks across\nthree key settings: social reasoning games, daily-life interactions, and\ndigital community platforms. Both automated and human validation are used to\nensure data quality. Our evaluation reveals several key insights: models vary\nsubstantially in their ability to handle dynamic interactions and integrate\ntemporally evolving information; models with strong chain-of-thought reasoning\nperform better on tasks requiring deeper inference beyond surface-level cues;\nand model reasoning degrades significantly under uncertainty. Furthermore, we\nshow that targeted fine-tuning on curated reasoning examples can greatly\nimprove model performance in complex social scenarios. The dataset is publicly\navailable at: https://huggingface.co/datasets/MBZUAI/SocialMaze",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly applied to socially grounded\ntasks, such as online community moderation, media content analysis, and social\nreasoning games. Success in these contexts depends on a model's social\nreasoning ability - the capacity to interpret social contexts, infer others'\nmental states, and assess the truthfulness of presented information. However,\nthere is currently no systematic evaluation framework that comprehensively\nassesses the social reasoning capabilities of LLMs. Existing efforts often\noversimplify real-world scenarios and consist of tasks that are too basic to\nchallenge advanced models. To address this gap, we introduce SocialMaze, a new\nbenchmark specifically designed to evaluate social reasoning. SocialMaze\nsystematically incorporates three core challenges: deep reasoning, dynamic\ninteraction, and information uncertainty. It provides six diverse tasks across\nthree key settings: social reasoning games, daily-life interactions, and\ndigital community platforms. Both automated and human validation are used to\nensure data quality. Our evaluation reveals several key insights: models vary\nsubstantially in their ability to handle dynamic interactions and integrate\ntemporally evolving information; models with strong chain-of-thought reasoning\nperform better on tasks requiring deeper inference beyond surface-level cues;\nand model reasoning degrades significantly under uncertainty. Furthermore, we\nshow that targeted fine-tuning on curated reasoning examples can greatly\nimprove model performance in complex social scenarios. The dataset is publicly\navailable at: https://huggingface.co/datasets/MBZUAI/SocialMaze"
                },
                "authors": [
                    {
                        "name": "Zixiang Xu"
                    },
                    {
                        "name": "Yanbo Wang"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Jiayi Ye"
                    },
                    {
                        "name": "Haomin Zhuang"
                    },
                    {
                        "name": "Zirui Song"
                    },
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Zhaorun Chen"
                    },
                    {
                        "name": "Yujun Zhou"
                    },
                    {
                        "name": "Sixian Li"
                    },
                    {
                        "name": "Wang Pan"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Jieyu Zhao"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "arxiv_comment": "Code available at https://github.com/xzx34/SocialMaze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23705v1",
                "updated": "2025-05-29T17:40:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    40,
                    9,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:40:09Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    40,
                    9,
                    3,
                    149,
                    0
                ],
                "title": "Knowledge Insulating Vision-Language-Action Models: Train Fast, Run\n  Fast, Generalize Better",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Insulating Vision-Language-Action Models: Train Fast, Run\n  Fast, Generalize Better"
                },
                "summary": "Vision-language-action (VLA) models provide a powerful approach to training\ncontrol policies for physical systems, such as robots, by combining end-to-end\nlearning with transfer of semantic knowledge from web-scale vision-language\nmodel (VLM) training. However, the constraints of real-time control are often\nat odds with the design of VLMs: the most powerful VLMs have tens or hundreds\nof billions of parameters, presenting an obstacle to real-time inference, and\noperate on discrete tokens rather than the continuous-valued outputs that are\nrequired for controlling robots. To address this challenge, recent VLA models\nhave used specialized modules for efficient continuous control, such as action\nexperts or continuous output heads, which typically require adding new\nuntrained parameters to the pretrained VLM backbone. While these modules\nimprove real-time and control capabilities, it remains an open question whether\nthey preserve or degrade the semantic knowledge contained in the pretrained\nVLM, and what effect they have on the VLA training dynamics. In this paper, we\nstudy this question in the context of VLAs that include a continuous diffusion\nor flow matching action expert, showing that naively including such experts\nsignificantly harms both training speed and knowledge transfer. We provide an\nextensive analysis of various design choices, their impact on performance and\nknowledge transfer, and propose a technique for insulating the VLM backbone\nduring VLA training that mitigates this issue. Videos are available at\nhttps://pi.website/research/knowledge_insulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) models provide a powerful approach to training\ncontrol policies for physical systems, such as robots, by combining end-to-end\nlearning with transfer of semantic knowledge from web-scale vision-language\nmodel (VLM) training. However, the constraints of real-time control are often\nat odds with the design of VLMs: the most powerful VLMs have tens or hundreds\nof billions of parameters, presenting an obstacle to real-time inference, and\noperate on discrete tokens rather than the continuous-valued outputs that are\nrequired for controlling robots. To address this challenge, recent VLA models\nhave used specialized modules for efficient continuous control, such as action\nexperts or continuous output heads, which typically require adding new\nuntrained parameters to the pretrained VLM backbone. While these modules\nimprove real-time and control capabilities, it remains an open question whether\nthey preserve or degrade the semantic knowledge contained in the pretrained\nVLM, and what effect they have on the VLA training dynamics. In this paper, we\nstudy this question in the context of VLAs that include a continuous diffusion\nor flow matching action expert, showing that naively including such experts\nsignificantly harms both training speed and knowledge transfer. We provide an\nextensive analysis of various design choices, their impact on performance and\nknowledge transfer, and propose a technique for insulating the VLM backbone\nduring VLA training that mitigates this issue. Videos are available at\nhttps://pi.website/research/knowledge_insulation."
                },
                "authors": [
                    {
                        "name": "Danny Driess"
                    },
                    {
                        "name": "Jost Tobias Springenberg"
                    },
                    {
                        "name": "Brian Ichter"
                    },
                    {
                        "name": "Lili Yu"
                    },
                    {
                        "name": "Adrian Li-Bell"
                    },
                    {
                        "name": "Karl Pertsch"
                    },
                    {
                        "name": "Allen Z. Ren"
                    },
                    {
                        "name": "Homer Walke"
                    },
                    {
                        "name": "Quan Vuong"
                    },
                    {
                        "name": "Lucy Xiaoyang Shi"
                    },
                    {
                        "name": "Sergey Levine"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Levine"
                },
                "author": "Sergey Levine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23703v1",
                "updated": "2025-05-29T17:39:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    39,
                    30,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:39:30Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    39,
                    30,
                    3,
                    149,
                    0
                ],
                "title": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's\n  Math Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's\n  Math Capability"
                },
                "summary": "Enhancing the mathematical reasoning capabilities of LLMs has garnered\nsignificant attention in both the mathematical and computer science\ncommunities. Recent works have made substantial progress in both Natural\nLanguage (NL) reasoning and Formal Language (FL) reasoning by leveraging the\npotential of pure Reinforcement Learning (RL) methods on base models. However,\nRL approaches struggle to impart new capabilities not presented in the base\nmodel, highlighting the need to integrate more knowledge like FL into NL math\nreasoning effectively. Yet, this integration is challenging due to inherent\ndisparities in problem structure and reasoning format between NL and FL. To\naddress these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end\nframework designed to incorporate the FL expert into NL math problem-solving.\nTo bridge the NL and FL input format gap, we propose the *NL-FL Problem\nAlignment* method, which reformulates the Question-Answering (QA) problems in\nNL as existence theorems in FL. Subsequently, the *Mixed Problem Input*\ntechnique we provide enables the FL reasoner to handle both QA and existence\nproblems concurrently. Lastly, we mitigate the NL and FL output format gap in\nreasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive\nexperiments demonstrate that the **HybridReasoning** framework achieves\n**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC\nbenchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.\nNotably, some problems resolved by our framework remain unsolved by the NL\nbaseline model even under a larger number of trials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the mathematical reasoning capabilities of LLMs has garnered\nsignificant attention in both the mathematical and computer science\ncommunities. Recent works have made substantial progress in both Natural\nLanguage (NL) reasoning and Formal Language (FL) reasoning by leveraging the\npotential of pure Reinforcement Learning (RL) methods on base models. However,\nRL approaches struggle to impart new capabilities not presented in the base\nmodel, highlighting the need to integrate more knowledge like FL into NL math\nreasoning effectively. Yet, this integration is challenging due to inherent\ndisparities in problem structure and reasoning format between NL and FL. To\naddress these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end\nframework designed to incorporate the FL expert into NL math problem-solving.\nTo bridge the NL and FL input format gap, we propose the *NL-FL Problem\nAlignment* method, which reformulates the Question-Answering (QA) problems in\nNL as existence theorems in FL. Subsequently, the *Mixed Problem Input*\ntechnique we provide enables the FL reasoner to handle both QA and existence\nproblems concurrently. Lastly, we mitigate the NL and FL output format gap in\nreasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive\nexperiments demonstrate that the **HybridReasoning** framework achieves\n**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC\nbenchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.\nNotably, some problems resolved by our framework remain unsolved by the NL\nbaseline model even under a larger number of trials."
                },
                "authors": [
                    {
                        "name": "Ruida Wang"
                    },
                    {
                        "name": "Yuxin Li"
                    },
                    {
                        "name": "Yi R."
                    },
                    {
                        "name": "Fung"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "arxiv_affiliation": "May",
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23701v1",
                "updated": "2025-05-29T17:37:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    37,
                    57,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:37:57Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    37,
                    57,
                    3,
                    149,
                    0
                ],
                "title": "Can LLMs Reason Abstractly Over Math Word Problems Without CoT?\n  Disentangling Abstract Formulation From Arithmetic Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Reason Abstractly Over Math Word Problems Without CoT?\n  Disentangling Abstract Formulation From Arithmetic Computation"
                },
                "summary": "Final-answer-based metrics are commonly used for evaluating large language\nmodels (LLMs) on math word problems, often taken as proxies for reasoning\nability. However, such metrics conflate two distinct sub-skills: abstract\nformulation (capturing mathematical relationships using expressions) and\narithmetic computation (executing the calculations). Through a disentangled\nevaluation on GSM8K and SVAMP, we find that the final-answer accuracy of\nLlama-3 and Qwen2.5 (1B-32B) without CoT is overwhelmingly bottlenecked by the\narithmetic computation step and not by the abstract formulation step. Contrary\nto the common belief, we show that CoT primarily aids in computation, with\nlimited impact on abstract formulation. Mechanistically, we show that these two\nskills are composed conjunctively even in a single forward pass without any\nreasoning steps via an abstract-then-compute mechanism: models first capture\nproblem abstractions, then handle computation. Causal patching confirms these\nabstractions are present, transferable, composable, and precede computation.\nThese behavioural and mechanistic findings highlight the need for disentangled\nevaluation to accurately assess LLM reasoning and to guide future improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Final-answer-based metrics are commonly used for evaluating large language\nmodels (LLMs) on math word problems, often taken as proxies for reasoning\nability. However, such metrics conflate two distinct sub-skills: abstract\nformulation (capturing mathematical relationships using expressions) and\narithmetic computation (executing the calculations). Through a disentangled\nevaluation on GSM8K and SVAMP, we find that the final-answer accuracy of\nLlama-3 and Qwen2.5 (1B-32B) without CoT is overwhelmingly bottlenecked by the\narithmetic computation step and not by the abstract formulation step. Contrary\nto the common belief, we show that CoT primarily aids in computation, with\nlimited impact on abstract formulation. Mechanistically, we show that these two\nskills are composed conjunctively even in a single forward pass without any\nreasoning steps via an abstract-then-compute mechanism: models first capture\nproblem abstractions, then handle computation. Causal patching confirms these\nabstractions are present, transferable, composable, and precede computation.\nThese behavioural and mechanistic findings highlight the need for disentangled\nevaluation to accurately assess LLM reasoning and to guide future improvements."
                },
                "authors": [
                    {
                        "name": "Ziling Cheng"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Leila Pishdad"
                    },
                    {
                        "name": "Yanshuai Cao"
                    },
                    {
                        "name": "Jackie Chi Kit Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Jackie Chi Kit Cheung"
                },
                "author": "Jackie Chi Kit Cheung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23700v1",
                "updated": "2025-05-29T17:37:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    37,
                    47,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:37:47Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    37,
                    47,
                    3,
                    149,
                    0
                ],
                "title": "DiCoFlex: Model-agnostic diverse counterfactuals with flexible control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCoFlex: Model-agnostic diverse counterfactuals with flexible control"
                },
                "summary": "Counterfactual explanations play a pivotal role in explainable artificial\nintelligence (XAI) by offering intuitive, human-understandable alternatives\nthat elucidate machine learning model decisions. Despite their significance,\nexisting methods for generating counterfactuals often require constant access\nto the predictive model, involve computationally intensive optimization for\neach instance and lack the flexibility to adapt to new user-defined constraints\nwithout retraining. In this paper, we propose DiCoFlex, a novel model-agnostic,\nconditional generative framework that produces multiple diverse counterfactuals\nin a single forward pass. Leveraging conditional normalizing flows trained\nsolely on labeled data, DiCoFlex addresses key limitations by enabling\nreal-time user-driven customization of constraints such as sparsity and\nactionability at inference time. Extensive experiments on standard benchmark\ndatasets show that DiCoFlex outperforms existing methods in terms of validity,\ndiversity, proximity, and constraint adherence, making it a practical and\nscalable solution for counterfactual generation in sensitive decision-making\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual explanations play a pivotal role in explainable artificial\nintelligence (XAI) by offering intuitive, human-understandable alternatives\nthat elucidate machine learning model decisions. Despite their significance,\nexisting methods for generating counterfactuals often require constant access\nto the predictive model, involve computationally intensive optimization for\neach instance and lack the flexibility to adapt to new user-defined constraints\nwithout retraining. In this paper, we propose DiCoFlex, a novel model-agnostic,\nconditional generative framework that produces multiple diverse counterfactuals\nin a single forward pass. Leveraging conditional normalizing flows trained\nsolely on labeled data, DiCoFlex addresses key limitations by enabling\nreal-time user-driven customization of constraints such as sparsity and\nactionability at inference time. Extensive experiments on standard benchmark\ndatasets show that DiCoFlex outperforms existing methods in terms of validity,\ndiversity, proximity, and constraint adherence, making it a practical and\nscalable solution for counterfactual generation in sensitive decision-making\ndomains."
                },
                "authors": [
                    {
                        "name": "Oleksii Furman"
                    },
                    {
                        "name": "Ulvi Movsum-zada"
                    },
                    {
                        "name": "Patryk Marszalek"
                    },
                    {
                        "name": "Maciej Zięba"
                    },
                    {
                        "name": "Marek Śmieja"
                    }
                ],
                "author_detail": {
                    "name": "Marek Śmieja"
                },
                "author": "Marek Śmieja",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06951v2",
                "updated": "2025-05-29T17:37:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    37,
                    26,
                    3,
                    149,
                    0
                ],
                "published": "2025-03-10T05:56:46Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    56,
                    46,
                    0,
                    69,
                    0
                ],
                "title": "ReAgent: Reversible Multi-Agent Reasoning for Knowledge-Enhanced\n  Multi-Hop QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReAgent: Reversible Multi-Agent Reasoning for Knowledge-Enhanced\n  Multi-Hop QA"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly improved\nmulti-hop question answering (QA) through direct Chain-of-Thought (CoT)\nreasoning. However, the irreversible nature of CoT leads to error accumulation,\nmaking it challenging to correct mistakes in multi-hop reasoning. This paper\nintroduces ReAgent: a Reversible multi-Agent collaborative framework augmented\nwith explicit backtracking mechanisms, enabling reversible multi-hop reasoning.\nBy incorporating text-based retrieval, information aggregation and validation,\nour system can detect and correct errors mid-reasoning, leading to more robust\nand interpretable QA outcomes. The framework and experiments serve as a\nfoundation for future work on error-tolerant QA systems. Empirical evaluations\nacross three benchmarks indicate ReAgent's efficacy, yielding average about 6\\%\nimprovements against baseline models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly improved\nmulti-hop question answering (QA) through direct Chain-of-Thought (CoT)\nreasoning. However, the irreversible nature of CoT leads to error accumulation,\nmaking it challenging to correct mistakes in multi-hop reasoning. This paper\nintroduces ReAgent: a Reversible multi-Agent collaborative framework augmented\nwith explicit backtracking mechanisms, enabling reversible multi-hop reasoning.\nBy incorporating text-based retrieval, information aggregation and validation,\nour system can detect and correct errors mid-reasoning, leading to more robust\nand interpretable QA outcomes. The framework and experiments serve as a\nfoundation for future work on error-tolerant QA systems. Empirical evaluations\nacross three benchmarks indicate ReAgent's efficacy, yielding average about 6\\%\nimprovements against baseline models."
                },
                "authors": [
                    {
                        "name": "Xinjie Zhao"
                    },
                    {
                        "name": "Fan Gao"
                    },
                    {
                        "name": "Xingyu Song"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Yanran Fu"
                    },
                    {
                        "name": "Yuyang Wang"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "arxiv_comment": "25pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11778v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11778v3",
                "updated": "2025-05-29T17:32:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    32,
                    38,
                    3,
                    149,
                    0
                ],
                "published": "2024-10-15T16:57:14Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    57,
                    14,
                    1,
                    289,
                    0
                ],
                "title": "On the Training Convergence of Transformers for In-Context\n  Classification of Gaussian Mixtures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Training Convergence of Transformers for In-Context\n  Classification of Gaussian Mixtures"
                },
                "summary": "Although transformers have demonstrated impressive capabilities for\nin-context learning (ICL) in practice, theoretical understanding of the\nunderlying mechanism that allows transformers to perform ICL is still in its\ninfancy. This work aims to theoretically study the training dynamics of\ntransformers for in-context classification tasks. We demonstrate that, for\nin-context classification of Gaussian mixtures under certain assumptions, a\nsingle-layer transformer trained via gradient descent converges to a globally\noptimal model at a linear rate. We further quantify the impact of the training\nand testing prompt lengths on the ICL inference error of the trained\ntransformer. We show that when the lengths of training and testing prompts are\nsufficiently large, the prediction of the trained transformer approaches the\nground truth distribution of the labels. Experimental results corroborate the\ntheoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although transformers have demonstrated impressive capabilities for\nin-context learning (ICL) in practice, theoretical understanding of the\nunderlying mechanism that allows transformers to perform ICL is still in its\ninfancy. This work aims to theoretically study the training dynamics of\ntransformers for in-context classification tasks. We demonstrate that, for\nin-context classification of Gaussian mixtures under certain assumptions, a\nsingle-layer transformer trained via gradient descent converges to a globally\noptimal model at a linear rate. We further quantify the impact of the training\nand testing prompt lengths on the ICL inference error of the trained\ntransformer. We show that when the lengths of training and testing prompts are\nsufficiently large, the prediction of the trained transformer approaches the\nground truth distribution of the labels. Experimental results corroborate the\ntheoretical findings."
                },
                "authors": [
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Ruida Zhou"
                    },
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Cong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Cong Shen"
                },
                "author": "Cong Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11778v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11778v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23695v1",
                "updated": "2025-05-29T17:32:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    32,
                    15,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:32:15Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    32,
                    15,
                    3,
                    149,
                    0
                ],
                "title": "Data-to-Dashboard: Multi-Agent LLM Framework for Insightful\n  Visualization in Enterprise Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-to-Dashboard: Multi-Agent LLM Framework for Insightful\n  Visualization in Enterprise Analytics"
                },
                "summary": "The rapid advancement of LLMs has led to the creation of diverse agentic\nsystems in data analysis, utilizing LLMs' capabilities to improve insight\ngeneration and visualization. In this paper, we present an agentic system that\nautomates the data-to-dashboard pipeline through modular LLM agents capable of\ndomain detection, concept extraction, multi-perspective analysis generation,\nand iterative self-reflection. Unlike existing chart QA systems, our framework\nsimulates the analytical reasoning process of business analysts by retrieving\ndomain-relevant knowledge and adapting to diverse datasets without relying on\nclosed ontologies or question templates.\n  We evaluate our system on three datasets across different domains.\nBenchmarked against GPT-4o with a single-prompt baseline, our approach shows\nimproved insightfulness, domain relevance, and analytical depth, as measured by\ntailored evaluation metrics and qualitative human assessment.\n  This work contributes a novel modular pipeline to bridge the path from raw\ndata to visualization, and opens new opportunities for human-in-the-loop\nvalidation by domain experts in business analytics. All code can be found here:\nhttps://github.com/77luvC/D2D_Data2Dashboard",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of LLMs has led to the creation of diverse agentic\nsystems in data analysis, utilizing LLMs' capabilities to improve insight\ngeneration and visualization. In this paper, we present an agentic system that\nautomates the data-to-dashboard pipeline through modular LLM agents capable of\ndomain detection, concept extraction, multi-perspective analysis generation,\nand iterative self-reflection. Unlike existing chart QA systems, our framework\nsimulates the analytical reasoning process of business analysts by retrieving\ndomain-relevant knowledge and adapting to diverse datasets without relying on\nclosed ontologies or question templates.\n  We evaluate our system on three datasets across different domains.\nBenchmarked against GPT-4o with a single-prompt baseline, our approach shows\nimproved insightfulness, domain relevance, and analytical depth, as measured by\ntailored evaluation metrics and qualitative human assessment.\n  This work contributes a novel modular pipeline to bridge the path from raw\ndata to visualization, and opens new opportunities for human-in-the-loop\nvalidation by domain experts in business analytics. All code can be found here:\nhttps://github.com/77luvC/D2D_Data2Dashboard"
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Mohannad Elhamod"
                    }
                ],
                "author_detail": {
                    "name": "Mohannad Elhamod"
                },
                "author": "Mohannad Elhamod",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23693v1",
                "updated": "2025-05-29T17:31:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    31,
                    13,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:31:13Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    31,
                    13,
                    3,
                    149,
                    0
                ],
                "title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC\n  Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC\n  Videos"
                },
                "summary": "MLLMs have been widely studied for video question answering recently.\nHowever, most existing assessments focus on natural videos, overlooking\nsynthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in\nvideo generation rely on MLLMs to evaluate the quality of generated videos, but\nthe capabilities of MLLMs on interpreting AIGC videos remain largely\nunderexplored. To address this, we propose a new benchmark, VF-Eval, which\nintroduces four tasks-coherence validation, error awareness, error type\ndetection, and reasoning evaluation-to comprehensively evaluate the abilities\nof MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that\neven the best-performing model, GPT-4.1, struggles to achieve consistently good\nperformance across all tasks. This highlights the challenging nature of our\nbenchmark. Additionally, to investigate the practical applications of VF-Eval\nin improving video generation, we conduct an experiment, RePrompt,\ndemonstrating that aligning MLLMs more closely with human feedback can benefit\nvideo generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLLMs have been widely studied for video question answering recently.\nHowever, most existing assessments focus on natural videos, overlooking\nsynthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in\nvideo generation rely on MLLMs to evaluate the quality of generated videos, but\nthe capabilities of MLLMs on interpreting AIGC videos remain largely\nunderexplored. To address this, we propose a new benchmark, VF-Eval, which\nintroduces four tasks-coherence validation, error awareness, error type\ndetection, and reasoning evaluation-to comprehensively evaluate the abilities\nof MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that\neven the best-performing model, GPT-4.1, struggles to achieve consistently good\nperformance across all tasks. This highlights the challenging nature of our\nbenchmark. Additionally, to investigate the practical applications of VF-Eval\nin improving video generation, we conduct an experiment, RePrompt,\ndemonstrating that aligning MLLMs more closely with human feedback can benefit\nvideo generation."
                },
                "authors": [
                    {
                        "name": "Tingyu Song"
                    },
                    {
                        "name": "Tongyan Hu"
                    },
                    {
                        "name": "Guo Gan"
                    },
                    {
                        "name": "Yilun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Zhao"
                },
                "author": "Yilun Zhao",
                "arxiv_comment": "ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08166v2",
                "updated": "2025-05-29T17:30:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    30,
                    22,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-12T07:11:33Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    11,
                    33,
                    2,
                    43,
                    0
                ],
                "title": "From Individual Experience to Collective Evidence: A Reporting-Based\n  Framework for Identifying Systemic Harms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Individual Experience to Collective Evidence: A Reporting-Based\n  Framework for Identifying Systemic Harms"
                },
                "summary": "When an individual reports a negative interaction with some system, how can\ntheir personal experience be contextualized within broader patterns of system\nbehavior? We study the reporting database problem, where individual reports of\nadverse events arrive sequentially, and are aggregated over time. In this work,\nour goal is to identify whether there are subgroups--defined by any combination\nof relevant features--that are disproportionately likely to experience harmful\ninteractions with the system. We formalize this problem as a sequential\nhypothesis test, and identify conditions on reporting behavior that are\nsufficient for making inferences about disparities in true rates of harm across\nsubgroups. We show that algorithms for sequential hypothesis tests can be\napplied to this problem with a standard multiple testing correction. We then\ndemonstrate our method on real-world datasets, including mortgage decisions and\nvaccine side effects; on each, our method (re-)identifies subgroups known to\nexperience disproportionate harm using only a fraction of the data that was\ninitially used to discover them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When an individual reports a negative interaction with some system, how can\ntheir personal experience be contextualized within broader patterns of system\nbehavior? We study the reporting database problem, where individual reports of\nadverse events arrive sequentially, and are aggregated over time. In this work,\nour goal is to identify whether there are subgroups--defined by any combination\nof relevant features--that are disproportionately likely to experience harmful\ninteractions with the system. We formalize this problem as a sequential\nhypothesis test, and identify conditions on reporting behavior that are\nsufficient for making inferences about disparities in true rates of harm across\nsubgroups. We show that algorithms for sequential hypothesis tests can be\napplied to this problem with a standard multiple testing correction. We then\ndemonstrate our method on real-world datasets, including mortgage decisions and\nvaccine side effects; on each, our method (re-)identifies subgroups known to\nexperience disproportionate harm using only a fraction of the data that was\ninitially used to discover them."
                },
                "authors": [
                    {
                        "name": "Jessica Dai"
                    },
                    {
                        "name": "Paula Gradu"
                    },
                    {
                        "name": "Inioluwa Deborah Raji"
                    },
                    {
                        "name": "Benjamin Recht"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Recht"
                },
                "author": "Benjamin Recht",
                "arxiv_comment": "Updated terminology from \"incident database\" to \"reporting database.\"\n  To be presented at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23671v1",
                "updated": "2025-05-29T17:14:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    14,
                    55,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:14:55Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    14,
                    55,
                    3,
                    149,
                    0
                ],
                "title": "GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents"
                },
                "summary": "Developing high-performance software is a complex task that requires\nspecialized expertise. We introduce GSO, a benchmark for evaluating language\nmodels' capabilities in developing high-performance software. We develop an\nautomated pipeline that generates and executes performance tests to analyze\nrepository commit histories to identify 102 challenging optimization tasks\nacross 10 codebases, spanning diverse domains and programming languages. An\nagent is provided with a codebase and performance test as a precise\nspecification, and tasked to improve the runtime efficiency, which is measured\nagainst the expert developer optimization. Our quantitative evaluation reveals\nthat leading SWE-Agents struggle significantly, achieving less than 5% success\nrate, with limited improvements even with inference-time scaling. Our\nqualitative analysis identifies key failure modes, including difficulties with\nlow-level languages, practicing lazy optimization strategies, and challenges in\naccurately localizing bottlenecks. We release the code and artifacts of our\nbenchmark along with agent trajectories to enable future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing high-performance software is a complex task that requires\nspecialized expertise. We introduce GSO, a benchmark for evaluating language\nmodels' capabilities in developing high-performance software. We develop an\nautomated pipeline that generates and executes performance tests to analyze\nrepository commit histories to identify 102 challenging optimization tasks\nacross 10 codebases, spanning diverse domains and programming languages. An\nagent is provided with a codebase and performance test as a precise\nspecification, and tasked to improve the runtime efficiency, which is measured\nagainst the expert developer optimization. Our quantitative evaluation reveals\nthat leading SWE-Agents struggle significantly, achieving less than 5% success\nrate, with limited improvements even with inference-time scaling. Our\nqualitative analysis identifies key failure modes, including difficulties with\nlow-level languages, practicing lazy optimization strategies, and challenges in\naccurately localizing bottlenecks. We release the code and artifacts of our\nbenchmark along with agent trajectories to enable future research."
                },
                "authors": [
                    {
                        "name": "Manish Shetty"
                    },
                    {
                        "name": "Naman Jain"
                    },
                    {
                        "name": "Jinjian Liu"
                    },
                    {
                        "name": "Vijay Kethanaboyina"
                    },
                    {
                        "name": "Koushik Sen"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_comment": "Website: https://gso-bench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09615v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09615v3",
                "updated": "2025-05-29T17:14:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    14,
                    16,
                    3,
                    149,
                    0
                ],
                "published": "2024-10-12T18:36:07Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    18,
                    36,
                    7,
                    5,
                    286,
                    0
                ],
                "title": "SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for\n  LLM Weight Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for\n  LLM Weight Compression"
                },
                "summary": "Conventional model compression techniques for LLMs address high memory\nconsumption and slow inference challenges but typically require computationally\nexpensive retraining to preserve accuracy. In contrast, one-shot compression\nmethods eliminate retraining cost, but struggle to achieve accuracy comparable\nto dense models. This paper presents SLIM, a new one-shot compression framework\nthat holistically integrates hardware-friendly quantization, sparsity, and\nlow-rank approximation into a unified process. First, we formulate the\nquantization process using a probabilistic approach (SLIM-Quant) that enables\nus to apply uniform quantization. Then, we use an existing one-shot pruning\nmethod to apply semi-structured sparsity on top of the quantized weights.\nFinally, to compensate for the introduced aggregated quantization and sparsity\nerror, we use a novel saliency function with unique invertible and additive\nfeatures that enables us to mathematically compute the value of low-rank\nadapters. SLIM improves model accuracy by up to 5.66% (LLaMA-2-7B) for 2:4\nsparsity with 4-bit weight quantization, outperforming prior methods. Models\ncompressed with SLIM achieve up to 4.3x and 3.8x on Nvidia RTX3060 and A100\nGPUs, respectively. Additionally, they achieve up to 0.23x end-to-end memory\nreduction in comparison to their dense counterparts. We also propose an\noptional PEFT recipe that further improves accuracy by up to 1.66%\n(LLaMA-2-13B) compared to SLIM without fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional model compression techniques for LLMs address high memory\nconsumption and slow inference challenges but typically require computationally\nexpensive retraining to preserve accuracy. In contrast, one-shot compression\nmethods eliminate retraining cost, but struggle to achieve accuracy comparable\nto dense models. This paper presents SLIM, a new one-shot compression framework\nthat holistically integrates hardware-friendly quantization, sparsity, and\nlow-rank approximation into a unified process. First, we formulate the\nquantization process using a probabilistic approach (SLIM-Quant) that enables\nus to apply uniform quantization. Then, we use an existing one-shot pruning\nmethod to apply semi-structured sparsity on top of the quantized weights.\nFinally, to compensate for the introduced aggregated quantization and sparsity\nerror, we use a novel saliency function with unique invertible and additive\nfeatures that enables us to mathematically compute the value of low-rank\nadapters. SLIM improves model accuracy by up to 5.66% (LLaMA-2-7B) for 2:4\nsparsity with 4-bit weight quantization, outperforming prior methods. Models\ncompressed with SLIM achieve up to 4.3x and 3.8x on Nvidia RTX3060 and A100\nGPUs, respectively. Additionally, they achieve up to 0.23x end-to-end memory\nreduction in comparison to their dense counterparts. We also propose an\noptional PEFT recipe that further improves accuracy by up to 1.66%\n(LLaMA-2-13B) compared to SLIM without fine-tuning."
                },
                "authors": [
                    {
                        "name": "Mohammad Mozaffari"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Maryam Mehri Dehnavi"
                    }
                ],
                "author_detail": {
                    "name": "Maryam Mehri Dehnavi"
                },
                "author": "Maryam Mehri Dehnavi",
                "arxiv_comment": "Published at Proceedings of the 42 nd International Conference on\n  Machine Learning (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09615v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09615v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23666v1",
                "updated": "2025-05-29T17:12:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "LoLA: Low-Rank Linear Attention With Sparse Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLA: Low-Rank Linear Attention With Sparse Caching"
                },
                "summary": "Transformer-based large language models suffer from quadratic complexity at\ninference on long sequences. Linear attention methods are efficient\nalternatives, however, they fail to provide an accurate approximation of\nsoftmax attention. By additionally incorporating sliding window attention into\neach linear attention head, this gap can be closed for short context-length\ntasks. Unfortunately, these approaches cannot recall important information from\nlong contexts due to \"memory collisions\". In this paper , we propose LoLA:\nLow-rank Linear Attention with sparse caching. LoLA separately stores\nadditional key-value pairs that would otherwise interfere with past associative\nmemories. Moreover, LoLA further closes the gap between linear attention models\nand transformers by distributing past key-value pairs into three forms of\nmemory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. As an inference-only strategy, LoLA enables\npass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks\nfrom RULER. It boosts the accuracy of the base subquadratic model from 0.6% to\n97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1\n8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning\ntasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an\nextremely lightweight approach: Nearly all of our results can be reproduced on\na single consumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models suffer from quadratic complexity at\ninference on long sequences. Linear attention methods are efficient\nalternatives, however, they fail to provide an accurate approximation of\nsoftmax attention. By additionally incorporating sliding window attention into\neach linear attention head, this gap can be closed for short context-length\ntasks. Unfortunately, these approaches cannot recall important information from\nlong contexts due to \"memory collisions\". In this paper , we propose LoLA:\nLow-rank Linear Attention with sparse caching. LoLA separately stores\nadditional key-value pairs that would otherwise interfere with past associative\nmemories. Moreover, LoLA further closes the gap between linear attention models\nand transformers by distributing past key-value pairs into three forms of\nmemory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. As an inference-only strategy, LoLA enables\npass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks\nfrom RULER. It boosts the accuracy of the base subquadratic model from 0.6% to\n97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1\n8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning\ntasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an\nextremely lightweight approach: Nearly all of our results can be reproduced on\na single consumer GPU."
                },
                "authors": [
                    {
                        "name": "Luke McDermott"
                    },
                    {
                        "name": "Robert W. Heath Jr."
                    },
                    {
                        "name": "Rahul Parhi"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Parhi"
                },
                "author": "Rahul Parhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18638v2",
                "updated": "2025-05-29T17:11:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    11,
                    54,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-24T11:01:05Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    11,
                    1,
                    5,
                    5,
                    144,
                    0
                ],
                "title": "Multilingual Question Answering in Low-Resource Settings: A\n  Dzongkha-English Benchmark for Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Question Answering in Low-Resource Settings: A\n  Dzongkha-English Benchmark for Foundation Models"
                },
                "summary": "In this work, we provide DZEN, a dataset of parallel Dzongkha and English\ntest questions for Bhutanese middle and high school students. The over 5K\nquestions in our collection span a variety of scientific topics and include\nfactual, application, and reasoning-based questions. We use our parallel\ndataset to test a number of Large Language Models (LLMs) and find a significant\nperformance difference between the models in English and Dzongkha. We also look\nat different prompting strategies and discover that Chain-of-Thought (CoT)\nprompting works well for reasoning questions but less well for factual ones. We\nalso find that adding English translations enhances the precision of Dzongkha\nquestion responses. Our results point to exciting avenues for further study to\nimprove LLM performance in Dzongkha and, more generally, in low-resource\nlanguages. We release the dataset at:\nhttps://github.com/kraritt/llm_dzongkha_evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we provide DZEN, a dataset of parallel Dzongkha and English\ntest questions for Bhutanese middle and high school students. The over 5K\nquestions in our collection span a variety of scientific topics and include\nfactual, application, and reasoning-based questions. We use our parallel\ndataset to test a number of Large Language Models (LLMs) and find a significant\nperformance difference between the models in English and Dzongkha. We also look\nat different prompting strategies and discover that Chain-of-Thought (CoT)\nprompting works well for reasoning questions but less well for factual ones. We\nalso find that adding English translations enhances the precision of Dzongkha\nquestion responses. Our results point to exciting avenues for further study to\nimprove LLM performance in Dzongkha and, more generally, in low-resource\nlanguages. We release the dataset at:\nhttps://github.com/kraritt/llm_dzongkha_evaluation."
                },
                "authors": [
                    {
                        "name": "Md. Tanzib Hosain"
                    },
                    {
                        "name": "Rajan Das Gupta"
                    },
                    {
                        "name": "Md. Kishor Morol"
                    }
                ],
                "author_detail": {
                    "name": "Md. Kishor Morol"
                },
                "author": "Md. Kishor Morol",
                "arxiv_comment": "24 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22869v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22869v3",
                "updated": "2025-05-29T17:11:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    11,
                    30,
                    3,
                    149,
                    0
                ],
                "published": "2025-03-28T20:53:20Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    20,
                    53,
                    20,
                    4,
                    87,
                    0
                ],
                "title": "SIGHT: Synthesizing Image-Text Conditioned and Geometry-Guided 3D\n  Hand-Object Trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIGHT: Synthesizing Image-Text Conditioned and Geometry-Guided 3D\n  Hand-Object Trajectories"
                },
                "summary": "When humans grasp an object, they naturally form trajectories in their minds\nto manipulate it for specific tasks. Modeling hand-object interaction priors\nholds significant potential to advance robotic and embodied AI systems in\nlearning to operate effectively within the physical world. We introduce SIGHT,\na novel task focused on generating realistic and physically plausible 3D\nhand-object interaction trajectories from a single image and a brief\nlanguage-based task description. Prior work on hand-object trajectory\ngeneration typically relies on textual input that lacks explicit grounding to\nthe target object, or assumes access to 3D object meshes, which are often\nconsiderably more difficult to obtain than 2D images. We propose SIGHT-Fusion,\na novel diffusion-based image-text conditioned generative model that tackles\nthis task by retrieving the most similar 3D object mesh from a database and\nenforcing geometric hand-object interaction constraints via a novel\ninference-time diffusion guidance. We benchmark our model on the HOI4D and H2O\ndatasets, adapting relevant baselines for this novel task. Experiments\ndemonstrate our superior performance in the diversity and quality of generated\ntrajectories, as well as in hand-object interaction geometry metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When humans grasp an object, they naturally form trajectories in their minds\nto manipulate it for specific tasks. Modeling hand-object interaction priors\nholds significant potential to advance robotic and embodied AI systems in\nlearning to operate effectively within the physical world. We introduce SIGHT,\na novel task focused on generating realistic and physically plausible 3D\nhand-object interaction trajectories from a single image and a brief\nlanguage-based task description. Prior work on hand-object trajectory\ngeneration typically relies on textual input that lacks explicit grounding to\nthe target object, or assumes access to 3D object meshes, which are often\nconsiderably more difficult to obtain than 2D images. We propose SIGHT-Fusion,\na novel diffusion-based image-text conditioned generative model that tackles\nthis task by retrieving the most similar 3D object mesh from a database and\nenforcing geometric hand-object interaction constraints via a novel\ninference-time diffusion guidance. We benchmark our model on the HOI4D and H2O\ndatasets, adapting relevant baselines for this novel task. Experiments\ndemonstrate our superior performance in the diversity and quality of generated\ntrajectories, as well as in hand-object interaction geometry metrics."
                },
                "authors": [
                    {
                        "name": "Alexey Gavryushin"
                    },
                    {
                        "name": "Alexandros Delitzas"
                    },
                    {
                        "name": "Luc Van Gool"
                    },
                    {
                        "name": "Marc Pollefeys"
                    },
                    {
                        "name": "Kaichun Mo"
                    },
                    {
                        "name": "Xi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xi Wang"
                },
                "author": "Xi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22869v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22869v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23662v1",
                "updated": "2025-05-29T17:10:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    10,
                    12,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:10:12Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    10,
                    12,
                    3,
                    149,
                    0
                ],
                "title": "ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic\n  Long-Term Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic\n  Long-Term Interactions"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in using\nexternal tools to address user inquiries. However, most existing evaluations\nassume tool use in short contexts, offering limited insight into model behavior\nduring realistic long-term interactions. To fill this gap, we introduce\nToolHaystack, a benchmark for testing the tool use capabilities in long-term\ninteractions. Each test instance in ToolHaystack includes multiple tasks\nexecution contexts and realistic noise within a continuous conversation,\nenabling assessment of how well models maintain context and handle various\ndisruptions. By applying this benchmark to 14 state-of-the-art LLMs, we find\nthat while current models perform well in standard multi-turn settings, they\noften significantly struggle in ToolHaystack, highlighting critical gaps in\ntheir long-term robustness not revealed by previous tool benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in using\nexternal tools to address user inquiries. However, most existing evaluations\nassume tool use in short contexts, offering limited insight into model behavior\nduring realistic long-term interactions. To fill this gap, we introduce\nToolHaystack, a benchmark for testing the tool use capabilities in long-term\ninteractions. Each test instance in ToolHaystack includes multiple tasks\nexecution contexts and realistic noise within a continuous conversation,\nenabling assessment of how well models maintain context and handle various\ndisruptions. By applying this benchmark to 14 state-of-the-art LLMs, we find\nthat while current models perform well in standard multi-turn settings, they\noften significantly struggle in ToolHaystack, highlighting critical gaps in\ntheir long-term robustness not revealed by previous tool benchmarks."
                },
                "authors": [
                    {
                        "name": "Beong-woo Kwak"
                    },
                    {
                        "name": "Minju Kim"
                    },
                    {
                        "name": "Dongha Lim"
                    },
                    {
                        "name": "Hyungjoo Chae"
                    },
                    {
                        "name": "Dongjin Kang"
                    },
                    {
                        "name": "Sunghwan Kim"
                    },
                    {
                        "name": "Dongil Yang"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    }
                ],
                "author_detail": {
                    "name": "Jinyoung Yeo"
                },
                "author": "Jinyoung Yeo",
                "arxiv_comment": "Our code and data are available at\n  https://github.com/bwookwak/ToolHaystack",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23661v1",
                "updated": "2025-05-29T17:09:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    9,
                    44,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:09:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    9,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and\n  Generation"
                },
                "summary": "In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni."
                },
                "authors": [
                    {
                        "name": "Size Wu"
                    },
                    {
                        "name": "Zhonghua Wu"
                    },
                    {
                        "name": "Zerui Gong"
                    },
                    {
                        "name": "Qingyi Tao"
                    },
                    {
                        "name": "Sheng Jin"
                    },
                    {
                        "name": "Qinyue Li"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Chen Change Loy"
                    }
                ],
                "author_detail": {
                    "name": "Chen Change Loy"
                },
                "author": "Chen Change Loy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23660v1",
                "updated": "2025-05-29T17:09:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    9,
                    25,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:09:25Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    9,
                    25,
                    3,
                    149,
                    0
                ],
                "title": "D-AR: Diffusion via Autoregressive Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D-AR: Diffusion via Autoregressive Models"
                },
                "summary": "This paper presents Diffusion via Autoregressive models (D-AR), a new\nparadigm recasting the image diffusion process as a vanilla autoregressive\nprocedure in the standard next-token-prediction fashion. We start by designing\nthe tokenizer that converts images into sequences of discrete tokens, where\ntokens in different positions can be decoded into different diffusion denoising\nsteps in the pixel space. Thanks to the diffusion properties, these tokens\nnaturally follow a coarse-to-fine order, which directly lends itself to\nautoregressive modeling. Therefore, we apply standard next-token prediction on\nthese tokens, without modifying any underlying designs (either causal masks or\ntraining/inference strategies), and such sequential autoregressive token\ngeneration directly mirrors the diffusion procedure in image space. That is,\nonce the autoregressive model generates an increment of tokens, we can directly\ndecode these tokens into the corresponding diffusion denoising step in the\nstreaming manner. Our pipeline naturally reveals several intriguing properties,\nfor example, it supports consistent previews when generating only a subset of\ntokens and enables zero-shot layout-controlled synthesis. On the standard\nImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone\nwith 256 discrete tokens. We hope our work can inspire future research on\nunified autoregressive architectures of visual synthesis, especially with large\nlanguage models. Code and models will be available at\nhttps://github.com/showlab/D-AR",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Diffusion via Autoregressive models (D-AR), a new\nparadigm recasting the image diffusion process as a vanilla autoregressive\nprocedure in the standard next-token-prediction fashion. We start by designing\nthe tokenizer that converts images into sequences of discrete tokens, where\ntokens in different positions can be decoded into different diffusion denoising\nsteps in the pixel space. Thanks to the diffusion properties, these tokens\nnaturally follow a coarse-to-fine order, which directly lends itself to\nautoregressive modeling. Therefore, we apply standard next-token prediction on\nthese tokens, without modifying any underlying designs (either causal masks or\ntraining/inference strategies), and such sequential autoregressive token\ngeneration directly mirrors the diffusion procedure in image space. That is,\nonce the autoregressive model generates an increment of tokens, we can directly\ndecode these tokens into the corresponding diffusion denoising step in the\nstreaming manner. Our pipeline naturally reveals several intriguing properties,\nfor example, it supports consistent previews when generating only a subset of\ntokens and enables zero-shot layout-controlled synthesis. On the standard\nImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone\nwith 256 discrete tokens. We hope our work can inspire future research on\nunified autoregressive architectures of visual synthesis, especially with large\nlanguage models. Code and models will be available at\nhttps://github.com/showlab/D-AR"
                },
                "authors": [
                    {
                        "name": "Ziteng Gao"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23658v1",
                "updated": "2025-05-29T17:08:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    8,
                    19,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:08:19Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    8,
                    19,
                    3,
                    149,
                    0
                ],
                "title": "Bayesian Perspective on Memorization and Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Perspective on Memorization and Reconstruction"
                },
                "summary": "We introduce a new Bayesian perspective on the concept of data\nreconstruction, and leverage this viewpoint to propose a new security\ndefinition that, in certain settings, provably prevents reconstruction attacks.\nWe use our paradigm to shed new light on one of the most notorious attacks in\nthe privacy and memorization literature - fingerprinting code attacks (FPC). We\nargue that these attacks are really a form of membership inference attacks,\nrather than reconstruction attacks. Furthermore, we show that if the goal is\nsolely to prevent reconstruction (but not membership inference), then in some\ncases the impossibility results derived from FPC no longer apply.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new Bayesian perspective on the concept of data\nreconstruction, and leverage this viewpoint to propose a new security\ndefinition that, in certain settings, provably prevents reconstruction attacks.\nWe use our paradigm to shed new light on one of the most notorious attacks in\nthe privacy and memorization literature - fingerprinting code attacks (FPC). We\nargue that these attacks are really a form of membership inference attacks,\nrather than reconstruction attacks. Furthermore, we show that if the goal is\nsolely to prevent reconstruction (but not membership inference), then in some\ncases the impossibility results derived from FPC no longer apply."
                },
                "authors": [
                    {
                        "name": "Haim Kaplan"
                    },
                    {
                        "name": "Yishay Mansour"
                    },
                    {
                        "name": "Kobbi Nissim"
                    },
                    {
                        "name": "Uri Stemmer"
                    }
                ],
                "author_detail": {
                    "name": "Uri Stemmer"
                },
                "author": "Uri Stemmer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23657v1",
                "updated": "2025-05-29T17:07:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    7,
                    24,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:07:24Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    7,
                    24,
                    3,
                    149,
                    0
                ],
                "title": "Active Layer-Contrastive Decoding Reduces Hallucination in Large\n  Language Model Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Layer-Contrastive Decoding Reduces Hallucination in Large\n  Language Model Generation"
                },
                "summary": "Recent decoding methods improve the factuality of large language\nmodels~(LLMs) by refining how the next token is selected during generation.\nThese methods typically operate at the token level, leveraging internal\nrepresentations to suppress superficial patterns. Nevertheless, LLMs remain\nprone to hallucinations, especially over longer contexts. In this paper, we\npropose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy\nthat actively decides when to apply contrasting layers during generation. By\ncasting decoding as a sequential decision-making problem, ActLCD employs a\nreinforcement learning policy guided by a reward-aware classifier to optimize\nfactuality beyond the token level. Our experiments demonstrate that ActLCD\nsurpasses state-of-the-art methods across five benchmarks, showcasing its\neffectiveness in mitigating hallucinations in diverse generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent decoding methods improve the factuality of large language\nmodels~(LLMs) by refining how the next token is selected during generation.\nThese methods typically operate at the token level, leveraging internal\nrepresentations to suppress superficial patterns. Nevertheless, LLMs remain\nprone to hallucinations, especially over longer contexts. In this paper, we\npropose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy\nthat actively decides when to apply contrasting layers during generation. By\ncasting decoding as a sequential decision-making problem, ActLCD employs a\nreinforcement learning policy guided by a reward-aware classifier to optimize\nfactuality beyond the token level. Our experiments demonstrate that ActLCD\nsurpasses state-of-the-art methods across five benchmarks, showcasing its\neffectiveness in mitigating hallucinations in diverse generation scenarios."
                },
                "authors": [
                    {
                        "name": "Hongxiang Zhang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Muhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Muhao Chen"
                },
                "author": "Muhao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23655v1",
                "updated": "2025-05-29T17:05:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    5,
                    42,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:05:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    5,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "Keyed Chaotic Tensor Transformations for Secure And Attributable Neural\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keyed Chaotic Tensor Transformations for Secure And Attributable Neural\n  Inference"
                },
                "summary": "This work introduces a novel framework for secure and privacy-preserving\nneural network inference based on keyed chaotic dynamical transformations. The\nproposed method applies a deterministic, cryptographically seeded chaotic\nsystem to tensors, producing non-invertible, user-specific transformations that\nenable authenticated inference, tensor-level watermarking, and data\nattribution. This framework offers a scalable and lightweight alternative to\nconventional cryptographic techniques, and establishes a new direction for\ntensor-level security in AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces a novel framework for secure and privacy-preserving\nneural network inference based on keyed chaotic dynamical transformations. The\nproposed method applies a deterministic, cryptographically seeded chaotic\nsystem to tensors, producing non-invertible, user-specific transformations that\nenable authenticated inference, tensor-level watermarking, and data\nattribution. This framework offers a scalable and lightweight alternative to\nconventional cryptographic techniques, and establishes a new direction for\ntensor-level security in AI systems."
                },
                "authors": [
                    {
                        "name": "Peter David Fagan"
                    }
                ],
                "author_detail": {
                    "name": "Peter David Fagan"
                },
                "author": "Peter David Fagan",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "94A60, 37N25, 68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23654v1",
                "updated": "2025-05-29T17:04:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    4,
                    2,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:04:02Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    4,
                    2,
                    3,
                    149,
                    0
                ],
                "title": "ARC: Argument Representation and Coverage Analysis for Zero-Shot Long\n  Document Summarization with Instruction Following LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: Argument Representation and Coverage Analysis for Zero-Shot Long\n  Document Summarization with Instruction Following LLMs"
                },
                "summary": "Integrating structured information has long improved the quality of\nabstractive summarization, particularly in retaining salient content. In this\nwork, we focus on a specific form of structure: argument roles, which are\ncrucial for summarizing documents in high-stakes domains such as law. We\ninvestigate whether instruction-tuned large language models (LLMs) adequately\npreserve this information. To this end, we introduce Argument Representation\nCoverage (ARC), a framework for measuring how well LLM-generated summaries\ncapture salient arguments. Using ARC, we analyze summaries produced by three\nopen-weight LLMs in two domains where argument roles are central: long legal\nopinions and scientific articles. Our results show that while LLMs cover\nsalient argument roles to some extent, critical information is often omitted in\ngenerated summaries, particularly when arguments are sparsely distributed\nthroughout the input. Further, we use ARC to uncover behavioral patterns --\nspecifically, how the positional bias of LLM context windows and role-specific\npreferences impact the coverage of key arguments in generated summaries,\nemphasizing the need for more argument-aware summarization strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating structured information has long improved the quality of\nabstractive summarization, particularly in retaining salient content. In this\nwork, we focus on a specific form of structure: argument roles, which are\ncrucial for summarizing documents in high-stakes domains such as law. We\ninvestigate whether instruction-tuned large language models (LLMs) adequately\npreserve this information. To this end, we introduce Argument Representation\nCoverage (ARC), a framework for measuring how well LLM-generated summaries\ncapture salient arguments. Using ARC, we analyze summaries produced by three\nopen-weight LLMs in two domains where argument roles are central: long legal\nopinions and scientific articles. Our results show that while LLMs cover\nsalient argument roles to some extent, critical information is often omitted in\ngenerated summaries, particularly when arguments are sparsely distributed\nthroughout the input. Further, we use ARC to uncover behavioral patterns --\nspecifically, how the positional bias of LLM context windows and role-specific\npreferences impact the coverage of key arguments in generated summaries,\nemphasizing the need for more argument-aware summarization strategies."
                },
                "authors": [
                    {
                        "name": "Mohamed Elaraby"
                    },
                    {
                        "name": "Diane Litman"
                    }
                ],
                "author_detail": {
                    "name": "Diane Litman"
                },
                "author": "Diane Litman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23653v1",
                "updated": "2025-05-29T17:02:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    2,
                    49,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:02:49Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    2,
                    49,
                    3,
                    149,
                    0
                ],
                "title": "How does Transformer Learn Implicit Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How does Transformer Learn Implicit Reasoning?"
                },
                "summary": "Recent work suggests that large language models (LLMs) can perform multi-hop\nreasoning implicitly -- producing correct answers without explicitly\nverbalizing intermediate steps -- but the underlying mechanisms remain poorly\nunderstood. In this paper, we study how such implicit reasoning emerges by\ntraining transformers from scratch in a controlled symbolic environment. Our\nanalysis reveals a three-stage developmental trajectory: early memorization,\nfollowed by in-distribution generalization, and eventually cross-distribution\ngeneralization. We find that training with atomic triples is not necessary but\naccelerates learning, and that second-hop generalization relies on query-level\nexposure to specific compositional structures. To interpret these behaviors, we\nintroduce two diagnostic tools: cross-query semantic patching, which identifies\nsemantically reusable intermediate representations, and a cosine-based\nrepresentational lens, which reveals that successful reasoning correlates with\nthe cosine-base clustering in hidden space. This clustering phenomenon in turn\nprovides a coherent explanation for the behavioral dynamics observed across\ntraining, linking representational structure to reasoning capability. These\nfindings provide new insights into the interpretability of implicit multi-hop\nreasoning in LLMs, helping to clarify how complex reasoning processes unfold\ninternally and offering pathways to enhance the transparency of such models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work suggests that large language models (LLMs) can perform multi-hop\nreasoning implicitly -- producing correct answers without explicitly\nverbalizing intermediate steps -- but the underlying mechanisms remain poorly\nunderstood. In this paper, we study how such implicit reasoning emerges by\ntraining transformers from scratch in a controlled symbolic environment. Our\nanalysis reveals a three-stage developmental trajectory: early memorization,\nfollowed by in-distribution generalization, and eventually cross-distribution\ngeneralization. We find that training with atomic triples is not necessary but\naccelerates learning, and that second-hop generalization relies on query-level\nexposure to specific compositional structures. To interpret these behaviors, we\nintroduce two diagnostic tools: cross-query semantic patching, which identifies\nsemantically reusable intermediate representations, and a cosine-based\nrepresentational lens, which reveals that successful reasoning correlates with\nthe cosine-base clustering in hidden space. This clustering phenomenon in turn\nprovides a coherent explanation for the behavioral dynamics observed across\ntraining, linking representational structure to reasoning capability. These\nfindings provide new insights into the interpretability of implicit multi-hop\nreasoning in LLMs, helping to clarify how complex reasoning processes unfold\ninternally and offering pathways to enhance the transparency of such models."
                },
                "authors": [
                    {
                        "name": "Jiaran Ye"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Zhidian Huang"
                    },
                    {
                        "name": "Liangming Pan"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Amy Xin"
                    },
                    {
                        "name": "Liu Weichuan"
                    },
                    {
                        "name": "Xiaoyin Che"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23648v1",
                "updated": "2025-05-29T16:58:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    58,
                    28,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:58:28Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    58,
                    28,
                    3,
                    149,
                    0
                ],
                "title": "Continuous Chain of Thought Enables Parallel Exploration and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Chain of Thought Enables Parallel Exploration and Reasoning"
                },
                "summary": "Current language models generate chain-of-thought traces by autoregressively\nsampling tokens from a finite vocabulary. While this discrete sampling has\nachieved remarkable success, conducting chain-of-thought with\ncontinuously-valued tokens (CoT2) offers a richer and more expressive\nalternative. Our work examines the benefits of CoT2 through logical reasoning\ntasks that inherently require search capabilities and provide optimization and\nexploration methods for CoT2. Theoretically, we show that CoT2 allows the model\nto track multiple traces in parallel and quantify its benefits for inference\nefficiency. Notably, one layer transformer equipped with CoT2 can provably\nsolve the combinatorial \"subset sum problem\" given sufficient embedding\ndimension. These insights lead to a novel and effective supervision strategy\nwhere we match the softmax outputs to the empirical token distributions of a\nset of target traces. Complementing this, we introduce sampling strategies that\nunlock policy optimization and self-improvement for CoT2. Our first strategy\nsamples and composes $K$ discrete tokens at each decoding step to control the\nlevel of parallelism, and reduces to standard CoT when $K=1$. Our second\nstrategy relies on continuous exploration over the probability simplex.\nExperiments confirm that policy optimization with CoT2 indeed improves the\nperformance of the model beyond its initial discrete or continuous supervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current language models generate chain-of-thought traces by autoregressively\nsampling tokens from a finite vocabulary. While this discrete sampling has\nachieved remarkable success, conducting chain-of-thought with\ncontinuously-valued tokens (CoT2) offers a richer and more expressive\nalternative. Our work examines the benefits of CoT2 through logical reasoning\ntasks that inherently require search capabilities and provide optimization and\nexploration methods for CoT2. Theoretically, we show that CoT2 allows the model\nto track multiple traces in parallel and quantify its benefits for inference\nefficiency. Notably, one layer transformer equipped with CoT2 can provably\nsolve the combinatorial \"subset sum problem\" given sufficient embedding\ndimension. These insights lead to a novel and effective supervision strategy\nwhere we match the softmax outputs to the empirical token distributions of a\nset of target traces. Complementing this, we introduce sampling strategies that\nunlock policy optimization and self-improvement for CoT2. Our first strategy\nsamples and composes $K$ discrete tokens at each decoding step to control the\nlevel of parallelism, and reduces to standard CoT when $K=1$. Our second\nstrategy relies on continuous exploration over the probability simplex.\nExperiments confirm that policy optimization with CoT2 indeed improves the\nperformance of the model beyond its initial discrete or continuous supervision."
                },
                "authors": [
                    {
                        "name": "Halil Alperen Gozeten"
                    },
                    {
                        "name": "M. Emrullah Ildiz"
                    },
                    {
                        "name": "Xuechen Zhang"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ankit Singh Rawat"
                    },
                    {
                        "name": "Samet Oymak"
                    }
                ],
                "author_detail": {
                    "name": "Samet Oymak"
                },
                "author": "Samet Oymak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23642v1",
                "updated": "2025-05-29T16:50:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    50,
                    28,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:50:28Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    50,
                    28,
                    3,
                    149,
                    0
                ],
                "title": "Radiant Triangle Soup with Soft Connectivity Forces for 3D\n  Reconstruction and Novel View Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiant Triangle Soup with Soft Connectivity Forces for 3D\n  Reconstruction and Novel View Synthesis"
                },
                "summary": "In this work, we introduce an inference-time optimization framework utilizing\ntriangles to represent the geometry and appearance of the scene. More\nspecifically, we develop a scene optimization algorithm for triangle soup, a\ncollection of disconnected semi-transparent triangle primitives. Compared to\nthe current most-widely used primitives for 3D scene representation, namely\nGaussian splats, triangles allow for more expressive color interpolation, and\nbenefit from a large algorithmic infrastructure for downstream tasks.\nTriangles, unlike full-rank Gaussian kernels, naturally combine to form\nsurfaces. We formulate connectivity forces between triangles during\noptimization, encouraging explicit, but soft, surface continuity in 3D. We\nperform experiments on a representative 3D reconstruction dataset and show\ncompetitive photometric and geometric results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce an inference-time optimization framework utilizing\ntriangles to represent the geometry and appearance of the scene. More\nspecifically, we develop a scene optimization algorithm for triangle soup, a\ncollection of disconnected semi-transparent triangle primitives. Compared to\nthe current most-widely used primitives for 3D scene representation, namely\nGaussian splats, triangles allow for more expressive color interpolation, and\nbenefit from a large algorithmic infrastructure for downstream tasks.\nTriangles, unlike full-rank Gaussian kernels, naturally combine to form\nsurfaces. We formulate connectivity forces between triangles during\noptimization, encouraging explicit, but soft, surface continuity in 3D. We\nperform experiments on a representative 3D reconstruction dataset and show\ncompetitive photometric and geometric results."
                },
                "authors": [
                    {
                        "name": "Nathaniel Burgdorfer"
                    },
                    {
                        "name": "Philippos Mordohai"
                    }
                ],
                "author_detail": {
                    "name": "Philippos Mordohai"
                },
                "author": "Philippos Mordohai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08805v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08805v3",
                "updated": "2025-05-29T16:48:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    48,
                    53,
                    3,
                    149,
                    0
                ],
                "published": "2024-12-11T22:37:45Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    22,
                    37,
                    45,
                    2,
                    346,
                    0
                ],
                "title": "Generative Agents for Multi-Agent Autoformalization of Interaction\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Agents for Multi-Agent Autoformalization of Interaction\n  Scenarios"
                },
                "summary": "Multi-agent simulations are versatile tools for exploring interactions among\nnatural and artificial agents, but their development typically demands domain\nexpertise and manual effort. This work introduces the Generative Agents for\nMulti-Agent Autoformalization (GAMA) framework, which automates the\nformalization of interaction scenarios in simulations using agents augmented\nwith large language models (LLMs). To demonstrate the application of GAMA, we\nuse natural language descriptions of game-theoretic scenarios representing\nsocial interactions, and we autoformalize them into executable logic programs\ndefining game rules, with syntactic correctness enforced through a solver-based\nvalidation. To ensure runtime validity, an iterative, tournament-based\nprocedure tests the generated rules and strategies, followed by exact semantic\nvalidation when ground truth outcomes are available. In experiments with 110\nnatural language descriptions across five 2x2 simultaneous-move games, GAMA\nachieves 100% syntactic and 76.5% semantic correctness with Claude 3.5 Sonnet,\nand 99.82% syntactic and 77% semantic correctness with GPT-4o. The framework\nalso shows high semantic accuracy in autoformalizing agents' strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent simulations are versatile tools for exploring interactions among\nnatural and artificial agents, but their development typically demands domain\nexpertise and manual effort. This work introduces the Generative Agents for\nMulti-Agent Autoformalization (GAMA) framework, which automates the\nformalization of interaction scenarios in simulations using agents augmented\nwith large language models (LLMs). To demonstrate the application of GAMA, we\nuse natural language descriptions of game-theoretic scenarios representing\nsocial interactions, and we autoformalize them into executable logic programs\ndefining game rules, with syntactic correctness enforced through a solver-based\nvalidation. To ensure runtime validity, an iterative, tournament-based\nprocedure tests the generated rules and strategies, followed by exact semantic\nvalidation when ground truth outcomes are available. In experiments with 110\nnatural language descriptions across five 2x2 simultaneous-move games, GAMA\nachieves 100% syntactic and 76.5% semantic correctness with Claude 3.5 Sonnet,\nand 99.82% syntactic and 77% semantic correctness with GPT-4o. The framework\nalso shows high semantic accuracy in autoformalizing agents' strategies."
                },
                "authors": [
                    {
                        "name": "Agnieszka Mensfelt"
                    },
                    {
                        "name": "Kostas Stathis"
                    },
                    {
                        "name": "Vince Trencsenyi"
                    }
                ],
                "author_detail": {
                    "name": "Vince Trencsenyi"
                },
                "author": "Vince Trencsenyi",
                "arxiv_comment": "code: https://github.com/dicelab-rhul/GAMA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08805v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08805v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04358v2",
                "updated": "2025-05-29T16:46:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    46,
                    0,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-04T20:47:43Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    20,
                    47,
                    43,
                    1,
                    35,
                    0
                ],
                "title": "Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM\n  Primitives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM\n  Primitives"
                },
                "summary": "Decomposing hard problems into subproblems often makes them easier and more\nefficient to solve. With large language models (LLMs) crossing critical\nreliability thresholds for a growing slate of capabilities, there is an\nincreasing effort to decompose systems into sets of LLM-based agents, each of\nwhom can be delegated sub-tasks. However, this decomposition (even when\nautomated) is often intuitive, e.g., based on how a human might assign roles to\nmembers of a human team. How close are these role decompositions to optimal?\nThis position paper argues that asymptotic analysis with LLM primitives is\nneeded to reason about the efficiency of such decomposed systems, and that\ninsights from such analysis will unlock opportunities for scaling them. By\ntreating the LLM forward pass as the atomic unit of computational cost, one can\nseparate out the (often opaque) inner workings of a particular LLM from the\ninherent efficiency of how a set of LLMs are orchestrated to solve hard\nproblems. In other words, if we want to scale the deployment of LLMs to the\nlimit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM\nprimitives should be used to reason about and develop more powerful\ndecompositions of large problems into LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decomposing hard problems into subproblems often makes them easier and more\nefficient to solve. With large language models (LLMs) crossing critical\nreliability thresholds for a growing slate of capabilities, there is an\nincreasing effort to decompose systems into sets of LLM-based agents, each of\nwhom can be delegated sub-tasks. However, this decomposition (even when\nautomated) is often intuitive, e.g., based on how a human might assign roles to\nmembers of a human team. How close are these role decompositions to optimal?\nThis position paper argues that asymptotic analysis with LLM primitives is\nneeded to reason about the efficiency of such decomposed systems, and that\ninsights from such analysis will unlock opportunities for scaling them. By\ntreating the LLM forward pass as the atomic unit of computational cost, one can\nseparate out the (often opaque) inner workings of a particular LLM from the\ninherent efficiency of how a set of LLMs are orchestrated to solve hard\nproblems. In other words, if we want to scale the deployment of LLMs to the\nlimit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM\nprimitives should be used to reason about and develop more powerful\ndecompositions of large problems into LLM agents."
                },
                "authors": [
                    {
                        "name": "Elliot Meyerson"
                    },
                    {
                        "name": "Xin Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Qiu"
                },
                "author": "Xin Qiu",
                "arxiv_comment": "In Proceedings of the 42nd International Conference on Machine\n  Learning (ICML 2025); 13 pages including references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14279v2",
                "updated": "2025-05-29T16:45:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    45,
                    0,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-20T12:30:46Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    12,
                    30,
                    46,
                    1,
                    140,
                    0
                ],
                "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering"
                },
                "summary": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry."
                },
                "authors": [
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Hamed Babaei Giglou"
                    },
                    {
                        "name": "Quentin Münch"
                    }
                ],
                "author_detail": {
                    "name": "Quentin Münch"
                },
                "author": "Quentin Münch",
                "arxiv_comment": "9 pages, 4 figures, Accepted as a Long Paper at the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23634v1",
                "updated": "2025-05-29T16:44:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    44,
                    29,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:44:29Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    44,
                    29,
                    3,
                    149,
                    0
                ],
                "title": "MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits\n  using Improved Preference Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits\n  using Improved Preference Alignment"
                },
                "summary": "The model context protocol (MCP) has been widely adapted as an open standard\nenabling the seamless integration of generative AI agents. However, recent work\nhas shown the MCP is susceptible to retrieval-based \"falsely benign\" attacks\n(FBAs), allowing malicious system access and credential theft, but requiring\nthat users download compromised files directly to their systems. Herein, we\nshow that the threat model of MCP-based attacks is significantly broader than\npreviously thought, i.e., attackers need only post malicious content online to\ndeceive MCP agents into carrying out their attacks on unsuspecting victims'\nsystems.\n  To improve alignment guardrails against such attacks, we introduce a new MCP\ndataset of FBAs and (truly) benign samples to explore the effectiveness of\ndirect preference optimization (DPO) for the refusal training of large language\nmodels (LLMs). While DPO improves model guardrails against such attacks, we\nshow that the efficacy of refusal learning varies drastically depending on the\nmodel's original post-training alignment scheme--e.g., GRPO-based LLMs learn to\nrefuse extremely poorly. Thus, to further improve FBA refusals, we introduce\nRetrieval Augmented Generation for Preference alignment (RAG-Pref), a novel\npreference alignment strategy based on RAG. We show that RAG-Pref significantly\nimproves the ability of LLMs to refuse FBAs, particularly when combined with\nDPO alignment, thus drastically improving guardrails against MCP-based attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The model context protocol (MCP) has been widely adapted as an open standard\nenabling the seamless integration of generative AI agents. However, recent work\nhas shown the MCP is susceptible to retrieval-based \"falsely benign\" attacks\n(FBAs), allowing malicious system access and credential theft, but requiring\nthat users download compromised files directly to their systems. Herein, we\nshow that the threat model of MCP-based attacks is significantly broader than\npreviously thought, i.e., attackers need only post malicious content online to\ndeceive MCP agents into carrying out their attacks on unsuspecting victims'\nsystems.\n  To improve alignment guardrails against such attacks, we introduce a new MCP\ndataset of FBAs and (truly) benign samples to explore the effectiveness of\ndirect preference optimization (DPO) for the refusal training of large language\nmodels (LLMs). While DPO improves model guardrails against such attacks, we\nshow that the efficacy of refusal learning varies drastically depending on the\nmodel's original post-training alignment scheme--e.g., GRPO-based LLMs learn to\nrefuse extremely poorly. Thus, to further improve FBA refusals, we introduce\nRetrieval Augmented Generation for Preference alignment (RAG-Pref), a novel\npreference alignment strategy based on RAG. We show that RAG-Pref significantly\nimproves the ability of LLMs to refuse FBAs, particularly when combined with\nDPO alignment, thus drastically improving guardrails against MCP-based attacks."
                },
                "authors": [
                    {
                        "name": "John Halloran"
                    }
                ],
                "author_detail": {
                    "name": "John Halloran"
                },
                "author": "John Halloran",
                "arxiv_comment": "27 pages, 19 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23628v1",
                "updated": "2025-05-29T16:34:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    34,
                    58,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:34:58Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    34,
                    58,
                    3,
                    149,
                    0
                ],
                "title": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic\n  Schema Induction from Web-Scale Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic\n  Schema Induction from Web-Scale Corpora"
                },
                "summary": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph\nconstruction that eliminates the need for predefined schemas. Our system\nleverages large language models to simultaneously extract knowledge triples and\ninduce comprehensive schemas directly from text, modeling both entities and\nevents while employing conceptualization to organize instances into semantic\ncategories. Processing over 50 million documents, we construct ATLAS (Automated\nTriple Linking And Schema induction), a family of knowledge graphs with 900+\nmillion nodes and 5.9 billion edges. This approach outperforms state-of-the-art\nbaselines on multi-hop QA tasks and enhances LLM factuality. Notably, our\nschema induction achieves 95\\% semantic alignment with human-crafted schemas\nwith zero manual intervention, demonstrating that billion-scale knowledge\ngraphs with dynamically induced schemas can effectively complement parametric\nknowledge in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph\nconstruction that eliminates the need for predefined schemas. Our system\nleverages large language models to simultaneously extract knowledge triples and\ninduce comprehensive schemas directly from text, modeling both entities and\nevents while employing conceptualization to organize instances into semantic\ncategories. Processing over 50 million documents, we construct ATLAS (Automated\nTriple Linking And Schema induction), a family of knowledge graphs with 900+\nmillion nodes and 5.9 billion edges. This approach outperforms state-of-the-art\nbaselines on multi-hop QA tasks and enhances LLM factuality. Notably, our\nschema induction achieves 95\\% semantic alignment with human-crafted schemas\nwith zero manual intervention, demonstrating that billion-scale knowledge\ngraphs with dynamically induced schemas can effectively complement parametric\nknowledge in large language models."
                },
                "authors": [
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Qing Zong"
                    },
                    {
                        "name": "Chunyang Li"
                    },
                    {
                        "name": "Hong Ting Tsang"
                    },
                    {
                        "name": "Hongyu Luo"
                    },
                    {
                        "name": "Yauwai Yim"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Xiao Zhou"
                    },
                    {
                        "name": "Feng Qin"
                    },
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Xi Peng"
                    },
                    {
                        "name": "Xin Yao"
                    },
                    {
                        "name": "Huiwen Yang"
                    },
                    {
                        "name": "Leijie Wu"
                    },
                    {
                        "name": "Yi Ji"
                    },
                    {
                        "name": "Gong Zhang"
                    },
                    {
                        "name": "Renhai Chen"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "9 pages, preprint, code:\n  https://github.com/HKUST-KnowComp/AutoSchemaKG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14669v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14669v2",
                "updated": "2025-05-29T16:32:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    32,
                    48,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-20T17:55:50Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    55,
                    50,
                    1,
                    140,
                    0
                ],
                "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models"
                },
                "summary": "Training large language models (LLMs) models directly in low-precision offers\na way to address computational costs by improving both throughput and energy\nefficiency. For those purposes, NVIDIA's recent Blackwell architecture\nfacilitates very low-precision operations using FP4 variants. Yet, current\nalgorithms for training LLMs in FP4 precision face significant accuracy\ndegradation and often rely on mixed-precision fallbacks. In this paper, we\ninvestigate hardware-supported FP4 training and introduce a new approach for\naccurate, end-to-end FP4 training with all the major computations (i.e., linear\nlayers) in low precision. Through extensive evaluations on Llama-type models,\nwe reveal a new low-precision scaling law that quantifies performance\ntrade-offs across bit-widths and training setups. Guided by this investigation,\nwe design an \"optimal\" technique in terms of accuracy-vs-computation, called\nQuartet. We implement Quartet using optimized CUDA kernels tailored for\nBlackwell, demonstrating that fully FP4-based training is a competitive\nalternative to FP16 half-precision and to FP8 training. Our code is available\nat https://github.com/IST-DASLab/Quartet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) models directly in low-precision offers\na way to address computational costs by improving both throughput and energy\nefficiency. For those purposes, NVIDIA's recent Blackwell architecture\nfacilitates very low-precision operations using FP4 variants. Yet, current\nalgorithms for training LLMs in FP4 precision face significant accuracy\ndegradation and often rely on mixed-precision fallbacks. In this paper, we\ninvestigate hardware-supported FP4 training and introduce a new approach for\naccurate, end-to-end FP4 training with all the major computations (i.e., linear\nlayers) in low precision. Through extensive evaluations on Llama-type models,\nwe reveal a new low-precision scaling law that quantifies performance\ntrade-offs across bit-widths and training setups. Guided by this investigation,\nwe design an \"optimal\" technique in terms of accuracy-vs-computation, called\nQuartet. We implement Quartet using optimized CUDA kernels tailored for\nBlackwell, demonstrating that fully FP4-based training is a competitive\nalternative to FP16 half-precision and to FP8 training. Our code is available\nat https://github.com/IST-DASLab/Quartet."
                },
                "authors": [
                    {
                        "name": "Roberto L. Castro"
                    },
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Soroush Tabesh"
                    },
                    {
                        "name": "Oliver Sieberling"
                    },
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Mahdi Nikdan"
                    },
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14669v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14669v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16502v2",
                "updated": "2025-05-29T16:31:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    31,
                    57,
                    3,
                    149,
                    0
                ],
                "published": "2024-10-21T20:48:16Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    20,
                    48,
                    16,
                    0,
                    295,
                    0
                ],
                "title": "RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic\n  and Human-like Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic\n  and Human-like Reasoning"
                },
                "summary": "Formal logic enables computers to reason in natural language by representing\nsentences in symbolic forms and applying rules to derive conclusions. However,\nin what our study characterizes as \"rulebreaker\" scenarios, this method can\nlead to conclusions that are typically not inferred or accepted by humans given\ntheir common sense and factual knowledge. Inspired by works in cognitive\nscience, we create RULEBREAKERS, the first dataset for rigorously evaluating\nthe ability of large language models (LLMs) to recognize and respond to\nrulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven\nLLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on\nRULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules\nunlike what is expected from typical human reasoners. Further analysis suggests\nthat this apparent failure is potentially associated with the models' poor\nutilization of their world knowledge and their attention distribution patterns.\nWhilst revealing a limitation of current LLMs, our study also provides a timely\ncounterbalance to a growing body of recent works that propose methods relying\non formal logic to improve LLMs' general reasoning capabilities, highlighting\ntheir risk of further increasing divergence between LLMs and human-like\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal logic enables computers to reason in natural language by representing\nsentences in symbolic forms and applying rules to derive conclusions. However,\nin what our study characterizes as \"rulebreaker\" scenarios, this method can\nlead to conclusions that are typically not inferred or accepted by humans given\ntheir common sense and factual knowledge. Inspired by works in cognitive\nscience, we create RULEBREAKERS, the first dataset for rigorously evaluating\nthe ability of large language models (LLMs) to recognize and respond to\nrulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven\nLLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on\nRULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules\nunlike what is expected from typical human reasoners. Further analysis suggests\nthat this apparent failure is potentially associated with the models' poor\nutilization of their world knowledge and their attention distribution patterns.\nWhilst revealing a limitation of current LLMs, our study also provides a timely\ncounterbalance to a growing body of recent works that propose methods relying\non formal logic to improve LLMs' general reasoning capabilities, highlighting\ntheir risk of further increasing divergence between LLMs and human-like\nreasoning."
                },
                "authors": [
                    {
                        "name": "Jason Chan"
                    },
                    {
                        "name": "Robert Gaizauskas"
                    },
                    {
                        "name": "Zhixue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhixue Zhao"
                },
                "author": "Zhixue Zhao",
                "arxiv_comment": "Preprint. Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23621v1",
                "updated": "2025-05-29T16:28:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    28,
                    50,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:28:50Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    28,
                    50,
                    3,
                    149,
                    0
                ],
                "title": "Table-R1: Inference-Time Scaling for Table Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table-R1: Inference-Time Scaling for Table Reasoning"
                },
                "summary": "In this work, we present the first study to explore inference-time scaling on\ntable reasoning tasks. We develop and evaluate two post-training strategies to\nenable inference-time scaling: distillation from frontier model reasoning\ntraces and reinforcement learning with verifiable rewards (RLVR). For\ndistillation, we introduce a large-scale dataset of reasoning traces generated\nby DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For\nRLVR, we propose task-specific verifiable reward functions and apply the GRPO\nalgorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series\nmodels across diverse table reasoning tasks, including short-form QA, fact\nverification, and free-form QA. Notably, the Table-R1-Zero model matches or\nexceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a\n7B-parameter LLM. It also demonstrates strong generalization to out-of-domain\ndatasets. Extensive ablation and qualitative analyses reveal the benefits of\ninstruction tuning, model architecture choices, and cross-task generalization,\nas well as emergence of essential table reasoning skills during RL training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present the first study to explore inference-time scaling on\ntable reasoning tasks. We develop and evaluate two post-training strategies to\nenable inference-time scaling: distillation from frontier model reasoning\ntraces and reinforcement learning with verifiable rewards (RLVR). For\ndistillation, we introduce a large-scale dataset of reasoning traces generated\nby DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For\nRLVR, we propose task-specific verifiable reward functions and apply the GRPO\nalgorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series\nmodels across diverse table reasoning tasks, including short-form QA, fact\nverification, and free-form QA. Notably, the Table-R1-Zero model matches or\nexceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a\n7B-parameter LLM. It also demonstrates strong generalization to out-of-domain\ndatasets. Extensive ablation and qualitative analyses reveal the benefits of\ninstruction tuning, model architecture choices, and cross-task generalization,\nas well as emergence of essential table reasoning skills during RL training."
                },
                "authors": [
                    {
                        "name": "Zheyuan Yang"
                    },
                    {
                        "name": "Lyuhao Chen"
                    },
                    {
                        "name": "Arman Cohan"
                    },
                    {
                        "name": "Yilun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Zhao"
                },
                "author": "Yilun Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23617v1",
                "updated": "2025-05-29T16:25:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    25,
                    35,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:25:35Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    25,
                    35,
                    3,
                    149,
                    0
                ],
                "title": "One Trajectory, One Token: Grounded Video Tokenization via Panoptic\n  Sub-object Trajectory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Trajectory, One Token: Grounded Video Tokenization via Panoptic\n  Sub-object Trajectory"
                },
                "summary": "Effective video tokenization is critical for scaling transformer models for\nlong videos. Current approaches tokenize videos using space-time patches,\nleading to excessive tokens and computational inefficiencies. The best token\nreduction strategies degrade performance and barely reduce the number of tokens\nwhen the camera moves. We introduce grounded video tokenization, a paradigm\nthat organizes tokens based on panoptic sub-object trajectories rather than\nfixed patches. Our method aligns with fundamental perceptual principles,\nensuring that tokenization reflects scene complexity rather than video\nduration. We propose TrajViT, a video encoder that extracts object trajectories\nand converts them into semantically meaningful tokens, significantly reducing\nredundancy while maintaining temporal coherence. Trained with contrastive\nlearning, TrajViT significantly outperforms space-time ViT (ViT3D) across\nmultiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a\nlarge margin of 6% top-5 recall in average at video-text retrieval task with\n10x token deduction. We also show TrajViT as a stronger model than ViT3D for\nbeing the video encoder for modern VideoLLM, obtaining an average of 5.2%\nperformance improvement across 6 VideoQA benchmarks while having 4x faster\ntraining time and 18x less inference FLOPs. TrajViT is the first efficient\nencoder to consistently outperform ViT3D across diverse video analysis tasks,\nmaking it a robust and scalable solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective video tokenization is critical for scaling transformer models for\nlong videos. Current approaches tokenize videos using space-time patches,\nleading to excessive tokens and computational inefficiencies. The best token\nreduction strategies degrade performance and barely reduce the number of tokens\nwhen the camera moves. We introduce grounded video tokenization, a paradigm\nthat organizes tokens based on panoptic sub-object trajectories rather than\nfixed patches. Our method aligns with fundamental perceptual principles,\nensuring that tokenization reflects scene complexity rather than video\nduration. We propose TrajViT, a video encoder that extracts object trajectories\nand converts them into semantically meaningful tokens, significantly reducing\nredundancy while maintaining temporal coherence. Trained with contrastive\nlearning, TrajViT significantly outperforms space-time ViT (ViT3D) across\nmultiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a\nlarge margin of 6% top-5 recall in average at video-text retrieval task with\n10x token deduction. We also show TrajViT as a stronger model than ViT3D for\nbeing the video encoder for modern VideoLLM, obtaining an average of 5.2%\nperformance improvement across 6 VideoQA benchmarks while having 4x faster\ntraining time and 18x less inference FLOPs. TrajViT is the first efficient\nencoder to consistently outperform ViT3D across diverse video analysis tasks,\nmaking it a robust and scalable solution."
                },
                "authors": [
                    {
                        "name": "Chenhao Zheng"
                    },
                    {
                        "name": "Jieyu Zhang"
                    },
                    {
                        "name": "Mohammadreza Salehi"
                    },
                    {
                        "name": "Ziqi Gao"
                    },
                    {
                        "name": "Vishnu Iyengar"
                    },
                    {
                        "name": "Norimasa Kobori"
                    },
                    {
                        "name": "Quan Kong"
                    },
                    {
                        "name": "Ranjay Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Ranjay Krishna"
                },
                "author": "Ranjay Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12147v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12147v2",
                "updated": "2025-05-29T16:25:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    25,
                    14,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-17T21:39:51Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    21,
                    39,
                    51,
                    5,
                    137,
                    0
                ],
                "title": "Causal Machine Learning in IoT-based Engineering Problems: A Tool\n  Comparison in the Case of Household Energy Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Machine Learning in IoT-based Engineering Problems: A Tool\n  Comparison in the Case of Household Energy Consumption"
                },
                "summary": "The rapid increase in computing power and the ability to store Big Data in\nthe infrastructure has enabled predictions in a large variety of domains by\nMachine Learning. However, in many cases, existing Machine Learning tools are\nconsidered insufficient or incorrect since they exploit only probabilistic\ndependencies rather than inference logic. Causal Machine Learning methods seem\nto close this gap. In this paper, two prevalent tools based on Causal Machine\nLearning methods are compared, as well as their mathematical underpinning\nbackground. The operation of the tools is demonstrated by examining their\nresponse to 18 queries, based on the IDEAL Household Energy Dataset, published\nby the University of Edinburgh. First, it was important to evaluate the causal\nrelations assumption that allowed the use of this approach; this was based on\nthe preexisting scientific knowledge of the domain and was implemented by use\nof the in-built validation tools. Results were encouraging and may easily be\nextended to other domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid increase in computing power and the ability to store Big Data in\nthe infrastructure has enabled predictions in a large variety of domains by\nMachine Learning. However, in many cases, existing Machine Learning tools are\nconsidered insufficient or incorrect since they exploit only probabilistic\ndependencies rather than inference logic. Causal Machine Learning methods seem\nto close this gap. In this paper, two prevalent tools based on Causal Machine\nLearning methods are compared, as well as their mathematical underpinning\nbackground. The operation of the tools is demonstrated by examining their\nresponse to 18 queries, based on the IDEAL Household Energy Dataset, published\nby the University of Edinburgh. First, it was important to evaluate the causal\nrelations assumption that allowed the use of this approach; this was based on\nthe preexisting scientific knowledge of the domain and was implemented by use\nof the in-built validation tools. Results were encouraging and may easily be\nextended to other domains."
                },
                "authors": [
                    {
                        "name": "Nikolaos-Lysias Kosioris"
                    },
                    {
                        "name": "Sotirios Nikoletseas"
                    },
                    {
                        "name": "Gavrilis Filios"
                    },
                    {
                        "name": "Stefanos Panagiotou"
                    }
                ],
                "author_detail": {
                    "name": "Stefanos Panagiotou"
                },
                "author": "Stefanos Panagiotou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12147v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12147v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23615v1",
                "updated": "2025-05-29T16:24:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    24,
                    18,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:24:18Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    24,
                    18,
                    3,
                    149,
                    0
                ],
                "title": "Learning Interpretable Differentiable Logic Networks for Tabular\n  Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Interpretable Differentiable Logic Networks for Tabular\n  Regression"
                },
                "summary": "Neural networks (NNs) achieve outstanding performance in many domains;\nhowever, their decision processes are often opaque and their inference can be\ncomputationally expensive in resource-constrained environments. We recently\nproposed Differentiable Logic Networks (DLNs) to address these issues for\ntabular classification based on relaxing discrete logic into a differentiable\nform, thereby enabling gradient-based learning of networks built from binary\nlogic operations. DLNs offer interpretable reasoning and substantially lower\ninference cost.\n  We extend the DLN framework to supervised tabular regression. Specifically,\nwe redesign the final output layer to support continuous targets and unify the\noriginal two-phase training procedure into a single differentiable stage. We\nevaluate the resulting model on 15 public regression benchmarks, comparing it\nwith modern neural networks and classical regression baselines. Regression DLNs\nmatch or exceed baseline accuracy while preserving interpretability and fast\ninference. Our results show that DLNs are a viable, cost-effective alternative\nfor regression tasks, especially where model transparency and computational\nefficiency are important.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks (NNs) achieve outstanding performance in many domains;\nhowever, their decision processes are often opaque and their inference can be\ncomputationally expensive in resource-constrained environments. We recently\nproposed Differentiable Logic Networks (DLNs) to address these issues for\ntabular classification based on relaxing discrete logic into a differentiable\nform, thereby enabling gradient-based learning of networks built from binary\nlogic operations. DLNs offer interpretable reasoning and substantially lower\ninference cost.\n  We extend the DLN framework to supervised tabular regression. Specifically,\nwe redesign the final output layer to support continuous targets and unify the\noriginal two-phase training procedure into a single differentiable stage. We\nevaluate the resulting model on 15 public regression benchmarks, comparing it\nwith modern neural networks and classical regression baselines. Regression DLNs\nmatch or exceed baseline accuracy while preserving interpretability and fast\ninference. Our results show that DLNs are a viable, cost-effective alternative\nfor regression tasks, especially where model transparency and computational\nefficiency are important."
                },
                "authors": [
                    {
                        "name": "Chang Yue"
                    },
                    {
                        "name": "Niraj K. Jha"
                    }
                ],
                "author_detail": {
                    "name": "Niraj K. Jha"
                },
                "author": "Niraj K. Jha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23614v1",
                "updated": "2025-05-29T16:22:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    22,
                    40,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:22:40Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    22,
                    40,
                    3,
                    149,
                    0
                ],
                "title": "Inference-time Scaling of Diffusion Models through Classical Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time Scaling of Diffusion Models through Classical Search"
                },
                "summary": "Classical search algorithms have long underpinned modern artificial\nintelligence. In this work, we tackle the challenge of inference-time control\nin diffusion models -- adapting generated outputs to meet diverse test-time\nobjectives -- using principles from classical search. We propose a general\nframework that orchestrates local and global search to efficiently navigate the\ngenerative space. It employs a theoretically grounded local search via annealed\nLangevin MCMC and performs compute-efficient global exploration using\nbreadth-first and depth-first tree search. We evaluate our approach on a range\nof challenging domains, including planning, offline reinforcement learning, and\nimage generation. Across all tasks, we observe significant gains in both\nperformance and efficiency. These results show that classical search provides a\nprincipled and practical foundation for inference-time scaling in diffusion\nmodels. Project page at diffusion-inference-scaling.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical search algorithms have long underpinned modern artificial\nintelligence. In this work, we tackle the challenge of inference-time control\nin diffusion models -- adapting generated outputs to meet diverse test-time\nobjectives -- using principles from classical search. We propose a general\nframework that orchestrates local and global search to efficiently navigate the\ngenerative space. It employs a theoretically grounded local search via annealed\nLangevin MCMC and performs compute-efficient global exploration using\nbreadth-first and depth-first tree search. We evaluate our approach on a range\nof challenging domains, including planning, offline reinforcement learning, and\nimage generation. Across all tasks, we observe significant gains in both\nperformance and efficiency. These results show that classical search provides a\nprincipled and practical foundation for inference-time scaling in diffusion\nmodels. Project page at diffusion-inference-scaling.github.io."
                },
                "authors": [
                    {
                        "name": "Xiangcheng Zhang"
                    },
                    {
                        "name": "Haowei Lin"
                    },
                    {
                        "name": "Haotian Ye"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Jianzhu Ma"
                    },
                    {
                        "name": "Yitao Liang"
                    },
                    {
                        "name": "Yilun Du"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Du"
                },
                "author": "Yilun Du",
                "arxiv_comment": "Website at https://diffusion-inference-scaling.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12559v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12559v3",
                "updated": "2025-05-29T16:18:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    18,
                    33,
                    3,
                    149,
                    0
                ],
                "published": "2024-12-17T05:38:27Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    38,
                    27,
                    1,
                    352,
                    0
                ],
                "title": "EXIT: Context-Aware Extractive Compression for Enhancing\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXIT: Context-Aware Extractive Compression for Enhancing\n  Retrieval-Augmented Generation"
                },
                "summary": "We introduce EXIT, an extractive context compression framework that enhances\nboth the effectiveness and efficiency of retrieval-augmented generation (RAG)\nin question answering (QA). Current RAG systems often struggle when retrieval\nmodels fail to rank the most relevant documents, leading to the inclusion of\nmore context at the expense of latency and accuracy. While abstractive\ncompression methods can drastically reduce token counts, their token-by-token\ngeneration process significantly increases end-to-end latency. Conversely,\nexisting extractive methods reduce latency but rely on independent,\nnon-adaptive sentence selection, failing to fully utilize contextual\ninformation. EXIT addresses these limitations by classifying sentences from\nretrieved documents - while preserving their contextual dependencies - enabling\nparallelizable, context-aware extraction that adapts to query complexity and\nretrieval quality. Our evaluations on both single-hop and multi-hop QA tasks\nshow that EXIT consistently surpasses existing compression methods and even\nuncompressed baselines in QA accuracy, while also delivering substantial\nreductions in inference time and token count. By improving both effectiveness\nand efficiency, EXIT provides a promising direction for developing scalable,\nhigh-quality QA solutions in RAG pipelines. Our code is available at\nhttps://github.com/ThisIsHwang/EXIT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce EXIT, an extractive context compression framework that enhances\nboth the effectiveness and efficiency of retrieval-augmented generation (RAG)\nin question answering (QA). Current RAG systems often struggle when retrieval\nmodels fail to rank the most relevant documents, leading to the inclusion of\nmore context at the expense of latency and accuracy. While abstractive\ncompression methods can drastically reduce token counts, their token-by-token\ngeneration process significantly increases end-to-end latency. Conversely,\nexisting extractive methods reduce latency but rely on independent,\nnon-adaptive sentence selection, failing to fully utilize contextual\ninformation. EXIT addresses these limitations by classifying sentences from\nretrieved documents - while preserving their contextual dependencies - enabling\nparallelizable, context-aware extraction that adapts to query complexity and\nretrieval quality. Our evaluations on both single-hop and multi-hop QA tasks\nshow that EXIT consistently surpasses existing compression methods and even\nuncompressed baselines in QA accuracy, while also delivering substantial\nreductions in inference time and token count. By improving both effectiveness\nand efficiency, EXIT provides a promising direction for developing scalable,\nhigh-quality QA solutions in RAG pipelines. Our code is available at\nhttps://github.com/ThisIsHwang/EXIT"
                },
                "authors": [
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Sukmin Cho"
                    },
                    {
                        "name": "Soyeong Jeong"
                    },
                    {
                        "name": "Hoyun Song"
                    },
                    {
                        "name": "SeungYoon Han"
                    },
                    {
                        "name": "Jong C. Park"
                    }
                ],
                "author_detail": {
                    "name": "Jong C. Park"
                },
                "author": "Jong C. Park",
                "arxiv_comment": "Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12559v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12559v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11942v3",
                "updated": "2025-05-30T02:28:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    28,
                    21,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-17T10:09:11Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    10,
                    9,
                    11,
                    5,
                    137,
                    0
                ],
                "title": "LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners"
                },
                "summary": "Lifelong learning is essential for intelligent agents operating in dynamic\nenvironments. Current large language model (LLM)-based agents, however, remain\nstateless and unable to accumulate or transfer knowledge over time. Existing\nbenchmarks treat agents as static systems and fail to evaluate lifelong\nlearning capabilities. We present LifelongAgentBench, the first unified\nbenchmark designed to systematically assess the lifelong learning ability of\nLLM agents. It provides skill-grounded, interdependent tasks across three\ninteractive environments, Database, Operating System, and Knowledge Graph, with\nautomatic label verification, reproducibility, and modular extensibility.\nExtensive experiments reveal that conventional experience replay has limited\neffectiveness for LLM agents due to irrelevant information and context length\nconstraints. We further introduce a group self-consistency mechanism that\nsignificantly improves lifelong learning performance. We hope\nLifelongAgentBench will advance the development of adaptive, memory-capable LLM\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifelong learning is essential for intelligent agents operating in dynamic\nenvironments. Current large language model (LLM)-based agents, however, remain\nstateless and unable to accumulate or transfer knowledge over time. Existing\nbenchmarks treat agents as static systems and fail to evaluate lifelong\nlearning capabilities. We present LifelongAgentBench, the first unified\nbenchmark designed to systematically assess the lifelong learning ability of\nLLM agents. It provides skill-grounded, interdependent tasks across three\ninteractive environments, Database, Operating System, and Knowledge Graph, with\nautomatic label verification, reproducibility, and modular extensibility.\nExtensive experiments reveal that conventional experience replay has limited\neffectiveness for LLM agents due to irrelevant information and context length\nconstraints. We further introduce a group self-consistency mechanism that\nsignificantly improves lifelong learning performance. We hope\nLifelongAgentBench will advance the development of adaptive, memory-capable LLM\nagents."
                },
                "authors": [
                    {
                        "name": "Junhao Zheng"
                    },
                    {
                        "name": "Xidi Cai"
                    },
                    {
                        "name": "Qiuke Li"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "ZhongZhi Li"
                    },
                    {
                        "name": "Yingying Zhang"
                    },
                    {
                        "name": "Le Song"
                    },
                    {
                        "name": "Qianli Ma"
                    }
                ],
                "author_detail": {
                    "name": "Qianli Ma"
                },
                "author": "Qianli Ma",
                "arxiv_comment": "Project Page: https://caixd-220529.github.io/LifelongAgentBench/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23606v1",
                "updated": "2025-05-29T16:15:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    15,
                    48,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:15:48Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    15,
                    48,
                    3,
                    149,
                    0
                ],
                "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified\n  Discrete Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified\n  Discrete Diffusion Model"
                },
                "summary": "Unified generation models aim to handle diverse tasks across modalities --\nsuch as text generation, image generation, and vision-language reasoning --\nwithin a single architecture and decoding paradigm. Autoregressive unified\nmodels suffer from slow inference due to sequential decoding, and\nnon-autoregressive unified models suffer from weak generalization due to\nlimited pretrained backbones. We introduce Muddit, a unified discrete diffusion\ntransformer that enables fast and parallel generation across both text and\nimage modalities. Unlike prior unified diffusion models trained from scratch,\nMuddit integrates strong visual priors from a pretrained text-to-image backbone\nwith a lightweight text decoder, enabling flexible and high-quality multimodal\ngeneration under a unified architecture. Empirical results show that Muddit\nachieves competitive or superior performance compared to significantly larger\nautoregressive models in both quality and efficiency. The work highlights the\npotential of purely discrete diffusion, when equipped with strong visual\npriors, as a scalable and effective backbone for unified generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified generation models aim to handle diverse tasks across modalities --\nsuch as text generation, image generation, and vision-language reasoning --\nwithin a single architecture and decoding paradigm. Autoregressive unified\nmodels suffer from slow inference due to sequential decoding, and\nnon-autoregressive unified models suffer from weak generalization due to\nlimited pretrained backbones. We introduce Muddit, a unified discrete diffusion\ntransformer that enables fast and parallel generation across both text and\nimage modalities. Unlike prior unified diffusion models trained from scratch,\nMuddit integrates strong visual priors from a pretrained text-to-image backbone\nwith a lightweight text decoder, enabling flexible and high-quality multimodal\ngeneration under a unified architecture. Empirical results show that Muddit\nachieves competitive or superior performance compared to significantly larger\nautoregressive models in both quality and efficiency. The work highlights the\npotential of purely discrete diffusion, when equipped with strong visual\npriors, as a scalable and effective backbone for unified generation."
                },
                "authors": [
                    {
                        "name": "Qingyu Shi"
                    },
                    {
                        "name": "Jinbin Bai"
                    },
                    {
                        "name": "Zhuoran Zhao"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Kaidong Yu"
                    },
                    {
                        "name": "Jianzong Wu"
                    },
                    {
                        "name": "Shuangyong Song"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Xuelong Li"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "arxiv_comment": "The code and model are available at\n  https://github.com/M-E-AGI-Lab/Muddit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23604v1",
                "updated": "2025-05-29T16:15:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    15,
                    36,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:15:36Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    15,
                    36,
                    3,
                    149,
                    0
                ],
                "title": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software\n  Engineering"
                },
                "summary": "Language models (LMs) perform well on standardized coding benchmarks but\nstruggle with real-world software engineering tasks such as resolving GitHub\nissues in SWE-Bench, especially when model parameters are less than 100B. While\nsmaller models are preferable in practice due to their lower computational\ncost, improving their performance remains challenging. Existing approaches\nprimarily rely on supervised fine-tuning (SFT) with high-quality data, which is\nexpensive to curate at scale. An alternative is test-time scaling: generating\nmultiple outputs, scoring them using a verifier, and selecting the best one.\nAlthough effective, this strategy often requires excessive sampling and costly\nscoring, limiting its practical application. We propose Evolutionary Test-Time\nScaling (EvoScale), a sample-efficient method that treats generation as an\nevolutionary process. By iteratively refining outputs via selection and\nmutation, EvoScale shifts the output distribution toward higher-scoring\nregions, reducing the number of samples needed to find correct solutions. To\nreduce the overhead from repeatedly sampling and selection, we train the model\nto self-evolve using reinforcement learning (RL). Rather than relying on\nexternal verifiers at inference time, the model learns to self-improve the\nscores of its own generations across iterations. Evaluated on\nSWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or\nexceed the performance of models with over 100B parameters while using a few\nsamples. Code, data, and models will be fully open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LMs) perform well on standardized coding benchmarks but\nstruggle with real-world software engineering tasks such as resolving GitHub\nissues in SWE-Bench, especially when model parameters are less than 100B. While\nsmaller models are preferable in practice due to their lower computational\ncost, improving their performance remains challenging. Existing approaches\nprimarily rely on supervised fine-tuning (SFT) with high-quality data, which is\nexpensive to curate at scale. An alternative is test-time scaling: generating\nmultiple outputs, scoring them using a verifier, and selecting the best one.\nAlthough effective, this strategy often requires excessive sampling and costly\nscoring, limiting its practical application. We propose Evolutionary Test-Time\nScaling (EvoScale), a sample-efficient method that treats generation as an\nevolutionary process. By iteratively refining outputs via selection and\nmutation, EvoScale shifts the output distribution toward higher-scoring\nregions, reducing the number of samples needed to find correct solutions. To\nreduce the overhead from repeatedly sampling and selection, we train the model\nto self-evolve using reinforcement learning (RL). Rather than relying on\nexternal verifiers at inference time, the model learns to self-improve the\nscores of its own generations across iterations. Evaluated on\nSWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or\nexceed the performance of models with over 100B parameters while using a few\nsamples. Code, data, and models will be fully open-sourced."
                },
                "authors": [
                    {
                        "name": "Guangtao Zeng"
                    },
                    {
                        "name": "Maohao Shen"
                    },
                    {
                        "name": "Delin Chen"
                    },
                    {
                        "name": "Zhenting Qi"
                    },
                    {
                        "name": "Subhro Das"
                    },
                    {
                        "name": "Dan Gutfreund"
                    },
                    {
                        "name": "David Cox"
                    },
                    {
                        "name": "Gregory Wornell"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Zhang-Wei Hong"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14276v2",
                "updated": "2025-05-29T16:13:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    13,
                    21,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-20T05:28:44Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    28,
                    44,
                    3,
                    51,
                    0
                ],
                "title": "STeCa: Step-level Trajectory Calibration for LLM Agent Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STeCa: Step-level Trajectory Calibration for LLM Agent Learning"
                },
                "summary": "Large language model (LLM)-based agents have shown promise in tackling\ncomplex tasks by interacting dynamically with the environment. Existing work\nprimarily focuses on behavior cloning from expert demonstrations or preference\nlearning through exploratory trajectory sampling. However, these methods often\nstruggle to address long-horizon tasks, where suboptimal actions accumulate\nstep by step, causing agents to deviate from correct task trajectories. To\naddress this, we highlight the importance of timely calibration and the need to\nautomatically construct calibration trajectories for training agents. We\npropose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM\nagent learning. Specifically, STeCa identifies suboptimal actions through a\nstep-level reward comparison during exploration. It constructs calibrated\ntrajectories using LLM-driven reflection, enabling agents to learn from\nimproved decision-making processes. We finally leverage these calibrated\ntrajectories with successful trajectories for reinforced training. Extensive\nexperiments demonstrate that STeCa significantly outperforms existing methods.\nFurther analysis highlights that timely calibration enables agents to complete\ntasks with greater robustness. Our code and data are available at\nhttps://github.com/WangHanLinHenry/STeCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based agents have shown promise in tackling\ncomplex tasks by interacting dynamically with the environment. Existing work\nprimarily focuses on behavior cloning from expert demonstrations or preference\nlearning through exploratory trajectory sampling. However, these methods often\nstruggle to address long-horizon tasks, where suboptimal actions accumulate\nstep by step, causing agents to deviate from correct task trajectories. To\naddress this, we highlight the importance of timely calibration and the need to\nautomatically construct calibration trajectories for training agents. We\npropose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM\nagent learning. Specifically, STeCa identifies suboptimal actions through a\nstep-level reward comparison during exploration. It constructs calibrated\ntrajectories using LLM-driven reflection, enabling agents to learn from\nimproved decision-making processes. We finally leverage these calibrated\ntrajectories with successful trajectories for reinforced training. Extensive\nexperiments demonstrate that STeCa significantly outperforms existing methods.\nFurther analysis highlights that timely calibration enables agents to complete\ntasks with greater robustness. Our code and data are available at\nhttps://github.com/WangHanLinHenry/STeCa."
                },
                "authors": [
                    {
                        "name": "Hanlin Wang"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Chak Tou Leong"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "arxiv_comment": "Accepted by ACL2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23598v1",
                "updated": "2025-05-29T16:11:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    11,
                    18,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:11:18Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    11,
                    18,
                    3,
                    149,
                    0
                ],
                "title": "LLM Performance for Code Generation on Noisy Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Performance for Code Generation on Noisy Tasks"
                },
                "summary": "This paper investigates the ability of large language models (LLMs) to\nrecognise and solve tasks which have been obfuscated beyond recognition.\nFocusing on competitive programming and benchmark tasks (LeetCode and MATH), we\ncompare performance across multiple models and obfuscation methods, such as\nnoise and redaction. We demonstrate that all evaluated LLMs can solve tasks\nobfuscated to a level where the text would be unintelligible to human readers,\nand does not contain key pieces of instruction or context. We introduce the\nconcept of eager pattern matching to describe this behaviour, which is not\nobserved in tasks published after the models' knowledge cutoff date, indicating\nstrong memorisation or overfitting to training data, rather than legitimate\nreasoning about the presented problem. We report empirical evidence of distinct\nperformance decay patterns between contaminated and unseen datasets. We discuss\nthe implications for benchmarking and evaluations of model behaviour, arguing\nfor caution when designing experiments using standard datasets. We also propose\nmeasuring the decay of performance under obfuscation as a possible strategy for\ndetecting dataset contamination and highlighting potential safety risks and\ninterpretability issues for automated software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the ability of large language models (LLMs) to\nrecognise and solve tasks which have been obfuscated beyond recognition.\nFocusing on competitive programming and benchmark tasks (LeetCode and MATH), we\ncompare performance across multiple models and obfuscation methods, such as\nnoise and redaction. We demonstrate that all evaluated LLMs can solve tasks\nobfuscated to a level where the text would be unintelligible to human readers,\nand does not contain key pieces of instruction or context. We introduce the\nconcept of eager pattern matching to describe this behaviour, which is not\nobserved in tasks published after the models' knowledge cutoff date, indicating\nstrong memorisation or overfitting to training data, rather than legitimate\nreasoning about the presented problem. We report empirical evidence of distinct\nperformance decay patterns between contaminated and unseen datasets. We discuss\nthe implications for benchmarking and evaluations of model behaviour, arguing\nfor caution when designing experiments using standard datasets. We also propose\nmeasuring the decay of performance under obfuscation as a possible strategy for\ndetecting dataset contamination and highlighting potential safety risks and\ninterpretability issues for automated software systems."
                },
                "authors": [
                    {
                        "name": "Radzim Sendyka"
                    },
                    {
                        "name": "Christian Cabrera"
                    },
                    {
                        "name": "Andrei Paleyes"
                    },
                    {
                        "name": "Diana Robinson"
                    },
                    {
                        "name": "Neil Lawrence"
                    }
                ],
                "author_detail": {
                    "name": "Neil Lawrence"
                },
                "author": "Neil Lawrence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09853v2",
                "updated": "2025-05-29T16:08:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    8,
                    23,
                    3,
                    149,
                    0
                ],
                "published": "2024-08-19T09:57:28Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    57,
                    28,
                    0,
                    232,
                    0
                ],
                "title": "X-TURING: Towards an Enhanced and Efficient Turing Test for Long-Term\n  Dialogue Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-TURING: Towards an Enhanced and Efficient Turing Test for Long-Term\n  Dialogue Agents"
                },
                "summary": "The Turing test examines whether AIs exhibit human-like behaviour in natural\nlanguage conversations. The traditional setting limits each participant to one\nmessage at a time and requires constant human participation. This fails to\nreflect a natural conversational style and hinders the evaluation of dialogue\nagents based on Large Language Models (LLMs) in complex and prolonged\ninteractions. This paper proposes \\textbf{\\textsc{X-Turing}}, which enhances\nthe original test with a \\textit{burst dialogue} pattern, allowing more dynamic\nexchanges using consecutive messages. It further reduces human workload by\niteratively generating dialogues that simulate the long-term interaction\nbetween the agent and a human to compose the majority of the test process. With\nthe \\textit{pseudo-dialogue} history, the agent then engages in a shorter\ndialogue with a real human, which is paired with a human-human conversation on\nthe same topic to be judged using questionnaires. We introduce the\n\\textit{X-Turn Pass-Rate} metric to assess the human likeness of LLMs across\nvarying durations. While LLMs like GPT-4 initially perform well, achieving pass\nrates of 51.9\\% and 38.9\\% during 3 turns and 10 turns of dialogues\nrespectively, their performance drops as the dialogue progresses, which\nunderscores the difficulty in maintaining consistency in the long term.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Turing test examines whether AIs exhibit human-like behaviour in natural\nlanguage conversations. The traditional setting limits each participant to one\nmessage at a time and requires constant human participation. This fails to\nreflect a natural conversational style and hinders the evaluation of dialogue\nagents based on Large Language Models (LLMs) in complex and prolonged\ninteractions. This paper proposes \\textbf{\\textsc{X-Turing}}, which enhances\nthe original test with a \\textit{burst dialogue} pattern, allowing more dynamic\nexchanges using consecutive messages. It further reduces human workload by\niteratively generating dialogues that simulate the long-term interaction\nbetween the agent and a human to compose the majority of the test process. With\nthe \\textit{pseudo-dialogue} history, the agent then engages in a shorter\ndialogue with a real human, which is paired with a human-human conversation on\nthe same topic to be judged using questionnaires. We introduce the\n\\textit{X-Turn Pass-Rate} metric to assess the human likeness of LLMs across\nvarying durations. While LLMs like GPT-4 initially perform well, achieving pass\nrates of 51.9\\% and 38.9\\% during 3 turns and 10 turns of dialogues\nrespectively, their performance drops as the dialogue progresses, which\nunderscores the difficulty in maintaining consistency in the long term."
                },
                "authors": [
                    {
                        "name": "Weiqi Wu"
                    },
                    {
                        "name": "Hongqiu Wu"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Accepted to ACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23592v1",
                "updated": "2025-05-29T16:04:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    4,
                    4,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:04:04Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    4,
                    4,
                    3,
                    149,
                    0
                ],
                "title": "A Modern Theory of Cross-Validation through the Lens of Stability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Modern Theory of Cross-Validation through the Lens of Stability"
                },
                "summary": "Modern data analysis and statistical learning are marked by complex data\nstructures and black-box algorithms. Data complexity stems from technologies\nlike imaging, remote sensing, wearables, and genomic sequencing.\nSimultaneously, black-box models -- especially deep neural networks -- have\nachieved impressive results. This combination raises new challenges for\nuncertainty quantification and statistical inference, which we term \"black-box\ninference.\"\n  Black-box inference is difficult due to the lack of traditional modeling\nassumptions and the opaque behavior of modern estimators. These make it hard to\ncharacterize the distribution of estimation errors. A popular solution is\npost-hoc randomization, which, under mild assumptions like exchangeability, can\nyield valid uncertainty quantification. Such methods range from classical\ntechniques like permutation tests, jackknife, and bootstrap, to recent\ninnovations like conformal inference. These approaches typically need little\nknowledge of data distributions or the internal working of estimators. Many\nrely on the idea that estimators behave similarly under small data changes -- a\nconcept formalized as stability. Over time, stability has become a key\nprinciple in data science, influencing generalization error, privacy, and\nadaptive inference.\n  This article investigates cross-validation (CV) -- a widely used resampling\nmethod -- through the lens of stability. We first review recent theoretical\nresults on CV for estimating generalization error and model selection under\nstability. We then examine uncertainty quantification for CV-based risk\nestimates. Together, these insights yield new theory and tools, which we apply\nto topics like model selection, selective inference, and conformal prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data analysis and statistical learning are marked by complex data\nstructures and black-box algorithms. Data complexity stems from technologies\nlike imaging, remote sensing, wearables, and genomic sequencing.\nSimultaneously, black-box models -- especially deep neural networks -- have\nachieved impressive results. This combination raises new challenges for\nuncertainty quantification and statistical inference, which we term \"black-box\ninference.\"\n  Black-box inference is difficult due to the lack of traditional modeling\nassumptions and the opaque behavior of modern estimators. These make it hard to\ncharacterize the distribution of estimation errors. A popular solution is\npost-hoc randomization, which, under mild assumptions like exchangeability, can\nyield valid uncertainty quantification. Such methods range from classical\ntechniques like permutation tests, jackknife, and bootstrap, to recent\ninnovations like conformal inference. These approaches typically need little\nknowledge of data distributions or the internal working of estimators. Many\nrely on the idea that estimators behave similarly under small data changes -- a\nconcept formalized as stability. Over time, stability has become a key\nprinciple in data science, influencing generalization error, privacy, and\nadaptive inference.\n  This article investigates cross-validation (CV) -- a widely used resampling\nmethod -- through the lens of stability. We first review recent theoretical\nresults on CV for estimating generalization error and model selection under\nstability. We then examine uncertainty quantification for CV-based risk\nestimates. Together, these insights yield new theory and tools, which we apply\nto topics like model selection, selective inference, and conformal prediction."
                },
                "authors": [
                    {
                        "name": "Jing Lei"
                    }
                ],
                "author_detail": {
                    "name": "Jing Lei"
                },
                "author": "Jing Lei",
                "arxiv_comment": "120 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07527v3",
                "updated": "2025-05-29T16:02:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    2,
                    19,
                    3,
                    149,
                    0
                ],
                "published": "2024-09-11T18:00:03Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    18,
                    0,
                    3,
                    2,
                    255,
                    0
                ],
                "title": "Can slow pulsars in Milky Way globular clusters form via partial\n  recycling?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can slow pulsars in Milky Way globular clusters form via partial\n  recycling?"
                },
                "summary": "Alongside the population of several hundred radio millisecond pulsars\ncurrently known in Milky Way globular clusters, a subset of six slowly spinning\npulsars (spin periods $0.3-4\\,$s) are also observed. With inferred magnetic\nfields $\\gtrsim 10^{11}\\,$G and characteristic ages $\\lesssim10^8\\,$yr,\nexplaining the formation of these apparently young pulsars in old stellar\npopulations poses a major challenge. One popular explanation is that these\nobjects are not actually young but instead have been partially spun up via\naccretion from a binary companion. In this scenario, accretion in a typical\nlow-mass X-ray binary is interrupted by a dynamical encounter with a\nneighboring object in the cluster. Instead of complete spin up to millisecond\nspin periods, the accretion is halted prematurely, leaving behind a ''partially\nrecycled'' neutron star. In this Letter, we use a combination of analytic\narguments motivated by low-mass X-ray binary evolution and $N$-body simulations\nto show that this partial-recycling mechanism is not viable. Realistic globular\nclusters are not sufficiently dense to interrupt mass transfer on the short\ntimescales required to achieve such slow spin periods. We argue that collapse\nof massive white dwarfs and/or neutron star collisions are more promising ways\nto form slow pulsars in old globular clusters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alongside the population of several hundred radio millisecond pulsars\ncurrently known in Milky Way globular clusters, a subset of six slowly spinning\npulsars (spin periods $0.3-4\\,$s) are also observed. With inferred magnetic\nfields $\\gtrsim 10^{11}\\,$G and characteristic ages $\\lesssim10^8\\,$yr,\nexplaining the formation of these apparently young pulsars in old stellar\npopulations poses a major challenge. One popular explanation is that these\nobjects are not actually young but instead have been partially spun up via\naccretion from a binary companion. In this scenario, accretion in a typical\nlow-mass X-ray binary is interrupted by a dynamical encounter with a\nneighboring object in the cluster. Instead of complete spin up to millisecond\nspin periods, the accretion is halted prematurely, leaving behind a ''partially\nrecycled'' neutron star. In this Letter, we use a combination of analytic\narguments motivated by low-mass X-ray binary evolution and $N$-body simulations\nto show that this partial-recycling mechanism is not viable. Realistic globular\nclusters are not sufficiently dense to interrupt mass transfer on the short\ntimescales required to achieve such slow spin periods. We argue that collapse\nof massive white dwarfs and/or neutron star collisions are more promising ways\nto form slow pulsars in old globular clusters."
                },
                "authors": [
                    {
                        "name": "Kyle Kremer"
                    },
                    {
                        "name": "Claire S. Ye"
                    },
                    {
                        "name": "Craig O. Heinke"
                    },
                    {
                        "name": "Anthony L. Piro"
                    },
                    {
                        "name": "Scott M. Ransom"
                    },
                    {
                        "name": "Frederic A. Rasio"
                    }
                ],
                "author_detail": {
                    "name": "Frederic A. Rasio"
                },
                "author": "Frederic A. Rasio",
                "arxiv_comment": "9 pages, 2 figures. Accepted for publication in ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23586v1",
                "updated": "2025-05-29T15:58:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    58,
                    29,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:58:29Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    58,
                    29,
                    3,
                    149,
                    0
                ],
                "title": "Weakly-supervised Localization of Manipulated Image Regions Using\n  Multi-resolution Learned Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly-supervised Localization of Manipulated Image Regions Using\n  Multi-resolution Learned Features"
                },
                "summary": "The explosive growth of digital images and the widespread availability of\nimage editing tools have made image manipulation detection an increasingly\ncritical challenge. Current deep learning-based manipulation detection methods\nexcel in achieving high image-level classification accuracy, they often fall\nshort in terms of interpretability and localization of manipulated regions.\nAdditionally, the absence of pixel-wise annotations in real-world scenarios\nlimits the existing fully-supervised manipulation localization techniques. To\naddress these challenges, we propose a novel weakly-supervised approach that\nintegrates activation maps generated by image-level manipulation detection\nnetworks with segmentation maps from pre-trained models. Specifically, we build\non our previous image-level work named WCBnet to produce multi-view feature\nmaps which are subsequently fused for coarse localization. These coarse maps\nare then refined using detailed segmented regional information provided by\npre-trained segmentation models (such as DeepLab, SegmentAnything and PSPnet),\nwith Bayesian inference employed to enhance the manipulation localization.\nExperimental results demonstrate the effectiveness of our approach,\nhighlighting the feasibility to localize image manipulations without relying on\npixel-level labels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The explosive growth of digital images and the widespread availability of\nimage editing tools have made image manipulation detection an increasingly\ncritical challenge. Current deep learning-based manipulation detection methods\nexcel in achieving high image-level classification accuracy, they often fall\nshort in terms of interpretability and localization of manipulated regions.\nAdditionally, the absence of pixel-wise annotations in real-world scenarios\nlimits the existing fully-supervised manipulation localization techniques. To\naddress these challenges, we propose a novel weakly-supervised approach that\nintegrates activation maps generated by image-level manipulation detection\nnetworks with segmentation maps from pre-trained models. Specifically, we build\non our previous image-level work named WCBnet to produce multi-view feature\nmaps which are subsequently fused for coarse localization. These coarse maps\nare then refined using detailed segmented regional information provided by\npre-trained segmentation models (such as DeepLab, SegmentAnything and PSPnet),\nwith Bayesian inference employed to enhance the manipulation localization.\nExperimental results demonstrate the effectiveness of our approach,\nhighlighting the feasibility to localize image manipulations without relying on\npixel-level labels."
                },
                "authors": [
                    {
                        "name": "Ziyong Wang"
                    },
                    {
                        "name": "Charith Abhayaratne"
                    }
                ],
                "author_detail": {
                    "name": "Charith Abhayaratne"
                },
                "author": "Charith Abhayaratne",
                "arxiv_comment": "This paper was presented at the British Machine Vision Conference\n  2024 workshop on Media authenticity in the age of artificial intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18463v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18463v3",
                "updated": "2025-05-29T15:56:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    56,
                    25,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-30T16:30:20Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    30,
                    20,
                    3,
                    30,
                    0
                ],
                "title": "A Benchmark and Evaluation for Real-World Out-of-Distribution Detection\n  Using Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Benchmark and Evaluation for Real-World Out-of-Distribution Detection\n  Using Vision-Language Models"
                },
                "summary": "Out-of-distribution (OOD) detection is a task that detects OOD samples during\ninference to ensure the safety of deployed models. However, conventional\nbenchmarks have reached performance saturation, making it difficult to compare\nrecent OOD detection methods. To address this challenge, we introduce three\nnovel OOD detection benchmarks that enable a deeper understanding of method\ncharacteristics and reflect real-world conditions. First, we present\nImageNet-X, designed to evaluate performance under challenging semantic shifts.\nSecond, we propose ImageNet-FS-X for full-spectrum OOD detection, assessing\nrobustness to covariate shifts (feature distribution shifts). Finally, we\npropose Wilds-FS-X, which extends these evaluations to real-world datasets,\noffering a more comprehensive testbed. Our experiments reveal that recent\nCLIP-based OOD detection methods struggle to varying degrees across the three\nproposed benchmarks, and none of them consistently outperforms the others. We\nhope the community goes beyond specific benchmarks and includes more\nchallenging conditions reflecting real-world scenarios. The code is\nhttps://github.com/hoshi23/OOD-X-Benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection is a task that detects OOD samples during\ninference to ensure the safety of deployed models. However, conventional\nbenchmarks have reached performance saturation, making it difficult to compare\nrecent OOD detection methods. To address this challenge, we introduce three\nnovel OOD detection benchmarks that enable a deeper understanding of method\ncharacteristics and reflect real-world conditions. First, we present\nImageNet-X, designed to evaluate performance under challenging semantic shifts.\nSecond, we propose ImageNet-FS-X for full-spectrum OOD detection, assessing\nrobustness to covariate shifts (feature distribution shifts). Finally, we\npropose Wilds-FS-X, which extends these evaluations to real-world datasets,\noffering a more comprehensive testbed. Our experiments reveal that recent\nCLIP-based OOD detection methods struggle to varying degrees across the three\nproposed benchmarks, and none of them consistently outperforms the others. We\nhope the community goes beyond specific benchmarks and includes more\nchallenging conditions reflecting real-world scenarios. The code is\nhttps://github.com/hoshi23/OOD-X-Benchmarks."
                },
                "authors": [
                    {
                        "name": "Shiho Noda"
                    },
                    {
                        "name": "Atsuyuki Miyai"
                    },
                    {
                        "name": "Qing Yu"
                    },
                    {
                        "name": "Go Irie"
                    },
                    {
                        "name": "Kiyoharu Aizawa"
                    }
                ],
                "author_detail": {
                    "name": "Kiyoharu Aizawa"
                },
                "author": "Kiyoharu Aizawa",
                "arxiv_comment": "Accepted at ICIP2025 Dataset and Benchmark Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18463v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18463v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23580v1",
                "updated": "2025-05-29T15:53:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    53,
                    21,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:53:21Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    53,
                    21,
                    3,
                    149,
                    0
                ],
                "title": "Engineering Serendipity through Recommendations of Items with Atypical\n  Aspects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering Serendipity through Recommendations of Items with Atypical\n  Aspects"
                },
                "summary": "A restaurant dinner or a hotel stay may lead to memorable experiences when\nguests encounter unexpected aspects that also match their interests. For\nexample, an origami-making station in the waiting area of a restaurant may be\nboth surprising and enjoyable for a customer who is passionate about paper\ncrafts. Similarly, an exhibit of 18th century harpsichords would be atypical\nfor a hotel lobby and likely pique the interest of a guest who has a passion\nfor Baroque music. Motivated by this insight, in this paper we introduce the\nnew task of engineering serendipity through recommendations of items with\natypical aspects. We describe an LLM-based system pipeline that extracts\natypical aspects from item reviews, then estimates and aggregates their\nuser-specific utility in a measure of serendipity potential that is used to\nrerank a list of items recommended to the user. To facilitate system\ndevelopment and evaluation, we introduce a dataset of Yelp reviews that are\nmanually annotated with atypical aspects and a dataset of artificially\ngenerated user profiles, together with crowdsourced annotations of user-aspect\nutility values. Furthermore, we introduce a custom procedure for dynamic\nselection of in-context learning examples, which is shown to improve LLM-based\njudgments of atypicality and utility. Experimental evaluations show that\nserendipity-based rankings generated by the system are highly correlated with\nground truth rankings for which serendipity scores are computed from manual\nannotations of atypical aspects and their user-dependent utility. Overall, we\nhope that the new recommendation task and the associated system presented in\nthis paper catalyze further research into recommendation approaches that go\nbeyond accuracy in their pursuit of enhanced user satisfaction.\n  The datasets and the code are made publicly available at\nhttps://github.com/ramituncc49er/ATARS .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A restaurant dinner or a hotel stay may lead to memorable experiences when\nguests encounter unexpected aspects that also match their interests. For\nexample, an origami-making station in the waiting area of a restaurant may be\nboth surprising and enjoyable for a customer who is passionate about paper\ncrafts. Similarly, an exhibit of 18th century harpsichords would be atypical\nfor a hotel lobby and likely pique the interest of a guest who has a passion\nfor Baroque music. Motivated by this insight, in this paper we introduce the\nnew task of engineering serendipity through recommendations of items with\natypical aspects. We describe an LLM-based system pipeline that extracts\natypical aspects from item reviews, then estimates and aggregates their\nuser-specific utility in a measure of serendipity potential that is used to\nrerank a list of items recommended to the user. To facilitate system\ndevelopment and evaluation, we introduce a dataset of Yelp reviews that are\nmanually annotated with atypical aspects and a dataset of artificially\ngenerated user profiles, together with crowdsourced annotations of user-aspect\nutility values. Furthermore, we introduce a custom procedure for dynamic\nselection of in-context learning examples, which is shown to improve LLM-based\njudgments of atypicality and utility. Experimental evaluations show that\nserendipity-based rankings generated by the system are highly correlated with\nground truth rankings for which serendipity scores are computed from manual\nannotations of atypical aspects and their user-dependent utility. Overall, we\nhope that the new recommendation task and the associated system presented in\nthis paper catalyze further research into recommendation approaches that go\nbeyond accuracy in their pursuit of enhanced user satisfaction.\n  The datasets and the code are made publicly available at\nhttps://github.com/ramituncc49er/ATARS ."
                },
                "authors": [
                    {
                        "name": "Ramit Aditya"
                    },
                    {
                        "name": "Razvan Bunescu"
                    },
                    {
                        "name": "Smita Nannaware"
                    },
                    {
                        "name": "Erfan Al-Hossami"
                    }
                ],
                "author_detail": {
                    "name": "Erfan Al-Hossami"
                },
                "author": "Erfan Al-Hossami",
                "arxiv_comment": "25 pages of content + references and appendix. arXiv admin note: text\n  overlap with arXiv:2311.02702",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23579v1",
                "updated": "2025-05-29T15:49:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    49,
                    27,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    49,
                    27,
                    3,
                    149,
                    0
                ],
                "title": "BioReason: Incentivizing Multimodal Biological Reasoning within a\n  DNA-LLM Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioReason: Incentivizing Multimodal Biological Reasoning within a\n  DNA-LLM Model"
                },
                "summary": "Unlocking deep, interpretable biological reasoning from complex genomic data\nis a major AI challenge hindering scientific discovery. Current DNA foundation\nmodels, despite strong sequence representation, struggle with multi-step\nreasoning and lack inherent transparent, biologically intuitive explanations.\nWe introduce BioReason, a pioneering architecture that, for the first time,\ndeeply integrates a DNA foundation model with a Large Language Model (LLM).\nThis novel connection enables the LLM to directly process and reason with\ngenomic information as a fundamental input, fostering a new form of multimodal\nbiological understanding. BioReason's sophisticated multi-step reasoning is\ndeveloped through supervised fine-tuning and targeted reinforcement learning,\nguiding the system to generate logical, biologically coherent deductions. On\nbiological reasoning benchmarks including KEGG-based disease pathway prediction\n- where accuracy improves from 88% to 97% - and variant effect prediction,\nBioReason demonstrates an average 15% performance gain over strong\nsingle-modality baselines. BioReason reasons over unseen biological entities\nand articulates decision-making through interpretable, step-by-step biological\ntraces, offering a transformative approach for AI in biology that enables\ndeeper mechanistic insights and accelerates testable hypothesis generation from\ngenomic data. Data, code, and checkpoints are publicly available at\nhttps://github.com/bowang-lab/BioReason",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking deep, interpretable biological reasoning from complex genomic data\nis a major AI challenge hindering scientific discovery. Current DNA foundation\nmodels, despite strong sequence representation, struggle with multi-step\nreasoning and lack inherent transparent, biologically intuitive explanations.\nWe introduce BioReason, a pioneering architecture that, for the first time,\ndeeply integrates a DNA foundation model with a Large Language Model (LLM).\nThis novel connection enables the LLM to directly process and reason with\ngenomic information as a fundamental input, fostering a new form of multimodal\nbiological understanding. BioReason's sophisticated multi-step reasoning is\ndeveloped through supervised fine-tuning and targeted reinforcement learning,\nguiding the system to generate logical, biologically coherent deductions. On\nbiological reasoning benchmarks including KEGG-based disease pathway prediction\n- where accuracy improves from 88% to 97% - and variant effect prediction,\nBioReason demonstrates an average 15% performance gain over strong\nsingle-modality baselines. BioReason reasons over unseen biological entities\nand articulates decision-making through interpretable, step-by-step biological\ntraces, offering a transformative approach for AI in biology that enables\ndeeper mechanistic insights and accelerates testable hypothesis generation from\ngenomic data. Data, code, and checkpoints are publicly available at\nhttps://github.com/bowang-lab/BioReason"
                },
                "authors": [
                    {
                        "name": "Adibvafa Fallahpour"
                    },
                    {
                        "name": "Andrew Magnuson"
                    },
                    {
                        "name": "Purav Gupta"
                    },
                    {
                        "name": "Shihao Ma"
                    },
                    {
                        "name": "Jack Naimer"
                    },
                    {
                        "name": "Arnav Shah"
                    },
                    {
                        "name": "Haonan Duan"
                    },
                    {
                        "name": "Omar Ibrahim"
                    },
                    {
                        "name": "Hani Goodarzi"
                    },
                    {
                        "name": "Chris J. Maddison"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "arxiv_comment": "16 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20088v2",
                "updated": "2025-05-29T15:47:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    47,
                    53,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-26T15:01:56Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    15,
                    1,
                    56,
                    0,
                    146,
                    0
                ],
                "title": "Multi-Domain Explainability of Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Domain Explainability of Preferences"
                },
                "summary": "Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and\nreward models, are central to aligning and evaluating large language models\n(LLMs). Yet, the underlying concepts that drive these preferences remain poorly\nunderstood. In this work, we propose a fully automated method for generating\nlocal and global concept-based explanations of preferences across multiple\ndomains. Our method utilizes an LLM to identify concepts that distinguish\nbetween chosen and rejected responses, and to represent them with concept-based\nvectors. To model the relationships between concepts and preferences, we\npropose a white-box Hierarchical Multi-Domain Regression model that captures\nboth domain-general and domain-specific effects. To evaluate our method, we\ncurate a dataset spanning eight challenging and diverse domains and explain\ntwelve mechanisms. Our method achieves strong preference prediction\nperformance, outperforming baselines while also being explainable.\nAdditionally, we assess explanations in two application-driven settings. First,\nguiding LLM outputs with concepts from LaaJ explanations yields responses that\nthose judges consistently prefer. Second, prompting LaaJs with concepts\nexplaining humans improves their preference predictions. Together, our work\nestablishes a new paradigm for explainability in the era of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and\nreward models, are central to aligning and evaluating large language models\n(LLMs). Yet, the underlying concepts that drive these preferences remain poorly\nunderstood. In this work, we propose a fully automated method for generating\nlocal and global concept-based explanations of preferences across multiple\ndomains. Our method utilizes an LLM to identify concepts that distinguish\nbetween chosen and rejected responses, and to represent them with concept-based\nvectors. To model the relationships between concepts and preferences, we\npropose a white-box Hierarchical Multi-Domain Regression model that captures\nboth domain-general and domain-specific effects. To evaluate our method, we\ncurate a dataset spanning eight challenging and diverse domains and explain\ntwelve mechanisms. Our method achieves strong preference prediction\nperformance, outperforming baselines while also being explainable.\nAdditionally, we assess explanations in two application-driven settings. First,\nguiding LLM outputs with concepts from LaaJ explanations yields responses that\nthose judges consistently prefer. Second, prompting LaaJs with concepts\nexplaining humans improves their preference predictions. Together, our work\nestablishes a new paradigm for explainability in the era of LLMs."
                },
                "authors": [
                    {
                        "name": "Nitay Calderon"
                    },
                    {
                        "name": "Liat Ein-Dor"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23576v1",
                "updated": "2025-05-29T15:47:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    47,
                    49,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:47:49Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    47,
                    49,
                    3,
                    149,
                    0
                ],
                "title": "Cognitive Guardrails for Open-World Decision Making in Autonomous Drone\n  Swarms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Guardrails for Open-World Decision Making in Autonomous Drone\n  Swarms"
                },
                "summary": "Small Uncrewed Aerial Systems (sUAS) are increasingly deployed as autonomous\nswarms in search-and-rescue and other disaster-response scenarios. In these\nsettings, they use computer vision (CV) to detect objects of interest and\nautonomously adapt their missions. However, traditional CV systems often\nstruggle to recognize unfamiliar objects in open-world environments or to infer\ntheir relevance for mission planning. To address this, we incorporate large\nlanguage models (LLMs) to reason about detected objects and their implications.\nWhile LLMs can offer valuable insights, they are also prone to hallucinations\nand may produce incorrect, misleading, or unsafe recommendations. To ensure\nsafe and sensible decision-making under uncertainty, high-level decisions must\nbe governed by cognitive guardrails. This article presents the design,\nsimulation, and real-world integration of these guardrails for sUAS swarms in\nsearch-and-rescue missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Uncrewed Aerial Systems (sUAS) are increasingly deployed as autonomous\nswarms in search-and-rescue and other disaster-response scenarios. In these\nsettings, they use computer vision (CV) to detect objects of interest and\nautonomously adapt their missions. However, traditional CV systems often\nstruggle to recognize unfamiliar objects in open-world environments or to infer\ntheir relevance for mission planning. To address this, we incorporate large\nlanguage models (LLMs) to reason about detected objects and their implications.\nWhile LLMs can offer valuable insights, they are also prone to hallucinations\nand may produce incorrect, misleading, or unsafe recommendations. To ensure\nsafe and sensible decision-making under uncertainty, high-level decisions must\nbe governed by cognitive guardrails. This article presents the design,\nsimulation, and real-world integration of these guardrails for sUAS swarms in\nsearch-and-rescue missions."
                },
                "authors": [
                    {
                        "name": "Jane Cleland-Huang"
                    },
                    {
                        "name": "Pedro Antonio Alarcon Granadeno"
                    },
                    {
                        "name": "Arturo Miguel Russell Bernal"
                    },
                    {
                        "name": "Demetrius Hernandez"
                    },
                    {
                        "name": "Michael Murphy"
                    },
                    {
                        "name": "Maureen Petterson"
                    },
                    {
                        "name": "Walter Scheirer"
                    }
                ],
                "author_detail": {
                    "name": "Walter Scheirer"
                },
                "author": "Walter Scheirer",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23570v1",
                "updated": "2025-05-29T15:44:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    44,
                    36,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:44:36Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    44,
                    36,
                    3,
                    149,
                    0
                ],
                "title": "Evaluating AI capabilities in detecting conspiracy theories on YouTube",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating AI capabilities in detecting conspiracy theories on YouTube"
                },
                "summary": "As a leading online platform with a vast global audience, YouTube's extensive\nreach also makes it susceptible to hosting harmful content, including\ndisinformation and conspiracy theories. This study explores the use of\nopen-weight Large Language Models (LLMs), both text-only and multimodal, for\nidentifying conspiracy theory videos shared on YouTube. Leveraging a labeled\ndataset of thousands of videos, we evaluate a variety of LLMs in a zero-shot\nsetting and compare their performance to a fine-tuned RoBERTa baseline. Results\nshow that text-based LLMs achieve high recall but lower precision, leading to\nincreased false positives. Multimodal models lag behind their text-only\ncounterparts, indicating limited benefits from visual data integration. To\nassess real-world applicability, we evaluate the most accurate models on an\nunlabeled dataset, finding that RoBERTa achieves performance close to LLMs with\na larger number of parameters. Our work highlights the strengths and\nlimitations of current LLM-based approaches for online harmful content\ndetection, emphasizing the need for more precise and robust systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a leading online platform with a vast global audience, YouTube's extensive\nreach also makes it susceptible to hosting harmful content, including\ndisinformation and conspiracy theories. This study explores the use of\nopen-weight Large Language Models (LLMs), both text-only and multimodal, for\nidentifying conspiracy theory videos shared on YouTube. Leveraging a labeled\ndataset of thousands of videos, we evaluate a variety of LLMs in a zero-shot\nsetting and compare their performance to a fine-tuned RoBERTa baseline. Results\nshow that text-based LLMs achieve high recall but lower precision, leading to\nincreased false positives. Multimodal models lag behind their text-only\ncounterparts, indicating limited benefits from visual data integration. To\nassess real-world applicability, we evaluate the most accurate models on an\nunlabeled dataset, finding that RoBERTa achieves performance close to LLMs with\na larger number of parameters. Our work highlights the strengths and\nlimitations of current LLM-based approaches for online harmful content\ndetection, emphasizing the need for more precise and robust systems."
                },
                "authors": [
                    {
                        "name": "Leonardo La Rocca"
                    },
                    {
                        "name": "Francesco Corso"
                    },
                    {
                        "name": "Francesco Pierri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pierri"
                },
                "author": "Francesco Pierri",
                "arxiv_comment": "Submitted for review to OSNEM Special Issue of April 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23569v1",
                "updated": "2025-05-29T15:44:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    44,
                    20,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:44:20Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    44,
                    20,
                    3,
                    149,
                    0
                ],
                "title": "Maximum Likelihood Learning of Latent Dynamics Without Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum Likelihood Learning of Latent Dynamics Without Reconstruction"
                },
                "summary": "We introduce a novel unsupervised learning method for time series data with\nlatent dynamical structure: the recognition-parametrized Gaussian state space\nmodel (RP-GSSM). The RP-GSSM is a probabilistic model that learns Markovian\nGaussian latents explaining statistical dependence between observations at\ndifferent time steps, combining the intuition of contrastive methods with the\nflexible tools of probabilistic generative models. Unlike contrastive\napproaches, the RP-GSSM is a valid probabilistic model learned via maximum\nlikelihood. Unlike generative approaches, the RP-GSSM has no need for an\nexplicit network mapping from latents to observations, allowing it to focus\nmodel capacity on inference of latents. The model is both tractable and\nexpressive: it admits exact inference thanks to its jointly Gaussian latent\nprior, while maintaining expressivity with an arbitrarily nonlinear neural\nnetwork link between observations and latents. These qualities allow the\nRP-GSSM to learn task-relevant latents without ad-hoc regularization, auxiliary\nlosses, or optimizer scheduling. We show how this approach outperforms\nalternatives on problems that include learning nonlinear stochastic dynamics\nfrom video, with or without background distractors. Our results position the\nRP-GSSM as a useful foundation model for a variety of downstream applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel unsupervised learning method for time series data with\nlatent dynamical structure: the recognition-parametrized Gaussian state space\nmodel (RP-GSSM). The RP-GSSM is a probabilistic model that learns Markovian\nGaussian latents explaining statistical dependence between observations at\ndifferent time steps, combining the intuition of contrastive methods with the\nflexible tools of probabilistic generative models. Unlike contrastive\napproaches, the RP-GSSM is a valid probabilistic model learned via maximum\nlikelihood. Unlike generative approaches, the RP-GSSM has no need for an\nexplicit network mapping from latents to observations, allowing it to focus\nmodel capacity on inference of latents. The model is both tractable and\nexpressive: it admits exact inference thanks to its jointly Gaussian latent\nprior, while maintaining expressivity with an arbitrarily nonlinear neural\nnetwork link between observations and latents. These qualities allow the\nRP-GSSM to learn task-relevant latents without ad-hoc regularization, auxiliary\nlosses, or optimizer scheduling. We show how this approach outperforms\nalternatives on problems that include learning nonlinear stochastic dynamics\nfrom video, with or without background distractors. Our results position the\nRP-GSSM as a useful foundation model for a variety of downstream applications."
                },
                "authors": [
                    {
                        "name": "Samo Hromadka"
                    },
                    {
                        "name": "Kai Biegun"
                    },
                    {
                        "name": "Lior Fox"
                    },
                    {
                        "name": "James Heald"
                    },
                    {
                        "name": "Maneesh Sahani"
                    }
                ],
                "author_detail": {
                    "name": "Maneesh Sahani"
                },
                "author": "Maneesh Sahani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23568v1",
                "updated": "2025-05-29T15:41:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    41,
                    51,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:41:51Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    41,
                    51,
                    3,
                    149,
                    0
                ],
                "title": "A Bayesian survival model induced by hurdle zero-modified power series\n  discrete frailty with dispersion: an application in lung cancer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian survival model induced by hurdle zero-modified power series\n  discrete frailty with dispersion: an application in lung cancer"
                },
                "summary": "Frailty survival models are widely used to capture unobserved heterogeneity\namong individuals in clinical and epidemiological research. This paper\nintroduces a Bayesian survival model that features discrete frailty induced by\nthe hurdle zero-modified power series (HZMPS) distribution. A key\ncharacteristic of HZMPS is the inclusion of a dispersion parameter, enhancing\nflexibility in capturing diverse heterogeneity patterns. Furthermore, this\nfrailty specification allows the model to distinguish individuals with higher\nsusceptibility to the event of interest from those potentially cured or no\nlonger at risk. We employ a Bayesian framework for parameter estimation,\nenabling the incorporation of prior information and robust inference, even with\nlimited data. A simulation study is performed to explore the limits of the\nmodel. Our proposal is also applied to a lung cancer study, in which patient\nvariability plays a crucial role in disease progression and treatment response.\nThe findings of this study highlight the importance of more flexible frailty\nmodels in survival data analysis and emphasize the potential of the Bayesian\napproach to modeling heterogeneity in biomedical studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frailty survival models are widely used to capture unobserved heterogeneity\namong individuals in clinical and epidemiological research. This paper\nintroduces a Bayesian survival model that features discrete frailty induced by\nthe hurdle zero-modified power series (HZMPS) distribution. A key\ncharacteristic of HZMPS is the inclusion of a dispersion parameter, enhancing\nflexibility in capturing diverse heterogeneity patterns. Furthermore, this\nfrailty specification allows the model to distinguish individuals with higher\nsusceptibility to the event of interest from those potentially cured or no\nlonger at risk. We employ a Bayesian framework for parameter estimation,\nenabling the incorporation of prior information and robust inference, even with\nlimited data. A simulation study is performed to explore the limits of the\nmodel. Our proposal is also applied to a lung cancer study, in which patient\nvariability plays a crucial role in disease progression and treatment response.\nThe findings of this study highlight the importance of more flexible frailty\nmodels in survival data analysis and emphasize the potential of the Bayesian\napproach to modeling heterogeneity in biomedical studies."
                },
                "authors": [
                    {
                        "name": "Katy C. Molina"
                    },
                    {
                        "name": "Joaquín Martínez-Minaya"
                    },
                    {
                        "name": "Danilo Alvares"
                    },
                    {
                        "name": "Vera D. Tomazella"
                    }
                ],
                "author_detail": {
                    "name": "Vera D. Tomazella"
                },
                "author": "Vera D. Tomazella",
                "arxiv_comment": "13 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12864v2",
                "updated": "2025-05-29T15:37:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    37,
                    57,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-19T08:48:12Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    8,
                    48,
                    12,
                    0,
                    139,
                    0
                ],
                "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams"
                },
                "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/"
                },
                "authors": [
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Jakob Merane"
                    },
                    {
                        "name": "Etienne Salimbeni"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Yoan Hermstrüwer"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Mubashara Akhtar"
                    },
                    {
                        "name": "Florian Geering"
                    },
                    {
                        "name": "Oliver Dreyer"
                    },
                    {
                        "name": "Daniel Brunner"
                    },
                    {
                        "name": "Markus Leippold"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Alexander Stremitzer"
                    },
                    {
                        "name": "Christoph Engel"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Joel Niklaus"
                    }
                ],
                "author_detail": {
                    "name": "Joel Niklaus"
                },
                "author": "Joel Niklaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23561v1",
                "updated": "2025-05-29T15:37:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    37,
                    23,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:37:23Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    37,
                    23,
                    3,
                    149,
                    0
                ],
                "title": "Merge Hijacking: Backdoor Attacks to Model Merging of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merge Hijacking: Backdoor Attacks to Model Merging of Large Language\n  Models"
                },
                "summary": "Model merging for Large Language Models (LLMs) directly fuses the parameters\nof different models finetuned on various tasks, creating a unified model for\nmulti-domain tasks. However, due to potential vulnerabilities in models\navailable on open-source platforms, model merging is susceptible to backdoor\nattacks. In this paper, we propose Merge Hijacking, the first backdoor attack\ntargeting model merging in LLMs. The attacker constructs a malicious upload\nmodel and releases it. Once a victim user merges it with any other models, the\nresulting merged model inherits the backdoor while maintaining utility across\ntasks. Merge Hijacking defines two main objectives-effectiveness and\nutility-and achieves them through four steps. Extensive experiments demonstrate\nthe effectiveness of our attack across different models, merging algorithms,\nand tasks. Additionally, we show that the attack remains effective even when\nmerging real-world models. Moreover, our attack demonstrates robustness against\ntwo inference-time defenses (Paraphrasing and CLEANGEN) and one training-time\ndefense (Fine-pruning).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging for Large Language Models (LLMs) directly fuses the parameters\nof different models finetuned on various tasks, creating a unified model for\nmulti-domain tasks. However, due to potential vulnerabilities in models\navailable on open-source platforms, model merging is susceptible to backdoor\nattacks. In this paper, we propose Merge Hijacking, the first backdoor attack\ntargeting model merging in LLMs. The attacker constructs a malicious upload\nmodel and releases it. Once a victim user merges it with any other models, the\nresulting merged model inherits the backdoor while maintaining utility across\ntasks. Merge Hijacking defines two main objectives-effectiveness and\nutility-and achieves them through four steps. Extensive experiments demonstrate\nthe effectiveness of our attack across different models, merging algorithms,\nand tasks. Additionally, we show that the attack remains effective even when\nmerging real-world models. Moreover, our attack demonstrates robustness against\ntwo inference-time defenses (Paraphrasing and CLEANGEN) and one training-time\ndefense (Fine-pruning)."
                },
                "authors": [
                    {
                        "name": "Zenghui Yuan"
                    },
                    {
                        "name": "Yangming Xu"
                    },
                    {
                        "name": "Jiawen Shi"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "This paper is accepted by ACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23559v1",
                "updated": "2025-05-29T15:35:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    35,
                    58,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:35:58Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    35,
                    58,
                    3,
                    149,
                    0
                ],
                "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents"
                },
                "summary": "Recent advancements in large language model (LLM) agents have significantly\naccelerated scientific discovery automation, yet concurrently raised critical\nethical and safety concerns. To systematically address these challenges, we\nintroduce \\textbf{SafeScientist}, an innovative AI scientist framework\nexplicitly designed to enhance safety and ethical responsibility in AI-driven\nscientific exploration. SafeScientist proactively refuses ethically\ninappropriate or high-risk tasks and rigorously emphasizes safety throughout\nthe research process. To achieve comprehensive safety oversight, we integrate\nmultiple defensive mechanisms, including prompt monitoring, agent-collaboration\nmonitoring, tool-use monitoring, and an ethical reviewer component.\nComplementing SafeScientist, we propose \\textbf{SciSafetyBench}, a novel\nbenchmark specifically designed to evaluate AI safety in scientific contexts,\ncomprising 240 high-risk scientific tasks across 6 domains, alongside 30\nspecially designed scientific tools and 120 tool-related risk tasks. Extensive\nexperiments demonstrate that SafeScientist significantly improves safety\nperformance by 35\\% compared to traditional AI scientist frameworks, without\ncompromising scientific output quality. Additionally, we rigorously validate\nthe robustness of our safety pipeline against diverse adversarial attack\nmethods, further confirming the effectiveness of our integrated approach. The\ncode and data will be available at https://github.com/ulab-uiuc/SafeScientist.\n\\textcolor{red}{Warning: this paper contains example data that may be offensive\nor harmful.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language model (LLM) agents have significantly\naccelerated scientific discovery automation, yet concurrently raised critical\nethical and safety concerns. To systematically address these challenges, we\nintroduce \\textbf{SafeScientist}, an innovative AI scientist framework\nexplicitly designed to enhance safety and ethical responsibility in AI-driven\nscientific exploration. SafeScientist proactively refuses ethically\ninappropriate or high-risk tasks and rigorously emphasizes safety throughout\nthe research process. To achieve comprehensive safety oversight, we integrate\nmultiple defensive mechanisms, including prompt monitoring, agent-collaboration\nmonitoring, tool-use monitoring, and an ethical reviewer component.\nComplementing SafeScientist, we propose \\textbf{SciSafetyBench}, a novel\nbenchmark specifically designed to evaluate AI safety in scientific contexts,\ncomprising 240 high-risk scientific tasks across 6 domains, alongside 30\nspecially designed scientific tools and 120 tool-related risk tasks. Extensive\nexperiments demonstrate that SafeScientist significantly improves safety\nperformance by 35\\% compared to traditional AI scientist frameworks, without\ncompromising scientific output quality. Additionally, we rigorously validate\nthe robustness of our safety pipeline against diverse adversarial attack\nmethods, further confirming the effectiveness of our integrated approach. The\ncode and data will be available at https://github.com/ulab-uiuc/SafeScientist.\n\\textcolor{red}{Warning: this paper contains example data that may be offensive\nor harmful.}"
                },
                "authors": [
                    {
                        "name": "Kunlun Zhu"
                    },
                    {
                        "name": "Jiaxun Zhang"
                    },
                    {
                        "name": "Ziheng Qi"
                    },
                    {
                        "name": "Nuoxing Shang"
                    },
                    {
                        "name": "Zijia Liu"
                    },
                    {
                        "name": "Peixuan Han"
                    },
                    {
                        "name": "Yue Su"
                    },
                    {
                        "name": "Haofei Yu"
                    },
                    {
                        "name": "Jiaxuan You"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan You"
                },
                "author": "Jiaxuan You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23558v2",
                "updated": "2025-05-30T07:37:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    7,
                    37,
                    34,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-29T15:34:15Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    34,
                    15,
                    3,
                    149,
                    0
                ],
                "title": "Qwen Look Again: Guiding Vision-Language Reasoning Models to\n  Re-attention Visual Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qwen Look Again: Guiding Vision-Language Reasoning Models to\n  Re-attention Visual Information"
                },
                "summary": "Inference time scaling drives extended reasoning to enhance the performance\nof Vision-Language Models (VLMs), thus forming powerful Vision-Language\nReasoning Models (VLRMs). However, long reasoning dilutes visual tokens,\ncausing visual information to receive less attention and may trigger\nhallucinations. Although introducing text-only reflection processes shows\npromise in language models, we demonstrate that it is insufficient to suppress\nhallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain\n(Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a\nvision-text reflection process that guides the model to re-attention visual\ninformation during reasoning. We first propose a reinforcement learning method\nBalanced Reflective Policy Optimization (BRPO), which guides the model to\ndecide when to generate vision-text reflection on its own and balance the\nnumber and length of reflections. Then, we formally prove that VLRMs lose\nattention to visual tokens as reasoning progresses, and demonstrate that\nsupplementing visual information during reflection enhances visual attention.\nTherefore, during training and inference, Visual Token COPY and Visual Token\nROUTE are introduced to force the model to re-attention visual information at\nthe visual level, addressing the limitations of text-only reflection.\nExperiments on multiple visual QA datasets and hallucination metrics indicate\nthat Qwen-LA achieves leading accuracy performance while reducing\nhallucinations. Our code is available at: https://github.com/Liar406/Look_Again",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference time scaling drives extended reasoning to enhance the performance\nof Vision-Language Models (VLMs), thus forming powerful Vision-Language\nReasoning Models (VLRMs). However, long reasoning dilutes visual tokens,\ncausing visual information to receive less attention and may trigger\nhallucinations. Although introducing text-only reflection processes shows\npromise in language models, we demonstrate that it is insufficient to suppress\nhallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain\n(Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a\nvision-text reflection process that guides the model to re-attention visual\ninformation during reasoning. We first propose a reinforcement learning method\nBalanced Reflective Policy Optimization (BRPO), which guides the model to\ndecide when to generate vision-text reflection on its own and balance the\nnumber and length of reflections. Then, we formally prove that VLRMs lose\nattention to visual tokens as reasoning progresses, and demonstrate that\nsupplementing visual information during reflection enhances visual attention.\nTherefore, during training and inference, Visual Token COPY and Visual Token\nROUTE are introduced to force the model to re-attention visual information at\nthe visual level, addressing the limitations of text-only reflection.\nExperiments on multiple visual QA datasets and hallucination metrics indicate\nthat Qwen-LA achieves leading accuracy performance while reducing\nhallucinations. Our code is available at: https://github.com/Liar406/Look_Again"
                },
                "authors": [
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Xinrong Chen"
                    },
                    {
                        "name": "Guanyu Wang"
                    },
                    {
                        "name": "Zhijie Tan"
                    },
                    {
                        "name": "Kui Huang"
                    },
                    {
                        "name": "Wenyu Lv"
                    },
                    {
                        "name": "Tong Mo"
                    },
                    {
                        "name": "Weiping Li"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Li"
                },
                "author": "Weiping Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23556v1",
                "updated": "2025-05-29T15:33:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    33,
                    39,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:33:39Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    33,
                    39,
                    3,
                    149,
                    0
                ],
                "title": "Understanding Refusal in Language Models with Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Refusal in Language Models with Sparse Autoencoders"
                },
                "summary": "Refusal is a key safety behavior in aligned language models, yet the internal\nmechanisms driving refusals remain opaque. In this work, we conduct a\nmechanistic study of refusal in instruction-tuned LLMs using sparse\nautoencoders to identify latent features that causally mediate refusal\nbehaviors. We apply our method to two open-source chat models and intervene on\nrefusal-related features to assess their influence on generation, validating\ntheir behavioral impact across multiple harmful datasets. This enables a\nfine-grained inspection of how refusal manifests at the activation level and\naddresses key research questions such as investigating upstream-downstream\nlatent relationship and understanding the mechanisms of adversarial\njailbreaking techniques. We also establish the usefulness of refusal features\nin enhancing generalization for linear probes to out-of-distribution\nadversarial samples in classification tasks. We open source our code in\nhttps://github.com/wj210/refusal_sae.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal is a key safety behavior in aligned language models, yet the internal\nmechanisms driving refusals remain opaque. In this work, we conduct a\nmechanistic study of refusal in instruction-tuned LLMs using sparse\nautoencoders to identify latent features that causally mediate refusal\nbehaviors. We apply our method to two open-source chat models and intervene on\nrefusal-related features to assess their influence on generation, validating\ntheir behavioral impact across multiple harmful datasets. This enables a\nfine-grained inspection of how refusal manifests at the activation level and\naddresses key research questions such as investigating upstream-downstream\nlatent relationship and understanding the mechanisms of adversarial\njailbreaking techniques. We also establish the usefulness of refusal features\nin enhancing generalization for linear probes to out-of-distribution\nadversarial samples in classification tasks. We open source our code in\nhttps://github.com/wj210/refusal_sae."
                },
                "authors": [
                    {
                        "name": "Wei Jie Yeo"
                    },
                    {
                        "name": "Nirmalendu Prakash"
                    },
                    {
                        "name": "Clement Neo"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Ranjan Satapathy"
                    }
                ],
                "author_detail": {
                    "name": "Ranjan Satapathy"
                },
                "author": "Ranjan Satapathy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04629v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04629v4",
                "updated": "2025-05-29T15:33:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    33,
                    8,
                    3,
                    149,
                    0
                ],
                "published": "2024-12-05T21:51:05Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    21,
                    51,
                    5,
                    3,
                    340,
                    0
                ],
                "title": "Argumentative Experience: Reducing Confirmation Bias on Controversial\n  Issues through LLM-Generated Multi-Persona Debates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argumentative Experience: Reducing Confirmation Bias on Controversial\n  Issues through LLM-Generated Multi-Persona Debates"
                },
                "summary": "Large language models (LLMs) are enabling designers to give life to exciting\nnew user experiences for information access. In this work, we present a system\nthat generates LLM personas to debate a topic of interest from different\nperspectives. How might information seekers use and benefit from such a system?\nCan centering information access around diverse viewpoints help to mitigate\nthorny challenges like confirmation bias in which information seekers\nover-trust search results matching existing beliefs? How do potential biases\nand hallucinations in LLMs play out alongside human users who are also fallible\nand possibly biased?\n  Our study exposes participants to multiple viewpoints on controversial issues\nvia a mixed-methods, within-subjects study. We use eye-tracking metrics to\nquantitatively assess cognitive engagement alongside qualitative feedback.\nCompared to a baseline search system, we see more creative interactions and\ndiverse information-seeking with our multi-persona debate system, which more\neffectively reduces user confirmation bias and conviction toward their initial\nbeliefs. Overall, our study contributes to the emerging design space of\nLLM-based information access systems, specifically investigating the potential\nof simulated personas to promote greater exposure to information diversity,\nemulate collective intelligence, and mitigate bias in information seeking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are enabling designers to give life to exciting\nnew user experiences for information access. In this work, we present a system\nthat generates LLM personas to debate a topic of interest from different\nperspectives. How might information seekers use and benefit from such a system?\nCan centering information access around diverse viewpoints help to mitigate\nthorny challenges like confirmation bias in which information seekers\nover-trust search results matching existing beliefs? How do potential biases\nand hallucinations in LLMs play out alongside human users who are also fallible\nand possibly biased?\n  Our study exposes participants to multiple viewpoints on controversial issues\nvia a mixed-methods, within-subjects study. We use eye-tracking metrics to\nquantitatively assess cognitive engagement alongside qualitative feedback.\nCompared to a baseline search system, we see more creative interactions and\ndiverse information-seeking with our multi-persona debate system, which more\neffectively reduces user confirmation bias and conviction toward their initial\nbeliefs. Overall, our study contributes to the emerging design space of\nLLM-based information access systems, specifically investigating the potential\nof simulated personas to promote greater exposure to information diversity,\nemulate collective intelligence, and mitigate bias in information seeking."
                },
                "authors": [
                    {
                        "name": "Li Shi"
                    },
                    {
                        "name": "Houjiang Liu"
                    },
                    {
                        "name": "Yian Wong"
                    },
                    {
                        "name": "Utkarsh Mujumdar"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Jacek Gwizdka"
                    },
                    {
                        "name": "Matthew Lease"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Lease"
                },
                "author": "Matthew Lease",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04629v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04629v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23555v1",
                "updated": "2025-05-29T15:31:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    31,
                    37,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:31:37Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    31,
                    37,
                    3,
                    149,
                    0
                ],
                "title": "Adaptive Federated LoRA in Heterogeneous Wireless Networks with\n  Independent Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Federated LoRA in Heterogeneous Wireless Networks with\n  Independent Sampling"
                },
                "summary": "Federated LoRA has emerged as a promising technique for efficiently\nfine-tuning large language models (LLMs) on distributed devices by reducing the\nnumber of trainable parameters. However, existing approaches often inadequately\noverlook the theoretical and practical implications of system and data\nheterogeneity, thereby failing to optimize the overall training efficiency,\nparticularly in terms of wall-clock time. In this paper, we propose an adaptive\nfederated LoRA strategy with independent client sampling to minimize the\nconvergence wall-clock time of federated fine-tuning under both computation and\ncommunication heterogeneity. We first derive a new convergence bound for\nfederated LoRA with arbitrary and independent client sampling, notably without\nrequiring the stringent bounded gradient assumption. Then, we introduce an\nadaptive bandwidth allocation scheme that accounts for heterogeneous client\nresources and system bandwidth constraints. Based on the derived theory, we\nformulate and solve a non-convex optimization problem to jointly determine the\nLoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock\nconvergence time. An efficient and low-complexity algorithm is developed to\napproximate the solution. Finally, extensive experiments demonstrate that our\napproach significantly reduces wall-clock training time compared to\nstate-of-the-art methods across various models and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated LoRA has emerged as a promising technique for efficiently\nfine-tuning large language models (LLMs) on distributed devices by reducing the\nnumber of trainable parameters. However, existing approaches often inadequately\noverlook the theoretical and practical implications of system and data\nheterogeneity, thereby failing to optimize the overall training efficiency,\nparticularly in terms of wall-clock time. In this paper, we propose an adaptive\nfederated LoRA strategy with independent client sampling to minimize the\nconvergence wall-clock time of federated fine-tuning under both computation and\ncommunication heterogeneity. We first derive a new convergence bound for\nfederated LoRA with arbitrary and independent client sampling, notably without\nrequiring the stringent bounded gradient assumption. Then, we introduce an\nadaptive bandwidth allocation scheme that accounts for heterogeneous client\nresources and system bandwidth constraints. Based on the derived theory, we\nformulate and solve a non-convex optimization problem to jointly determine the\nLoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock\nconvergence time. An efficient and low-complexity algorithm is developed to\napproximate the solution. Finally, extensive experiments demonstrate that our\napproach significantly reduces wall-clock training time compared to\nstate-of-the-art methods across various models and datasets."
                },
                "authors": [
                    {
                        "name": "Yanzhao Hou"
                    },
                    {
                        "name": "Jiaxiang Geng"
                    },
                    {
                        "name": "Boyu Li"
                    },
                    {
                        "name": "Xiaofeng Tao"
                    },
                    {
                        "name": "Juncheng Wang"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Bing Luo"
                    }
                ],
                "author_detail": {
                    "name": "Bing Luo"
                },
                "author": "Bing Luo",
                "arxiv_comment": "13 pages, Submitted to IEEE Journal on Selected Areas in\n  Communications (JSAC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23554v1",
                "updated": "2025-05-29T15:31:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    31,
                    28,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:31:28Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    31,
                    28,
                    3,
                    149,
                    0
                ],
                "title": "Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in\n  Geo-Distributed Cloud Datacenters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in\n  Geo-Distributed Cloud Datacenters"
                },
                "summary": "In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and\nGemini have been widely adopted in different areas. As the use of LLMs\ncontinues to grow, many efforts have focused on reducing the massive training\noverheads of these models. But it is the environmental impact of handling user\nrequests to LLMs that is increasingly becoming a concern. Recent studies\nestimate that the costs of operating LLMs in their inference phase can exceed\ntraining costs by 25x per year. As LLMs are queried incessantly, the cumulative\ncarbon footprint for the operational phase has been shown to far exceed the\nfootprint during the training phase. Further, estimates indicate that 500 ml of\nfresh water is expended for every 20-50 requests to LLMs during inference. To\naddress these important sustainability issues with LLMs, we propose a novel\nframework called SLIT to co-optimize LLM quality of service (time-to-first\ntoken), carbon emissions, water usage, and energy costs. The framework utilizes\na machine learning (ML) based metaheuristic to enhance the sustainability of\nLLM hosting across geo-distributed cloud datacenters. Such a framework will\nbecome increasingly vital as LLMs proliferate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and\nGemini have been widely adopted in different areas. As the use of LLMs\ncontinues to grow, many efforts have focused on reducing the massive training\noverheads of these models. But it is the environmental impact of handling user\nrequests to LLMs that is increasingly becoming a concern. Recent studies\nestimate that the costs of operating LLMs in their inference phase can exceed\ntraining costs by 25x per year. As LLMs are queried incessantly, the cumulative\ncarbon footprint for the operational phase has been shown to far exceed the\nfootprint during the training phase. Further, estimates indicate that 500 ml of\nfresh water is expended for every 20-50 requests to LLMs during inference. To\naddress these important sustainability issues with LLMs, we propose a novel\nframework called SLIT to co-optimize LLM quality of service (time-to-first\ntoken), carbon emissions, water usage, and energy costs. The framework utilizes\na machine learning (ML) based metaheuristic to enhance the sustainability of\nLLM hosting across geo-distributed cloud datacenters. Such a framework will\nbecome increasingly vital as LLMs proliferate."
                },
                "authors": [
                    {
                        "name": "Hayden Moore"
                    },
                    {
                        "name": "Sirui Qi"
                    },
                    {
                        "name": "Ninad Hogade"
                    },
                    {
                        "name": "Dejan Milojicic"
                    },
                    {
                        "name": "Cullen Bash"
                    },
                    {
                        "name": "Sudeep Pasricha"
                    }
                ],
                "author_detail": {
                    "name": "Sudeep Pasricha"
                },
                "author": "Sudeep Pasricha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23553v1",
                "updated": "2025-05-29T15:31:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    31,
                    13,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:31:13Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    31,
                    13,
                    3,
                    149,
                    0
                ],
                "title": "A Unified Framework for Mapping and Synthesis of Approximate R-Blocks\n  CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Mapping and Synthesis of Approximate R-Blocks\n  CGRAs"
                },
                "summary": "The ever-increasing complexity and operational diversity of modern Neural\nNetworks (NNs) have caused the need for low-power and, at the same time,\nhigh-performance edge devices for AI applications. Coarse Grained\nReconfigurable Architectures (CGRAs) form a promising design paradigm to\naddress these challenges, delivering a close-to-ASIC performance while allowing\nfor hardware programmability. In this paper, we introduce a novel end-to-end\nexploration and synthesis framework for approximate CGRA processors that\nenables transparent and optimized integration and mapping of state-of-the-art\napproximate multiplication components into CGRAs. Our methodology introduces a\nper-channel exploration strategy that maps specific output features onto\napproximate components based on accuracy degradation constraints. This enables\nthe optimization of the system's energy consumption while retaining the\naccuracy above a certain threshold. At the circuit level, the integration of\napproximate components enables the creation of voltage islands that operate at\nreduced voltage levels, which is attributed to their inherently shorter\ncritical paths. This key enabler allows us to effectively reduce the overall\npower consumption by an average of 30% across our analyzed architectures,\ncompared to their baseline counterparts, while incurring only a minimal 2% area\noverhead. The proposed methodology was evaluated on a widely used NN model,\nMobileNetV2, on the ImageNet dataset, demonstrating that the generated\narchitectures can deliver up to 440 GOPS/W with relatively small output error\nduring inference, outperforming several State-of-the-Art CGRA architectures in\nterms of throughput and energy efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-increasing complexity and operational diversity of modern Neural\nNetworks (NNs) have caused the need for low-power and, at the same time,\nhigh-performance edge devices for AI applications. Coarse Grained\nReconfigurable Architectures (CGRAs) form a promising design paradigm to\naddress these challenges, delivering a close-to-ASIC performance while allowing\nfor hardware programmability. In this paper, we introduce a novel end-to-end\nexploration and synthesis framework for approximate CGRA processors that\nenables transparent and optimized integration and mapping of state-of-the-art\napproximate multiplication components into CGRAs. Our methodology introduces a\nper-channel exploration strategy that maps specific output features onto\napproximate components based on accuracy degradation constraints. This enables\nthe optimization of the system's energy consumption while retaining the\naccuracy above a certain threshold. At the circuit level, the integration of\napproximate components enables the creation of voltage islands that operate at\nreduced voltage levels, which is attributed to their inherently shorter\ncritical paths. This key enabler allows us to effectively reduce the overall\npower consumption by an average of 30% across our analyzed architectures,\ncompared to their baseline counterparts, while incurring only a minimal 2% area\noverhead. The proposed methodology was evaluated on a widely used NN model,\nMobileNetV2, on the ImageNet dataset, demonstrating that the generated\narchitectures can deliver up to 440 GOPS/W with relatively small output error\nduring inference, outperforming several State-of-the-Art CGRA architectures in\nterms of throughput and energy efficiency."
                },
                "authors": [
                    {
                        "name": "Georgios Alexandris"
                    },
                    {
                        "name": "Panagiotis Chaidos"
                    },
                    {
                        "name": "Alexis Maras"
                    },
                    {
                        "name": "Barry de Bruin"
                    },
                    {
                        "name": "Manil Dev Gomony"
                    },
                    {
                        "name": "Henk Corporaal"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    },
                    {
                        "name": "Sotirios Xydis"
                    }
                ],
                "author_detail": {
                    "name": "Sotirios Xydis"
                },
                "author": "Sotirios Xydis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23549v1",
                "updated": "2025-05-29T15:27:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    27,
                    52,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:27:52Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    27,
                    52,
                    3,
                    149,
                    0
                ],
                "title": "LLM-based Property-based Test Generation for Guardrailing Cyber-Physical\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Property-based Test Generation for Guardrailing Cyber-Physical\n  Systems"
                },
                "summary": "Cyber-physical systems (CPSs) are complex systems that integrate physical,\ncomputational, and communication subsystems. The heterogeneous nature of these\nsystems makes their safety assurance challenging. In this paper, we propose a\nnovel automated approach for guardrailing cyber-physical systems using\nproperty-based tests (PBTs) generated by Large Language Models (LLMs). Our\napproach employs an LLM to extract properties from the code and documentation\nof CPSs. Next, we use the LLM to generate PBTs that verify the extracted\nproperties on the CPS. The generated PBTs have two uses. First, they are used\nto test the CPS before it is deployed, i.e., at design time. Secondly, these\nPBTs can be used after deployment, i.e., at run time, to monitor the behavior\nof the system and guardrail it against unsafe states. We implement our approach\nin ChekProp and conduct preliminary experiments to evaluate the generated PBTs\nin terms of their relevance (how well they match manually crafted properties),\nexecutability (how many run with minimal manual modification), and\neffectiveness (coverage of the input space partitions). The results of our\nexperiments and evaluation demonstrate a promising path forward for creating\nguardrails for CPSs using LLM-generated property-based tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber-physical systems (CPSs) are complex systems that integrate physical,\ncomputational, and communication subsystems. The heterogeneous nature of these\nsystems makes their safety assurance challenging. In this paper, we propose a\nnovel automated approach for guardrailing cyber-physical systems using\nproperty-based tests (PBTs) generated by Large Language Models (LLMs). Our\napproach employs an LLM to extract properties from the code and documentation\nof CPSs. Next, we use the LLM to generate PBTs that verify the extracted\nproperties on the CPS. The generated PBTs have two uses. First, they are used\nto test the CPS before it is deployed, i.e., at design time. Secondly, these\nPBTs can be used after deployment, i.e., at run time, to monitor the behavior\nof the system and guardrail it against unsafe states. We implement our approach\nin ChekProp and conduct preliminary experiments to evaluate the generated PBTs\nin terms of their relevance (how well they match manually crafted properties),\nexecutability (how many run with minimal manual modification), and\neffectiveness (coverage of the input space partitions). The results of our\nexperiments and evaluation demonstrate a promising path forward for creating\nguardrails for CPSs using LLM-generated property-based tests."
                },
                "authors": [
                    {
                        "name": "Khashayar Etemadi"
                    },
                    {
                        "name": "Marjan Sirjani"
                    },
                    {
                        "name": "Mahshid Helali Moghadam"
                    },
                    {
                        "name": "Per Strandberg"
                    },
                    {
                        "name": "Paul Pettersson"
                    }
                ],
                "author_detail": {
                    "name": "Paul Pettersson"
                },
                "author": "Paul Pettersson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08319v2",
                "updated": "2025-05-29T15:26:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    26,
                    6,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-14T18:53:00Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    53,
                    0,
                    1,
                    14,
                    0
                ],
                "title": "Enhancing Automated Interpretability with Output-Centric Feature\n  Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Automated Interpretability with Output-Centric Feature\n  Descriptions"
                },
                "summary": "Automated interpretability pipelines generate natural language descriptions\nfor the concepts represented by features in large language models (LLMs), such\nas plants or the first word in a sentence. These descriptions are derived using\ninputs that activate the feature, which may be a dimension or a direction in\nthe model's representation space. However, identifying activating inputs is\ncostly, and the mechanistic role of a feature in model behavior is determined\nboth by how inputs cause a feature to activate and by how feature activation\naffects outputs. Using steering evaluations, we reveal that current pipelines\nprovide descriptions that fail to capture the causal effect of the feature on\noutputs. To fix this, we propose efficient, output-centric methods for\nautomatically generating feature descriptions. These methods use the tokens\nweighted higher after feature stimulation or the highest weight tokens after\napplying the vocabulary \"unembedding\" head directly to the feature. Our\noutput-centric descriptions better capture the causal effect of a feature on\nmodel outputs than input-centric descriptions, but combining the two leads to\nthe best performance on both input and output evaluations. Lastly, we show that\noutput-centric descriptions can be used to find inputs that activate features\npreviously thought to be \"dead\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated interpretability pipelines generate natural language descriptions\nfor the concepts represented by features in large language models (LLMs), such\nas plants or the first word in a sentence. These descriptions are derived using\ninputs that activate the feature, which may be a dimension or a direction in\nthe model's representation space. However, identifying activating inputs is\ncostly, and the mechanistic role of a feature in model behavior is determined\nboth by how inputs cause a feature to activate and by how feature activation\naffects outputs. Using steering evaluations, we reveal that current pipelines\nprovide descriptions that fail to capture the causal effect of the feature on\noutputs. To fix this, we propose efficient, output-centric methods for\nautomatically generating feature descriptions. These methods use the tokens\nweighted higher after feature stimulation or the highest weight tokens after\napplying the vocabulary \"unembedding\" head directly to the feature. Our\noutput-centric descriptions better capture the causal effect of a feature on\nmodel outputs than input-centric descriptions, but combining the two leads to\nthe best performance on both input and output evaluations. Lastly, we show that\noutput-centric descriptions can be used to find inputs that activate features\npreviously thought to be \"dead\"."
                },
                "authors": [
                    {
                        "name": "Yoav Gur-Arieh"
                    },
                    {
                        "name": "Roy Mayan"
                    },
                    {
                        "name": "Chen Agassy"
                    },
                    {
                        "name": "Atticus Geiger"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "arxiv_comment": "Accepted to ACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23548v1",
                "updated": "2025-05-29T15:26:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    26,
                    4,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:26:04Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    26,
                    4,
                    3,
                    149,
                    0
                ],
                "title": "Translation in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translation in the Wild"
                },
                "summary": "Large Language Models (LLMs) excel in translation among other things,\ndemonstrating competitive performance for many language pairs in zero- and\nfew-shot settings. But unlike dedicated neural machine translation models, LLMs\nare not trained on any translation-related objective. What explains their\nremarkable translation abilities? Are these abilities grounded in \"incidental\nbilingualism\" (Briakou et al. 2023) in training data? Does instruction tuning\ncontribute to it? Are LLMs capable of aligning and leveraging semantically\nidentical or similar monolingual contents from different corners of the\ninternet that are unlikely to fit in a single context window? I offer some\nreflections on this topic, informed by recent studies and growing user\nexperience. My working hypothesis is that LLMs' translation abilities originate\nin two different types of pre-training data that may be internalized by the\nmodels in different ways. I discuss the prospects for testing the \"duality\"\nhypothesis empirically and its implications for reconceptualizing translation,\nhuman and machine, in the age of deep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in translation among other things,\ndemonstrating competitive performance for many language pairs in zero- and\nfew-shot settings. But unlike dedicated neural machine translation models, LLMs\nare not trained on any translation-related objective. What explains their\nremarkable translation abilities? Are these abilities grounded in \"incidental\nbilingualism\" (Briakou et al. 2023) in training data? Does instruction tuning\ncontribute to it? Are LLMs capable of aligning and leveraging semantically\nidentical or similar monolingual contents from different corners of the\ninternet that are unlikely to fit in a single context window? I offer some\nreflections on this topic, informed by recent studies and growing user\nexperience. My working hypothesis is that LLMs' translation abilities originate\nin two different types of pre-training data that may be internalized by the\nmodels in different ways. I discuss the prospects for testing the \"duality\"\nhypothesis empirically and its implications for reconceptualizing translation,\nhuman and machine, in the age of deep learning."
                },
                "authors": [
                    {
                        "name": "Yuri Balashov"
                    }
                ],
                "author_detail": {
                    "name": "Yuri Balashov"
                },
                "author": "Yuri Balashov",
                "arxiv_comment": "4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23546v1",
                "updated": "2025-05-29T15:24:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    24,
                    23,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:24:23Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    24,
                    23,
                    3,
                    149,
                    0
                ],
                "title": "Going from a Representative Agent to Counterfactuals in Combinatorial\n  Choice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Going from a Representative Agent to Counterfactuals in Combinatorial\n  Choice"
                },
                "summary": "We study decision-making problems where data comprises points from a\ncollection of binary polytopes, capturing aggregate information stemming from\nvarious combinatorial selection environments. We propose a nonparametric\napproach for counterfactual inference in this setting based on a representative\nagent model, where the available data is viewed as arising from maximizing\nseparable concave utility functions over the respective binary polytopes. Our\nfirst contribution is to precisely characterize the selection probabilities\nrepresentable under this model and show that verifying the consistency of any\ngiven aggregated selection dataset reduces to solving a polynomial-sized linear\nprogram. Building on this characterization, we develop a nonparametric method\nfor counterfactual prediction. When data is inconsistent with the model,\nfinding a best-fitting approximation for prediction reduces to solving a\ncompact mixed-integer convex program. Numerical experiments based on synthetic\ndata demonstrate the method's flexibility, predictive accuracy, and strong\nrepresentational power even under model misspecification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study decision-making problems where data comprises points from a\ncollection of binary polytopes, capturing aggregate information stemming from\nvarious combinatorial selection environments. We propose a nonparametric\napproach for counterfactual inference in this setting based on a representative\nagent model, where the available data is viewed as arising from maximizing\nseparable concave utility functions over the respective binary polytopes. Our\nfirst contribution is to precisely characterize the selection probabilities\nrepresentable under this model and show that verifying the consistency of any\ngiven aggregated selection dataset reduces to solving a polynomial-sized linear\nprogram. Building on this characterization, we develop a nonparametric method\nfor counterfactual prediction. When data is inconsistent with the model,\nfinding a best-fitting approximation for prediction reduces to solving a\ncompact mixed-integer convex program. Numerical experiments based on synthetic\ndata demonstrate the method's flexibility, predictive accuracy, and strong\nrepresentational power even under model misspecification."
                },
                "authors": [
                    {
                        "name": "Yanqiu Ruan"
                    },
                    {
                        "name": "Karthyek Murthy"
                    },
                    {
                        "name": "Karthik Natarajan"
                    }
                ],
                "author_detail": {
                    "name": "Karthik Natarajan"
                },
                "author": "Karthik Natarajan",
                "arxiv_comment": "22 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23543v1",
                "updated": "2025-05-29T15:22:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    22,
                    18,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:22:18Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    22,
                    18,
                    3,
                    149,
                    0
                ],
                "title": "Position Paper: Metadata Enrichment Model: Integrating Neural Networks\n  and Semantic Knowledge Graphs for Cultural Heritage Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position Paper: Metadata Enrichment Model: Integrating Neural Networks\n  and Semantic Knowledge Graphs for Cultural Heritage Applications"
                },
                "summary": "The digitization of cultural heritage collections has opened new directions\nfor research, yet the lack of enriched metadata poses a substantial challenge\nto accessibility, interoperability, and cross-institutional collaboration. In\nseveral past years neural networks models such as YOLOv11 and Detectron2 have\nrevolutionized visual data analysis, but their application to domain-specific\ncultural artifacts - such as manuscripts and incunabula - remains limited by\nthe absence of methodologies that address structural feature extraction and\nsemantic interoperability. In this position paper, we argue, that the\nintegration of neural networks with semantic technologies represents a paradigm\nshift in cultural heritage digitization processes. We present the Metadata\nEnrichment Model (MEM), a conceptual framework designed to enrich metadata for\ndigitized collections by combining fine-tuned computer vision models, large\nlanguage models (LLMs) and structured knowledge graphs. The Multilayer Vision\nMechanism (MVM) appears as the key innovation of MEM. This iterative process\nimproves visual analysis by dynamically detecting nested features, such as text\nwithin seals or images within stamps. To expose MEM's potential, we apply it to\na dataset of digitized incunabula from the Jagiellonian Digital Library and\nrelease a manually annotated dataset of 105 manuscript pages. We examine the\npractical challenges of MEM's usage in real-world GLAM institutions, including\nthe need for domain-specific fine-tuning, the adjustment of enriched metadata\nwith Linked Data standards and computational costs. We present MEM as a\nflexible and extensible methodology. This paper contributes to the discussion\non how artificial intelligence and semantic web technologies can advance\ncultural heritage research, and also use these technologies in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The digitization of cultural heritage collections has opened new directions\nfor research, yet the lack of enriched metadata poses a substantial challenge\nto accessibility, interoperability, and cross-institutional collaboration. In\nseveral past years neural networks models such as YOLOv11 and Detectron2 have\nrevolutionized visual data analysis, but their application to domain-specific\ncultural artifacts - such as manuscripts and incunabula - remains limited by\nthe absence of methodologies that address structural feature extraction and\nsemantic interoperability. In this position paper, we argue, that the\nintegration of neural networks with semantic technologies represents a paradigm\nshift in cultural heritage digitization processes. We present the Metadata\nEnrichment Model (MEM), a conceptual framework designed to enrich metadata for\ndigitized collections by combining fine-tuned computer vision models, large\nlanguage models (LLMs) and structured knowledge graphs. The Multilayer Vision\nMechanism (MVM) appears as the key innovation of MEM. This iterative process\nimproves visual analysis by dynamically detecting nested features, such as text\nwithin seals or images within stamps. To expose MEM's potential, we apply it to\na dataset of digitized incunabula from the Jagiellonian Digital Library and\nrelease a manually annotated dataset of 105 manuscript pages. We examine the\npractical challenges of MEM's usage in real-world GLAM institutions, including\nthe need for domain-specific fine-tuning, the adjustment of enriched metadata\nwith Linked Data standards and computational costs. We present MEM as a\nflexible and extensible methodology. This paper contributes to the discussion\non how artificial intelligence and semantic web technologies can advance\ncultural heritage research, and also use these technologies in practice."
                },
                "authors": [
                    {
                        "name": "Jan Ignatowicz"
                    },
                    {
                        "name": "Krzysztof Kutt"
                    },
                    {
                        "name": "Grzegorz J. Nalepa"
                    }
                ],
                "author_detail": {
                    "name": "Grzegorz J. Nalepa"
                },
                "author": "Grzegorz J. Nalepa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11077v2",
                "updated": "2025-05-29T15:22:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    22,
                    9,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-16T10:08:25Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    10,
                    8,
                    25,
                    4,
                    136,
                    0
                ],
                "title": "LLM-Enhanced Symbolic Control for Safety-Critical Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced Symbolic Control for Safety-Critical Applications"
                },
                "summary": "Motivated by Smart Manufacturing and Industry 4.0, we introduce a framework\nfor synthesizing Abstraction-Based Controller Design (ABCD) for reach-avoid\nproblems from Natural Language (NL) specifications using Large Language Models\n(LLMs). A Code Agent interprets an NL description of the control problem and\ntranslates it into a formal language interpretable by state-of-the-art symbolic\ncontrol software, while a Checker Agent verifies the correctness of the\ngenerated code and enhances safety by identifying specification mismatches.\nEvaluations show that the system handles linguistic variability and improves\nrobustness over direct planning with LLMs. The proposed approach lowers the\nbarrier to formal control synthesis by enabling intuitive, NL-based task\ndefinition while maintaining safety guarantees through automated validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by Smart Manufacturing and Industry 4.0, we introduce a framework\nfor synthesizing Abstraction-Based Controller Design (ABCD) for reach-avoid\nproblems from Natural Language (NL) specifications using Large Language Models\n(LLMs). A Code Agent interprets an NL description of the control problem and\ntranslates it into a formal language interpretable by state-of-the-art symbolic\ncontrol software, while a Checker Agent verifies the correctness of the\ngenerated code and enhances safety by identifying specification mismatches.\nEvaluations show that the system handles linguistic variability and improves\nrobustness over direct planning with LLMs. The proposed approach lowers the\nbarrier to formal control synthesis by enabling intuitive, NL-based task\ndefinition while maintaining safety guarantees through automated validation."
                },
                "authors": [
                    {
                        "name": "Amir Bayat"
                    },
                    {
                        "name": "Alessandro Abate"
                    },
                    {
                        "name": "Necmiye Ozay"
                    },
                    {
                        "name": "Raphael M. Jungers"
                    }
                ],
                "author_detail": {
                    "name": "Raphael M. Jungers"
                },
                "author": "Raphael M. Jungers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23542v1",
                "updated": "2025-05-29T15:21:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    21,
                    10,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:21:10Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    21,
                    10,
                    3,
                    149,
                    0
                ],
                "title": "A Gibbs Sampler for Efficient Bayesian Inference in Sign-Identified\n  SVARs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Gibbs Sampler for Efficient Bayesian Inference in Sign-Identified\n  SVARs"
                },
                "summary": "We develop a new algorithm for inference based on SVARs identified with sign\nrestrictions. The key insight of our algorithm is to break apart from the\naccept-reject tradition associated with sign-identified SVARs. We show that\nembedding an elliptical slice sampling within a Gibbs sampler approach can\ndeliver dramatic gains in speed and turn previously infeasible applications\ninto feasible ones. We provide a tractable example to illustrate the power of\nthe elliptical slice sampling applied to sign-identified SVARs. We demonstrate\nthe usefulness of our algorithm by applying it to a well-known small-SVAR model\nof the oil market featuring a tight identified set as well as to large SVAR\nmodel with more than 100 sign restrictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a new algorithm for inference based on SVARs identified with sign\nrestrictions. The key insight of our algorithm is to break apart from the\naccept-reject tradition associated with sign-identified SVARs. We show that\nembedding an elliptical slice sampling within a Gibbs sampler approach can\ndeliver dramatic gains in speed and turn previously infeasible applications\ninto feasible ones. We provide a tractable example to illustrate the power of\nthe elliptical slice sampling applied to sign-identified SVARs. We demonstrate\nthe usefulness of our algorithm by applying it to a well-known small-SVAR model\nof the oil market featuring a tight identified set as well as to large SVAR\nmodel with more than 100 sign restrictions."
                },
                "authors": [
                    {
                        "name": "Jonas E. Arias"
                    },
                    {
                        "name": "Juan F. Rubio-Ramírez"
                    },
                    {
                        "name": "Minchul Shin"
                    }
                ],
                "author_detail": {
                    "name": "Minchul Shin"
                },
                "author": "Minchul Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23540v1",
                "updated": "2025-05-29T15:20:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    20,
                    44,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:20:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    20,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "Probability-Consistent Preference Optimization for Enhanced LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probability-Consistent Preference Optimization for Enhanced LLM\n  Reasoning"
                },
                "summary": "Recent advances in preference optimization have demonstrated significant\npotential for improving mathematical reasoning capabilities in large language\nmodels (LLMs). While current approaches leverage high-quality pairwise\npreference data through outcome-based criteria like answer correctness or\nconsistency, they fundamentally neglect the internal logical coherence of\nresponses. To overcome this, we propose Probability-Consistent Preference\nOptimization (PCPO), a novel framework that establishes dual quantitative\nmetrics for preference selection: (1) surface-level answer correctness and (2)\nintrinsic token-level probability consistency across responses. Extensive\nexperiments show that our PCPO consistently outperforms existing outcome-only\ncriterion approaches across a diverse range of LLMs and benchmarks. Our code is\npublicly available at https://github.com/YunqiaoYang/PCPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in preference optimization have demonstrated significant\npotential for improving mathematical reasoning capabilities in large language\nmodels (LLMs). While current approaches leverage high-quality pairwise\npreference data through outcome-based criteria like answer correctness or\nconsistency, they fundamentally neglect the internal logical coherence of\nresponses. To overcome this, we propose Probability-Consistent Preference\nOptimization (PCPO), a novel framework that establishes dual quantitative\nmetrics for preference selection: (1) surface-level answer correctness and (2)\nintrinsic token-level probability consistency across responses. Extensive\nexperiments show that our PCPO consistently outperforms existing outcome-only\ncriterion approaches across a diverse range of LLMs and benchmarks. Our code is\npublicly available at https://github.com/YunqiaoYang/PCPO."
                },
                "authors": [
                    {
                        "name": "Yunqiao Yang"
                    },
                    {
                        "name": "Houxing Ren"
                    },
                    {
                        "name": "Zimu Lu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Weikang Shi"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Mingjie Zhan"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "14 pages, to be published in ACL 2025 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01662v2",
                "updated": "2025-05-29T15:20:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    20,
                    23,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-01T05:22:11Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    5,
                    22,
                    11,
                    5,
                    32,
                    0
                ],
                "title": "Fast Large Language Model Collaborative Decoding via Speculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Large Language Model Collaborative Decoding via Speculation"
                },
                "summary": "Large Language Model (LLM) collaborative decoding techniques improve output\nquality by combining the outputs of multiple models at each generation step,\nbut they incur high computational costs. In this paper, we introduce\nCollaborative decoding via Speculation (CoS), a novel framework that\naccelerates collaborative decoding without compromising performance. Inspired\nby Speculative Decoding--where a small proposal model generates tokens\nsequentially, and a larger target model verifies them in parallel, our approach\nbuilds on two key insights: (1) the verification distribution can be the\ncombined distribution of both the proposal and target models, and (2)\nalternating each model as the proposer and verifier can further enhance\nefficiency. We generalize this method to collaboration among n models and\ntheoretically prove that CoS is never slower than standard collaborative\ndecoding, typically achieving faster speed. Extensive experiments demonstrate\nCoS is 1.11x-2.23x faster than standard collaborative decoding without\ncompromising generation quality. Our code is available at\nhttps://github.com/Kamichanw/CoS/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) collaborative decoding techniques improve output\nquality by combining the outputs of multiple models at each generation step,\nbut they incur high computational costs. In this paper, we introduce\nCollaborative decoding via Speculation (CoS), a novel framework that\naccelerates collaborative decoding without compromising performance. Inspired\nby Speculative Decoding--where a small proposal model generates tokens\nsequentially, and a larger target model verifies them in parallel, our approach\nbuilds on two key insights: (1) the verification distribution can be the\ncombined distribution of both the proposal and target models, and (2)\nalternating each model as the proposer and verifier can further enhance\nefficiency. We generalize this method to collaboration among n models and\ntheoretically prove that CoS is never slower than standard collaborative\ndecoding, typically achieving faster speed. Extensive experiments demonstrate\nCoS is 1.11x-2.23x faster than standard collaborative decoding without\ncompromising generation quality. Our code is available at\nhttps://github.com/Kamichanw/CoS/."
                },
                "authors": [
                    {
                        "name": "Jiale Fu"
                    },
                    {
                        "name": "Yuchu Jiang"
                    },
                    {
                        "name": "Junkai Chen"
                    },
                    {
                        "name": "Jiaming Fan"
                    },
                    {
                        "name": "Xin Geng"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23537v1",
                "updated": "2025-05-29T15:18:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    18,
                    33,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:18:33Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    18,
                    33,
                    3,
                    149,
                    0
                ],
                "title": "Domain-Aware Tensor Network Structure Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-Aware Tensor Network Structure Search"
                },
                "summary": "Tensor networks (TNs) provide efficient representations of high-dimensional\ndata, yet identification of the optimal TN structures, the so called tensor\nnetwork structure search (TN-SS) problem, remains a challenge. Current\nstate-of-the-art (SOTA) algorithms are computationally expensive as they\nrequire extensive function evaluations, which is prohibitive for real-world\napplications. In addition, existing methods ignore valuable domain information\ninherent in real-world tensor data and lack transparency in their identified TN\nstructures. To this end, we propose a novel TN-SS framework, termed the tnLLM,\nwhich incorporates domain information about the data and harnesses the\nreasoning capabilities of large language models (LLMs) to directly predict\nsuitable TN structures. The proposed framework involves a domain-aware\nprompting pipeline which instructs the LLM to infer suitable TN structures\nbased on the real-world relationships between tensor modes. In this way, our\napproach is capable of not only iteratively optimizing the objective function,\nbut also generating domain-aware explanations for the identified structures.\nExperimental results demonstrate that tnLLM achieves comparable TN-SS objective\nfunction values with much fewer function evaluations compared to SOTA\nalgorithms. Furthermore, we demonstrate that the LLM-enabled domain information\ncan be used to find good initializations in the search space for sampling-based\nSOTA methods to accelerate their convergence while preserving theoretical\nperformance guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor networks (TNs) provide efficient representations of high-dimensional\ndata, yet identification of the optimal TN structures, the so called tensor\nnetwork structure search (TN-SS) problem, remains a challenge. Current\nstate-of-the-art (SOTA) algorithms are computationally expensive as they\nrequire extensive function evaluations, which is prohibitive for real-world\napplications. In addition, existing methods ignore valuable domain information\ninherent in real-world tensor data and lack transparency in their identified TN\nstructures. To this end, we propose a novel TN-SS framework, termed the tnLLM,\nwhich incorporates domain information about the data and harnesses the\nreasoning capabilities of large language models (LLMs) to directly predict\nsuitable TN structures. The proposed framework involves a domain-aware\nprompting pipeline which instructs the LLM to infer suitable TN structures\nbased on the real-world relationships between tensor modes. In this way, our\napproach is capable of not only iteratively optimizing the objective function,\nbut also generating domain-aware explanations for the identified structures.\nExperimental results demonstrate that tnLLM achieves comparable TN-SS objective\nfunction values with much fewer function evaluations compared to SOTA\nalgorithms. Furthermore, we demonstrate that the LLM-enabled domain information\ncan be used to find good initializations in the search space for sampling-based\nSOTA methods to accelerate their convergence while preserving theoretical\nperformance guarantees."
                },
                "authors": [
                    {
                        "name": "Giorgos Iacovides"
                    },
                    {
                        "name": "Wuyang Zhou"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Qibin Zhao"
                    },
                    {
                        "name": "Danilo Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo Mandic"
                },
                "author": "Danilo Mandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13330v2",
                "updated": "2025-05-29T15:16:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    16,
                    42,
                    3,
                    149,
                    0
                ],
                "published": "2024-01-24T09:48:12Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    9,
                    48,
                    12,
                    2,
                    24,
                    0
                ],
                "title": "NACHOS: Neural Architecture Search for Hardware Constrained Early Exit\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACHOS: Neural Architecture Search for Hardware Constrained Early Exit\n  Neural Networks"
                },
                "summary": "Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN)\nwith Early Exit Classifiers (EECs), to provide predictions at intermediate\npoints of the processing when enough confidence in classification is achieved.\nThis leads to many benefits in terms of effectiveness and efficiency.\nCurrently, the design of EENNs is carried out manually by experts, a complex\nand time-consuming task that requires accounting for many aspects, including\nthe correct placement, the thresholding, and the computational overhead of the\nEECs. For this reason, the research is exploring the use of Neural Architecture\nSearch (NAS) to automatize the design of EENNs. Currently, few comprehensive\nNAS solutions for EENNs have been proposed in the literature, and a fully\nautomated, joint design strategy taking into consideration both the backbone\nand the EECs remains an open problem. To this end, this work presents Neural\nArchitecture Search for Hardware Constrained Early Exit Neural Networks\n(NACHOS), the first NAS framework for the design of optimal EENNs satisfying\nconstraints on the accuracy and the number of Multiply and Accumulate (MAC)\noperations performed by the EENNs at inference time. In particular, this\nprovides the joint design of backbone and EECs to select a set of admissible\n(i.e., respecting the constraints) Pareto Optimal Solutions in terms of best\ntradeoff between the accuracy and number of MACs. The results show that the\nmodels designed by NACHOS are competitive with the state-of-the-art EENNs.\nAdditionally, this work investigates the effectiveness of two novel\nregularization terms designed for the optimization of the auxiliary classifiers\nof the EENN",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN)\nwith Early Exit Classifiers (EECs), to provide predictions at intermediate\npoints of the processing when enough confidence in classification is achieved.\nThis leads to many benefits in terms of effectiveness and efficiency.\nCurrently, the design of EENNs is carried out manually by experts, a complex\nand time-consuming task that requires accounting for many aspects, including\nthe correct placement, the thresholding, and the computational overhead of the\nEECs. For this reason, the research is exploring the use of Neural Architecture\nSearch (NAS) to automatize the design of EENNs. Currently, few comprehensive\nNAS solutions for EENNs have been proposed in the literature, and a fully\nautomated, joint design strategy taking into consideration both the backbone\nand the EECs remains an open problem. To this end, this work presents Neural\nArchitecture Search for Hardware Constrained Early Exit Neural Networks\n(NACHOS), the first NAS framework for the design of optimal EENNs satisfying\nconstraints on the accuracy and the number of Multiply and Accumulate (MAC)\noperations performed by the EENNs at inference time. In particular, this\nprovides the joint design of backbone and EECs to select a set of admissible\n(i.e., respecting the constraints) Pareto Optimal Solutions in terms of best\ntradeoff between the accuracy and number of MACs. The results show that the\nmodels designed by NACHOS are competitive with the state-of-the-art EENNs.\nAdditionally, this work investigates the effectiveness of two novel\nregularization terms designed for the optimization of the auxiliary classifiers\nof the EENN"
                },
                "authors": [
                    {
                        "name": "Matteo Gambella"
                    },
                    {
                        "name": "Jary Pomponi"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Manuel Roveri"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Roveri"
                },
                "author": "Manuel Roveri",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.13330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21432v2",
                "updated": "2025-05-29T15:15:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    15,
                    19,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-27T17:04:21Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    4,
                    21,
                    1,
                    147,
                    0
                ],
                "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model"
                },
                "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments."
                },
                "authors": [
                    {
                        "name": "Haoming Song"
                    },
                    {
                        "name": "Delin Qu"
                    },
                    {
                        "name": "Yuanqi Yao"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Qi Lv"
                    },
                    {
                        "name": "Yiwen Tang"
                    },
                    {
                        "name": "Modi Shi"
                    },
                    {
                        "name": "Guanghui Ren"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Bin Zhao"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23523v1",
                "updated": "2025-05-29T15:03:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    3,
                    56,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:03:56Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    3,
                    56,
                    3,
                    149,
                    0
                ],
                "title": "Accelerating AllReduce with a Persistent Straggler",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating AllReduce with a Persistent Straggler"
                },
                "summary": "Distributed machine learning workloads use data and tensor parallelism for\ntraining and inference, both of which rely on the AllReduce collective to\nsynchronize gradients or activations. However, bulk-synchronous AllReduce\nalgorithms can be delayed by a persistent straggler that is slower to reach the\nsynchronization barrier required to begin the collective. To address this\nchallenge, we propose StragglAR: an AllReduce algorithm that accelerates\ndistributed training and inference in the presence of persistent stragglers.\nStragglAR implements a ReduceScatter among the remaining GPUs during the\nstraggler-induced delay, and then executes a novel collective algorithm to\ncomplete the AllReduce once the straggler reaches the synchronization barrier.\nStragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient\nAllReduce algorithms (e.g., Ring) for large GPU clusters with persistent\nstragglers. On an 8-GPU server, our implementation of StragglAR yields a 22%\nspeedup over state-of-the-art AllReduce algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed machine learning workloads use data and tensor parallelism for\ntraining and inference, both of which rely on the AllReduce collective to\nsynchronize gradients or activations. However, bulk-synchronous AllReduce\nalgorithms can be delayed by a persistent straggler that is slower to reach the\nsynchronization barrier required to begin the collective. To address this\nchallenge, we propose StragglAR: an AllReduce algorithm that accelerates\ndistributed training and inference in the presence of persistent stragglers.\nStragglAR implements a ReduceScatter among the remaining GPUs during the\nstraggler-induced delay, and then executes a novel collective algorithm to\ncomplete the AllReduce once the straggler reaches the synchronization barrier.\nStragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient\nAllReduce algorithms (e.g., Ring) for large GPU clusters with persistent\nstragglers. On an 8-GPU server, our implementation of StragglAR yields a 22%\nspeedup over state-of-the-art AllReduce algorithms."
                },
                "authors": [
                    {
                        "name": "Arjun Devraj"
                    },
                    {
                        "name": "Eric Ding"
                    },
                    {
                        "name": "Abhishek Vijaya Kumar"
                    },
                    {
                        "name": "Robert Kleinberg"
                    },
                    {
                        "name": "Rachee Singh"
                    }
                ],
                "author_detail": {
                    "name": "Rachee Singh"
                },
                "author": "Rachee Singh",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23520v1",
                "updated": "2025-05-29T14:59:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    59,
                    6,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:59:06Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    59,
                    6,
                    3,
                    149,
                    0
                ],
                "title": "AnchorAttention: Difference-Aware Sparse Attention with Stripe\n  Granularity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnchorAttention: Difference-Aware Sparse Attention with Stripe\n  Granularity"
                },
                "summary": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Dong Guo"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Guoliang Zhu"
                    },
                    {
                        "name": "Dian Ding"
                    },
                    {
                        "name": "Yiming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Zhang"
                },
                "author": "Yiming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01179v4",
                "updated": "2025-05-29T14:57:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    57,
                    31,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-03T09:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    13,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "Joint Localization and Activation Editing for Low-Resource Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Localization and Activation Editing for Low-Resource Fine-Tuning"
                },
                "summary": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly\nused to adapt LLMs. However, the effectiveness of standard PEFT methods is\nlimited in low-resource scenarios with only a few hundred examples. Recent\nadvances in interpretability research have inspired the emergence of activation\nediting (or steering) techniques, which modify the activations of specific\nmodel components. Due to their extremely small parameter counts, these methods\nshow promise for small datasets. However, their performance is highly dependent\non identifying the correct modules to edit and often lacks stability across\ndifferent datasets. In this paper, we propose Joint Localization and Activation\nEditing (JoLA), a method that jointly learns (1) which heads in the Transformer\nto edit (2) whether the intervention should be additive, multiplicative, or\nboth and (3) the intervention parameters themselves - the vectors applied as\nadditive offsets or multiplicative scalings to the head output. Through\nevaluations on three benchmarks spanning commonsense reasoning, natural\nlanguage understanding, and natural language generation, we demonstrate that\nJoLA consistently outperforms existing methods. The code for the method is\nreleased at https://github.com/wenlai-lavine/jola.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly\nused to adapt LLMs. However, the effectiveness of standard PEFT methods is\nlimited in low-resource scenarios with only a few hundred examples. Recent\nadvances in interpretability research have inspired the emergence of activation\nediting (or steering) techniques, which modify the activations of specific\nmodel components. Due to their extremely small parameter counts, these methods\nshow promise for small datasets. However, their performance is highly dependent\non identifying the correct modules to edit and often lacks stability across\ndifferent datasets. In this paper, we propose Joint Localization and Activation\nEditing (JoLA), a method that jointly learns (1) which heads in the Transformer\nto edit (2) whether the intervention should be additive, multiplicative, or\nboth and (3) the intervention parameters themselves - the vectors applied as\nadditive offsets or multiplicative scalings to the head output. Through\nevaluations on three benchmarks spanning commonsense reasoning, natural\nlanguage understanding, and natural language generation, we demonstrate that\nJoLA consistently outperforms existing methods. The code for the method is\nreleased at https://github.com/wenlai-lavine/jola."
                },
                "authors": [
                    {
                        "name": "Wen Lai"
                    },
                    {
                        "name": "Alexander Fraser"
                    },
                    {
                        "name": "Ivan Titov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Titov"
                },
                "author": "Ivan Titov",
                "arxiv_comment": "Accepted by ICML 2025 (camera-ready version). The code is released at\n  https://github.com/wenlai-lavine/jola",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16081v2",
                "updated": "2025-05-29T14:53:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    53,
                    45,
                    3,
                    149,
                    0
                ],
                "published": "2024-08-28T18:25:35Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    18,
                    25,
                    35,
                    2,
                    241,
                    0
                ],
                "title": "Towards Logically Sound Natural Language Reasoning with Logic-Enhanced\n  Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Logically Sound Natural Language Reasoning with Logic-Enhanced\n  Language Model Agents"
                },
                "summary": "Large language models (LLMs) are increasingly explored as general-purpose\nreasoners, particularly in agentic contexts. However, their outputs remain\nprone to mathematical and logical errors. This is especially challenging in\nopen-ended tasks, where unstructured outputs lack explicit ground truth and may\ncontain subtle inconsistencies. To address this issue, we propose\nLogic-Enhanced Language Model Agents (LELMA), a framework that integrates LLMs\nwith formal logic to enable validation and refinement of natural language\nreasoning. LELMA comprises three components: an LLM-Reasoner, an\nLLM-Translator, and a Solver, and employs autoformalization to translate\nreasoning into logic representations, which are then used to assess logical\nvalidity. Using game-theoretic scenarios such as the Prisoner's Dilemma as\ntestbeds, we highlight the limitations of both less capable (Gemini 1.0 Pro)\nand advanced (GPT-4o) models in generating logically sound reasoning. LELMA\nachieves high accuracy in error detection and improves reasoning correctness\nvia self-refinement, particularly in GPT-4o. The study also highlights\nchallenges in autoformalization accuracy and in evaluation of inherently\nambiguous open-ended reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly explored as general-purpose\nreasoners, particularly in agentic contexts. However, their outputs remain\nprone to mathematical and logical errors. This is especially challenging in\nopen-ended tasks, where unstructured outputs lack explicit ground truth and may\ncontain subtle inconsistencies. To address this issue, we propose\nLogic-Enhanced Language Model Agents (LELMA), a framework that integrates LLMs\nwith formal logic to enable validation and refinement of natural language\nreasoning. LELMA comprises three components: an LLM-Reasoner, an\nLLM-Translator, and a Solver, and employs autoformalization to translate\nreasoning into logic representations, which are then used to assess logical\nvalidity. Using game-theoretic scenarios such as the Prisoner's Dilemma as\ntestbeds, we highlight the limitations of both less capable (Gemini 1.0 Pro)\nand advanced (GPT-4o) models in generating logically sound reasoning. LELMA\nachieves high accuracy in error detection and improves reasoning correctness\nvia self-refinement, particularly in GPT-4o. The study also highlights\nchallenges in autoformalization accuracy and in evaluation of inherently\nambiguous open-ended reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Agnieszka Mensfelt"
                    },
                    {
                        "name": "Kostas Stathis"
                    },
                    {
                        "name": "Vince Trencsenyi"
                    }
                ],
                "author_detail": {
                    "name": "Vince Trencsenyi"
                },
                "author": "Vince Trencsenyi",
                "arxiv_comment": "Source code: https://github.com/dicelab-rhul/LELMA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09955v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09955v3",
                "updated": "2025-05-29T14:51:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    51,
                    49,
                    3,
                    149,
                    0
                ],
                "published": "2024-08-19T12:55:16Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    55,
                    16,
                    0,
                    232,
                    0
                ],
                "title": "MegaAgent: A Large-Scale Autonomous LLM-based Multi-Agent System Without\n  Predefined SOPs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaAgent: A Large-Scale Autonomous LLM-based Multi-Agent System Without\n  Predefined SOPs"
                },
                "summary": "LLM-based multi-agent systems (MAS) have shown promise in tackling complex\ntasks. However, existing solutions often suffer from limited agent coordination\nand heavy reliance on predefined Standard Operating Procedures (SOPs), which\ndemand extensive human input. To address these limitations, we propose\nMegaAgent, a large-scale autonomous LLM-based multi-agent system. MegaAgent\ngenerates agents based on task complexity and enables dynamic task\ndecomposition, parallel execution, efficient communication, and comprehensive\nsystem monitoring of agents. In evaluations, MegaAgent demonstrates exceptional\nperformance, successfully developing a Gobang game within 800 seconds and\nscaling up to 590 agents in a national policy simulation to generate\nmulti-domain policies. It significantly outperforms existing systems, such as\nMetaGPT, in both task completion efficiency and scalability. By eliminating the\nneed for predefined SOPs, MegaAgent demonstrates exceptional scalability and\nautonomy, setting a foundation for advancing true autonomy in MAS. Our code is\navailable at https://github.com/Xtra-Computing/MegaAgent .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based multi-agent systems (MAS) have shown promise in tackling complex\ntasks. However, existing solutions often suffer from limited agent coordination\nand heavy reliance on predefined Standard Operating Procedures (SOPs), which\ndemand extensive human input. To address these limitations, we propose\nMegaAgent, a large-scale autonomous LLM-based multi-agent system. MegaAgent\ngenerates agents based on task complexity and enables dynamic task\ndecomposition, parallel execution, efficient communication, and comprehensive\nsystem monitoring of agents. In evaluations, MegaAgent demonstrates exceptional\nperformance, successfully developing a Gobang game within 800 seconds and\nscaling up to 590 agents in a national policy simulation to generate\nmulti-domain policies. It significantly outperforms existing systems, such as\nMetaGPT, in both task completion efficiency and scalability. By eliminating the\nneed for predefined SOPs, MegaAgent demonstrates exceptional scalability and\nautonomy, setting a foundation for advancing true autonomy in MAS. Our code is\navailable at https://github.com/Xtra-Computing/MegaAgent ."
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Qinbin Li"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Jingsheng Liang"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09955v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09955v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09948v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09948v3",
                "updated": "2025-05-29T14:49:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    49,
                    44,
                    3,
                    149,
                    0
                ],
                "published": "2023-11-16T15:01:48Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    15,
                    1,
                    48,
                    3,
                    320,
                    0
                ],
                "title": "Hijacking Large Language Models via Adversarial In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hijacking Large Language Models via Adversarial In-Context Learning"
                },
                "summary": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs\nfor specific downstream tasks by utilizing labeled examples as demonstrations\n(demos) in the preconditioned prompts. Despite its promising performance,\ncrafted adversarial attacks pose a notable threat to the robustness of LLMs.\nExisting attacks are either easy to detect, require a trigger in user input, or\nlack specificity towards ICL. To address these issues, this work introduces a\nnovel transferable prompt injection attack against ICL, aiming to hijack LLMs\nto generate the target output or elicit harmful responses. In our threat model,\nthe hacker acts as a model publisher who leverages a gradient-based prompt\nsearch method to learn and append imperceptible adversarial suffixes to the\nin-context demos via prompt injection. We also propose effective defense\nstrategies using a few shots of clean demos, enhancing the robustness of LLMs\nduring ICL. Extensive experimental results across various classification and\njailbreak tasks demonstrate the effectiveness of the proposed attack and\ndefense strategies. This work highlights the significant security\nvulnerabilities of LLMs during ICL and underscores the need for further\nin-depth studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs\nfor specific downstream tasks by utilizing labeled examples as demonstrations\n(demos) in the preconditioned prompts. Despite its promising performance,\ncrafted adversarial attacks pose a notable threat to the robustness of LLMs.\nExisting attacks are either easy to detect, require a trigger in user input, or\nlack specificity towards ICL. To address these issues, this work introduces a\nnovel transferable prompt injection attack against ICL, aiming to hijack LLMs\nto generate the target output or elicit harmful responses. In our threat model,\nthe hacker acts as a model publisher who leverages a gradient-based prompt\nsearch method to learn and append imperceptible adversarial suffixes to the\nin-context demos via prompt injection. We also propose effective defense\nstrategies using a few shots of clean demos, enhancing the robustness of LLMs\nduring ICL. Extensive experimental results across various classification and\njailbreak tasks demonstrate the effectiveness of the proposed attack and\ndefense strategies. This work highlights the significant security\nvulnerabilities of LLMs during ICL and underscores the need for further\nin-depth studies."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhou"
                    },
                    {
                        "name": "Yao Qiang"
                    },
                    {
                        "name": "Saleh Zare Zade"
                    },
                    {
                        "name": "Prashant Khanduri"
                    },
                    {
                        "name": "Dongxiao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Dongxiao Zhu"
                },
                "author": "Dongxiao Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09948v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09948v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23503v1",
                "updated": "2025-05-29T14:48:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    48,
                    9,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:48:09Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    48,
                    9,
                    3,
                    149,
                    0
                ],
                "title": "Can Large Language Models Challenge CNNS in Medical Image Analysis?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Challenge CNNS in Medical Image Analysis?"
                },
                "summary": "This study presents a multimodal AI framework designed for precisely\nclassifying medical diagnostic images. Utilizing publicly available datasets,\nthe proposed system compares the strengths of convolutional neural networks\n(CNNs) and different large language models (LLMs). This in-depth comparative\nanalysis highlights key differences in diagnostic performance, execution\nefficiency, and environmental impacts. Model evaluation was based on accuracy,\nF1-score, average execution time, average energy consumption, and estimated\n$CO_2$ emission. The findings indicate that although CNN-based models can\noutperform various multimodal techniques that incorporate both images and\ncontextual information, applying additional filtering on top of LLMs can lead\nto substantial performance gains. These findings highlight the transformative\npotential of multimodal AI systems to enhance the reliability, efficiency, and\nscalability of medical diagnostics in clinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a multimodal AI framework designed for precisely\nclassifying medical diagnostic images. Utilizing publicly available datasets,\nthe proposed system compares the strengths of convolutional neural networks\n(CNNs) and different large language models (LLMs). This in-depth comparative\nanalysis highlights key differences in diagnostic performance, execution\nefficiency, and environmental impacts. Model evaluation was based on accuracy,\nF1-score, average execution time, average energy consumption, and estimated\n$CO_2$ emission. The findings indicate that although CNN-based models can\noutperform various multimodal techniques that incorporate both images and\ncontextual information, applying additional filtering on top of LLMs can lead\nto substantial performance gains. These findings highlight the transformative\npotential of multimodal AI systems to enhance the reliability, efficiency, and\nscalability of medical diagnostics in clinical settings."
                },
                "authors": [
                    {
                        "name": "Shibbir Ahmed"
                    },
                    {
                        "name": "Shahnewaz Karim Sakib"
                    },
                    {
                        "name": "Anindya Bijoy Das"
                    }
                ],
                "author_detail": {
                    "name": "Anindya Bijoy Das"
                },
                "author": "Anindya Bijoy Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.23765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23765v1",
                "updated": "2025-05-29T17:59:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    55,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:59:55Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    55,
                    3,
                    149,
                    0
                ],
                "title": "From Chat Logs to Collective Insights: Aggregative Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Chat Logs to Collective Insights: Aggregative Question Answering"
                },
                "summary": "Conversational agents powered by large language models (LLMs) are rapidly\nbecoming integral to our daily interactions, generating unprecedented amounts\nof conversational data. Such datasets offer a powerful lens into societal\ninterests, trending topics, and collective concerns. Yet, existing approaches\ntypically treat these interactions as independent and miss critical insights\nthat could emerge from aggregating and reasoning across large-scale\nconversation logs. In this paper, we introduce Aggregative Question Answering,\na novel task requiring models to reason explicitly over thousands of\nuser-chatbot interactions to answer aggregative queries, such as identifying\nemerging concerns among specific demographics. To enable research in this\ndirection, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative\nquestions derived from 182,330 real-world chatbot conversations. Experiments\nshow that existing methods either struggle to reason effectively or incur\nprohibitive computational costs, underscoring the need for new approaches\ncapable of extracting collective insights from large-scale conversational data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational agents powered by large language models (LLMs) are rapidly\nbecoming integral to our daily interactions, generating unprecedented amounts\nof conversational data. Such datasets offer a powerful lens into societal\ninterests, trending topics, and collective concerns. Yet, existing approaches\ntypically treat these interactions as independent and miss critical insights\nthat could emerge from aggregating and reasoning across large-scale\nconversation logs. In this paper, we introduce Aggregative Question Answering,\na novel task requiring models to reason explicitly over thousands of\nuser-chatbot interactions to answer aggregative queries, such as identifying\nemerging concerns among specific demographics. To enable research in this\ndirection, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative\nquestions derived from 182,330 real-world chatbot conversations. Experiments\nshow that existing methods either struggle to reason effectively or incur\nprohibitive computational costs, underscoring the need for new approaches\ncapable of extracting collective insights from large-scale conversational data."
                },
                "authors": [
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Woojeong Kim"
                    },
                    {
                        "name": "Yuntian Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yuntian Deng"
                },
                "author": "Yuntian Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23764v1",
                "updated": "2025-05-29T17:59:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    52,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:59:52Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    52,
                    3,
                    149,
                    0
                ],
                "title": "MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence"
                },
                "summary": "Spatial intelligence is essential for multimodal large language models\n(MLLMs) operating in the complex physical world. Existing benchmarks, however,\nprobe only single-image relations and thus fail to assess the multi-image\nspatial reasoning that real-world deployments demand. We introduce MMSI-Bench,\na VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision\nresearchers spent more than 300 hours meticulously crafting 1,000 challenging,\nunambiguous multiple-choice questions from over 120,000 images, each paired\nwith carefully designed distractors and a step-by-step reasoning process. We\nconduct extensive experiments and thoroughly evaluate 34 open-source and\nproprietary MLLMs, observing a wide gap: the strongest open-source model\nattains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while\nhumans score 97%. These results underscore the challenging nature of MMSI-Bench\nand the substantial headroom for future research. Leveraging the annotated\nreasoning processes, we also provide an automated error analysis pipeline that\ndiagnoses four dominant failure modes, including (1) grounding errors, (2)\noverlap-matching and scene-reconstruction errors, (3) situation-transformation\nreasoning errors, and (4) spatial-logic errors, offering valuable insights for\nadvancing multi-image spatial intelligence. Project page:\nhttps://runsenxu.com/projects/MMSI_Bench .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial intelligence is essential for multimodal large language models\n(MLLMs) operating in the complex physical world. Existing benchmarks, however,\nprobe only single-image relations and thus fail to assess the multi-image\nspatial reasoning that real-world deployments demand. We introduce MMSI-Bench,\na VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision\nresearchers spent more than 300 hours meticulously crafting 1,000 challenging,\nunambiguous multiple-choice questions from over 120,000 images, each paired\nwith carefully designed distractors and a step-by-step reasoning process. We\nconduct extensive experiments and thoroughly evaluate 34 open-source and\nproprietary MLLMs, observing a wide gap: the strongest open-source model\nattains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while\nhumans score 97%. These results underscore the challenging nature of MMSI-Bench\nand the substantial headroom for future research. Leveraging the annotated\nreasoning processes, we also provide an automated error analysis pipeline that\ndiagnoses four dominant failure modes, including (1) grounding errors, (2)\noverlap-matching and scene-reconstruction errors, (3) situation-transformation\nreasoning errors, and (4) spatial-logic errors, offering valuable insights for\nadvancing multi-image spatial intelligence. Project page:\nhttps://runsenxu.com/projects/MMSI_Bench ."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Yiman Xie"
                    },
                    {
                        "name": "Sizhe Yang"
                    },
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Jingli Lin"
                    },
                    {
                        "name": "Chenming Zhu"
                    },
                    {
                        "name": "Xiaochen Chen"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "34 pages. A comprehensive, fully human-curated, multi-image-based\n  spatial intelligence benchmark with reasoning annotation for MLLMs. Project\n  page: https://runsenxu.com/projects/MMSI_Bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23754v1",
                "updated": "2025-05-29T17:59:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    39,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:59:39Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    39,
                    3,
                    149,
                    0
                ],
                "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural\n  Language and Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural\n  Language and Reinforcement Learning"
                },
                "summary": "Theorem proving serves as a major testbed for evaluating complex reasoning\nabilities in large language models (LLMs). However, traditional automated\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\npoorly align with LLMs' strength derived from informal, natural language\nknowledge acquired during pre-training. In this work, we propose DeepTheorem, a\ncomprehensive informal theorem-proving framework exploiting natural language to\nenhance LLM mathematical reasoning. DeepTheorem includes a large-scale\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\nand proofs spanning diverse mathematical domains, rigorously annotated for\ncorrectness, difficulty, and topic categories, accompanied by systematically\nconstructed verifiable theorem variants. We devise a novel reinforcement\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\nleveraging the verified theorem variants to incentivize robust mathematical\ninference. Additionally, we propose comprehensive outcome and process\nevaluation metrics examining proof correctness and the quality of reasoning\nsteps. Extensive experimental analyses demonstrate DeepTheorem significantly\nimproves LLM theorem-proving performance compared to existing datasets and\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy and\nreasoning quality. Our findings highlight DeepTheorem's potential to\nfundamentally advance automated informal theorem proving and mathematical\nexploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theorem proving serves as a major testbed for evaluating complex reasoning\nabilities in large language models (LLMs). However, traditional automated\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\npoorly align with LLMs' strength derived from informal, natural language\nknowledge acquired during pre-training. In this work, we propose DeepTheorem, a\ncomprehensive informal theorem-proving framework exploiting natural language to\nenhance LLM mathematical reasoning. DeepTheorem includes a large-scale\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\nand proofs spanning diverse mathematical domains, rigorously annotated for\ncorrectness, difficulty, and topic categories, accompanied by systematically\nconstructed verifiable theorem variants. We devise a novel reinforcement\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\nleveraging the verified theorem variants to incentivize robust mathematical\ninference. Additionally, we propose comprehensive outcome and process\nevaluation metrics examining proof correctness and the quality of reasoning\nsteps. Extensive experimental analyses demonstrate DeepTheorem significantly\nimproves LLM theorem-proving performance compared to existing datasets and\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy and\nreasoning quality. Our findings highlight DeepTheorem's potential to\nfundamentally advance automated informal theorem proving and mathematical\nexploration."
                },
                "authors": [
                    {
                        "name": "Ziyin Zhang"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Yansi Li"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Zhengwen Liang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23752v1",
                "updated": "2025-05-29T17:59:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    38,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:59:38Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    38,
                    3,
                    149,
                    0
                ],
                "title": "ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks"
                },
                "summary": "Recent progress in large language models (LLMs) has enabled tool-augmented\nagents capable of solving complex real-world tasks through step-by-step\nreasoning. However, existing evaluations often focus on general-purpose or\nmultimodal scenarios, leaving a gap in domain-specific benchmarks that assess\ntool-use capabilities in complex remote sensing use cases. We present ThinkGeo,\nan agentic benchmark designed to evaluate LLM-driven agents on remote sensing\ntasks via structured tool use and multi-step planning. Inspired by\ntool-interaction paradigms, ThinkGeo includes human-curated queries spanning a\nwide range of real-world applications such as urban planning, disaster\nassessment and change analysis, environmental monitoring, transportation\nanalysis, aviation monitoring, recreational infrastructure, and industrial site\nanalysis. Each query is grounded in satellite or aerial imagery and requires\nagents to reason through a diverse toolset. We implement a ReAct-style\ninteraction loop and evaluate both open and closed-source LLMs (e.g., GPT-4o,\nQwen2.5) on 436 structured agentic tasks. The benchmark reports both step-wise\nexecution metrics and final answer correctness. Our analysis reveals notable\ndisparities in tool accuracy and planning consistency across models. ThinkGeo\nprovides the first extensive testbed for evaluating how tool-enabled LLMs\nhandle spatial reasoning in remote sensing. Our code and dataset are publicly\navailable",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has enabled tool-augmented\nagents capable of solving complex real-world tasks through step-by-step\nreasoning. However, existing evaluations often focus on general-purpose or\nmultimodal scenarios, leaving a gap in domain-specific benchmarks that assess\ntool-use capabilities in complex remote sensing use cases. We present ThinkGeo,\nan agentic benchmark designed to evaluate LLM-driven agents on remote sensing\ntasks via structured tool use and multi-step planning. Inspired by\ntool-interaction paradigms, ThinkGeo includes human-curated queries spanning a\nwide range of real-world applications such as urban planning, disaster\nassessment and change analysis, environmental monitoring, transportation\nanalysis, aviation monitoring, recreational infrastructure, and industrial site\nanalysis. Each query is grounded in satellite or aerial imagery and requires\nagents to reason through a diverse toolset. We implement a ReAct-style\ninteraction loop and evaluate both open and closed-source LLMs (e.g., GPT-4o,\nQwen2.5) on 436 structured agentic tasks. The benchmark reports both step-wise\nexecution metrics and final answer correctness. Our analysis reveals notable\ndisparities in tool accuracy and planning consistency across models. ThinkGeo\nprovides the first extensive testbed for evaluating how tool-enabled LLMs\nhandle spatial reasoning in remote sensing. Our code and dataset are publicly\navailable"
                },
                "authors": [
                    {
                        "name": "Akashah Shabbir"
                    },
                    {
                        "name": "Muhammad Akhtar Munir"
                    },
                    {
                        "name": "Akshay Dudhane"
                    },
                    {
                        "name": "Muhammad Umer Sheikh"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    },
                    {
                        "name": "Paolo Fraccaro"
                    },
                    {
                        "name": "Juan Bernabe Moreno"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    },
                    {
                        "name": "Salman Khan"
                    }
                ],
                "author_detail": {
                    "name": "Salman Khan"
                },
                "author": "Salman Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23745v1",
                "updated": "2025-05-29T17:59:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    1,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:59:01Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    1,
                    3,
                    149,
                    0
                ],
                "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Trust Or Not To Trust Your Vision-Language Model's Prediction"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated strong capabilities in\naligning visual and textual modalities, enabling a wide range of applications\nin multimodal understanding and generation. While they excel in zero-shot and\ntransfer learning scenarios, VLMs remain susceptible to misclassification,\noften yielding confident yet incorrect predictions. This limitation poses a\nsignificant risk in safety-critical domains, where erroneous predictions can\nlead to severe consequences. In this work, we introduce TrustVLM, a\ntraining-free framework designed to address the critical challenge of\nestimating when VLM's predictions can be trusted. Motivated by the observed\nmodality gap in VLMs and the insight that certain concepts are more distinctly\nrepresented in the image embedding space, we propose a novel confidence-scoring\nfunction that leverages this space to improve misclassification detection. We\nrigorously evaluate our approach across 17 diverse datasets, employing 4\narchitectures and 2 VLMs, and demonstrate state-of-the-art performance, with\nimprovements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95\ncompared to existing baselines. By improving the reliability of the model\nwithout requiring retraining, TrustVLM paves the way for safer deployment of\nVLMs in real-world applications. The code will be available at\nhttps://github.com/EPFL-IMOS/TrustVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated strong capabilities in\naligning visual and textual modalities, enabling a wide range of applications\nin multimodal understanding and generation. While they excel in zero-shot and\ntransfer learning scenarios, VLMs remain susceptible to misclassification,\noften yielding confident yet incorrect predictions. This limitation poses a\nsignificant risk in safety-critical domains, where erroneous predictions can\nlead to severe consequences. In this work, we introduce TrustVLM, a\ntraining-free framework designed to address the critical challenge of\nestimating when VLM's predictions can be trusted. Motivated by the observed\nmodality gap in VLMs and the insight that certain concepts are more distinctly\nrepresented in the image embedding space, we propose a novel confidence-scoring\nfunction that leverages this space to improve misclassification detection. We\nrigorously evaluate our approach across 17 diverse datasets, employing 4\narchitectures and 2 VLMs, and demonstrate state-of-the-art performance, with\nimprovements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95\ncompared to existing baselines. By improving the reliability of the model\nwithout requiring retraining, TrustVLM paves the way for safer deployment of\nVLMs in real-world applications. The code will be available at\nhttps://github.com/EPFL-IMOS/TrustVLM."
                },
                "authors": [
                    {
                        "name": "Hao Dong"
                    },
                    {
                        "name": "Moru Liu"
                    },
                    {
                        "name": "Jian Liang"
                    },
                    {
                        "name": "Eleni Chatzi"
                    },
                    {
                        "name": "Olga Fink"
                    }
                ],
                "author_detail": {
                    "name": "Olga Fink"
                },
                "author": "Olga Fink",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17690v2",
                "updated": "2025-05-29T17:58:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    58,
                    2,
                    3,
                    149,
                    0
                ],
                "published": "2024-11-26T18:57:29Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    57,
                    29,
                    1,
                    331,
                    0
                ],
                "title": "Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis"
                },
                "summary": "The rapid progress of foundation models and large language models (LLMs) has\nfueled significantly improvement in the capabilities of machine learning\nsystems that benefit from mutlimodal input data. However, existing multimodal\nmodels are predominantly built on top of pre-trained LLMs, which can limit\naccurate modeling of temporal dependencies across other modalities and thus\nlimit the model's ability to jointly process and leverage multimodal inputs. To\nspecifically investigate the alignment of text, video, and speech modalities in\nLLM-style (decoder-only) models, we consider a simplified multimodal generation\ntask, Video-Text to Speech (VTTS): speech generation conditioned on both its\ncorresponding text and video of talking people. The ultimate goal is to\ngenerate speech that not only follows the text but also aligns temporally with\nthe video and is consistent with the facial expressions. In this paper, we\nfirst introduce Visatronic, a unified multimodal decoder-only transformer model\nthat adopts an LLM-style architecture to embed visual, textual, and speech\ninputs into a shared subspace, treating all modalities as temporally aligned\ntoken streams. Next, we carefully explore different token mixing strategies to\nunderstand the best way to propagate information from the steps where video and\ntext conditioning is input to the steps where the audio is generated. We\nextensively evaluate Visatronic on the challenging VoxCeleb2 dataset and\ndemonstrate zero-shot generalization to LRS3, where Visatronic, trained on\nVoxCeleb2, achieves a 4.5% WER, outperforming prior SOTA methods trained only\non LRS3, which report a 21.4% WER. Additionally, we propose a new objective\nmetric, TimeSync, specifically designed to measure phoneme-level temporal\nalignment between generated and reference speech, further ensuring\nsynchronization quality. Demo: https://apple.github.io/visatronic-demo/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of foundation models and large language models (LLMs) has\nfueled significantly improvement in the capabilities of machine learning\nsystems that benefit from mutlimodal input data. However, existing multimodal\nmodels are predominantly built on top of pre-trained LLMs, which can limit\naccurate modeling of temporal dependencies across other modalities and thus\nlimit the model's ability to jointly process and leverage multimodal inputs. To\nspecifically investigate the alignment of text, video, and speech modalities in\nLLM-style (decoder-only) models, we consider a simplified multimodal generation\ntask, Video-Text to Speech (VTTS): speech generation conditioned on both its\ncorresponding text and video of talking people. The ultimate goal is to\ngenerate speech that not only follows the text but also aligns temporally with\nthe video and is consistent with the facial expressions. In this paper, we\nfirst introduce Visatronic, a unified multimodal decoder-only transformer model\nthat adopts an LLM-style architecture to embed visual, textual, and speech\ninputs into a shared subspace, treating all modalities as temporally aligned\ntoken streams. Next, we carefully explore different token mixing strategies to\nunderstand the best way to propagate information from the steps where video and\ntext conditioning is input to the steps where the audio is generated. We\nextensively evaluate Visatronic on the challenging VoxCeleb2 dataset and\ndemonstrate zero-shot generalization to LRS3, where Visatronic, trained on\nVoxCeleb2, achieves a 4.5% WER, outperforming prior SOTA methods trained only\non LRS3, which report a 21.4% WER. Additionally, we propose a new objective\nmetric, TimeSync, specifically designed to measure phoneme-level temporal\nalignment between generated and reference speech, further ensuring\nsynchronization quality. Demo: https://apple.github.io/visatronic-demo/"
                },
                "authors": [
                    {
                        "name": "Akshita Gupta"
                    },
                    {
                        "name": "Tatiana Likhomanenko"
                    },
                    {
                        "name": "Karren Dai Yang"
                    },
                    {
                        "name": "Richard He Bai"
                    },
                    {
                        "name": "Zakaria Aldeneh"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    }
                ],
                "author_detail": {
                    "name": "Navdeep Jaitly"
                },
                "author": "Navdeep Jaitly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23729v1",
                "updated": "2025-05-29T17:56:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    56,
                    5,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:56:05Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    56,
                    5,
                    3,
                    149,
                    0
                ],
                "title": "Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time"
                },
                "summary": "Aligning large language models with humans is challenging due to the\ninherently multifaceted nature of preference feedback. While existing\napproaches typically frame this as a multi-objective optimization problem, they\noften overlook how humans actually make decisions. Research on bounded\nrationality suggests that human decision making follows satisficing\nstrategies-optimizing primary objectives while ensuring others meet acceptable\nthresholds. To bridge this gap and operationalize the notion of satisficing\nalignment, we propose SITAlign: an inference time framework that addresses the\nmultifaceted nature of alignment by maximizing a primary objective while\nsatisfying threshold-based constraints on secondary criteria. We provide\ntheoretical insights by deriving sub-optimality bounds of our satisficing based\ninference alignment approach. We empirically validate SITAlign's performance\nthrough extensive experimentation on multiple benchmarks. For instance, on the\nPKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while\nensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art\nmulti objective decoding strategy by a margin of 22.3% in terms of GPT-4\nwin-tie rate for helpfulness reward while adhering to the threshold on\nharmlessness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models with humans is challenging due to the\ninherently multifaceted nature of preference feedback. While existing\napproaches typically frame this as a multi-objective optimization problem, they\noften overlook how humans actually make decisions. Research on bounded\nrationality suggests that human decision making follows satisficing\nstrategies-optimizing primary objectives while ensuring others meet acceptable\nthresholds. To bridge this gap and operationalize the notion of satisficing\nalignment, we propose SITAlign: an inference time framework that addresses the\nmultifaceted nature of alignment by maximizing a primary objective while\nsatisfying threshold-based constraints on secondary criteria. We provide\ntheoretical insights by deriving sub-optimality bounds of our satisficing based\ninference alignment approach. We empirically validate SITAlign's performance\nthrough extensive experimentation on multiple benchmarks. For instance, on the\nPKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while\nensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art\nmulti objective decoding strategy by a margin of 22.3% in terms of GPT-4\nwin-tie rate for helpfulness reward while adhering to the threshold on\nharmlessness."
                },
                "authors": [
                    {
                        "name": "Mohamad Chehade"
                    },
                    {
                        "name": "Soumya Suvra Ghosal"
                    },
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Avinash Reddy"
                    },
                    {
                        "name": "Dinesh Manocha"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    }
                ],
                "author_detail": {
                    "name": "Amrit Singh Bedi"
                },
                "author": "Amrit Singh Bedi",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23725v1",
                "updated": "2025-05-29T17:55:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    55,
                    37,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:55:37Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    55,
                    37,
                    3,
                    149,
                    0
                ],
                "title": "MuLoCo: Muon is a practical inner optimizer for DiLoCo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuLoCo: Muon is a practical inner optimizer for DiLoCo"
                },
                "summary": "DiLoCo is a powerful framework for training large language models (LLMs)\nunder networking constraints with advantages for increasing parallelism and\naccelerator utilization in data center settings. Despite significantly reducing\ncommunication frequency, however, DiLoCo's communication steps still involve\nall-reducing a complete copy of the model's parameters. While existing works\nhave explored ways to reduce communication in DiLoCo, the role of error\nfeedback accumulators and the effect of the inner-optimizer on compressibility\nremain under-explored. In this work, we investigate the effectiveness of\nstandard compression methods including Top-k sparsification and quantization\nfor reducing the communication overhead of DiLoCo when paired with two local\noptimizers (AdamW and Muon). Our experiments pre-training decoder-only\ntransformer language models (LMs) reveal that leveraging Muon as the inner\noptimizer for DiLoCo along with an error-feedback accumulator allows to\naggressively compress the communicated delta to 2-bits with next to no\nperformance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo)\nsignificantly outperforms DiLoCo while communicating 8X less and having\nidentical memory complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiLoCo is a powerful framework for training large language models (LLMs)\nunder networking constraints with advantages for increasing parallelism and\naccelerator utilization in data center settings. Despite significantly reducing\ncommunication frequency, however, DiLoCo's communication steps still involve\nall-reducing a complete copy of the model's parameters. While existing works\nhave explored ways to reduce communication in DiLoCo, the role of error\nfeedback accumulators and the effect of the inner-optimizer on compressibility\nremain under-explored. In this work, we investigate the effectiveness of\nstandard compression methods including Top-k sparsification and quantization\nfor reducing the communication overhead of DiLoCo when paired with two local\noptimizers (AdamW and Muon). Our experiments pre-training decoder-only\ntransformer language models (LMs) reveal that leveraging Muon as the inner\noptimizer for DiLoCo along with an error-feedback accumulator allows to\naggressively compress the communicated delta to 2-bits with next to no\nperformance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo)\nsignificantly outperforms DiLoCo while communicating 8X less and having\nidentical memory complexity."
                },
                "authors": [
                    {
                        "name": "Benjamin Thérien"
                    },
                    {
                        "name": "Xiaolong Huang"
                    },
                    {
                        "name": "Irina Rish"
                    },
                    {
                        "name": "Eugene Belilovsky"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Belilovsky"
                },
                "author": "Eugene Belilovsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23724v1",
                "updated": "2025-05-29T17:55:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    55,
                    21,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:55:21Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    55,
                    21,
                    3,
                    149,
                    0
                ],
                "title": "SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via\n  Subspace-Constrained LoRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via\n  Subspace-Constrained LoRA"
                },
                "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), are indispensable for efficiently customizing Large Language\nModels (LLMs). However, vanilla LoRA suffers from slow convergence speed and\nknowledge forgetting problems. Recent studies have leveraged the power of\ndesigned LoRA initialization, to enhance the fine-tuning efficiency, or to\npreserve knowledge in the pre-trained LLM. However, none of these works can\naddress the two cases at the same time. To this end, we introduce\nSubspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework\nengineered to navigate the trade-off between efficient fine-tuning and\nknowledge preservation. We achieve this by constraining the output of trainable\nLoRA adapters in a low-rank subspace, where the context information of\nfine-tuning data is most preserved while the context information of preserved\nknowledge is least retained, in a balanced way. Such constraint enables the\ntrainable weights to primarily focus on the main features of fine-tuning data\nwhile avoiding damaging the preserved knowledge features. We provide\ntheoretical analysis on our method, and conduct extensive experiments including\nsafety preservation and world knowledge preservation, on various downstream\ntasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning\nperformance while markedly diminishing knowledge forgetting, surpassing\ncontemporary LoRA initialization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), are indispensable for efficiently customizing Large Language\nModels (LLMs). However, vanilla LoRA suffers from slow convergence speed and\nknowledge forgetting problems. Recent studies have leveraged the power of\ndesigned LoRA initialization, to enhance the fine-tuning efficiency, or to\npreserve knowledge in the pre-trained LLM. However, none of these works can\naddress the two cases at the same time. To this end, we introduce\nSubspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework\nengineered to navigate the trade-off between efficient fine-tuning and\nknowledge preservation. We achieve this by constraining the output of trainable\nLoRA adapters in a low-rank subspace, where the context information of\nfine-tuning data is most preserved while the context information of preserved\nknowledge is least retained, in a balanced way. Such constraint enables the\ntrainable weights to primarily focus on the main features of fine-tuning data\nwhile avoiding damaging the preserved knowledge features. We provide\ntheoretical analysis on our method, and conduct extensive experiments including\nsafety preservation and world knowledge preservation, on various downstream\ntasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning\nperformance while markedly diminishing knowledge forgetting, surpassing\ncontemporary LoRA initialization methods."
                },
                "authors": [
                    {
                        "name": "Minrui Luo"
                    },
                    {
                        "name": "Fuhang Kuang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Tianxing He"
                    }
                ],
                "author_detail": {
                    "name": "Tianxing He"
                },
                "author": "Tianxing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23723v1",
                "updated": "2025-05-29T17:54:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    54,
                    44,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:54:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    54,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning\n  Engineering"
                },
                "summary": "The emergence of large language model (LLM)-based agents has significantly\nadvanced the development of autonomous machine learning (ML) engineering.\nHowever, most existing approaches rely heavily on manual prompt engineering,\nfailing to adapt and optimize based on diverse experimental experiences.\nFocusing on this, for the first time, we explore the paradigm of learning-based\nagentic ML, where an LLM agent learns through interactive experimentation on ML\ntasks using online reinforcement learning (RL). To realize this, we propose a\nnovel agentic ML training framework with three key components: (1)\nexploration-enriched fine-tuning, which enables LLM agents to generate diverse\nactions for enhanced RL exploration; (2) step-wise RL, which enables training\non a single action step, accelerating experience collection and improving\ntraining efficiency; (3) an agentic ML-specific reward module, which unifies\nvaried ML feedback signals into consistent rewards for RL optimization.\nLeveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM\nfor autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our\n7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it\nachieves continuous performance improvements and demonstrates exceptional\ncross-task generalization capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language model (LLM)-based agents has significantly\nadvanced the development of autonomous machine learning (ML) engineering.\nHowever, most existing approaches rely heavily on manual prompt engineering,\nfailing to adapt and optimize based on diverse experimental experiences.\nFocusing on this, for the first time, we explore the paradigm of learning-based\nagentic ML, where an LLM agent learns through interactive experimentation on ML\ntasks using online reinforcement learning (RL). To realize this, we propose a\nnovel agentic ML training framework with three key components: (1)\nexploration-enriched fine-tuning, which enables LLM agents to generate diverse\nactions for enhanced RL exploration; (2) step-wise RL, which enables training\non a single action step, accelerating experience collection and improving\ntraining efficiency; (3) an agentic ML-specific reward module, which unifies\nvaried ML feedback signals into consistent rewards for RL optimization.\nLeveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM\nfor autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our\n7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it\nachieves continuous performance improvements and demonstrates exceptional\ncross-task generalization capabilities."
                },
                "authors": [
                    {
                        "name": "Zexi Liu"
                    },
                    {
                        "name": "Jingyi Chai"
                    },
                    {
                        "name": "Xinyu Zhu"
                    },
                    {
                        "name": "Shuo Tang"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23722v1",
                "updated": "2025-05-29T17:54:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    54,
                    32,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    54,
                    32,
                    3,
                    149,
                    0
                ],
                "title": "Label-Guided In-Context Learning for Named Entity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Label-Guided In-Context Learning for Named Entity Recognition"
                },
                "summary": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks using only a few demonstrations. In Named Entity Recognition (NER),\ndemonstrations are typically selected based on semantic similarity to the test\ninstance, ignoring training labels and resulting in suboptimal performance. We\nintroduce DEER, a new method that leverages training labels through token-level\nstatistics to improve ICL performance. DEER first enhances example selection\nwith a label-guided, token-based retriever that prioritizes tokens most\ninformative for entity recognition. It then prompts the LLM to revisit\nerror-prone tokens, which are also identified using label statistics, and make\ntargeted corrections. Evaluated on five NER datasets using four different LLMs,\nDEER consistently outperforms existing ICL methods and approaches the\nperformance of supervised fine-tuning. Further analysis shows its effectiveness\non both seen and unseen entities and its robustness in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks using only a few demonstrations. In Named Entity Recognition (NER),\ndemonstrations are typically selected based on semantic similarity to the test\ninstance, ignoring training labels and resulting in suboptimal performance. We\nintroduce DEER, a new method that leverages training labels through token-level\nstatistics to improve ICL performance. DEER first enhances example selection\nwith a label-guided, token-based retriever that prioritizes tokens most\ninformative for entity recognition. It then prompts the LLM to revisit\nerror-prone tokens, which are also identified using label statistics, and make\ntargeted corrections. Evaluated on five NER datasets using four different LLMs,\nDEER consistently outperforms existing ICL methods and approaches the\nperformance of supervised fine-tuning. Further analysis shows its effectiveness\non both seen and unseen entities and its robustness in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Fan Bai"
                    },
                    {
                        "name": "Hamid Hassanzadeh"
                    },
                    {
                        "name": "Ardavan Saeedi"
                    },
                    {
                        "name": "Mark Dredze"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dredze"
                },
                "author": "Mark Dredze",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23715v1",
                "updated": "2025-05-29T17:49:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    49,
                    44,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:49:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    49,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "Don't Take the Premise for Granted: Evaluating the Premise Critique\n  Ability of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Take the Premise for Granted: Evaluating the Premise Critique\n  Ability of Large Language Models"
                },
                "summary": "Large language models (LLMs) have witnessed rapid advancements, demonstrating\nremarkable capabilities. However, a notable vulnerability persists: LLMs often\nuncritically accept flawed or contradictory premises, leading to inefficient\nreasoning and unreliable outputs. This emphasizes the significance of\npossessing the \\textbf{Premise Critique Ability} for LLMs, defined as the\ncapacity to proactively identify and articulate errors in input premises. Most\nexisting studies assess LLMs' reasoning ability in ideal settings, largely\nignoring their vulnerabilities when faced with flawed premises. Thus, we\nintroduce the \\textbf{Premise Critique Bench (PCBench)}, designed by\nincorporating four error types across three difficulty levels, paired with\nmulti-faceted evaluation metrics. We conducted systematic evaluations of 15\nrepresentative LLMs. Our findings reveal: (1) Most models rely heavily on\nexplicit prompts to detect errors, with limited autonomous critique; (2)\nPremise critique ability depends on question difficulty and error type, with\ndirect contradictions being easier to detect than complex or procedural errors;\n(3) Reasoning ability does not consistently correlate with the premise critique\nability; (4) Flawed premises trigger overthinking in reasoning models, markedly\nlengthening responses due to repeated attempts at resolving conflicts. These\ninsights underscore the urgent need to enhance LLMs' proactive evaluation of\ninput validity, positioning premise critique as a foundational capability for\ndeveloping reliable, human-centric systems. The code is available at\nhttps://github.com/MLGroupJLU/Premise_Critique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have witnessed rapid advancements, demonstrating\nremarkable capabilities. However, a notable vulnerability persists: LLMs often\nuncritically accept flawed or contradictory premises, leading to inefficient\nreasoning and unreliable outputs. This emphasizes the significance of\npossessing the \\textbf{Premise Critique Ability} for LLMs, defined as the\ncapacity to proactively identify and articulate errors in input premises. Most\nexisting studies assess LLMs' reasoning ability in ideal settings, largely\nignoring their vulnerabilities when faced with flawed premises. Thus, we\nintroduce the \\textbf{Premise Critique Bench (PCBench)}, designed by\nincorporating four error types across three difficulty levels, paired with\nmulti-faceted evaluation metrics. We conducted systematic evaluations of 15\nrepresentative LLMs. Our findings reveal: (1) Most models rely heavily on\nexplicit prompts to detect errors, with limited autonomous critique; (2)\nPremise critique ability depends on question difficulty and error type, with\ndirect contradictions being easier to detect than complex or procedural errors;\n(3) Reasoning ability does not consistently correlate with the premise critique\nability; (4) Flawed premises trigger overthinking in reasoning models, markedly\nlengthening responses due to repeated attempts at resolving conflicts. These\ninsights underscore the urgent need to enhance LLMs' proactive evaluation of\ninput validity, positioning premise critique as a foundational capability for\ndeveloping reliable, human-centric systems. The code is available at\nhttps://github.com/MLGroupJLU/Premise_Critique."
                },
                "authors": [
                    {
                        "name": "Jinzhe Li"
                    },
                    {
                        "name": "Gengxu Li"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "31 pages,13 figures,15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23713v1",
                "updated": "2025-05-29T17:47:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    47,
                    36,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:47:36Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    47,
                    36,
                    3,
                    149,
                    0
                ],
                "title": "SocialMaze: A Benchmark for Evaluating Social Reasoning in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SocialMaze: A Benchmark for Evaluating Social Reasoning in Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) are increasingly applied to socially grounded\ntasks, such as online community moderation, media content analysis, and social\nreasoning games. Success in these contexts depends on a model's social\nreasoning ability - the capacity to interpret social contexts, infer others'\nmental states, and assess the truthfulness of presented information. However,\nthere is currently no systematic evaluation framework that comprehensively\nassesses the social reasoning capabilities of LLMs. Existing efforts often\noversimplify real-world scenarios and consist of tasks that are too basic to\nchallenge advanced models. To address this gap, we introduce SocialMaze, a new\nbenchmark specifically designed to evaluate social reasoning. SocialMaze\nsystematically incorporates three core challenges: deep reasoning, dynamic\ninteraction, and information uncertainty. It provides six diverse tasks across\nthree key settings: social reasoning games, daily-life interactions, and\ndigital community platforms. Both automated and human validation are used to\nensure data quality. Our evaluation reveals several key insights: models vary\nsubstantially in their ability to handle dynamic interactions and integrate\ntemporally evolving information; models with strong chain-of-thought reasoning\nperform better on tasks requiring deeper inference beyond surface-level cues;\nand model reasoning degrades significantly under uncertainty. Furthermore, we\nshow that targeted fine-tuning on curated reasoning examples can greatly\nimprove model performance in complex social scenarios. The dataset is publicly\navailable at: https://huggingface.co/datasets/MBZUAI/SocialMaze",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly applied to socially grounded\ntasks, such as online community moderation, media content analysis, and social\nreasoning games. Success in these contexts depends on a model's social\nreasoning ability - the capacity to interpret social contexts, infer others'\nmental states, and assess the truthfulness of presented information. However,\nthere is currently no systematic evaluation framework that comprehensively\nassesses the social reasoning capabilities of LLMs. Existing efforts often\noversimplify real-world scenarios and consist of tasks that are too basic to\nchallenge advanced models. To address this gap, we introduce SocialMaze, a new\nbenchmark specifically designed to evaluate social reasoning. SocialMaze\nsystematically incorporates three core challenges: deep reasoning, dynamic\ninteraction, and information uncertainty. It provides six diverse tasks across\nthree key settings: social reasoning games, daily-life interactions, and\ndigital community platforms. Both automated and human validation are used to\nensure data quality. Our evaluation reveals several key insights: models vary\nsubstantially in their ability to handle dynamic interactions and integrate\ntemporally evolving information; models with strong chain-of-thought reasoning\nperform better on tasks requiring deeper inference beyond surface-level cues;\nand model reasoning degrades significantly under uncertainty. Furthermore, we\nshow that targeted fine-tuning on curated reasoning examples can greatly\nimprove model performance in complex social scenarios. The dataset is publicly\navailable at: https://huggingface.co/datasets/MBZUAI/SocialMaze"
                },
                "authors": [
                    {
                        "name": "Zixiang Xu"
                    },
                    {
                        "name": "Yanbo Wang"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Jiayi Ye"
                    },
                    {
                        "name": "Haomin Zhuang"
                    },
                    {
                        "name": "Zirui Song"
                    },
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Zhaorun Chen"
                    },
                    {
                        "name": "Yujun Zhou"
                    },
                    {
                        "name": "Sixian Li"
                    },
                    {
                        "name": "Wang Pan"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Jieyu Zhao"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "arxiv_comment": "Code available at https://github.com/xzx34/SocialMaze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.03207v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.03207v2",
                "updated": "2025-05-29T17:39:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    39,
                    41,
                    3,
                    149,
                    0
                ],
                "published": "2023-12-06T00:48:50Z",
                "published_parsed": [
                    2023,
                    12,
                    6,
                    0,
                    48,
                    50,
                    2,
                    340,
                    0
                ],
                "title": "Satellite Imagery and AI: A New Era in Ocean Conservation, from Research\n  to Deployment and Impact (Version. 2.0)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satellite Imagery and AI: A New Era in Ocean Conservation, from Research\n  to Deployment and Impact (Version. 2.0)"
                },
                "summary": "Illegal, unreported, and unregulated (IUU) fishing poses a global threat to\nocean habitats. Publicly available satellite data offered by NASA, the European\nSpace Agency (ESA), and the U.S. Geological Survey (USGS), provide an\nopportunity to actively monitor this activity. Effectively leveraging satellite\ndata for maritime conservation requires highly reliable machine learning models\noperating globally with minimal latency. This paper introduces four specialized\ncomputer vision models designed for a variety of sensors including Sentinel-1\n(synthetic aperture radar), Sentinel-2 (optical imagery), Landsat 8-9 (optical\nimagery), and Suomi-NPP/NOAA-20/NOAA-21 (nighttime lights). It also presents\nbest practices for developing and deploying global-scale real-time satellite\nbased computer vision. All of the models are open sourced under permissive\nlicenses. These models have all been deployed in Skylight, a real-time maritime\nmonitoring platform, which is provided at no cost to users worldwide.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Illegal, unreported, and unregulated (IUU) fishing poses a global threat to\nocean habitats. Publicly available satellite data offered by NASA, the European\nSpace Agency (ESA), and the U.S. Geological Survey (USGS), provide an\nopportunity to actively monitor this activity. Effectively leveraging satellite\ndata for maritime conservation requires highly reliable machine learning models\noperating globally with minimal latency. This paper introduces four specialized\ncomputer vision models designed for a variety of sensors including Sentinel-1\n(synthetic aperture radar), Sentinel-2 (optical imagery), Landsat 8-9 (optical\nimagery), and Suomi-NPP/NOAA-20/NOAA-21 (nighttime lights). It also presents\nbest practices for developing and deploying global-scale real-time satellite\nbased computer vision. All of the models are open sourced under permissive\nlicenses. These models have all been deployed in Skylight, a real-time maritime\nmonitoring platform, which is provided at no cost to users worldwide."
                },
                "authors": [
                    {
                        "name": "Patrick Beukema"
                    },
                    {
                        "name": "Favyen Bastani"
                    },
                    {
                        "name": "Yawen Zheng"
                    },
                    {
                        "name": "Piper Wolters"
                    },
                    {
                        "name": "Henry Herzog"
                    },
                    {
                        "name": "Joe Ferdinando"
                    }
                ],
                "author_detail": {
                    "name": "Joe Ferdinando"
                },
                "author": "Joe Ferdinando",
                "arxiv_comment": "8 pages, 3 figures, NeurIPS Computational Sustainability 2023 best\n  paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.03207v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.03207v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23703v1",
                "updated": "2025-05-29T17:39:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    39,
                    30,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:39:30Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    39,
                    30,
                    3,
                    149,
                    0
                ],
                "title": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's\n  Math Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's\n  Math Capability"
                },
                "summary": "Enhancing the mathematical reasoning capabilities of LLMs has garnered\nsignificant attention in both the mathematical and computer science\ncommunities. Recent works have made substantial progress in both Natural\nLanguage (NL) reasoning and Formal Language (FL) reasoning by leveraging the\npotential of pure Reinforcement Learning (RL) methods on base models. However,\nRL approaches struggle to impart new capabilities not presented in the base\nmodel, highlighting the need to integrate more knowledge like FL into NL math\nreasoning effectively. Yet, this integration is challenging due to inherent\ndisparities in problem structure and reasoning format between NL and FL. To\naddress these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end\nframework designed to incorporate the FL expert into NL math problem-solving.\nTo bridge the NL and FL input format gap, we propose the *NL-FL Problem\nAlignment* method, which reformulates the Question-Answering (QA) problems in\nNL as existence theorems in FL. Subsequently, the *Mixed Problem Input*\ntechnique we provide enables the FL reasoner to handle both QA and existence\nproblems concurrently. Lastly, we mitigate the NL and FL output format gap in\nreasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive\nexperiments demonstrate that the **HybridReasoning** framework achieves\n**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC\nbenchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.\nNotably, some problems resolved by our framework remain unsolved by the NL\nbaseline model even under a larger number of trials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the mathematical reasoning capabilities of LLMs has garnered\nsignificant attention in both the mathematical and computer science\ncommunities. Recent works have made substantial progress in both Natural\nLanguage (NL) reasoning and Formal Language (FL) reasoning by leveraging the\npotential of pure Reinforcement Learning (RL) methods on base models. However,\nRL approaches struggle to impart new capabilities not presented in the base\nmodel, highlighting the need to integrate more knowledge like FL into NL math\nreasoning effectively. Yet, this integration is challenging due to inherent\ndisparities in problem structure and reasoning format between NL and FL. To\naddress these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end\nframework designed to incorporate the FL expert into NL math problem-solving.\nTo bridge the NL and FL input format gap, we propose the *NL-FL Problem\nAlignment* method, which reformulates the Question-Answering (QA) problems in\nNL as existence theorems in FL. Subsequently, the *Mixed Problem Input*\ntechnique we provide enables the FL reasoner to handle both QA and existence\nproblems concurrently. Lastly, we mitigate the NL and FL output format gap in\nreasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive\nexperiments demonstrate that the **HybridReasoning** framework achieves\n**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC\nbenchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.\nNotably, some problems resolved by our framework remain unsolved by the NL\nbaseline model even under a larger number of trials."
                },
                "authors": [
                    {
                        "name": "Ruida Wang"
                    },
                    {
                        "name": "Yuxin Li"
                    },
                    {
                        "name": "Yi R."
                    },
                    {
                        "name": "Fung"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "arxiv_affiliation": "May",
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23701v1",
                "updated": "2025-05-29T17:37:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    37,
                    57,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:37:57Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    37,
                    57,
                    3,
                    149,
                    0
                ],
                "title": "Can LLMs Reason Abstractly Over Math Word Problems Without CoT?\n  Disentangling Abstract Formulation From Arithmetic Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Reason Abstractly Over Math Word Problems Without CoT?\n  Disentangling Abstract Formulation From Arithmetic Computation"
                },
                "summary": "Final-answer-based metrics are commonly used for evaluating large language\nmodels (LLMs) on math word problems, often taken as proxies for reasoning\nability. However, such metrics conflate two distinct sub-skills: abstract\nformulation (capturing mathematical relationships using expressions) and\narithmetic computation (executing the calculations). Through a disentangled\nevaluation on GSM8K and SVAMP, we find that the final-answer accuracy of\nLlama-3 and Qwen2.5 (1B-32B) without CoT is overwhelmingly bottlenecked by the\narithmetic computation step and not by the abstract formulation step. Contrary\nto the common belief, we show that CoT primarily aids in computation, with\nlimited impact on abstract formulation. Mechanistically, we show that these two\nskills are composed conjunctively even in a single forward pass without any\nreasoning steps via an abstract-then-compute mechanism: models first capture\nproblem abstractions, then handle computation. Causal patching confirms these\nabstractions are present, transferable, composable, and precede computation.\nThese behavioural and mechanistic findings highlight the need for disentangled\nevaluation to accurately assess LLM reasoning and to guide future improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Final-answer-based metrics are commonly used for evaluating large language\nmodels (LLMs) on math word problems, often taken as proxies for reasoning\nability. However, such metrics conflate two distinct sub-skills: abstract\nformulation (capturing mathematical relationships using expressions) and\narithmetic computation (executing the calculations). Through a disentangled\nevaluation on GSM8K and SVAMP, we find that the final-answer accuracy of\nLlama-3 and Qwen2.5 (1B-32B) without CoT is overwhelmingly bottlenecked by the\narithmetic computation step and not by the abstract formulation step. Contrary\nto the common belief, we show that CoT primarily aids in computation, with\nlimited impact on abstract formulation. Mechanistically, we show that these two\nskills are composed conjunctively even in a single forward pass without any\nreasoning steps via an abstract-then-compute mechanism: models first capture\nproblem abstractions, then handle computation. Causal patching confirms these\nabstractions are present, transferable, composable, and precede computation.\nThese behavioural and mechanistic findings highlight the need for disentangled\nevaluation to accurately assess LLM reasoning and to guide future improvements."
                },
                "authors": [
                    {
                        "name": "Ziling Cheng"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Leila Pishdad"
                    },
                    {
                        "name": "Yanshuai Cao"
                    },
                    {
                        "name": "Jackie Chi Kit Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Jackie Chi Kit Cheung"
                },
                "author": "Jackie Chi Kit Cheung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06951v2",
                "updated": "2025-05-29T17:37:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    37,
                    26,
                    3,
                    149,
                    0
                ],
                "published": "2025-03-10T05:56:46Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    56,
                    46,
                    0,
                    69,
                    0
                ],
                "title": "ReAgent: Reversible Multi-Agent Reasoning for Knowledge-Enhanced\n  Multi-Hop QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReAgent: Reversible Multi-Agent Reasoning for Knowledge-Enhanced\n  Multi-Hop QA"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly improved\nmulti-hop question answering (QA) through direct Chain-of-Thought (CoT)\nreasoning. However, the irreversible nature of CoT leads to error accumulation,\nmaking it challenging to correct mistakes in multi-hop reasoning. This paper\nintroduces ReAgent: a Reversible multi-Agent collaborative framework augmented\nwith explicit backtracking mechanisms, enabling reversible multi-hop reasoning.\nBy incorporating text-based retrieval, information aggregation and validation,\nour system can detect and correct errors mid-reasoning, leading to more robust\nand interpretable QA outcomes. The framework and experiments serve as a\nfoundation for future work on error-tolerant QA systems. Empirical evaluations\nacross three benchmarks indicate ReAgent's efficacy, yielding average about 6\\%\nimprovements against baseline models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly improved\nmulti-hop question answering (QA) through direct Chain-of-Thought (CoT)\nreasoning. However, the irreversible nature of CoT leads to error accumulation,\nmaking it challenging to correct mistakes in multi-hop reasoning. This paper\nintroduces ReAgent: a Reversible multi-Agent collaborative framework augmented\nwith explicit backtracking mechanisms, enabling reversible multi-hop reasoning.\nBy incorporating text-based retrieval, information aggregation and validation,\nour system can detect and correct errors mid-reasoning, leading to more robust\nand interpretable QA outcomes. The framework and experiments serve as a\nfoundation for future work on error-tolerant QA systems. Empirical evaluations\nacross three benchmarks indicate ReAgent's efficacy, yielding average about 6\\%\nimprovements against baseline models."
                },
                "authors": [
                    {
                        "name": "Xinjie Zhao"
                    },
                    {
                        "name": "Fan Gao"
                    },
                    {
                        "name": "Xingyu Song"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Yanran Fu"
                    },
                    {
                        "name": "Yuyang Wang"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "arxiv_comment": "25pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23695v1",
                "updated": "2025-05-29T17:32:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    32,
                    15,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:32:15Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    32,
                    15,
                    3,
                    149,
                    0
                ],
                "title": "Data-to-Dashboard: Multi-Agent LLM Framework for Insightful\n  Visualization in Enterprise Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-to-Dashboard: Multi-Agent LLM Framework for Insightful\n  Visualization in Enterprise Analytics"
                },
                "summary": "The rapid advancement of LLMs has led to the creation of diverse agentic\nsystems in data analysis, utilizing LLMs' capabilities to improve insight\ngeneration and visualization. In this paper, we present an agentic system that\nautomates the data-to-dashboard pipeline through modular LLM agents capable of\ndomain detection, concept extraction, multi-perspective analysis generation,\nand iterative self-reflection. Unlike existing chart QA systems, our framework\nsimulates the analytical reasoning process of business analysts by retrieving\ndomain-relevant knowledge and adapting to diverse datasets without relying on\nclosed ontologies or question templates.\n  We evaluate our system on three datasets across different domains.\nBenchmarked against GPT-4o with a single-prompt baseline, our approach shows\nimproved insightfulness, domain relevance, and analytical depth, as measured by\ntailored evaluation metrics and qualitative human assessment.\n  This work contributes a novel modular pipeline to bridge the path from raw\ndata to visualization, and opens new opportunities for human-in-the-loop\nvalidation by domain experts in business analytics. All code can be found here:\nhttps://github.com/77luvC/D2D_Data2Dashboard",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of LLMs has led to the creation of diverse agentic\nsystems in data analysis, utilizing LLMs' capabilities to improve insight\ngeneration and visualization. In this paper, we present an agentic system that\nautomates the data-to-dashboard pipeline through modular LLM agents capable of\ndomain detection, concept extraction, multi-perspective analysis generation,\nand iterative self-reflection. Unlike existing chart QA systems, our framework\nsimulates the analytical reasoning process of business analysts by retrieving\ndomain-relevant knowledge and adapting to diverse datasets without relying on\nclosed ontologies or question templates.\n  We evaluate our system on three datasets across different domains.\nBenchmarked against GPT-4o with a single-prompt baseline, our approach shows\nimproved insightfulness, domain relevance, and analytical depth, as measured by\ntailored evaluation metrics and qualitative human assessment.\n  This work contributes a novel modular pipeline to bridge the path from raw\ndata to visualization, and opens new opportunities for human-in-the-loop\nvalidation by domain experts in business analytics. All code can be found here:\nhttps://github.com/77luvC/D2D_Data2Dashboard"
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Mohannad Elhamod"
                    }
                ],
                "author_detail": {
                    "name": "Mohannad Elhamod"
                },
                "author": "Mohannad Elhamod",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23693v1",
                "updated": "2025-05-29T17:31:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    31,
                    13,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:31:13Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    31,
                    13,
                    3,
                    149,
                    0
                ],
                "title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC\n  Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC\n  Videos"
                },
                "summary": "MLLMs have been widely studied for video question answering recently.\nHowever, most existing assessments focus on natural videos, overlooking\nsynthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in\nvideo generation rely on MLLMs to evaluate the quality of generated videos, but\nthe capabilities of MLLMs on interpreting AIGC videos remain largely\nunderexplored. To address this, we propose a new benchmark, VF-Eval, which\nintroduces four tasks-coherence validation, error awareness, error type\ndetection, and reasoning evaluation-to comprehensively evaluate the abilities\nof MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that\neven the best-performing model, GPT-4.1, struggles to achieve consistently good\nperformance across all tasks. This highlights the challenging nature of our\nbenchmark. Additionally, to investigate the practical applications of VF-Eval\nin improving video generation, we conduct an experiment, RePrompt,\ndemonstrating that aligning MLLMs more closely with human feedback can benefit\nvideo generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLLMs have been widely studied for video question answering recently.\nHowever, most existing assessments focus on natural videos, overlooking\nsynthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in\nvideo generation rely on MLLMs to evaluate the quality of generated videos, but\nthe capabilities of MLLMs on interpreting AIGC videos remain largely\nunderexplored. To address this, we propose a new benchmark, VF-Eval, which\nintroduces four tasks-coherence validation, error awareness, error type\ndetection, and reasoning evaluation-to comprehensively evaluate the abilities\nof MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that\neven the best-performing model, GPT-4.1, struggles to achieve consistently good\nperformance across all tasks. This highlights the challenging nature of our\nbenchmark. Additionally, to investigate the practical applications of VF-Eval\nin improving video generation, we conduct an experiment, RePrompt,\ndemonstrating that aligning MLLMs more closely with human feedback can benefit\nvideo generation."
                },
                "authors": [
                    {
                        "name": "Tingyu Song"
                    },
                    {
                        "name": "Tongyan Hu"
                    },
                    {
                        "name": "Guo Gan"
                    },
                    {
                        "name": "Yilun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Zhao"
                },
                "author": "Yilun Zhao",
                "arxiv_comment": "ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23686v1",
                "updated": "2025-05-29T17:24:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    24,
                    54,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:24:54Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    24,
                    54,
                    3,
                    149,
                    0
                ],
                "title": "ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork"
                },
                "summary": "Developing AI agents capable of collaborating with previously unseen partners\nis a fundamental generalization challenge in multi-agent learning, known as Ad\nHoc Teamwork (AHT). Existing AHT approaches typically adopt a two-stage\npipeline, where first, a fixed population of teammates is generated with the\nidea that they should be representative of the teammates that will be seen at\ndeployment time, and second, an AHT agent is trained to collaborate well with\nagents in the population. To date, the research community has focused on\ndesigning separate algorithms for each stage. This separation has led to\nalgorithms that generate teammate pools with limited coverage of possible\nbehaviors, and that ignore whether the generated teammates are easy to learn\nfrom for the AHT agent. Furthermore, algorithms for training AHT agents\ntypically treat the set of training teammates as static, thus attempting to\ngeneralize to previously unseen partner agents without assuming any control\nover the distribution of training teammates. In this paper, we present a\nunified framework for AHT by reformulating the problem as an open-ended\nlearning process between an ad hoc agent and an adversarial teammate generator.\nWe introduce ROTATE, a regret-driven, open-ended training algorithm that\nalternates between improving the AHT agent and generating teammates that probe\nits deficiencies. Extensive experiments across diverse AHT environments\ndemonstrate that ROTATE significantly outperforms baselines at generalizing to\nan unseen set of evaluation teammates, thus establishing a new standard for\nrobust and generalizable teamwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing AI agents capable of collaborating with previously unseen partners\nis a fundamental generalization challenge in multi-agent learning, known as Ad\nHoc Teamwork (AHT). Existing AHT approaches typically adopt a two-stage\npipeline, where first, a fixed population of teammates is generated with the\nidea that they should be representative of the teammates that will be seen at\ndeployment time, and second, an AHT agent is trained to collaborate well with\nagents in the population. To date, the research community has focused on\ndesigning separate algorithms for each stage. This separation has led to\nalgorithms that generate teammate pools with limited coverage of possible\nbehaviors, and that ignore whether the generated teammates are easy to learn\nfrom for the AHT agent. Furthermore, algorithms for training AHT agents\ntypically treat the set of training teammates as static, thus attempting to\ngeneralize to previously unseen partner agents without assuming any control\nover the distribution of training teammates. In this paper, we present a\nunified framework for AHT by reformulating the problem as an open-ended\nlearning process between an ad hoc agent and an adversarial teammate generator.\nWe introduce ROTATE, a regret-driven, open-ended training algorithm that\nalternates between improving the AHT agent and generating teammates that probe\nits deficiencies. Extensive experiments across diverse AHT environments\ndemonstrate that ROTATE significantly outperforms baselines at generalizing to\nan unseen set of evaluation teammates, thus establishing a new standard for\nrobust and generalizable teamwork."
                },
                "authors": [
                    {
                        "name": "Caroline Wang"
                    },
                    {
                        "name": "Arrasy Rahman"
                    },
                    {
                        "name": "Jiaxun Cui"
                    },
                    {
                        "name": "Yoonchang Sung"
                    },
                    {
                        "name": "Peter Stone"
                    }
                ],
                "author_detail": {
                    "name": "Peter Stone"
                },
                "author": "Peter Stone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.1; I.2.6; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09615v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09615v3",
                "updated": "2025-05-29T17:14:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    14,
                    16,
                    3,
                    149,
                    0
                ],
                "published": "2024-10-12T18:36:07Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    18,
                    36,
                    7,
                    5,
                    286,
                    0
                ],
                "title": "SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for\n  LLM Weight Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for\n  LLM Weight Compression"
                },
                "summary": "Conventional model compression techniques for LLMs address high memory\nconsumption and slow inference challenges but typically require computationally\nexpensive retraining to preserve accuracy. In contrast, one-shot compression\nmethods eliminate retraining cost, but struggle to achieve accuracy comparable\nto dense models. This paper presents SLIM, a new one-shot compression framework\nthat holistically integrates hardware-friendly quantization, sparsity, and\nlow-rank approximation into a unified process. First, we formulate the\nquantization process using a probabilistic approach (SLIM-Quant) that enables\nus to apply uniform quantization. Then, we use an existing one-shot pruning\nmethod to apply semi-structured sparsity on top of the quantized weights.\nFinally, to compensate for the introduced aggregated quantization and sparsity\nerror, we use a novel saliency function with unique invertible and additive\nfeatures that enables us to mathematically compute the value of low-rank\nadapters. SLIM improves model accuracy by up to 5.66% (LLaMA-2-7B) for 2:4\nsparsity with 4-bit weight quantization, outperforming prior methods. Models\ncompressed with SLIM achieve up to 4.3x and 3.8x on Nvidia RTX3060 and A100\nGPUs, respectively. Additionally, they achieve up to 0.23x end-to-end memory\nreduction in comparison to their dense counterparts. We also propose an\noptional PEFT recipe that further improves accuracy by up to 1.66%\n(LLaMA-2-13B) compared to SLIM without fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional model compression techniques for LLMs address high memory\nconsumption and slow inference challenges but typically require computationally\nexpensive retraining to preserve accuracy. In contrast, one-shot compression\nmethods eliminate retraining cost, but struggle to achieve accuracy comparable\nto dense models. This paper presents SLIM, a new one-shot compression framework\nthat holistically integrates hardware-friendly quantization, sparsity, and\nlow-rank approximation into a unified process. First, we formulate the\nquantization process using a probabilistic approach (SLIM-Quant) that enables\nus to apply uniform quantization. Then, we use an existing one-shot pruning\nmethod to apply semi-structured sparsity on top of the quantized weights.\nFinally, to compensate for the introduced aggregated quantization and sparsity\nerror, we use a novel saliency function with unique invertible and additive\nfeatures that enables us to mathematically compute the value of low-rank\nadapters. SLIM improves model accuracy by up to 5.66% (LLaMA-2-7B) for 2:4\nsparsity with 4-bit weight quantization, outperforming prior methods. Models\ncompressed with SLIM achieve up to 4.3x and 3.8x on Nvidia RTX3060 and A100\nGPUs, respectively. Additionally, they achieve up to 0.23x end-to-end memory\nreduction in comparison to their dense counterparts. We also propose an\noptional PEFT recipe that further improves accuracy by up to 1.66%\n(LLaMA-2-13B) compared to SLIM without fine-tuning."
                },
                "authors": [
                    {
                        "name": "Mohammad Mozaffari"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Maryam Mehri Dehnavi"
                    }
                ],
                "author_detail": {
                    "name": "Maryam Mehri Dehnavi"
                },
                "author": "Maryam Mehri Dehnavi",
                "arxiv_comment": "Published at Proceedings of the 42 nd International Conference on\n  Machine Learning (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09615v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09615v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18638v2",
                "updated": "2025-05-29T17:11:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    11,
                    54,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-24T11:01:05Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    11,
                    1,
                    5,
                    5,
                    144,
                    0
                ],
                "title": "Multilingual Question Answering in Low-Resource Settings: A\n  Dzongkha-English Benchmark for Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Question Answering in Low-Resource Settings: A\n  Dzongkha-English Benchmark for Foundation Models"
                },
                "summary": "In this work, we provide DZEN, a dataset of parallel Dzongkha and English\ntest questions for Bhutanese middle and high school students. The over 5K\nquestions in our collection span a variety of scientific topics and include\nfactual, application, and reasoning-based questions. We use our parallel\ndataset to test a number of Large Language Models (LLMs) and find a significant\nperformance difference between the models in English and Dzongkha. We also look\nat different prompting strategies and discover that Chain-of-Thought (CoT)\nprompting works well for reasoning questions but less well for factual ones. We\nalso find that adding English translations enhances the precision of Dzongkha\nquestion responses. Our results point to exciting avenues for further study to\nimprove LLM performance in Dzongkha and, more generally, in low-resource\nlanguages. We release the dataset at:\nhttps://github.com/kraritt/llm_dzongkha_evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we provide DZEN, a dataset of parallel Dzongkha and English\ntest questions for Bhutanese middle and high school students. The over 5K\nquestions in our collection span a variety of scientific topics and include\nfactual, application, and reasoning-based questions. We use our parallel\ndataset to test a number of Large Language Models (LLMs) and find a significant\nperformance difference between the models in English and Dzongkha. We also look\nat different prompting strategies and discover that Chain-of-Thought (CoT)\nprompting works well for reasoning questions but less well for factual ones. We\nalso find that adding English translations enhances the precision of Dzongkha\nquestion responses. Our results point to exciting avenues for further study to\nimprove LLM performance in Dzongkha and, more generally, in low-resource\nlanguages. We release the dataset at:\nhttps://github.com/kraritt/llm_dzongkha_evaluation."
                },
                "authors": [
                    {
                        "name": "Md. Tanzib Hosain"
                    },
                    {
                        "name": "Rajan Das Gupta"
                    },
                    {
                        "name": "Md. Kishor Morol"
                    }
                ],
                "author_detail": {
                    "name": "Md. Kishor Morol"
                },
                "author": "Md. Kishor Morol",
                "arxiv_comment": "24 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23662v1",
                "updated": "2025-05-29T17:10:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    10,
                    12,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:10:12Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    10,
                    12,
                    3,
                    149,
                    0
                ],
                "title": "ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic\n  Long-Term Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic\n  Long-Term Interactions"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in using\nexternal tools to address user inquiries. However, most existing evaluations\nassume tool use in short contexts, offering limited insight into model behavior\nduring realistic long-term interactions. To fill this gap, we introduce\nToolHaystack, a benchmark for testing the tool use capabilities in long-term\ninteractions. Each test instance in ToolHaystack includes multiple tasks\nexecution contexts and realistic noise within a continuous conversation,\nenabling assessment of how well models maintain context and handle various\ndisruptions. By applying this benchmark to 14 state-of-the-art LLMs, we find\nthat while current models perform well in standard multi-turn settings, they\noften significantly struggle in ToolHaystack, highlighting critical gaps in\ntheir long-term robustness not revealed by previous tool benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in using\nexternal tools to address user inquiries. However, most existing evaluations\nassume tool use in short contexts, offering limited insight into model behavior\nduring realistic long-term interactions. To fill this gap, we introduce\nToolHaystack, a benchmark for testing the tool use capabilities in long-term\ninteractions. Each test instance in ToolHaystack includes multiple tasks\nexecution contexts and realistic noise within a continuous conversation,\nenabling assessment of how well models maintain context and handle various\ndisruptions. By applying this benchmark to 14 state-of-the-art LLMs, we find\nthat while current models perform well in standard multi-turn settings, they\noften significantly struggle in ToolHaystack, highlighting critical gaps in\ntheir long-term robustness not revealed by previous tool benchmarks."
                },
                "authors": [
                    {
                        "name": "Beong-woo Kwak"
                    },
                    {
                        "name": "Minju Kim"
                    },
                    {
                        "name": "Dongha Lim"
                    },
                    {
                        "name": "Hyungjoo Chae"
                    },
                    {
                        "name": "Dongjin Kang"
                    },
                    {
                        "name": "Sunghwan Kim"
                    },
                    {
                        "name": "Dongil Yang"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    }
                ],
                "author_detail": {
                    "name": "Jinyoung Yeo"
                },
                "author": "Jinyoung Yeo",
                "arxiv_comment": "Our code and data are available at\n  https://github.com/bwookwak/ToolHaystack",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23661v1",
                "updated": "2025-05-29T17:09:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    9,
                    44,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:09:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    9,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and\n  Generation"
                },
                "summary": "In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni."
                },
                "authors": [
                    {
                        "name": "Size Wu"
                    },
                    {
                        "name": "Zhonghua Wu"
                    },
                    {
                        "name": "Zerui Gong"
                    },
                    {
                        "name": "Qingyi Tao"
                    },
                    {
                        "name": "Sheng Jin"
                    },
                    {
                        "name": "Qinyue Li"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Chen Change Loy"
                    }
                ],
                "author_detail": {
                    "name": "Chen Change Loy"
                },
                "author": "Chen Change Loy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23657v1",
                "updated": "2025-05-29T17:07:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    7,
                    24,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:07:24Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    7,
                    24,
                    3,
                    149,
                    0
                ],
                "title": "Active Layer-Contrastive Decoding Reduces Hallucination in Large\n  Language Model Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Layer-Contrastive Decoding Reduces Hallucination in Large\n  Language Model Generation"
                },
                "summary": "Recent decoding methods improve the factuality of large language\nmodels~(LLMs) by refining how the next token is selected during generation.\nThese methods typically operate at the token level, leveraging internal\nrepresentations to suppress superficial patterns. Nevertheless, LLMs remain\nprone to hallucinations, especially over longer contexts. In this paper, we\npropose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy\nthat actively decides when to apply contrasting layers during generation. By\ncasting decoding as a sequential decision-making problem, ActLCD employs a\nreinforcement learning policy guided by a reward-aware classifier to optimize\nfactuality beyond the token level. Our experiments demonstrate that ActLCD\nsurpasses state-of-the-art methods across five benchmarks, showcasing its\neffectiveness in mitigating hallucinations in diverse generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent decoding methods improve the factuality of large language\nmodels~(LLMs) by refining how the next token is selected during generation.\nThese methods typically operate at the token level, leveraging internal\nrepresentations to suppress superficial patterns. Nevertheless, LLMs remain\nprone to hallucinations, especially over longer contexts. In this paper, we\npropose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy\nthat actively decides when to apply contrasting layers during generation. By\ncasting decoding as a sequential decision-making problem, ActLCD employs a\nreinforcement learning policy guided by a reward-aware classifier to optimize\nfactuality beyond the token level. Our experiments demonstrate that ActLCD\nsurpasses state-of-the-art methods across five benchmarks, showcasing its\neffectiveness in mitigating hallucinations in diverse generation scenarios."
                },
                "authors": [
                    {
                        "name": "Hongxiang Zhang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Muhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Muhao Chen"
                },
                "author": "Muhao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23654v1",
                "updated": "2025-05-29T17:04:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    4,
                    2,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:04:02Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    4,
                    2,
                    3,
                    149,
                    0
                ],
                "title": "ARC: Argument Representation and Coverage Analysis for Zero-Shot Long\n  Document Summarization with Instruction Following LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: Argument Representation and Coverage Analysis for Zero-Shot Long\n  Document Summarization with Instruction Following LLMs"
                },
                "summary": "Integrating structured information has long improved the quality of\nabstractive summarization, particularly in retaining salient content. In this\nwork, we focus on a specific form of structure: argument roles, which are\ncrucial for summarizing documents in high-stakes domains such as law. We\ninvestigate whether instruction-tuned large language models (LLMs) adequately\npreserve this information. To this end, we introduce Argument Representation\nCoverage (ARC), a framework for measuring how well LLM-generated summaries\ncapture salient arguments. Using ARC, we analyze summaries produced by three\nopen-weight LLMs in two domains where argument roles are central: long legal\nopinions and scientific articles. Our results show that while LLMs cover\nsalient argument roles to some extent, critical information is often omitted in\ngenerated summaries, particularly when arguments are sparsely distributed\nthroughout the input. Further, we use ARC to uncover behavioral patterns --\nspecifically, how the positional bias of LLM context windows and role-specific\npreferences impact the coverage of key arguments in generated summaries,\nemphasizing the need for more argument-aware summarization strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating structured information has long improved the quality of\nabstractive summarization, particularly in retaining salient content. In this\nwork, we focus on a specific form of structure: argument roles, which are\ncrucial for summarizing documents in high-stakes domains such as law. We\ninvestigate whether instruction-tuned large language models (LLMs) adequately\npreserve this information. To this end, we introduce Argument Representation\nCoverage (ARC), a framework for measuring how well LLM-generated summaries\ncapture salient arguments. Using ARC, we analyze summaries produced by three\nopen-weight LLMs in two domains where argument roles are central: long legal\nopinions and scientific articles. Our results show that while LLMs cover\nsalient argument roles to some extent, critical information is often omitted in\ngenerated summaries, particularly when arguments are sparsely distributed\nthroughout the input. Further, we use ARC to uncover behavioral patterns --\nspecifically, how the positional bias of LLM context windows and role-specific\npreferences impact the coverage of key arguments in generated summaries,\nemphasizing the need for more argument-aware summarization strategies."
                },
                "authors": [
                    {
                        "name": "Mohamed Elaraby"
                    },
                    {
                        "name": "Diane Litman"
                    }
                ],
                "author_detail": {
                    "name": "Diane Litman"
                },
                "author": "Diane Litman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23653v1",
                "updated": "2025-05-29T17:02:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    2,
                    49,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:02:49Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    2,
                    49,
                    3,
                    149,
                    0
                ],
                "title": "How does Transformer Learn Implicit Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How does Transformer Learn Implicit Reasoning?"
                },
                "summary": "Recent work suggests that large language models (LLMs) can perform multi-hop\nreasoning implicitly -- producing correct answers without explicitly\nverbalizing intermediate steps -- but the underlying mechanisms remain poorly\nunderstood. In this paper, we study how such implicit reasoning emerges by\ntraining transformers from scratch in a controlled symbolic environment. Our\nanalysis reveals a three-stage developmental trajectory: early memorization,\nfollowed by in-distribution generalization, and eventually cross-distribution\ngeneralization. We find that training with atomic triples is not necessary but\naccelerates learning, and that second-hop generalization relies on query-level\nexposure to specific compositional structures. To interpret these behaviors, we\nintroduce two diagnostic tools: cross-query semantic patching, which identifies\nsemantically reusable intermediate representations, and a cosine-based\nrepresentational lens, which reveals that successful reasoning correlates with\nthe cosine-base clustering in hidden space. This clustering phenomenon in turn\nprovides a coherent explanation for the behavioral dynamics observed across\ntraining, linking representational structure to reasoning capability. These\nfindings provide new insights into the interpretability of implicit multi-hop\nreasoning in LLMs, helping to clarify how complex reasoning processes unfold\ninternally and offering pathways to enhance the transparency of such models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work suggests that large language models (LLMs) can perform multi-hop\nreasoning implicitly -- producing correct answers without explicitly\nverbalizing intermediate steps -- but the underlying mechanisms remain poorly\nunderstood. In this paper, we study how such implicit reasoning emerges by\ntraining transformers from scratch in a controlled symbolic environment. Our\nanalysis reveals a three-stage developmental trajectory: early memorization,\nfollowed by in-distribution generalization, and eventually cross-distribution\ngeneralization. We find that training with atomic triples is not necessary but\naccelerates learning, and that second-hop generalization relies on query-level\nexposure to specific compositional structures. To interpret these behaviors, we\nintroduce two diagnostic tools: cross-query semantic patching, which identifies\nsemantically reusable intermediate representations, and a cosine-based\nrepresentational lens, which reveals that successful reasoning correlates with\nthe cosine-base clustering in hidden space. This clustering phenomenon in turn\nprovides a coherent explanation for the behavioral dynamics observed across\ntraining, linking representational structure to reasoning capability. These\nfindings provide new insights into the interpretability of implicit multi-hop\nreasoning in LLMs, helping to clarify how complex reasoning processes unfold\ninternally and offering pathways to enhance the transparency of such models."
                },
                "authors": [
                    {
                        "name": "Jiaran Ye"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Zhidian Huang"
                    },
                    {
                        "name": "Liangming Pan"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Amy Xin"
                    },
                    {
                        "name": "Liu Weichuan"
                    },
                    {
                        "name": "Xiaoyin Che"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19529v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19529v2",
                "updated": "2025-05-29T16:57:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    57,
                    36,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-26T05:29:47Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    29,
                    47,
                    0,
                    146,
                    0
                ],
                "title": "Small Language Models: Architectures, Techniques, Evaluation, Problems\n  and Future Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Models: Architectures, Techniques, Evaluation, Problems\n  and Future Adaptation"
                },
                "summary": "Small Language Models (SLMs) have gained substantial attention due to their\nability to execute diverse language tasks successfully while using fewer\ncomputer resources. These models are particularly ideal for deployment in\nlimited environments, such as mobile devices, on-device processing, and edge\nsystems. In this study, we present a complete assessment of SLMs, focussing on\ntheir design frameworks, training approaches, and techniques for lowering model\nsize and complexity. We offer a novel classification system to organize the\noptimization approaches applied for SLMs, encompassing strategies like pruning,\nquantization, and model compression. Furthermore, we assemble SLM's studies of\nevaluation suite with some existing datasets, establishing a rigorous platform\nfor measuring SLM capabilities. Alongside this, we discuss the important\ndifficulties that remain unresolved in this sector, including trade-offs\nbetween efficiency and performance, and we suggest directions for future study.\nWe anticipate this study to serve as a beneficial guide for researchers and\npractitioners who aim to construct compact, efficient, and high-performing\nlanguage models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Models (SLMs) have gained substantial attention due to their\nability to execute diverse language tasks successfully while using fewer\ncomputer resources. These models are particularly ideal for deployment in\nlimited environments, such as mobile devices, on-device processing, and edge\nsystems. In this study, we present a complete assessment of SLMs, focussing on\ntheir design frameworks, training approaches, and techniques for lowering model\nsize and complexity. We offer a novel classification system to organize the\noptimization approaches applied for SLMs, encompassing strategies like pruning,\nquantization, and model compression. Furthermore, we assemble SLM's studies of\nevaluation suite with some existing datasets, establishing a rigorous platform\nfor measuring SLM capabilities. Alongside this, we discuss the important\ndifficulties that remain unresolved in this sector, including trade-offs\nbetween efficiency and performance, and we suggest directions for future study.\nWe anticipate this study to serve as a beneficial guide for researchers and\npractitioners who aim to construct compact, efficient, and high-performing\nlanguage models."
                },
                "authors": [
                    {
                        "name": "Tanjil Hasan Sakib"
                    },
                    {
                        "name": "Md. Tanzib Hosain"
                    },
                    {
                        "name": "Md. Kishor Morol"
                    }
                ],
                "author_detail": {
                    "name": "Md. Kishor Morol"
                },
                "author": "Md. Kishor Morol",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19529v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19529v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08805v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08805v3",
                "updated": "2025-05-29T16:48:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    48,
                    53,
                    3,
                    149,
                    0
                ],
                "published": "2024-12-11T22:37:45Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    22,
                    37,
                    45,
                    2,
                    346,
                    0
                ],
                "title": "Generative Agents for Multi-Agent Autoformalization of Interaction\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Agents for Multi-Agent Autoformalization of Interaction\n  Scenarios"
                },
                "summary": "Multi-agent simulations are versatile tools for exploring interactions among\nnatural and artificial agents, but their development typically demands domain\nexpertise and manual effort. This work introduces the Generative Agents for\nMulti-Agent Autoformalization (GAMA) framework, which automates the\nformalization of interaction scenarios in simulations using agents augmented\nwith large language models (LLMs). To demonstrate the application of GAMA, we\nuse natural language descriptions of game-theoretic scenarios representing\nsocial interactions, and we autoformalize them into executable logic programs\ndefining game rules, with syntactic correctness enforced through a solver-based\nvalidation. To ensure runtime validity, an iterative, tournament-based\nprocedure tests the generated rules and strategies, followed by exact semantic\nvalidation when ground truth outcomes are available. In experiments with 110\nnatural language descriptions across five 2x2 simultaneous-move games, GAMA\nachieves 100% syntactic and 76.5% semantic correctness with Claude 3.5 Sonnet,\nand 99.82% syntactic and 77% semantic correctness with GPT-4o. The framework\nalso shows high semantic accuracy in autoformalizing agents' strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent simulations are versatile tools for exploring interactions among\nnatural and artificial agents, but their development typically demands domain\nexpertise and manual effort. This work introduces the Generative Agents for\nMulti-Agent Autoformalization (GAMA) framework, which automates the\nformalization of interaction scenarios in simulations using agents augmented\nwith large language models (LLMs). To demonstrate the application of GAMA, we\nuse natural language descriptions of game-theoretic scenarios representing\nsocial interactions, and we autoformalize them into executable logic programs\ndefining game rules, with syntactic correctness enforced through a solver-based\nvalidation. To ensure runtime validity, an iterative, tournament-based\nprocedure tests the generated rules and strategies, followed by exact semantic\nvalidation when ground truth outcomes are available. In experiments with 110\nnatural language descriptions across five 2x2 simultaneous-move games, GAMA\nachieves 100% syntactic and 76.5% semantic correctness with Claude 3.5 Sonnet,\nand 99.82% syntactic and 77% semantic correctness with GPT-4o. The framework\nalso shows high semantic accuracy in autoformalizing agents' strategies."
                },
                "authors": [
                    {
                        "name": "Agnieszka Mensfelt"
                    },
                    {
                        "name": "Kostas Stathis"
                    },
                    {
                        "name": "Vince Trencsenyi"
                    }
                ],
                "author_detail": {
                    "name": "Vince Trencsenyi"
                },
                "author": "Vince Trencsenyi",
                "arxiv_comment": "code: https://github.com/dicelab-rhul/GAMA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08805v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08805v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04358v2",
                "updated": "2025-05-29T16:46:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    46,
                    0,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-04T20:47:43Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    20,
                    47,
                    43,
                    1,
                    35,
                    0
                ],
                "title": "Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM\n  Primitives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM\n  Primitives"
                },
                "summary": "Decomposing hard problems into subproblems often makes them easier and more\nefficient to solve. With large language models (LLMs) crossing critical\nreliability thresholds for a growing slate of capabilities, there is an\nincreasing effort to decompose systems into sets of LLM-based agents, each of\nwhom can be delegated sub-tasks. However, this decomposition (even when\nautomated) is often intuitive, e.g., based on how a human might assign roles to\nmembers of a human team. How close are these role decompositions to optimal?\nThis position paper argues that asymptotic analysis with LLM primitives is\nneeded to reason about the efficiency of such decomposed systems, and that\ninsights from such analysis will unlock opportunities for scaling them. By\ntreating the LLM forward pass as the atomic unit of computational cost, one can\nseparate out the (often opaque) inner workings of a particular LLM from the\ninherent efficiency of how a set of LLMs are orchestrated to solve hard\nproblems. In other words, if we want to scale the deployment of LLMs to the\nlimit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM\nprimitives should be used to reason about and develop more powerful\ndecompositions of large problems into LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decomposing hard problems into subproblems often makes them easier and more\nefficient to solve. With large language models (LLMs) crossing critical\nreliability thresholds for a growing slate of capabilities, there is an\nincreasing effort to decompose systems into sets of LLM-based agents, each of\nwhom can be delegated sub-tasks. However, this decomposition (even when\nautomated) is often intuitive, e.g., based on how a human might assign roles to\nmembers of a human team. How close are these role decompositions to optimal?\nThis position paper argues that asymptotic analysis with LLM primitives is\nneeded to reason about the efficiency of such decomposed systems, and that\ninsights from such analysis will unlock opportunities for scaling them. By\ntreating the LLM forward pass as the atomic unit of computational cost, one can\nseparate out the (often opaque) inner workings of a particular LLM from the\ninherent efficiency of how a set of LLMs are orchestrated to solve hard\nproblems. In other words, if we want to scale the deployment of LLMs to the\nlimit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM\nprimitives should be used to reason about and develop more powerful\ndecompositions of large problems into LLM agents."
                },
                "authors": [
                    {
                        "name": "Elliot Meyerson"
                    },
                    {
                        "name": "Xin Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Qiu"
                },
                "author": "Xin Qiu",
                "arxiv_comment": "In Proceedings of the 42nd International Conference on Machine\n  Learning (ICML 2025); 13 pages including references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14279v2",
                "updated": "2025-05-29T16:45:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    45,
                    0,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-20T12:30:46Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    12,
                    30,
                    46,
                    1,
                    140,
                    0
                ],
                "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering"
                },
                "summary": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry."
                },
                "authors": [
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Hamed Babaei Giglou"
                    },
                    {
                        "name": "Quentin Münch"
                    }
                ],
                "author_detail": {
                    "name": "Quentin Münch"
                },
                "author": "Quentin Münch",
                "arxiv_comment": "9 pages, 4 figures, Accepted as a Long Paper at the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23634v1",
                "updated": "2025-05-29T16:44:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    44,
                    29,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:44:29Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    44,
                    29,
                    3,
                    149,
                    0
                ],
                "title": "MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits\n  using Improved Preference Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits\n  using Improved Preference Alignment"
                },
                "summary": "The model context protocol (MCP) has been widely adapted as an open standard\nenabling the seamless integration of generative AI agents. However, recent work\nhas shown the MCP is susceptible to retrieval-based \"falsely benign\" attacks\n(FBAs), allowing malicious system access and credential theft, but requiring\nthat users download compromised files directly to their systems. Herein, we\nshow that the threat model of MCP-based attacks is significantly broader than\npreviously thought, i.e., attackers need only post malicious content online to\ndeceive MCP agents into carrying out their attacks on unsuspecting victims'\nsystems.\n  To improve alignment guardrails against such attacks, we introduce a new MCP\ndataset of FBAs and (truly) benign samples to explore the effectiveness of\ndirect preference optimization (DPO) for the refusal training of large language\nmodels (LLMs). While DPO improves model guardrails against such attacks, we\nshow that the efficacy of refusal learning varies drastically depending on the\nmodel's original post-training alignment scheme--e.g., GRPO-based LLMs learn to\nrefuse extremely poorly. Thus, to further improve FBA refusals, we introduce\nRetrieval Augmented Generation for Preference alignment (RAG-Pref), a novel\npreference alignment strategy based on RAG. We show that RAG-Pref significantly\nimproves the ability of LLMs to refuse FBAs, particularly when combined with\nDPO alignment, thus drastically improving guardrails against MCP-based attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The model context protocol (MCP) has been widely adapted as an open standard\nenabling the seamless integration of generative AI agents. However, recent work\nhas shown the MCP is susceptible to retrieval-based \"falsely benign\" attacks\n(FBAs), allowing malicious system access and credential theft, but requiring\nthat users download compromised files directly to their systems. Herein, we\nshow that the threat model of MCP-based attacks is significantly broader than\npreviously thought, i.e., attackers need only post malicious content online to\ndeceive MCP agents into carrying out their attacks on unsuspecting victims'\nsystems.\n  To improve alignment guardrails against such attacks, we introduce a new MCP\ndataset of FBAs and (truly) benign samples to explore the effectiveness of\ndirect preference optimization (DPO) for the refusal training of large language\nmodels (LLMs). While DPO improves model guardrails against such attacks, we\nshow that the efficacy of refusal learning varies drastically depending on the\nmodel's original post-training alignment scheme--e.g., GRPO-based LLMs learn to\nrefuse extremely poorly. Thus, to further improve FBA refusals, we introduce\nRetrieval Augmented Generation for Preference alignment (RAG-Pref), a novel\npreference alignment strategy based on RAG. We show that RAG-Pref significantly\nimproves the ability of LLMs to refuse FBAs, particularly when combined with\nDPO alignment, thus drastically improving guardrails against MCP-based attacks."
                },
                "authors": [
                    {
                        "name": "John Halloran"
                    }
                ],
                "author_detail": {
                    "name": "John Halloran"
                },
                "author": "John Halloran",
                "arxiv_comment": "27 pages, 19 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23628v1",
                "updated": "2025-05-29T16:34:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    34,
                    58,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:34:58Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    34,
                    58,
                    3,
                    149,
                    0
                ],
                "title": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic\n  Schema Induction from Web-Scale Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic\n  Schema Induction from Web-Scale Corpora"
                },
                "summary": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph\nconstruction that eliminates the need for predefined schemas. Our system\nleverages large language models to simultaneously extract knowledge triples and\ninduce comprehensive schemas directly from text, modeling both entities and\nevents while employing conceptualization to organize instances into semantic\ncategories. Processing over 50 million documents, we construct ATLAS (Automated\nTriple Linking And Schema induction), a family of knowledge graphs with 900+\nmillion nodes and 5.9 billion edges. This approach outperforms state-of-the-art\nbaselines on multi-hop QA tasks and enhances LLM factuality. Notably, our\nschema induction achieves 95\\% semantic alignment with human-crafted schemas\nwith zero manual intervention, demonstrating that billion-scale knowledge\ngraphs with dynamically induced schemas can effectively complement parametric\nknowledge in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph\nconstruction that eliminates the need for predefined schemas. Our system\nleverages large language models to simultaneously extract knowledge triples and\ninduce comprehensive schemas directly from text, modeling both entities and\nevents while employing conceptualization to organize instances into semantic\ncategories. Processing over 50 million documents, we construct ATLAS (Automated\nTriple Linking And Schema induction), a family of knowledge graphs with 900+\nmillion nodes and 5.9 billion edges. This approach outperforms state-of-the-art\nbaselines on multi-hop QA tasks and enhances LLM factuality. Notably, our\nschema induction achieves 95\\% semantic alignment with human-crafted schemas\nwith zero manual intervention, demonstrating that billion-scale knowledge\ngraphs with dynamically induced schemas can effectively complement parametric\nknowledge in large language models."
                },
                "authors": [
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Qing Zong"
                    },
                    {
                        "name": "Chunyang Li"
                    },
                    {
                        "name": "Hong Ting Tsang"
                    },
                    {
                        "name": "Hongyu Luo"
                    },
                    {
                        "name": "Yauwai Yim"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Xiao Zhou"
                    },
                    {
                        "name": "Feng Qin"
                    },
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Xi Peng"
                    },
                    {
                        "name": "Xin Yao"
                    },
                    {
                        "name": "Huiwen Yang"
                    },
                    {
                        "name": "Leijie Wu"
                    },
                    {
                        "name": "Yi Ji"
                    },
                    {
                        "name": "Gong Zhang"
                    },
                    {
                        "name": "Renhai Chen"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "9 pages, preprint, code:\n  https://github.com/HKUST-KnowComp/AutoSchemaKG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14669v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14669v2",
                "updated": "2025-05-29T16:32:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    32,
                    48,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-20T17:55:50Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    55,
                    50,
                    1,
                    140,
                    0
                ],
                "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models"
                },
                "summary": "Training large language models (LLMs) models directly in low-precision offers\na way to address computational costs by improving both throughput and energy\nefficiency. For those purposes, NVIDIA's recent Blackwell architecture\nfacilitates very low-precision operations using FP4 variants. Yet, current\nalgorithms for training LLMs in FP4 precision face significant accuracy\ndegradation and often rely on mixed-precision fallbacks. In this paper, we\ninvestigate hardware-supported FP4 training and introduce a new approach for\naccurate, end-to-end FP4 training with all the major computations (i.e., linear\nlayers) in low precision. Through extensive evaluations on Llama-type models,\nwe reveal a new low-precision scaling law that quantifies performance\ntrade-offs across bit-widths and training setups. Guided by this investigation,\nwe design an \"optimal\" technique in terms of accuracy-vs-computation, called\nQuartet. We implement Quartet using optimized CUDA kernels tailored for\nBlackwell, demonstrating that fully FP4-based training is a competitive\nalternative to FP16 half-precision and to FP8 training. Our code is available\nat https://github.com/IST-DASLab/Quartet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) models directly in low-precision offers\na way to address computational costs by improving both throughput and energy\nefficiency. For those purposes, NVIDIA's recent Blackwell architecture\nfacilitates very low-precision operations using FP4 variants. Yet, current\nalgorithms for training LLMs in FP4 precision face significant accuracy\ndegradation and often rely on mixed-precision fallbacks. In this paper, we\ninvestigate hardware-supported FP4 training and introduce a new approach for\naccurate, end-to-end FP4 training with all the major computations (i.e., linear\nlayers) in low precision. Through extensive evaluations on Llama-type models,\nwe reveal a new low-precision scaling law that quantifies performance\ntrade-offs across bit-widths and training setups. Guided by this investigation,\nwe design an \"optimal\" technique in terms of accuracy-vs-computation, called\nQuartet. We implement Quartet using optimized CUDA kernels tailored for\nBlackwell, demonstrating that fully FP4-based training is a competitive\nalternative to FP16 half-precision and to FP8 training. Our code is available\nat https://github.com/IST-DASLab/Quartet."
                },
                "authors": [
                    {
                        "name": "Roberto L. Castro"
                    },
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Soroush Tabesh"
                    },
                    {
                        "name": "Oliver Sieberling"
                    },
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Mahdi Nikdan"
                    },
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14669v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14669v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16502v2",
                "updated": "2025-05-29T16:31:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    31,
                    57,
                    3,
                    149,
                    0
                ],
                "published": "2024-10-21T20:48:16Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    20,
                    48,
                    16,
                    0,
                    295,
                    0
                ],
                "title": "RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic\n  and Human-like Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic\n  and Human-like Reasoning"
                },
                "summary": "Formal logic enables computers to reason in natural language by representing\nsentences in symbolic forms and applying rules to derive conclusions. However,\nin what our study characterizes as \"rulebreaker\" scenarios, this method can\nlead to conclusions that are typically not inferred or accepted by humans given\ntheir common sense and factual knowledge. Inspired by works in cognitive\nscience, we create RULEBREAKERS, the first dataset for rigorously evaluating\nthe ability of large language models (LLMs) to recognize and respond to\nrulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven\nLLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on\nRULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules\nunlike what is expected from typical human reasoners. Further analysis suggests\nthat this apparent failure is potentially associated with the models' poor\nutilization of their world knowledge and their attention distribution patterns.\nWhilst revealing a limitation of current LLMs, our study also provides a timely\ncounterbalance to a growing body of recent works that propose methods relying\non formal logic to improve LLMs' general reasoning capabilities, highlighting\ntheir risk of further increasing divergence between LLMs and human-like\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal logic enables computers to reason in natural language by representing\nsentences in symbolic forms and applying rules to derive conclusions. However,\nin what our study characterizes as \"rulebreaker\" scenarios, this method can\nlead to conclusions that are typically not inferred or accepted by humans given\ntheir common sense and factual knowledge. Inspired by works in cognitive\nscience, we create RULEBREAKERS, the first dataset for rigorously evaluating\nthe ability of large language models (LLMs) to recognize and respond to\nrulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven\nLLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on\nRULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules\nunlike what is expected from typical human reasoners. Further analysis suggests\nthat this apparent failure is potentially associated with the models' poor\nutilization of their world knowledge and their attention distribution patterns.\nWhilst revealing a limitation of current LLMs, our study also provides a timely\ncounterbalance to a growing body of recent works that propose methods relying\non formal logic to improve LLMs' general reasoning capabilities, highlighting\ntheir risk of further increasing divergence between LLMs and human-like\nreasoning."
                },
                "authors": [
                    {
                        "name": "Jason Chan"
                    },
                    {
                        "name": "Robert Gaizauskas"
                    },
                    {
                        "name": "Zhixue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhixue Zhao"
                },
                "author": "Zhixue Zhao",
                "arxiv_comment": "Preprint. Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23621v1",
                "updated": "2025-05-29T16:28:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    28,
                    50,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:28:50Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    28,
                    50,
                    3,
                    149,
                    0
                ],
                "title": "Table-R1: Inference-Time Scaling for Table Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table-R1: Inference-Time Scaling for Table Reasoning"
                },
                "summary": "In this work, we present the first study to explore inference-time scaling on\ntable reasoning tasks. We develop and evaluate two post-training strategies to\nenable inference-time scaling: distillation from frontier model reasoning\ntraces and reinforcement learning with verifiable rewards (RLVR). For\ndistillation, we introduce a large-scale dataset of reasoning traces generated\nby DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For\nRLVR, we propose task-specific verifiable reward functions and apply the GRPO\nalgorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series\nmodels across diverse table reasoning tasks, including short-form QA, fact\nverification, and free-form QA. Notably, the Table-R1-Zero model matches or\nexceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a\n7B-parameter LLM. It also demonstrates strong generalization to out-of-domain\ndatasets. Extensive ablation and qualitative analyses reveal the benefits of\ninstruction tuning, model architecture choices, and cross-task generalization,\nas well as emergence of essential table reasoning skills during RL training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present the first study to explore inference-time scaling on\ntable reasoning tasks. We develop and evaluate two post-training strategies to\nenable inference-time scaling: distillation from frontier model reasoning\ntraces and reinforcement learning with verifiable rewards (RLVR). For\ndistillation, we introduce a large-scale dataset of reasoning traces generated\nby DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For\nRLVR, we propose task-specific verifiable reward functions and apply the GRPO\nalgorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series\nmodels across diverse table reasoning tasks, including short-form QA, fact\nverification, and free-form QA. Notably, the Table-R1-Zero model matches or\nexceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a\n7B-parameter LLM. It also demonstrates strong generalization to out-of-domain\ndatasets. Extensive ablation and qualitative analyses reveal the benefits of\ninstruction tuning, model architecture choices, and cross-task generalization,\nas well as emergence of essential table reasoning skills during RL training."
                },
                "authors": [
                    {
                        "name": "Zheyuan Yang"
                    },
                    {
                        "name": "Lyuhao Chen"
                    },
                    {
                        "name": "Arman Cohan"
                    },
                    {
                        "name": "Yilun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Zhao"
                },
                "author": "Yilun Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23611v1",
                "updated": "2025-05-29T16:19:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    19,
                    12,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:19:12Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    19,
                    12,
                    3,
                    149,
                    0
                ],
                "title": "Optimizing Flexible Complex Systems with Coupled and Co-Evolving\n  Subsystems under Operational Uncertainties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Flexible Complex Systems with Coupled and Co-Evolving\n  Subsystems under Operational Uncertainties"
                },
                "summary": "The paper develops a novel design optimization framework and associated\ncomputational techniques for staged deployment optimization of complex systems\nunder operational uncertainties. It proposes a local scenario discretization\nmethod that offers a computationally efficient approach to optimize staged\nco-deployment of multiple coupled subsystems by decoupling weak dynamic\ninteraction among subsystems. The proposed method is applied to a case study\nand is demonstrated to provide an effective and scalable strategy to determine\nthe optimal and flexible systems design under uncertainty. The developed\noptimization framework is expected to improve the staged deployment design of\nvarious complex engineering systems, such as water, energy, food, and other\ninfrastructure systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper develops a novel design optimization framework and associated\ncomputational techniques for staged deployment optimization of complex systems\nunder operational uncertainties. It proposes a local scenario discretization\nmethod that offers a computationally efficient approach to optimize staged\nco-deployment of multiple coupled subsystems by decoupling weak dynamic\ninteraction among subsystems. The proposed method is applied to a case study\nand is demonstrated to provide an effective and scalable strategy to determine\nthe optimal and flexible systems design under uncertainty. The developed\noptimization framework is expected to improve the staged deployment design of\nvarious complex engineering systems, such as water, energy, food, and other\ninfrastructure systems."
                },
                "authors": [
                    {
                        "name": "Koki Ho"
                    },
                    {
                        "name": "Masafumi Isaji"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Isaji"
                },
                "author": "Masafumi Isaji",
                "arxiv_comment": "15 pages, 1 figure; Under Review by Systems Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11942v3",
                "updated": "2025-05-30T02:28:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    28,
                    21,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-17T10:09:11Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    10,
                    9,
                    11,
                    5,
                    137,
                    0
                ],
                "title": "LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners"
                },
                "summary": "Lifelong learning is essential for intelligent agents operating in dynamic\nenvironments. Current large language model (LLM)-based agents, however, remain\nstateless and unable to accumulate or transfer knowledge over time. Existing\nbenchmarks treat agents as static systems and fail to evaluate lifelong\nlearning capabilities. We present LifelongAgentBench, the first unified\nbenchmark designed to systematically assess the lifelong learning ability of\nLLM agents. It provides skill-grounded, interdependent tasks across three\ninteractive environments, Database, Operating System, and Knowledge Graph, with\nautomatic label verification, reproducibility, and modular extensibility.\nExtensive experiments reveal that conventional experience replay has limited\neffectiveness for LLM agents due to irrelevant information and context length\nconstraints. We further introduce a group self-consistency mechanism that\nsignificantly improves lifelong learning performance. We hope\nLifelongAgentBench will advance the development of adaptive, memory-capable LLM\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifelong learning is essential for intelligent agents operating in dynamic\nenvironments. Current large language model (LLM)-based agents, however, remain\nstateless and unable to accumulate or transfer knowledge over time. Existing\nbenchmarks treat agents as static systems and fail to evaluate lifelong\nlearning capabilities. We present LifelongAgentBench, the first unified\nbenchmark designed to systematically assess the lifelong learning ability of\nLLM agents. It provides skill-grounded, interdependent tasks across three\ninteractive environments, Database, Operating System, and Knowledge Graph, with\nautomatic label verification, reproducibility, and modular extensibility.\nExtensive experiments reveal that conventional experience replay has limited\neffectiveness for LLM agents due to irrelevant information and context length\nconstraints. We further introduce a group self-consistency mechanism that\nsignificantly improves lifelong learning performance. We hope\nLifelongAgentBench will advance the development of adaptive, memory-capable LLM\nagents."
                },
                "authors": [
                    {
                        "name": "Junhao Zheng"
                    },
                    {
                        "name": "Xidi Cai"
                    },
                    {
                        "name": "Qiuke Li"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "ZhongZhi Li"
                    },
                    {
                        "name": "Yingying Zhang"
                    },
                    {
                        "name": "Le Song"
                    },
                    {
                        "name": "Qianli Ma"
                    }
                ],
                "author_detail": {
                    "name": "Qianli Ma"
                },
                "author": "Qianli Ma",
                "arxiv_comment": "Project Page: https://caixd-220529.github.io/LifelongAgentBench/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14276v2",
                "updated": "2025-05-29T16:13:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    13,
                    21,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-20T05:28:44Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    28,
                    44,
                    3,
                    51,
                    0
                ],
                "title": "STeCa: Step-level Trajectory Calibration for LLM Agent Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STeCa: Step-level Trajectory Calibration for LLM Agent Learning"
                },
                "summary": "Large language model (LLM)-based agents have shown promise in tackling\ncomplex tasks by interacting dynamically with the environment. Existing work\nprimarily focuses on behavior cloning from expert demonstrations or preference\nlearning through exploratory trajectory sampling. However, these methods often\nstruggle to address long-horizon tasks, where suboptimal actions accumulate\nstep by step, causing agents to deviate from correct task trajectories. To\naddress this, we highlight the importance of timely calibration and the need to\nautomatically construct calibration trajectories for training agents. We\npropose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM\nagent learning. Specifically, STeCa identifies suboptimal actions through a\nstep-level reward comparison during exploration. It constructs calibrated\ntrajectories using LLM-driven reflection, enabling agents to learn from\nimproved decision-making processes. We finally leverage these calibrated\ntrajectories with successful trajectories for reinforced training. Extensive\nexperiments demonstrate that STeCa significantly outperforms existing methods.\nFurther analysis highlights that timely calibration enables agents to complete\ntasks with greater robustness. Our code and data are available at\nhttps://github.com/WangHanLinHenry/STeCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based agents have shown promise in tackling\ncomplex tasks by interacting dynamically with the environment. Existing work\nprimarily focuses on behavior cloning from expert demonstrations or preference\nlearning through exploratory trajectory sampling. However, these methods often\nstruggle to address long-horizon tasks, where suboptimal actions accumulate\nstep by step, causing agents to deviate from correct task trajectories. To\naddress this, we highlight the importance of timely calibration and the need to\nautomatically construct calibration trajectories for training agents. We\npropose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM\nagent learning. Specifically, STeCa identifies suboptimal actions through a\nstep-level reward comparison during exploration. It constructs calibrated\ntrajectories using LLM-driven reflection, enabling agents to learn from\nimproved decision-making processes. We finally leverage these calibrated\ntrajectories with successful trajectories for reinforced training. Extensive\nexperiments demonstrate that STeCa significantly outperforms existing methods.\nFurther analysis highlights that timely calibration enables agents to complete\ntasks with greater robustness. Our code and data are available at\nhttps://github.com/WangHanLinHenry/STeCa."
                },
                "authors": [
                    {
                        "name": "Hanlin Wang"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Chak Tou Leong"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "arxiv_comment": "Accepted by ACL2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23598v1",
                "updated": "2025-05-29T16:11:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    11,
                    18,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:11:18Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    11,
                    18,
                    3,
                    149,
                    0
                ],
                "title": "LLM Performance for Code Generation on Noisy Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Performance for Code Generation on Noisy Tasks"
                },
                "summary": "This paper investigates the ability of large language models (LLMs) to\nrecognise and solve tasks which have been obfuscated beyond recognition.\nFocusing on competitive programming and benchmark tasks (LeetCode and MATH), we\ncompare performance across multiple models and obfuscation methods, such as\nnoise and redaction. We demonstrate that all evaluated LLMs can solve tasks\nobfuscated to a level where the text would be unintelligible to human readers,\nand does not contain key pieces of instruction or context. We introduce the\nconcept of eager pattern matching to describe this behaviour, which is not\nobserved in tasks published after the models' knowledge cutoff date, indicating\nstrong memorisation or overfitting to training data, rather than legitimate\nreasoning about the presented problem. We report empirical evidence of distinct\nperformance decay patterns between contaminated and unseen datasets. We discuss\nthe implications for benchmarking and evaluations of model behaviour, arguing\nfor caution when designing experiments using standard datasets. We also propose\nmeasuring the decay of performance under obfuscation as a possible strategy for\ndetecting dataset contamination and highlighting potential safety risks and\ninterpretability issues for automated software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the ability of large language models (LLMs) to\nrecognise and solve tasks which have been obfuscated beyond recognition.\nFocusing on competitive programming and benchmark tasks (LeetCode and MATH), we\ncompare performance across multiple models and obfuscation methods, such as\nnoise and redaction. We demonstrate that all evaluated LLMs can solve tasks\nobfuscated to a level where the text would be unintelligible to human readers,\nand does not contain key pieces of instruction or context. We introduce the\nconcept of eager pattern matching to describe this behaviour, which is not\nobserved in tasks published after the models' knowledge cutoff date, indicating\nstrong memorisation or overfitting to training data, rather than legitimate\nreasoning about the presented problem. We report empirical evidence of distinct\nperformance decay patterns between contaminated and unseen datasets. We discuss\nthe implications for benchmarking and evaluations of model behaviour, arguing\nfor caution when designing experiments using standard datasets. We also propose\nmeasuring the decay of performance under obfuscation as a possible strategy for\ndetecting dataset contamination and highlighting potential safety risks and\ninterpretability issues for automated software systems."
                },
                "authors": [
                    {
                        "name": "Radzim Sendyka"
                    },
                    {
                        "name": "Christian Cabrera"
                    },
                    {
                        "name": "Andrei Paleyes"
                    },
                    {
                        "name": "Diana Robinson"
                    },
                    {
                        "name": "Neil Lawrence"
                    }
                ],
                "author_detail": {
                    "name": "Neil Lawrence"
                },
                "author": "Neil Lawrence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23595v1",
                "updated": "2025-05-29T16:08:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    8,
                    26,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T16:08:26Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    8,
                    26,
                    3,
                    149,
                    0
                ],
                "title": "DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task\n  Learning in Chest X-ray Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task\n  Learning in Chest X-ray Classification"
                },
                "summary": "While Multi-Task Learning (MTL) offers inherent advantages in complex domains\nsuch as medical imaging by enabling shared representation learning, effectively\nbalancing task contributions remains a significant challenge. This paper\naddresses this critical issue by introducing DeepChest, a novel,\ncomputationally efficient and effective dynamic task-weighting framework\nspecifically designed for multi-label chest X-ray (CXR) classification. Unlike\nexisting heuristic or gradient-based methods that often incur substantial\noverhead, DeepChest leverages a performance-driven weighting mechanism based on\neffective analysis of task-specific loss trends. Given a network architecture\n(e.g., ResNet18), our model-agnostic approach adaptively adjusts task\nimportance without requiring gradient access, thereby significantly reducing\nmemory usage and achieving a threefold increase in training speed. It can be\neasily applied to improve various state-of-the-art methods. Extensive\nexperiments on a large-scale CXR dataset demonstrate that DeepChest not only\noutperforms state-of-the-art MTL methods by 7% in overall accuracy but also\nyields substantial reductions in individual task losses, indicating improved\ngeneralization and effective mitigation of negative transfer. The efficiency\nand performance gains of DeepChest pave the way for more practical and robust\ndeployment of deep learning in critical medical diagnostic applications. The\ncode is publicly available at https://github.com/youssefkhalil320/DeepChest-MTL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multi-Task Learning (MTL) offers inherent advantages in complex domains\nsuch as medical imaging by enabling shared representation learning, effectively\nbalancing task contributions remains a significant challenge. This paper\naddresses this critical issue by introducing DeepChest, a novel,\ncomputationally efficient and effective dynamic task-weighting framework\nspecifically designed for multi-label chest X-ray (CXR) classification. Unlike\nexisting heuristic or gradient-based methods that often incur substantial\noverhead, DeepChest leverages a performance-driven weighting mechanism based on\neffective analysis of task-specific loss trends. Given a network architecture\n(e.g., ResNet18), our model-agnostic approach adaptively adjusts task\nimportance without requiring gradient access, thereby significantly reducing\nmemory usage and achieving a threefold increase in training speed. It can be\neasily applied to improve various state-of-the-art methods. Extensive\nexperiments on a large-scale CXR dataset demonstrate that DeepChest not only\noutperforms state-of-the-art MTL methods by 7% in overall accuracy but also\nyields substantial reductions in individual task losses, indicating improved\ngeneralization and effective mitigation of negative transfer. The efficiency\nand performance gains of DeepChest pave the way for more practical and robust\ndeployment of deep learning in critical medical diagnostic applications. The\ncode is publicly available at https://github.com/youssefkhalil320/DeepChest-MTL"
                },
                "authors": [
                    {
                        "name": "Youssef Mohamed"
                    },
                    {
                        "name": "Noran Mohamed"
                    },
                    {
                        "name": "Khaled Abouhashad"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Sara Atito"
                    },
                    {
                        "name": "Shoaib Jameel"
                    },
                    {
                        "name": "Imran Razzak"
                    },
                    {
                        "name": "Ahmed B. Zaky"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed B. Zaky"
                },
                "author": "Ahmed B. Zaky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09853v2",
                "updated": "2025-05-29T16:08:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    8,
                    23,
                    3,
                    149,
                    0
                ],
                "published": "2024-08-19T09:57:28Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    57,
                    28,
                    0,
                    232,
                    0
                ],
                "title": "X-TURING: Towards an Enhanced and Efficient Turing Test for Long-Term\n  Dialogue Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-TURING: Towards an Enhanced and Efficient Turing Test for Long-Term\n  Dialogue Agents"
                },
                "summary": "The Turing test examines whether AIs exhibit human-like behaviour in natural\nlanguage conversations. The traditional setting limits each participant to one\nmessage at a time and requires constant human participation. This fails to\nreflect a natural conversational style and hinders the evaluation of dialogue\nagents based on Large Language Models (LLMs) in complex and prolonged\ninteractions. This paper proposes \\textbf{\\textsc{X-Turing}}, which enhances\nthe original test with a \\textit{burst dialogue} pattern, allowing more dynamic\nexchanges using consecutive messages. It further reduces human workload by\niteratively generating dialogues that simulate the long-term interaction\nbetween the agent and a human to compose the majority of the test process. With\nthe \\textit{pseudo-dialogue} history, the agent then engages in a shorter\ndialogue with a real human, which is paired with a human-human conversation on\nthe same topic to be judged using questionnaires. We introduce the\n\\textit{X-Turn Pass-Rate} metric to assess the human likeness of LLMs across\nvarying durations. While LLMs like GPT-4 initially perform well, achieving pass\nrates of 51.9\\% and 38.9\\% during 3 turns and 10 turns of dialogues\nrespectively, their performance drops as the dialogue progresses, which\nunderscores the difficulty in maintaining consistency in the long term.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Turing test examines whether AIs exhibit human-like behaviour in natural\nlanguage conversations. The traditional setting limits each participant to one\nmessage at a time and requires constant human participation. This fails to\nreflect a natural conversational style and hinders the evaluation of dialogue\nagents based on Large Language Models (LLMs) in complex and prolonged\ninteractions. This paper proposes \\textbf{\\textsc{X-Turing}}, which enhances\nthe original test with a \\textit{burst dialogue} pattern, allowing more dynamic\nexchanges using consecutive messages. It further reduces human workload by\niteratively generating dialogues that simulate the long-term interaction\nbetween the agent and a human to compose the majority of the test process. With\nthe \\textit{pseudo-dialogue} history, the agent then engages in a shorter\ndialogue with a real human, which is paired with a human-human conversation on\nthe same topic to be judged using questionnaires. We introduce the\n\\textit{X-Turn Pass-Rate} metric to assess the human likeness of LLMs across\nvarying durations. While LLMs like GPT-4 initially perform well, achieving pass\nrates of 51.9\\% and 38.9\\% during 3 turns and 10 turns of dialogues\nrespectively, their performance drops as the dialogue progresses, which\nunderscores the difficulty in maintaining consistency in the long term."
                },
                "authors": [
                    {
                        "name": "Weiqi Wu"
                    },
                    {
                        "name": "Hongqiu Wu"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Accepted to ACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23580v1",
                "updated": "2025-05-29T15:53:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    53,
                    21,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:53:21Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    53,
                    21,
                    3,
                    149,
                    0
                ],
                "title": "Engineering Serendipity through Recommendations of Items with Atypical\n  Aspects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering Serendipity through Recommendations of Items with Atypical\n  Aspects"
                },
                "summary": "A restaurant dinner or a hotel stay may lead to memorable experiences when\nguests encounter unexpected aspects that also match their interests. For\nexample, an origami-making station in the waiting area of a restaurant may be\nboth surprising and enjoyable for a customer who is passionate about paper\ncrafts. Similarly, an exhibit of 18th century harpsichords would be atypical\nfor a hotel lobby and likely pique the interest of a guest who has a passion\nfor Baroque music. Motivated by this insight, in this paper we introduce the\nnew task of engineering serendipity through recommendations of items with\natypical aspects. We describe an LLM-based system pipeline that extracts\natypical aspects from item reviews, then estimates and aggregates their\nuser-specific utility in a measure of serendipity potential that is used to\nrerank a list of items recommended to the user. To facilitate system\ndevelopment and evaluation, we introduce a dataset of Yelp reviews that are\nmanually annotated with atypical aspects and a dataset of artificially\ngenerated user profiles, together with crowdsourced annotations of user-aspect\nutility values. Furthermore, we introduce a custom procedure for dynamic\nselection of in-context learning examples, which is shown to improve LLM-based\njudgments of atypicality and utility. Experimental evaluations show that\nserendipity-based rankings generated by the system are highly correlated with\nground truth rankings for which serendipity scores are computed from manual\nannotations of atypical aspects and their user-dependent utility. Overall, we\nhope that the new recommendation task and the associated system presented in\nthis paper catalyze further research into recommendation approaches that go\nbeyond accuracy in their pursuit of enhanced user satisfaction.\n  The datasets and the code are made publicly available at\nhttps://github.com/ramituncc49er/ATARS .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A restaurant dinner or a hotel stay may lead to memorable experiences when\nguests encounter unexpected aspects that also match their interests. For\nexample, an origami-making station in the waiting area of a restaurant may be\nboth surprising and enjoyable for a customer who is passionate about paper\ncrafts. Similarly, an exhibit of 18th century harpsichords would be atypical\nfor a hotel lobby and likely pique the interest of a guest who has a passion\nfor Baroque music. Motivated by this insight, in this paper we introduce the\nnew task of engineering serendipity through recommendations of items with\natypical aspects. We describe an LLM-based system pipeline that extracts\natypical aspects from item reviews, then estimates and aggregates their\nuser-specific utility in a measure of serendipity potential that is used to\nrerank a list of items recommended to the user. To facilitate system\ndevelopment and evaluation, we introduce a dataset of Yelp reviews that are\nmanually annotated with atypical aspects and a dataset of artificially\ngenerated user profiles, together with crowdsourced annotations of user-aspect\nutility values. Furthermore, we introduce a custom procedure for dynamic\nselection of in-context learning examples, which is shown to improve LLM-based\njudgments of atypicality and utility. Experimental evaluations show that\nserendipity-based rankings generated by the system are highly correlated with\nground truth rankings for which serendipity scores are computed from manual\nannotations of atypical aspects and their user-dependent utility. Overall, we\nhope that the new recommendation task and the associated system presented in\nthis paper catalyze further research into recommendation approaches that go\nbeyond accuracy in their pursuit of enhanced user satisfaction.\n  The datasets and the code are made publicly available at\nhttps://github.com/ramituncc49er/ATARS ."
                },
                "authors": [
                    {
                        "name": "Ramit Aditya"
                    },
                    {
                        "name": "Razvan Bunescu"
                    },
                    {
                        "name": "Smita Nannaware"
                    },
                    {
                        "name": "Erfan Al-Hossami"
                    }
                ],
                "author_detail": {
                    "name": "Erfan Al-Hossami"
                },
                "author": "Erfan Al-Hossami",
                "arxiv_comment": "25 pages of content + references and appendix. arXiv admin note: text\n  overlap with arXiv:2311.02702",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23579v1",
                "updated": "2025-05-29T15:49:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    49,
                    27,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    49,
                    27,
                    3,
                    149,
                    0
                ],
                "title": "BioReason: Incentivizing Multimodal Biological Reasoning within a\n  DNA-LLM Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioReason: Incentivizing Multimodal Biological Reasoning within a\n  DNA-LLM Model"
                },
                "summary": "Unlocking deep, interpretable biological reasoning from complex genomic data\nis a major AI challenge hindering scientific discovery. Current DNA foundation\nmodels, despite strong sequence representation, struggle with multi-step\nreasoning and lack inherent transparent, biologically intuitive explanations.\nWe introduce BioReason, a pioneering architecture that, for the first time,\ndeeply integrates a DNA foundation model with a Large Language Model (LLM).\nThis novel connection enables the LLM to directly process and reason with\ngenomic information as a fundamental input, fostering a new form of multimodal\nbiological understanding. BioReason's sophisticated multi-step reasoning is\ndeveloped through supervised fine-tuning and targeted reinforcement learning,\nguiding the system to generate logical, biologically coherent deductions. On\nbiological reasoning benchmarks including KEGG-based disease pathway prediction\n- where accuracy improves from 88% to 97% - and variant effect prediction,\nBioReason demonstrates an average 15% performance gain over strong\nsingle-modality baselines. BioReason reasons over unseen biological entities\nand articulates decision-making through interpretable, step-by-step biological\ntraces, offering a transformative approach for AI in biology that enables\ndeeper mechanistic insights and accelerates testable hypothesis generation from\ngenomic data. Data, code, and checkpoints are publicly available at\nhttps://github.com/bowang-lab/BioReason",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking deep, interpretable biological reasoning from complex genomic data\nis a major AI challenge hindering scientific discovery. Current DNA foundation\nmodels, despite strong sequence representation, struggle with multi-step\nreasoning and lack inherent transparent, biologically intuitive explanations.\nWe introduce BioReason, a pioneering architecture that, for the first time,\ndeeply integrates a DNA foundation model with a Large Language Model (LLM).\nThis novel connection enables the LLM to directly process and reason with\ngenomic information as a fundamental input, fostering a new form of multimodal\nbiological understanding. BioReason's sophisticated multi-step reasoning is\ndeveloped through supervised fine-tuning and targeted reinforcement learning,\nguiding the system to generate logical, biologically coherent deductions. On\nbiological reasoning benchmarks including KEGG-based disease pathway prediction\n- where accuracy improves from 88% to 97% - and variant effect prediction,\nBioReason demonstrates an average 15% performance gain over strong\nsingle-modality baselines. BioReason reasons over unseen biological entities\nand articulates decision-making through interpretable, step-by-step biological\ntraces, offering a transformative approach for AI in biology that enables\ndeeper mechanistic insights and accelerates testable hypothesis generation from\ngenomic data. Data, code, and checkpoints are publicly available at\nhttps://github.com/bowang-lab/BioReason"
                },
                "authors": [
                    {
                        "name": "Adibvafa Fallahpour"
                    },
                    {
                        "name": "Andrew Magnuson"
                    },
                    {
                        "name": "Purav Gupta"
                    },
                    {
                        "name": "Shihao Ma"
                    },
                    {
                        "name": "Jack Naimer"
                    },
                    {
                        "name": "Arnav Shah"
                    },
                    {
                        "name": "Haonan Duan"
                    },
                    {
                        "name": "Omar Ibrahim"
                    },
                    {
                        "name": "Hani Goodarzi"
                    },
                    {
                        "name": "Chris J. Maddison"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "arxiv_comment": "16 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20088v2",
                "updated": "2025-05-29T15:47:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    47,
                    53,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-26T15:01:56Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    15,
                    1,
                    56,
                    0,
                    146,
                    0
                ],
                "title": "Multi-Domain Explainability of Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Domain Explainability of Preferences"
                },
                "summary": "Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and\nreward models, are central to aligning and evaluating large language models\n(LLMs). Yet, the underlying concepts that drive these preferences remain poorly\nunderstood. In this work, we propose a fully automated method for generating\nlocal and global concept-based explanations of preferences across multiple\ndomains. Our method utilizes an LLM to identify concepts that distinguish\nbetween chosen and rejected responses, and to represent them with concept-based\nvectors. To model the relationships between concepts and preferences, we\npropose a white-box Hierarchical Multi-Domain Regression model that captures\nboth domain-general and domain-specific effects. To evaluate our method, we\ncurate a dataset spanning eight challenging and diverse domains and explain\ntwelve mechanisms. Our method achieves strong preference prediction\nperformance, outperforming baselines while also being explainable.\nAdditionally, we assess explanations in two application-driven settings. First,\nguiding LLM outputs with concepts from LaaJ explanations yields responses that\nthose judges consistently prefer. Second, prompting LaaJs with concepts\nexplaining humans improves their preference predictions. Together, our work\nestablishes a new paradigm for explainability in the era of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and\nreward models, are central to aligning and evaluating large language models\n(LLMs). Yet, the underlying concepts that drive these preferences remain poorly\nunderstood. In this work, we propose a fully automated method for generating\nlocal and global concept-based explanations of preferences across multiple\ndomains. Our method utilizes an LLM to identify concepts that distinguish\nbetween chosen and rejected responses, and to represent them with concept-based\nvectors. To model the relationships between concepts and preferences, we\npropose a white-box Hierarchical Multi-Domain Regression model that captures\nboth domain-general and domain-specific effects. To evaluate our method, we\ncurate a dataset spanning eight challenging and diverse domains and explain\ntwelve mechanisms. Our method achieves strong preference prediction\nperformance, outperforming baselines while also being explainable.\nAdditionally, we assess explanations in two application-driven settings. First,\nguiding LLM outputs with concepts from LaaJ explanations yields responses that\nthose judges consistently prefer. Second, prompting LaaJs with concepts\nexplaining humans improves their preference predictions. Together, our work\nestablishes a new paradigm for explainability in the era of LLMs."
                },
                "authors": [
                    {
                        "name": "Nitay Calderon"
                    },
                    {
                        "name": "Liat Ein-Dor"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23576v1",
                "updated": "2025-05-29T15:47:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    47,
                    49,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:47:49Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    47,
                    49,
                    3,
                    149,
                    0
                ],
                "title": "Cognitive Guardrails for Open-World Decision Making in Autonomous Drone\n  Swarms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Guardrails for Open-World Decision Making in Autonomous Drone\n  Swarms"
                },
                "summary": "Small Uncrewed Aerial Systems (sUAS) are increasingly deployed as autonomous\nswarms in search-and-rescue and other disaster-response scenarios. In these\nsettings, they use computer vision (CV) to detect objects of interest and\nautonomously adapt their missions. However, traditional CV systems often\nstruggle to recognize unfamiliar objects in open-world environments or to infer\ntheir relevance for mission planning. To address this, we incorporate large\nlanguage models (LLMs) to reason about detected objects and their implications.\nWhile LLMs can offer valuable insights, they are also prone to hallucinations\nand may produce incorrect, misleading, or unsafe recommendations. To ensure\nsafe and sensible decision-making under uncertainty, high-level decisions must\nbe governed by cognitive guardrails. This article presents the design,\nsimulation, and real-world integration of these guardrails for sUAS swarms in\nsearch-and-rescue missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Uncrewed Aerial Systems (sUAS) are increasingly deployed as autonomous\nswarms in search-and-rescue and other disaster-response scenarios. In these\nsettings, they use computer vision (CV) to detect objects of interest and\nautonomously adapt their missions. However, traditional CV systems often\nstruggle to recognize unfamiliar objects in open-world environments or to infer\ntheir relevance for mission planning. To address this, we incorporate large\nlanguage models (LLMs) to reason about detected objects and their implications.\nWhile LLMs can offer valuable insights, they are also prone to hallucinations\nand may produce incorrect, misleading, or unsafe recommendations. To ensure\nsafe and sensible decision-making under uncertainty, high-level decisions must\nbe governed by cognitive guardrails. This article presents the design,\nsimulation, and real-world integration of these guardrails for sUAS swarms in\nsearch-and-rescue missions."
                },
                "authors": [
                    {
                        "name": "Jane Cleland-Huang"
                    },
                    {
                        "name": "Pedro Antonio Alarcon Granadeno"
                    },
                    {
                        "name": "Arturo Miguel Russell Bernal"
                    },
                    {
                        "name": "Demetrius Hernandez"
                    },
                    {
                        "name": "Michael Murphy"
                    },
                    {
                        "name": "Maureen Petterson"
                    },
                    {
                        "name": "Walter Scheirer"
                    }
                ],
                "author_detail": {
                    "name": "Walter Scheirer"
                },
                "author": "Walter Scheirer",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23570v1",
                "updated": "2025-05-29T15:44:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    44,
                    36,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:44:36Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    44,
                    36,
                    3,
                    149,
                    0
                ],
                "title": "Evaluating AI capabilities in detecting conspiracy theories on YouTube",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating AI capabilities in detecting conspiracy theories on YouTube"
                },
                "summary": "As a leading online platform with a vast global audience, YouTube's extensive\nreach also makes it susceptible to hosting harmful content, including\ndisinformation and conspiracy theories. This study explores the use of\nopen-weight Large Language Models (LLMs), both text-only and multimodal, for\nidentifying conspiracy theory videos shared on YouTube. Leveraging a labeled\ndataset of thousands of videos, we evaluate a variety of LLMs in a zero-shot\nsetting and compare their performance to a fine-tuned RoBERTa baseline. Results\nshow that text-based LLMs achieve high recall but lower precision, leading to\nincreased false positives. Multimodal models lag behind their text-only\ncounterparts, indicating limited benefits from visual data integration. To\nassess real-world applicability, we evaluate the most accurate models on an\nunlabeled dataset, finding that RoBERTa achieves performance close to LLMs with\na larger number of parameters. Our work highlights the strengths and\nlimitations of current LLM-based approaches for online harmful content\ndetection, emphasizing the need for more precise and robust systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a leading online platform with a vast global audience, YouTube's extensive\nreach also makes it susceptible to hosting harmful content, including\ndisinformation and conspiracy theories. This study explores the use of\nopen-weight Large Language Models (LLMs), both text-only and multimodal, for\nidentifying conspiracy theory videos shared on YouTube. Leveraging a labeled\ndataset of thousands of videos, we evaluate a variety of LLMs in a zero-shot\nsetting and compare their performance to a fine-tuned RoBERTa baseline. Results\nshow that text-based LLMs achieve high recall but lower precision, leading to\nincreased false positives. Multimodal models lag behind their text-only\ncounterparts, indicating limited benefits from visual data integration. To\nassess real-world applicability, we evaluate the most accurate models on an\nunlabeled dataset, finding that RoBERTa achieves performance close to LLMs with\na larger number of parameters. Our work highlights the strengths and\nlimitations of current LLM-based approaches for online harmful content\ndetection, emphasizing the need for more precise and robust systems."
                },
                "authors": [
                    {
                        "name": "Leonardo La Rocca"
                    },
                    {
                        "name": "Francesco Corso"
                    },
                    {
                        "name": "Francesco Pierri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pierri"
                },
                "author": "Francesco Pierri",
                "arxiv_comment": "Submitted for review to OSNEM Special Issue of April 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12864v2",
                "updated": "2025-05-29T15:37:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    37,
                    57,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-19T08:48:12Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    8,
                    48,
                    12,
                    0,
                    139,
                    0
                ],
                "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams"
                },
                "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/"
                },
                "authors": [
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Jakob Merane"
                    },
                    {
                        "name": "Etienne Salimbeni"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Yoan Hermstrüwer"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Mubashara Akhtar"
                    },
                    {
                        "name": "Florian Geering"
                    },
                    {
                        "name": "Oliver Dreyer"
                    },
                    {
                        "name": "Daniel Brunner"
                    },
                    {
                        "name": "Markus Leippold"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Alexander Stremitzer"
                    },
                    {
                        "name": "Christoph Engel"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Joel Niklaus"
                    }
                ],
                "author_detail": {
                    "name": "Joel Niklaus"
                },
                "author": "Joel Niklaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23561v1",
                "updated": "2025-05-29T15:37:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    37,
                    23,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:37:23Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    37,
                    23,
                    3,
                    149,
                    0
                ],
                "title": "Merge Hijacking: Backdoor Attacks to Model Merging of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merge Hijacking: Backdoor Attacks to Model Merging of Large Language\n  Models"
                },
                "summary": "Model merging for Large Language Models (LLMs) directly fuses the parameters\nof different models finetuned on various tasks, creating a unified model for\nmulti-domain tasks. However, due to potential vulnerabilities in models\navailable on open-source platforms, model merging is susceptible to backdoor\nattacks. In this paper, we propose Merge Hijacking, the first backdoor attack\ntargeting model merging in LLMs. The attacker constructs a malicious upload\nmodel and releases it. Once a victim user merges it with any other models, the\nresulting merged model inherits the backdoor while maintaining utility across\ntasks. Merge Hijacking defines two main objectives-effectiveness and\nutility-and achieves them through four steps. Extensive experiments demonstrate\nthe effectiveness of our attack across different models, merging algorithms,\nand tasks. Additionally, we show that the attack remains effective even when\nmerging real-world models. Moreover, our attack demonstrates robustness against\ntwo inference-time defenses (Paraphrasing and CLEANGEN) and one training-time\ndefense (Fine-pruning).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging for Large Language Models (LLMs) directly fuses the parameters\nof different models finetuned on various tasks, creating a unified model for\nmulti-domain tasks. However, due to potential vulnerabilities in models\navailable on open-source platforms, model merging is susceptible to backdoor\nattacks. In this paper, we propose Merge Hijacking, the first backdoor attack\ntargeting model merging in LLMs. The attacker constructs a malicious upload\nmodel and releases it. Once a victim user merges it with any other models, the\nresulting merged model inherits the backdoor while maintaining utility across\ntasks. Merge Hijacking defines two main objectives-effectiveness and\nutility-and achieves them through four steps. Extensive experiments demonstrate\nthe effectiveness of our attack across different models, merging algorithms,\nand tasks. Additionally, we show that the attack remains effective even when\nmerging real-world models. Moreover, our attack demonstrates robustness against\ntwo inference-time defenses (Paraphrasing and CLEANGEN) and one training-time\ndefense (Fine-pruning)."
                },
                "authors": [
                    {
                        "name": "Zenghui Yuan"
                    },
                    {
                        "name": "Yangming Xu"
                    },
                    {
                        "name": "Jiawen Shi"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "This paper is accepted by ACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23560v1",
                "updated": "2025-05-29T15:37:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    37,
                    6,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:37:06Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    37,
                    6,
                    3,
                    149,
                    0
                ],
                "title": "Ambulance Allocation for Patient-Centered Care",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambulance Allocation for Patient-Centered Care"
                },
                "summary": "Emergency Medical Services (EMS) in the United States and similar systems\ntypically utilize a single treatment pathway, transporting all patients to\nemergency departments (EDs), regardless of their actual care needs or\npreferences. Recent policy reforms have sought to introduce alternative\ntreatment pathways to divert lower acuity patients from the ED, but\noperationalizing these options has proven difficult. This paper proposes a\npatient-centered EMS (PC-EMS) ambulance allocation model that supports multiple\ncare pathways by aligning EMS responses with individual patient needs. We\ndevelop a two-stage mixed-integer optimization framework that incorporates\nmultiple dispatch and secondary assignment strategies which enable dynamic\nresource deployment. The model maximizes appropriate ED diversions while\nmaintaining ambulance availability using a queueing-based availability\nconstraint. We leverage national EMS data and machine learning to estimate\ndispatcher accuracy and diversion potential. Simulations across diverse\ngeographic regions suggest that agencies can achieve up to 80% of possible ED\ndiversions by equipping only 15 to 25% of their fleet with diversion capable\nunits. Adaptive dispatch strategies improve diversion rates by 3.4 to 8.6 times\ncompared to conventional single unit dispatch. These results provide actionable\nguidance for PC-EMS implementation by quantifying the trade off between\nequipment investment and operational coordination. Using the allocation model,\nagencies can strategically choose between upgrading fewer units with advanced\ndispatching protocols versus larger fleet investments with simpler operations.\nThis approach offers flexible pathways suited to different organizational\ncapabilities and implementation readiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency Medical Services (EMS) in the United States and similar systems\ntypically utilize a single treatment pathway, transporting all patients to\nemergency departments (EDs), regardless of their actual care needs or\npreferences. Recent policy reforms have sought to introduce alternative\ntreatment pathways to divert lower acuity patients from the ED, but\noperationalizing these options has proven difficult. This paper proposes a\npatient-centered EMS (PC-EMS) ambulance allocation model that supports multiple\ncare pathways by aligning EMS responses with individual patient needs. We\ndevelop a two-stage mixed-integer optimization framework that incorporates\nmultiple dispatch and secondary assignment strategies which enable dynamic\nresource deployment. The model maximizes appropriate ED diversions while\nmaintaining ambulance availability using a queueing-based availability\nconstraint. We leverage national EMS data and machine learning to estimate\ndispatcher accuracy and diversion potential. Simulations across diverse\ngeographic regions suggest that agencies can achieve up to 80% of possible ED\ndiversions by equipping only 15 to 25% of their fleet with diversion capable\nunits. Adaptive dispatch strategies improve diversion rates by 3.4 to 8.6 times\ncompared to conventional single unit dispatch. These results provide actionable\nguidance for PC-EMS implementation by quantifying the trade off between\nequipment investment and operational coordination. Using the allocation model,\nagencies can strategically choose between upgrading fewer units with advanced\ndispatching protocols versus larger fleet investments with simpler operations.\nThis approach offers flexible pathways suited to different organizational\ncapabilities and implementation readiness."
                },
                "authors": [
                    {
                        "name": "Eric G. Stratman"
                    },
                    {
                        "name": "Justin J. Boutilier"
                    },
                    {
                        "name": "Laura A. Albert"
                    }
                ],
                "author_detail": {
                    "name": "Laura A. Albert"
                },
                "author": "Laura A. Albert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23559v1",
                "updated": "2025-05-29T15:35:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    35,
                    58,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:35:58Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    35,
                    58,
                    3,
                    149,
                    0
                ],
                "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents"
                },
                "summary": "Recent advancements in large language model (LLM) agents have significantly\naccelerated scientific discovery automation, yet concurrently raised critical\nethical and safety concerns. To systematically address these challenges, we\nintroduce \\textbf{SafeScientist}, an innovative AI scientist framework\nexplicitly designed to enhance safety and ethical responsibility in AI-driven\nscientific exploration. SafeScientist proactively refuses ethically\ninappropriate or high-risk tasks and rigorously emphasizes safety throughout\nthe research process. To achieve comprehensive safety oversight, we integrate\nmultiple defensive mechanisms, including prompt monitoring, agent-collaboration\nmonitoring, tool-use monitoring, and an ethical reviewer component.\nComplementing SafeScientist, we propose \\textbf{SciSafetyBench}, a novel\nbenchmark specifically designed to evaluate AI safety in scientific contexts,\ncomprising 240 high-risk scientific tasks across 6 domains, alongside 30\nspecially designed scientific tools and 120 tool-related risk tasks. Extensive\nexperiments demonstrate that SafeScientist significantly improves safety\nperformance by 35\\% compared to traditional AI scientist frameworks, without\ncompromising scientific output quality. Additionally, we rigorously validate\nthe robustness of our safety pipeline against diverse adversarial attack\nmethods, further confirming the effectiveness of our integrated approach. The\ncode and data will be available at https://github.com/ulab-uiuc/SafeScientist.\n\\textcolor{red}{Warning: this paper contains example data that may be offensive\nor harmful.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language model (LLM) agents have significantly\naccelerated scientific discovery automation, yet concurrently raised critical\nethical and safety concerns. To systematically address these challenges, we\nintroduce \\textbf{SafeScientist}, an innovative AI scientist framework\nexplicitly designed to enhance safety and ethical responsibility in AI-driven\nscientific exploration. SafeScientist proactively refuses ethically\ninappropriate or high-risk tasks and rigorously emphasizes safety throughout\nthe research process. To achieve comprehensive safety oversight, we integrate\nmultiple defensive mechanisms, including prompt monitoring, agent-collaboration\nmonitoring, tool-use monitoring, and an ethical reviewer component.\nComplementing SafeScientist, we propose \\textbf{SciSafetyBench}, a novel\nbenchmark specifically designed to evaluate AI safety in scientific contexts,\ncomprising 240 high-risk scientific tasks across 6 domains, alongside 30\nspecially designed scientific tools and 120 tool-related risk tasks. Extensive\nexperiments demonstrate that SafeScientist significantly improves safety\nperformance by 35\\% compared to traditional AI scientist frameworks, without\ncompromising scientific output quality. Additionally, we rigorously validate\nthe robustness of our safety pipeline against diverse adversarial attack\nmethods, further confirming the effectiveness of our integrated approach. The\ncode and data will be available at https://github.com/ulab-uiuc/SafeScientist.\n\\textcolor{red}{Warning: this paper contains example data that may be offensive\nor harmful.}"
                },
                "authors": [
                    {
                        "name": "Kunlun Zhu"
                    },
                    {
                        "name": "Jiaxun Zhang"
                    },
                    {
                        "name": "Ziheng Qi"
                    },
                    {
                        "name": "Nuoxing Shang"
                    },
                    {
                        "name": "Zijia Liu"
                    },
                    {
                        "name": "Peixuan Han"
                    },
                    {
                        "name": "Yue Su"
                    },
                    {
                        "name": "Haofei Yu"
                    },
                    {
                        "name": "Jiaxuan You"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan You"
                },
                "author": "Jiaxuan You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23556v1",
                "updated": "2025-05-29T15:33:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    33,
                    39,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:33:39Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    33,
                    39,
                    3,
                    149,
                    0
                ],
                "title": "Understanding Refusal in Language Models with Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Refusal in Language Models with Sparse Autoencoders"
                },
                "summary": "Refusal is a key safety behavior in aligned language models, yet the internal\nmechanisms driving refusals remain opaque. In this work, we conduct a\nmechanistic study of refusal in instruction-tuned LLMs using sparse\nautoencoders to identify latent features that causally mediate refusal\nbehaviors. We apply our method to two open-source chat models and intervene on\nrefusal-related features to assess their influence on generation, validating\ntheir behavioral impact across multiple harmful datasets. This enables a\nfine-grained inspection of how refusal manifests at the activation level and\naddresses key research questions such as investigating upstream-downstream\nlatent relationship and understanding the mechanisms of adversarial\njailbreaking techniques. We also establish the usefulness of refusal features\nin enhancing generalization for linear probes to out-of-distribution\nadversarial samples in classification tasks. We open source our code in\nhttps://github.com/wj210/refusal_sae.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal is a key safety behavior in aligned language models, yet the internal\nmechanisms driving refusals remain opaque. In this work, we conduct a\nmechanistic study of refusal in instruction-tuned LLMs using sparse\nautoencoders to identify latent features that causally mediate refusal\nbehaviors. We apply our method to two open-source chat models and intervene on\nrefusal-related features to assess their influence on generation, validating\ntheir behavioral impact across multiple harmful datasets. This enables a\nfine-grained inspection of how refusal manifests at the activation level and\naddresses key research questions such as investigating upstream-downstream\nlatent relationship and understanding the mechanisms of adversarial\njailbreaking techniques. We also establish the usefulness of refusal features\nin enhancing generalization for linear probes to out-of-distribution\nadversarial samples in classification tasks. We open source our code in\nhttps://github.com/wj210/refusal_sae."
                },
                "authors": [
                    {
                        "name": "Wei Jie Yeo"
                    },
                    {
                        "name": "Nirmalendu Prakash"
                    },
                    {
                        "name": "Clement Neo"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Ranjan Satapathy"
                    }
                ],
                "author_detail": {
                    "name": "Ranjan Satapathy"
                },
                "author": "Ranjan Satapathy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04629v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04629v4",
                "updated": "2025-05-29T15:33:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    33,
                    8,
                    3,
                    149,
                    0
                ],
                "published": "2024-12-05T21:51:05Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    21,
                    51,
                    5,
                    3,
                    340,
                    0
                ],
                "title": "Argumentative Experience: Reducing Confirmation Bias on Controversial\n  Issues through LLM-Generated Multi-Persona Debates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argumentative Experience: Reducing Confirmation Bias on Controversial\n  Issues through LLM-Generated Multi-Persona Debates"
                },
                "summary": "Large language models (LLMs) are enabling designers to give life to exciting\nnew user experiences for information access. In this work, we present a system\nthat generates LLM personas to debate a topic of interest from different\nperspectives. How might information seekers use and benefit from such a system?\nCan centering information access around diverse viewpoints help to mitigate\nthorny challenges like confirmation bias in which information seekers\nover-trust search results matching existing beliefs? How do potential biases\nand hallucinations in LLMs play out alongside human users who are also fallible\nand possibly biased?\n  Our study exposes participants to multiple viewpoints on controversial issues\nvia a mixed-methods, within-subjects study. We use eye-tracking metrics to\nquantitatively assess cognitive engagement alongside qualitative feedback.\nCompared to a baseline search system, we see more creative interactions and\ndiverse information-seeking with our multi-persona debate system, which more\neffectively reduces user confirmation bias and conviction toward their initial\nbeliefs. Overall, our study contributes to the emerging design space of\nLLM-based information access systems, specifically investigating the potential\nof simulated personas to promote greater exposure to information diversity,\nemulate collective intelligence, and mitigate bias in information seeking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are enabling designers to give life to exciting\nnew user experiences for information access. In this work, we present a system\nthat generates LLM personas to debate a topic of interest from different\nperspectives. How might information seekers use and benefit from such a system?\nCan centering information access around diverse viewpoints help to mitigate\nthorny challenges like confirmation bias in which information seekers\nover-trust search results matching existing beliefs? How do potential biases\nand hallucinations in LLMs play out alongside human users who are also fallible\nand possibly biased?\n  Our study exposes participants to multiple viewpoints on controversial issues\nvia a mixed-methods, within-subjects study. We use eye-tracking metrics to\nquantitatively assess cognitive engagement alongside qualitative feedback.\nCompared to a baseline search system, we see more creative interactions and\ndiverse information-seeking with our multi-persona debate system, which more\neffectively reduces user confirmation bias and conviction toward their initial\nbeliefs. Overall, our study contributes to the emerging design space of\nLLM-based information access systems, specifically investigating the potential\nof simulated personas to promote greater exposure to information diversity,\nemulate collective intelligence, and mitigate bias in information seeking."
                },
                "authors": [
                    {
                        "name": "Li Shi"
                    },
                    {
                        "name": "Houjiang Liu"
                    },
                    {
                        "name": "Yian Wong"
                    },
                    {
                        "name": "Utkarsh Mujumdar"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Jacek Gwizdka"
                    },
                    {
                        "name": "Matthew Lease"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Lease"
                },
                "author": "Matthew Lease",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04629v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04629v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23555v1",
                "updated": "2025-05-29T15:31:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    31,
                    37,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:31:37Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    31,
                    37,
                    3,
                    149,
                    0
                ],
                "title": "Adaptive Federated LoRA in Heterogeneous Wireless Networks with\n  Independent Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Federated LoRA in Heterogeneous Wireless Networks with\n  Independent Sampling"
                },
                "summary": "Federated LoRA has emerged as a promising technique for efficiently\nfine-tuning large language models (LLMs) on distributed devices by reducing the\nnumber of trainable parameters. However, existing approaches often inadequately\noverlook the theoretical and practical implications of system and data\nheterogeneity, thereby failing to optimize the overall training efficiency,\nparticularly in terms of wall-clock time. In this paper, we propose an adaptive\nfederated LoRA strategy with independent client sampling to minimize the\nconvergence wall-clock time of federated fine-tuning under both computation and\ncommunication heterogeneity. We first derive a new convergence bound for\nfederated LoRA with arbitrary and independent client sampling, notably without\nrequiring the stringent bounded gradient assumption. Then, we introduce an\nadaptive bandwidth allocation scheme that accounts for heterogeneous client\nresources and system bandwidth constraints. Based on the derived theory, we\nformulate and solve a non-convex optimization problem to jointly determine the\nLoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock\nconvergence time. An efficient and low-complexity algorithm is developed to\napproximate the solution. Finally, extensive experiments demonstrate that our\napproach significantly reduces wall-clock training time compared to\nstate-of-the-art methods across various models and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated LoRA has emerged as a promising technique for efficiently\nfine-tuning large language models (LLMs) on distributed devices by reducing the\nnumber of trainable parameters. However, existing approaches often inadequately\noverlook the theoretical and practical implications of system and data\nheterogeneity, thereby failing to optimize the overall training efficiency,\nparticularly in terms of wall-clock time. In this paper, we propose an adaptive\nfederated LoRA strategy with independent client sampling to minimize the\nconvergence wall-clock time of federated fine-tuning under both computation and\ncommunication heterogeneity. We first derive a new convergence bound for\nfederated LoRA with arbitrary and independent client sampling, notably without\nrequiring the stringent bounded gradient assumption. Then, we introduce an\nadaptive bandwidth allocation scheme that accounts for heterogeneous client\nresources and system bandwidth constraints. Based on the derived theory, we\nformulate and solve a non-convex optimization problem to jointly determine the\nLoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock\nconvergence time. An efficient and low-complexity algorithm is developed to\napproximate the solution. Finally, extensive experiments demonstrate that our\napproach significantly reduces wall-clock training time compared to\nstate-of-the-art methods across various models and datasets."
                },
                "authors": [
                    {
                        "name": "Yanzhao Hou"
                    },
                    {
                        "name": "Jiaxiang Geng"
                    },
                    {
                        "name": "Boyu Li"
                    },
                    {
                        "name": "Xiaofeng Tao"
                    },
                    {
                        "name": "Juncheng Wang"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Bing Luo"
                    }
                ],
                "author_detail": {
                    "name": "Bing Luo"
                },
                "author": "Bing Luo",
                "arxiv_comment": "13 pages, Submitted to IEEE Journal on Selected Areas in\n  Communications (JSAC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23554v1",
                "updated": "2025-05-29T15:31:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    31,
                    28,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:31:28Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    31,
                    28,
                    3,
                    149,
                    0
                ],
                "title": "Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in\n  Geo-Distributed Cloud Datacenters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in\n  Geo-Distributed Cloud Datacenters"
                },
                "summary": "In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and\nGemini have been widely adopted in different areas. As the use of LLMs\ncontinues to grow, many efforts have focused on reducing the massive training\noverheads of these models. But it is the environmental impact of handling user\nrequests to LLMs that is increasingly becoming a concern. Recent studies\nestimate that the costs of operating LLMs in their inference phase can exceed\ntraining costs by 25x per year. As LLMs are queried incessantly, the cumulative\ncarbon footprint for the operational phase has been shown to far exceed the\nfootprint during the training phase. Further, estimates indicate that 500 ml of\nfresh water is expended for every 20-50 requests to LLMs during inference. To\naddress these important sustainability issues with LLMs, we propose a novel\nframework called SLIT to co-optimize LLM quality of service (time-to-first\ntoken), carbon emissions, water usage, and energy costs. The framework utilizes\na machine learning (ML) based metaheuristic to enhance the sustainability of\nLLM hosting across geo-distributed cloud datacenters. Such a framework will\nbecome increasingly vital as LLMs proliferate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and\nGemini have been widely adopted in different areas. As the use of LLMs\ncontinues to grow, many efforts have focused on reducing the massive training\noverheads of these models. But it is the environmental impact of handling user\nrequests to LLMs that is increasingly becoming a concern. Recent studies\nestimate that the costs of operating LLMs in their inference phase can exceed\ntraining costs by 25x per year. As LLMs are queried incessantly, the cumulative\ncarbon footprint for the operational phase has been shown to far exceed the\nfootprint during the training phase. Further, estimates indicate that 500 ml of\nfresh water is expended for every 20-50 requests to LLMs during inference. To\naddress these important sustainability issues with LLMs, we propose a novel\nframework called SLIT to co-optimize LLM quality of service (time-to-first\ntoken), carbon emissions, water usage, and energy costs. The framework utilizes\na machine learning (ML) based metaheuristic to enhance the sustainability of\nLLM hosting across geo-distributed cloud datacenters. Such a framework will\nbecome increasingly vital as LLMs proliferate."
                },
                "authors": [
                    {
                        "name": "Hayden Moore"
                    },
                    {
                        "name": "Sirui Qi"
                    },
                    {
                        "name": "Ninad Hogade"
                    },
                    {
                        "name": "Dejan Milojicic"
                    },
                    {
                        "name": "Cullen Bash"
                    },
                    {
                        "name": "Sudeep Pasricha"
                    }
                ],
                "author_detail": {
                    "name": "Sudeep Pasricha"
                },
                "author": "Sudeep Pasricha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23549v1",
                "updated": "2025-05-29T15:27:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    27,
                    52,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:27:52Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    27,
                    52,
                    3,
                    149,
                    0
                ],
                "title": "LLM-based Property-based Test Generation for Guardrailing Cyber-Physical\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Property-based Test Generation for Guardrailing Cyber-Physical\n  Systems"
                },
                "summary": "Cyber-physical systems (CPSs) are complex systems that integrate physical,\ncomputational, and communication subsystems. The heterogeneous nature of these\nsystems makes their safety assurance challenging. In this paper, we propose a\nnovel automated approach for guardrailing cyber-physical systems using\nproperty-based tests (PBTs) generated by Large Language Models (LLMs). Our\napproach employs an LLM to extract properties from the code and documentation\nof CPSs. Next, we use the LLM to generate PBTs that verify the extracted\nproperties on the CPS. The generated PBTs have two uses. First, they are used\nto test the CPS before it is deployed, i.e., at design time. Secondly, these\nPBTs can be used after deployment, i.e., at run time, to monitor the behavior\nof the system and guardrail it against unsafe states. We implement our approach\nin ChekProp and conduct preliminary experiments to evaluate the generated PBTs\nin terms of their relevance (how well they match manually crafted properties),\nexecutability (how many run with minimal manual modification), and\neffectiveness (coverage of the input space partitions). The results of our\nexperiments and evaluation demonstrate a promising path forward for creating\nguardrails for CPSs using LLM-generated property-based tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber-physical systems (CPSs) are complex systems that integrate physical,\ncomputational, and communication subsystems. The heterogeneous nature of these\nsystems makes their safety assurance challenging. In this paper, we propose a\nnovel automated approach for guardrailing cyber-physical systems using\nproperty-based tests (PBTs) generated by Large Language Models (LLMs). Our\napproach employs an LLM to extract properties from the code and documentation\nof CPSs. Next, we use the LLM to generate PBTs that verify the extracted\nproperties on the CPS. The generated PBTs have two uses. First, they are used\nto test the CPS before it is deployed, i.e., at design time. Secondly, these\nPBTs can be used after deployment, i.e., at run time, to monitor the behavior\nof the system and guardrail it against unsafe states. We implement our approach\nin ChekProp and conduct preliminary experiments to evaluate the generated PBTs\nin terms of their relevance (how well they match manually crafted properties),\nexecutability (how many run with minimal manual modification), and\neffectiveness (coverage of the input space partitions). The results of our\nexperiments and evaluation demonstrate a promising path forward for creating\nguardrails for CPSs using LLM-generated property-based tests."
                },
                "authors": [
                    {
                        "name": "Khashayar Etemadi"
                    },
                    {
                        "name": "Marjan Sirjani"
                    },
                    {
                        "name": "Mahshid Helali Moghadam"
                    },
                    {
                        "name": "Per Strandberg"
                    },
                    {
                        "name": "Paul Pettersson"
                    }
                ],
                "author_detail": {
                    "name": "Paul Pettersson"
                },
                "author": "Paul Pettersson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08319v2",
                "updated": "2025-05-29T15:26:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    26,
                    6,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-14T18:53:00Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    53,
                    0,
                    1,
                    14,
                    0
                ],
                "title": "Enhancing Automated Interpretability with Output-Centric Feature\n  Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Automated Interpretability with Output-Centric Feature\n  Descriptions"
                },
                "summary": "Automated interpretability pipelines generate natural language descriptions\nfor the concepts represented by features in large language models (LLMs), such\nas plants or the first word in a sentence. These descriptions are derived using\ninputs that activate the feature, which may be a dimension or a direction in\nthe model's representation space. However, identifying activating inputs is\ncostly, and the mechanistic role of a feature in model behavior is determined\nboth by how inputs cause a feature to activate and by how feature activation\naffects outputs. Using steering evaluations, we reveal that current pipelines\nprovide descriptions that fail to capture the causal effect of the feature on\noutputs. To fix this, we propose efficient, output-centric methods for\nautomatically generating feature descriptions. These methods use the tokens\nweighted higher after feature stimulation or the highest weight tokens after\napplying the vocabulary \"unembedding\" head directly to the feature. Our\noutput-centric descriptions better capture the causal effect of a feature on\nmodel outputs than input-centric descriptions, but combining the two leads to\nthe best performance on both input and output evaluations. Lastly, we show that\noutput-centric descriptions can be used to find inputs that activate features\npreviously thought to be \"dead\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated interpretability pipelines generate natural language descriptions\nfor the concepts represented by features in large language models (LLMs), such\nas plants or the first word in a sentence. These descriptions are derived using\ninputs that activate the feature, which may be a dimension or a direction in\nthe model's representation space. However, identifying activating inputs is\ncostly, and the mechanistic role of a feature in model behavior is determined\nboth by how inputs cause a feature to activate and by how feature activation\naffects outputs. Using steering evaluations, we reveal that current pipelines\nprovide descriptions that fail to capture the causal effect of the feature on\noutputs. To fix this, we propose efficient, output-centric methods for\nautomatically generating feature descriptions. These methods use the tokens\nweighted higher after feature stimulation or the highest weight tokens after\napplying the vocabulary \"unembedding\" head directly to the feature. Our\noutput-centric descriptions better capture the causal effect of a feature on\nmodel outputs than input-centric descriptions, but combining the two leads to\nthe best performance on both input and output evaluations. Lastly, we show that\noutput-centric descriptions can be used to find inputs that activate features\npreviously thought to be \"dead\"."
                },
                "authors": [
                    {
                        "name": "Yoav Gur-Arieh"
                    },
                    {
                        "name": "Roy Mayan"
                    },
                    {
                        "name": "Chen Agassy"
                    },
                    {
                        "name": "Atticus Geiger"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "arxiv_comment": "Accepted to ACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23548v1",
                "updated": "2025-05-29T15:26:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    26,
                    4,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:26:04Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    26,
                    4,
                    3,
                    149,
                    0
                ],
                "title": "Translation in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translation in the Wild"
                },
                "summary": "Large Language Models (LLMs) excel in translation among other things,\ndemonstrating competitive performance for many language pairs in zero- and\nfew-shot settings. But unlike dedicated neural machine translation models, LLMs\nare not trained on any translation-related objective. What explains their\nremarkable translation abilities? Are these abilities grounded in \"incidental\nbilingualism\" (Briakou et al. 2023) in training data? Does instruction tuning\ncontribute to it? Are LLMs capable of aligning and leveraging semantically\nidentical or similar monolingual contents from different corners of the\ninternet that are unlikely to fit in a single context window? I offer some\nreflections on this topic, informed by recent studies and growing user\nexperience. My working hypothesis is that LLMs' translation abilities originate\nin two different types of pre-training data that may be internalized by the\nmodels in different ways. I discuss the prospects for testing the \"duality\"\nhypothesis empirically and its implications for reconceptualizing translation,\nhuman and machine, in the age of deep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in translation among other things,\ndemonstrating competitive performance for many language pairs in zero- and\nfew-shot settings. But unlike dedicated neural machine translation models, LLMs\nare not trained on any translation-related objective. What explains their\nremarkable translation abilities? Are these abilities grounded in \"incidental\nbilingualism\" (Briakou et al. 2023) in training data? Does instruction tuning\ncontribute to it? Are LLMs capable of aligning and leveraging semantically\nidentical or similar monolingual contents from different corners of the\ninternet that are unlikely to fit in a single context window? I offer some\nreflections on this topic, informed by recent studies and growing user\nexperience. My working hypothesis is that LLMs' translation abilities originate\nin two different types of pre-training data that may be internalized by the\nmodels in different ways. I discuss the prospects for testing the \"duality\"\nhypothesis empirically and its implications for reconceptualizing translation,\nhuman and machine, in the age of deep learning."
                },
                "authors": [
                    {
                        "name": "Yuri Balashov"
                    }
                ],
                "author_detail": {
                    "name": "Yuri Balashov"
                },
                "author": "Yuri Balashov",
                "arxiv_comment": "4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23543v1",
                "updated": "2025-05-29T15:22:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    22,
                    18,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:22:18Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    22,
                    18,
                    3,
                    149,
                    0
                ],
                "title": "Position Paper: Metadata Enrichment Model: Integrating Neural Networks\n  and Semantic Knowledge Graphs for Cultural Heritage Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position Paper: Metadata Enrichment Model: Integrating Neural Networks\n  and Semantic Knowledge Graphs for Cultural Heritage Applications"
                },
                "summary": "The digitization of cultural heritage collections has opened new directions\nfor research, yet the lack of enriched metadata poses a substantial challenge\nto accessibility, interoperability, and cross-institutional collaboration. In\nseveral past years neural networks models such as YOLOv11 and Detectron2 have\nrevolutionized visual data analysis, but their application to domain-specific\ncultural artifacts - such as manuscripts and incunabula - remains limited by\nthe absence of methodologies that address structural feature extraction and\nsemantic interoperability. In this position paper, we argue, that the\nintegration of neural networks with semantic technologies represents a paradigm\nshift in cultural heritage digitization processes. We present the Metadata\nEnrichment Model (MEM), a conceptual framework designed to enrich metadata for\ndigitized collections by combining fine-tuned computer vision models, large\nlanguage models (LLMs) and structured knowledge graphs. The Multilayer Vision\nMechanism (MVM) appears as the key innovation of MEM. This iterative process\nimproves visual analysis by dynamically detecting nested features, such as text\nwithin seals or images within stamps. To expose MEM's potential, we apply it to\na dataset of digitized incunabula from the Jagiellonian Digital Library and\nrelease a manually annotated dataset of 105 manuscript pages. We examine the\npractical challenges of MEM's usage in real-world GLAM institutions, including\nthe need for domain-specific fine-tuning, the adjustment of enriched metadata\nwith Linked Data standards and computational costs. We present MEM as a\nflexible and extensible methodology. This paper contributes to the discussion\non how artificial intelligence and semantic web technologies can advance\ncultural heritage research, and also use these technologies in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The digitization of cultural heritage collections has opened new directions\nfor research, yet the lack of enriched metadata poses a substantial challenge\nto accessibility, interoperability, and cross-institutional collaboration. In\nseveral past years neural networks models such as YOLOv11 and Detectron2 have\nrevolutionized visual data analysis, but their application to domain-specific\ncultural artifacts - such as manuscripts and incunabula - remains limited by\nthe absence of methodologies that address structural feature extraction and\nsemantic interoperability. In this position paper, we argue, that the\nintegration of neural networks with semantic technologies represents a paradigm\nshift in cultural heritage digitization processes. We present the Metadata\nEnrichment Model (MEM), a conceptual framework designed to enrich metadata for\ndigitized collections by combining fine-tuned computer vision models, large\nlanguage models (LLMs) and structured knowledge graphs. The Multilayer Vision\nMechanism (MVM) appears as the key innovation of MEM. This iterative process\nimproves visual analysis by dynamically detecting nested features, such as text\nwithin seals or images within stamps. To expose MEM's potential, we apply it to\na dataset of digitized incunabula from the Jagiellonian Digital Library and\nrelease a manually annotated dataset of 105 manuscript pages. We examine the\npractical challenges of MEM's usage in real-world GLAM institutions, including\nthe need for domain-specific fine-tuning, the adjustment of enriched metadata\nwith Linked Data standards and computational costs. We present MEM as a\nflexible and extensible methodology. This paper contributes to the discussion\non how artificial intelligence and semantic web technologies can advance\ncultural heritage research, and also use these technologies in practice."
                },
                "authors": [
                    {
                        "name": "Jan Ignatowicz"
                    },
                    {
                        "name": "Krzysztof Kutt"
                    },
                    {
                        "name": "Grzegorz J. Nalepa"
                    }
                ],
                "author_detail": {
                    "name": "Grzegorz J. Nalepa"
                },
                "author": "Grzegorz J. Nalepa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11077v2",
                "updated": "2025-05-29T15:22:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    22,
                    9,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-16T10:08:25Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    10,
                    8,
                    25,
                    4,
                    136,
                    0
                ],
                "title": "LLM-Enhanced Symbolic Control for Safety-Critical Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced Symbolic Control for Safety-Critical Applications"
                },
                "summary": "Motivated by Smart Manufacturing and Industry 4.0, we introduce a framework\nfor synthesizing Abstraction-Based Controller Design (ABCD) for reach-avoid\nproblems from Natural Language (NL) specifications using Large Language Models\n(LLMs). A Code Agent interprets an NL description of the control problem and\ntranslates it into a formal language interpretable by state-of-the-art symbolic\ncontrol software, while a Checker Agent verifies the correctness of the\ngenerated code and enhances safety by identifying specification mismatches.\nEvaluations show that the system handles linguistic variability and improves\nrobustness over direct planning with LLMs. The proposed approach lowers the\nbarrier to formal control synthesis by enabling intuitive, NL-based task\ndefinition while maintaining safety guarantees through automated validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by Smart Manufacturing and Industry 4.0, we introduce a framework\nfor synthesizing Abstraction-Based Controller Design (ABCD) for reach-avoid\nproblems from Natural Language (NL) specifications using Large Language Models\n(LLMs). A Code Agent interprets an NL description of the control problem and\ntranslates it into a formal language interpretable by state-of-the-art symbolic\ncontrol software, while a Checker Agent verifies the correctness of the\ngenerated code and enhances safety by identifying specification mismatches.\nEvaluations show that the system handles linguistic variability and improves\nrobustness over direct planning with LLMs. The proposed approach lowers the\nbarrier to formal control synthesis by enabling intuitive, NL-based task\ndefinition while maintaining safety guarantees through automated validation."
                },
                "authors": [
                    {
                        "name": "Amir Bayat"
                    },
                    {
                        "name": "Alessandro Abate"
                    },
                    {
                        "name": "Necmiye Ozay"
                    },
                    {
                        "name": "Raphael M. Jungers"
                    }
                ],
                "author_detail": {
                    "name": "Raphael M. Jungers"
                },
                "author": "Raphael M. Jungers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23540v1",
                "updated": "2025-05-29T15:20:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    20,
                    44,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:20:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    20,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "Probability-Consistent Preference Optimization for Enhanced LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probability-Consistent Preference Optimization for Enhanced LLM\n  Reasoning"
                },
                "summary": "Recent advances in preference optimization have demonstrated significant\npotential for improving mathematical reasoning capabilities in large language\nmodels (LLMs). While current approaches leverage high-quality pairwise\npreference data through outcome-based criteria like answer correctness or\nconsistency, they fundamentally neglect the internal logical coherence of\nresponses. To overcome this, we propose Probability-Consistent Preference\nOptimization (PCPO), a novel framework that establishes dual quantitative\nmetrics for preference selection: (1) surface-level answer correctness and (2)\nintrinsic token-level probability consistency across responses. Extensive\nexperiments show that our PCPO consistently outperforms existing outcome-only\ncriterion approaches across a diverse range of LLMs and benchmarks. Our code is\npublicly available at https://github.com/YunqiaoYang/PCPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in preference optimization have demonstrated significant\npotential for improving mathematical reasoning capabilities in large language\nmodels (LLMs). While current approaches leverage high-quality pairwise\npreference data through outcome-based criteria like answer correctness or\nconsistency, they fundamentally neglect the internal logical coherence of\nresponses. To overcome this, we propose Probability-Consistent Preference\nOptimization (PCPO), a novel framework that establishes dual quantitative\nmetrics for preference selection: (1) surface-level answer correctness and (2)\nintrinsic token-level probability consistency across responses. Extensive\nexperiments show that our PCPO consistently outperforms existing outcome-only\ncriterion approaches across a diverse range of LLMs and benchmarks. Our code is\npublicly available at https://github.com/YunqiaoYang/PCPO."
                },
                "authors": [
                    {
                        "name": "Yunqiao Yang"
                    },
                    {
                        "name": "Houxing Ren"
                    },
                    {
                        "name": "Zimu Lu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Weikang Shi"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Mingjie Zhan"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "14 pages, to be published in ACL 2025 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01662v2",
                "updated": "2025-05-29T15:20:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    20,
                    23,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-01T05:22:11Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    5,
                    22,
                    11,
                    5,
                    32,
                    0
                ],
                "title": "Fast Large Language Model Collaborative Decoding via Speculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Large Language Model Collaborative Decoding via Speculation"
                },
                "summary": "Large Language Model (LLM) collaborative decoding techniques improve output\nquality by combining the outputs of multiple models at each generation step,\nbut they incur high computational costs. In this paper, we introduce\nCollaborative decoding via Speculation (CoS), a novel framework that\naccelerates collaborative decoding without compromising performance. Inspired\nby Speculative Decoding--where a small proposal model generates tokens\nsequentially, and a larger target model verifies them in parallel, our approach\nbuilds on two key insights: (1) the verification distribution can be the\ncombined distribution of both the proposal and target models, and (2)\nalternating each model as the proposer and verifier can further enhance\nefficiency. We generalize this method to collaboration among n models and\ntheoretically prove that CoS is never slower than standard collaborative\ndecoding, typically achieving faster speed. Extensive experiments demonstrate\nCoS is 1.11x-2.23x faster than standard collaborative decoding without\ncompromising generation quality. Our code is available at\nhttps://github.com/Kamichanw/CoS/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) collaborative decoding techniques improve output\nquality by combining the outputs of multiple models at each generation step,\nbut they incur high computational costs. In this paper, we introduce\nCollaborative decoding via Speculation (CoS), a novel framework that\naccelerates collaborative decoding without compromising performance. Inspired\nby Speculative Decoding--where a small proposal model generates tokens\nsequentially, and a larger target model verifies them in parallel, our approach\nbuilds on two key insights: (1) the verification distribution can be the\ncombined distribution of both the proposal and target models, and (2)\nalternating each model as the proposer and verifier can further enhance\nefficiency. We generalize this method to collaboration among n models and\ntheoretically prove that CoS is never slower than standard collaborative\ndecoding, typically achieving faster speed. Extensive experiments demonstrate\nCoS is 1.11x-2.23x faster than standard collaborative decoding without\ncompromising generation quality. Our code is available at\nhttps://github.com/Kamichanw/CoS/."
                },
                "authors": [
                    {
                        "name": "Jiale Fu"
                    },
                    {
                        "name": "Yuchu Jiang"
                    },
                    {
                        "name": "Junkai Chen"
                    },
                    {
                        "name": "Jiaming Fan"
                    },
                    {
                        "name": "Xin Geng"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23537v1",
                "updated": "2025-05-29T15:18:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    18,
                    33,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T15:18:33Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    18,
                    33,
                    3,
                    149,
                    0
                ],
                "title": "Domain-Aware Tensor Network Structure Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-Aware Tensor Network Structure Search"
                },
                "summary": "Tensor networks (TNs) provide efficient representations of high-dimensional\ndata, yet identification of the optimal TN structures, the so called tensor\nnetwork structure search (TN-SS) problem, remains a challenge. Current\nstate-of-the-art (SOTA) algorithms are computationally expensive as they\nrequire extensive function evaluations, which is prohibitive for real-world\napplications. In addition, existing methods ignore valuable domain information\ninherent in real-world tensor data and lack transparency in their identified TN\nstructures. To this end, we propose a novel TN-SS framework, termed the tnLLM,\nwhich incorporates domain information about the data and harnesses the\nreasoning capabilities of large language models (LLMs) to directly predict\nsuitable TN structures. The proposed framework involves a domain-aware\nprompting pipeline which instructs the LLM to infer suitable TN structures\nbased on the real-world relationships between tensor modes. In this way, our\napproach is capable of not only iteratively optimizing the objective function,\nbut also generating domain-aware explanations for the identified structures.\nExperimental results demonstrate that tnLLM achieves comparable TN-SS objective\nfunction values with much fewer function evaluations compared to SOTA\nalgorithms. Furthermore, we demonstrate that the LLM-enabled domain information\ncan be used to find good initializations in the search space for sampling-based\nSOTA methods to accelerate their convergence while preserving theoretical\nperformance guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor networks (TNs) provide efficient representations of high-dimensional\ndata, yet identification of the optimal TN structures, the so called tensor\nnetwork structure search (TN-SS) problem, remains a challenge. Current\nstate-of-the-art (SOTA) algorithms are computationally expensive as they\nrequire extensive function evaluations, which is prohibitive for real-world\napplications. In addition, existing methods ignore valuable domain information\ninherent in real-world tensor data and lack transparency in their identified TN\nstructures. To this end, we propose a novel TN-SS framework, termed the tnLLM,\nwhich incorporates domain information about the data and harnesses the\nreasoning capabilities of large language models (LLMs) to directly predict\nsuitable TN structures. The proposed framework involves a domain-aware\nprompting pipeline which instructs the LLM to infer suitable TN structures\nbased on the real-world relationships between tensor modes. In this way, our\napproach is capable of not only iteratively optimizing the objective function,\nbut also generating domain-aware explanations for the identified structures.\nExperimental results demonstrate that tnLLM achieves comparable TN-SS objective\nfunction values with much fewer function evaluations compared to SOTA\nalgorithms. Furthermore, we demonstrate that the LLM-enabled domain information\ncan be used to find good initializations in the search space for sampling-based\nSOTA methods to accelerate their convergence while preserving theoretical\nperformance guarantees."
                },
                "authors": [
                    {
                        "name": "Giorgos Iacovides"
                    },
                    {
                        "name": "Wuyang Zhou"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Qibin Zhao"
                    },
                    {
                        "name": "Danilo Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo Mandic"
                },
                "author": "Danilo Mandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21432v2",
                "updated": "2025-05-29T15:15:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    15,
                    19,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-27T17:04:21Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    4,
                    21,
                    1,
                    147,
                    0
                ],
                "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model"
                },
                "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments."
                },
                "authors": [
                    {
                        "name": "Haoming Song"
                    },
                    {
                        "name": "Delin Qu"
                    },
                    {
                        "name": "Yuanqi Yao"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Qi Lv"
                    },
                    {
                        "name": "Yiwen Tang"
                    },
                    {
                        "name": "Modi Shi"
                    },
                    {
                        "name": "Guanghui Ren"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Bin Zhao"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23520v1",
                "updated": "2025-05-29T14:59:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    59,
                    6,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:59:06Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    59,
                    6,
                    3,
                    149,
                    0
                ],
                "title": "AnchorAttention: Difference-Aware Sparse Attention with Stripe\n  Granularity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnchorAttention: Difference-Aware Sparse Attention with Stripe\n  Granularity"
                },
                "summary": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Dong Guo"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Guoliang Zhu"
                    },
                    {
                        "name": "Dian Ding"
                    },
                    {
                        "name": "Yiming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Zhang"
                },
                "author": "Yiming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01179v4",
                "updated": "2025-05-29T14:57:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    57,
                    31,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-03T09:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    13,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "Joint Localization and Activation Editing for Low-Resource Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Localization and Activation Editing for Low-Resource Fine-Tuning"
                },
                "summary": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly\nused to adapt LLMs. However, the effectiveness of standard PEFT methods is\nlimited in low-resource scenarios with only a few hundred examples. Recent\nadvances in interpretability research have inspired the emergence of activation\nediting (or steering) techniques, which modify the activations of specific\nmodel components. Due to their extremely small parameter counts, these methods\nshow promise for small datasets. However, their performance is highly dependent\non identifying the correct modules to edit and often lacks stability across\ndifferent datasets. In this paper, we propose Joint Localization and Activation\nEditing (JoLA), a method that jointly learns (1) which heads in the Transformer\nto edit (2) whether the intervention should be additive, multiplicative, or\nboth and (3) the intervention parameters themselves - the vectors applied as\nadditive offsets or multiplicative scalings to the head output. Through\nevaluations on three benchmarks spanning commonsense reasoning, natural\nlanguage understanding, and natural language generation, we demonstrate that\nJoLA consistently outperforms existing methods. The code for the method is\nreleased at https://github.com/wenlai-lavine/jola.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly\nused to adapt LLMs. However, the effectiveness of standard PEFT methods is\nlimited in low-resource scenarios with only a few hundred examples. Recent\nadvances in interpretability research have inspired the emergence of activation\nediting (or steering) techniques, which modify the activations of specific\nmodel components. Due to their extremely small parameter counts, these methods\nshow promise for small datasets. However, their performance is highly dependent\non identifying the correct modules to edit and often lacks stability across\ndifferent datasets. In this paper, we propose Joint Localization and Activation\nEditing (JoLA), a method that jointly learns (1) which heads in the Transformer\nto edit (2) whether the intervention should be additive, multiplicative, or\nboth and (3) the intervention parameters themselves - the vectors applied as\nadditive offsets or multiplicative scalings to the head output. Through\nevaluations on three benchmarks spanning commonsense reasoning, natural\nlanguage understanding, and natural language generation, we demonstrate that\nJoLA consistently outperforms existing methods. The code for the method is\nreleased at https://github.com/wenlai-lavine/jola."
                },
                "authors": [
                    {
                        "name": "Wen Lai"
                    },
                    {
                        "name": "Alexander Fraser"
                    },
                    {
                        "name": "Ivan Titov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Titov"
                },
                "author": "Ivan Titov",
                "arxiv_comment": "Accepted by ICML 2025 (camera-ready version). The code is released at\n  https://github.com/wenlai-lavine/jola",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23518v1",
                "updated": "2025-05-29T14:57:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    57,
                    16,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:57:16Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    57,
                    16,
                    3,
                    149,
                    0
                ],
                "title": "TRAP: Targeted Redirecting of Agentic Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRAP: Targeted Redirecting of Agentic Preferences"
                },
                "summary": "Autonomous agentic AI systems powered by vision-language models (VLMs) are\nrapidly advancing toward real-world deployment, yet their cross-modal reasoning\ncapabilities introduce new attack surfaces for adversarial manipulation that\nexploit semantic reasoning across modalities. Existing adversarial attacks\ntypically rely on visible pixel perturbations or require privileged model or\nenvironment access, making them impractical for stealthy, real-world\nexploitation. We introduce TRAP, a generative adversarial framework that\nmanipulates the agent's decision-making using diffusion-based semantic\ninjections. Our method combines negative prompt-based degradation with positive\nsemantic optimization, guided by a Siamese semantic network and layout-aware\nspatial masking. Without requiring access to model internals, TRAP produces\nvisually natural images yet induces consistent selection biases in agentic AI\nsystems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO)\ndataset, building multi-candidate decision scenarios. Across these scenarios,\nTRAP achieves a 100% attack success rate on leading models, including\nLLaVA-34B, Gemma3, and Mistral-3.1, significantly outperforming baselines such\nas SPSA, Bandit, and standard diffusion approaches. These results expose a\ncritical vulnerability: Autonomous agents can be consistently misled through\nhuman-imperceptible cross-modal manipulations. These findings highlight the\nneed for defense strategies beyond pixel-level robustness to address semantic\nvulnerabilities in cross-modal decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agentic AI systems powered by vision-language models (VLMs) are\nrapidly advancing toward real-world deployment, yet their cross-modal reasoning\ncapabilities introduce new attack surfaces for adversarial manipulation that\nexploit semantic reasoning across modalities. Existing adversarial attacks\ntypically rely on visible pixel perturbations or require privileged model or\nenvironment access, making them impractical for stealthy, real-world\nexploitation. We introduce TRAP, a generative adversarial framework that\nmanipulates the agent's decision-making using diffusion-based semantic\ninjections. Our method combines negative prompt-based degradation with positive\nsemantic optimization, guided by a Siamese semantic network and layout-aware\nspatial masking. Without requiring access to model internals, TRAP produces\nvisually natural images yet induces consistent selection biases in agentic AI\nsystems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO)\ndataset, building multi-candidate decision scenarios. Across these scenarios,\nTRAP achieves a 100% attack success rate on leading models, including\nLLaVA-34B, Gemma3, and Mistral-3.1, significantly outperforming baselines such\nas SPSA, Bandit, and standard diffusion approaches. These results expose a\ncritical vulnerability: Autonomous agents can be consistently misled through\nhuman-imperceptible cross-modal manipulations. These findings highlight the\nneed for defense strategies beyond pixel-level robustness to address semantic\nvulnerabilities in cross-modal decision-making."
                },
                "authors": [
                    {
                        "name": "Hangoo Kang"
                    },
                    {
                        "name": "Jehyeok Yeon"
                    },
                    {
                        "name": "Gagandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Gagandeep Singh"
                },
                "author": "Gagandeep Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23516v1",
                "updated": "2025-05-29T14:56:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    56,
                    26,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:56:26Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    56,
                    26,
                    3,
                    149,
                    0
                ],
                "title": "The CASE Framework -- A New Architecture for Participatory Research and\n  Digital Health Surveillance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CASE Framework -- A New Architecture for Participatory Research and\n  Digital Health Surveillance"
                },
                "summary": "We present the CASE framework, an open-source platform for adaptive,\ncontext-aware participatory research, and pandemic preparedness. CASE\nimplements an event-driven architecture that enables dynamic survey workflows,\nallowing real-time adaptation based on participant responses, external data,\ntemporal conditions, and evolving user states. The framework supports a broad\nrange of research needs, from simple one-time questionnaires to complex\nlongitudinal studies with advanced conditional logic. Built on over a decade of\npractical experience, CASE underwent a major architectural rework in 2024,\ntransitioning from a microservice-based design to a streamlined monolithic\narchitecture. This evolution significantly improved maintainability,\nflexibility, and accessibility to deployment, particularly for institutions\nwith limited technical capacity. CASE has been successfully deployed across\ndiverse domains, powering national disease surveillance platforms, supporting\npost-COVID cohort studies, and enabling real-time sentiment analysis during\npolitical events. These applications, involving tens of thousands of\nparticipants, demonstrate the framework's scalability, versatility, and\npractical value. This paper describes the foundations of CASE, details its\narchitectural evolution, and presents lessons learned from real-world\ndeployments. We establish CASE as a mature and reusable research infrastructure\nthat balances sophisticated functionality with practical implementation,\naddressing the critical global need for sustainable and institutionally\ncontrolled data collection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the CASE framework, an open-source platform for adaptive,\ncontext-aware participatory research, and pandemic preparedness. CASE\nimplements an event-driven architecture that enables dynamic survey workflows,\nallowing real-time adaptation based on participant responses, external data,\ntemporal conditions, and evolving user states. The framework supports a broad\nrange of research needs, from simple one-time questionnaires to complex\nlongitudinal studies with advanced conditional logic. Built on over a decade of\npractical experience, CASE underwent a major architectural rework in 2024,\ntransitioning from a microservice-based design to a streamlined monolithic\narchitecture. This evolution significantly improved maintainability,\nflexibility, and accessibility to deployment, particularly for institutions\nwith limited technical capacity. CASE has been successfully deployed across\ndiverse domains, powering national disease surveillance platforms, supporting\npost-COVID cohort studies, and enabling real-time sentiment analysis during\npolitical events. These applications, involving tens of thousands of\nparticipants, demonstrate the framework's scalability, versatility, and\npractical value. This paper describes the foundations of CASE, details its\narchitectural evolution, and presents lessons learned from real-world\ndeployments. We establish CASE as a mature and reusable research infrastructure\nthat balances sophisticated functionality with practical implementation,\naddressing the critical global need for sustainable and institutionally\ncontrolled data collection systems."
                },
                "authors": [
                    {
                        "name": "Marco Hirsch"
                    },
                    {
                        "name": "Peter Hevesi"
                    },
                    {
                        "name": "Paul Lukowicz"
                    }
                ],
                "author_detail": {
                    "name": "Paul Lukowicz"
                },
                "author": "Paul Lukowicz",
                "arxiv_comment": "10 pages, 5 figures, submitted as a preprint to arXiv (no prior\n  publication)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.3; D.2.11; H.3.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16081v2",
                "updated": "2025-05-29T14:53:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    53,
                    45,
                    3,
                    149,
                    0
                ],
                "published": "2024-08-28T18:25:35Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    18,
                    25,
                    35,
                    2,
                    241,
                    0
                ],
                "title": "Towards Logically Sound Natural Language Reasoning with Logic-Enhanced\n  Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Logically Sound Natural Language Reasoning with Logic-Enhanced\n  Language Model Agents"
                },
                "summary": "Large language models (LLMs) are increasingly explored as general-purpose\nreasoners, particularly in agentic contexts. However, their outputs remain\nprone to mathematical and logical errors. This is especially challenging in\nopen-ended tasks, where unstructured outputs lack explicit ground truth and may\ncontain subtle inconsistencies. To address this issue, we propose\nLogic-Enhanced Language Model Agents (LELMA), a framework that integrates LLMs\nwith formal logic to enable validation and refinement of natural language\nreasoning. LELMA comprises three components: an LLM-Reasoner, an\nLLM-Translator, and a Solver, and employs autoformalization to translate\nreasoning into logic representations, which are then used to assess logical\nvalidity. Using game-theoretic scenarios such as the Prisoner's Dilemma as\ntestbeds, we highlight the limitations of both less capable (Gemini 1.0 Pro)\nand advanced (GPT-4o) models in generating logically sound reasoning. LELMA\nachieves high accuracy in error detection and improves reasoning correctness\nvia self-refinement, particularly in GPT-4o. The study also highlights\nchallenges in autoformalization accuracy and in evaluation of inherently\nambiguous open-ended reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly explored as general-purpose\nreasoners, particularly in agentic contexts. However, their outputs remain\nprone to mathematical and logical errors. This is especially challenging in\nopen-ended tasks, where unstructured outputs lack explicit ground truth and may\ncontain subtle inconsistencies. To address this issue, we propose\nLogic-Enhanced Language Model Agents (LELMA), a framework that integrates LLMs\nwith formal logic to enable validation and refinement of natural language\nreasoning. LELMA comprises three components: an LLM-Reasoner, an\nLLM-Translator, and a Solver, and employs autoformalization to translate\nreasoning into logic representations, which are then used to assess logical\nvalidity. Using game-theoretic scenarios such as the Prisoner's Dilemma as\ntestbeds, we highlight the limitations of both less capable (Gemini 1.0 Pro)\nand advanced (GPT-4o) models in generating logically sound reasoning. LELMA\nachieves high accuracy in error detection and improves reasoning correctness\nvia self-refinement, particularly in GPT-4o. The study also highlights\nchallenges in autoformalization accuracy and in evaluation of inherently\nambiguous open-ended reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Agnieszka Mensfelt"
                    },
                    {
                        "name": "Kostas Stathis"
                    },
                    {
                        "name": "Vince Trencsenyi"
                    }
                ],
                "author_detail": {
                    "name": "Vince Trencsenyi"
                },
                "author": "Vince Trencsenyi",
                "arxiv_comment": "Source code: https://github.com/dicelab-rhul/LELMA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16143v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16143v2",
                "updated": "2025-05-29T14:52:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    52,
                    7,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-27T15:36:51Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    36,
                    51,
                    0,
                    27,
                    0
                ],
                "title": "Disruption-aware Microservice Re-orchestration for Cost-efficient\n  Multi-cloud Deployments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disruption-aware Microservice Re-orchestration for Cost-efficient\n  Multi-cloud Deployments"
                },
                "summary": "Multi-cloud environments enable a cost-efficient scaling of cloud-native\napplications across geographically distributed virtual nodes with different\npricing models. In this context, the resource fragmentation caused by frequent\nchanges in the resource demands of deployed microservices, along with the\nallocation or termination of new and existing microservices, increases the\ndeployment cost. Therefore, re-orchestrating deployed microservices on a\ncheaper configuration of multi-cloud nodes offers a practical solution to\nrestore the cost efficiency of deployment. However, the rescheduling procedure\ncauses frequent service interruptions due to the continuous termination and\nrebooting of the containerized microservices. Moreover, it may potentially\ninterfere with and delay other deployment operations, compromising the\nstability of the running applications. To address this issue, we formulate a\nmulti-objective integer linear programming (ILP) problem that computes a\nmicroservice rescheduling solution capable of providing minimum deployment cost\nwithout significantly affecting the service continuity. At the same time, the\nproposed formulation also preserves the quality of service (QoS) requirements,\nincluding latency, expressed through microservice co-location constraints.\nAdditionally, we present a heuristic algorithm to approximate the optimal\nsolution, striking a balance between cost reduction and service disruption\nmitigation. We integrate the proposed approach as a custom plugin of the\nKubernetes (K8s) scheduler. Results reveal that our approach significantly\nreduces multi-cloud deployment costs and service disruptions compared to the\nbenchmark schemes, while ensuring QoS requirements are consistently met.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-cloud environments enable a cost-efficient scaling of cloud-native\napplications across geographically distributed virtual nodes with different\npricing models. In this context, the resource fragmentation caused by frequent\nchanges in the resource demands of deployed microservices, along with the\nallocation or termination of new and existing microservices, increases the\ndeployment cost. Therefore, re-orchestrating deployed microservices on a\ncheaper configuration of multi-cloud nodes offers a practical solution to\nrestore the cost efficiency of deployment. However, the rescheduling procedure\ncauses frequent service interruptions due to the continuous termination and\nrebooting of the containerized microservices. Moreover, it may potentially\ninterfere with and delay other deployment operations, compromising the\nstability of the running applications. To address this issue, we formulate a\nmulti-objective integer linear programming (ILP) problem that computes a\nmicroservice rescheduling solution capable of providing minimum deployment cost\nwithout significantly affecting the service continuity. At the same time, the\nproposed formulation also preserves the quality of service (QoS) requirements,\nincluding latency, expressed through microservice co-location constraints.\nAdditionally, we present a heuristic algorithm to approximate the optimal\nsolution, striking a balance between cost reduction and service disruption\nmitigation. We integrate the proposed approach as a custom plugin of the\nKubernetes (K8s) scheduler. Results reveal that our approach significantly\nreduces multi-cloud deployment costs and service disruptions compared to the\nbenchmark schemes, while ensuring QoS requirements are consistently met."
                },
                "authors": [
                    {
                        "name": "Marco Zambianco"
                    },
                    {
                        "name": "Silvio Cretti"
                    },
                    {
                        "name": "Domenico Siracusa"
                    }
                ],
                "author_detail": {
                    "name": "Domenico Siracusa"
                },
                "author": "Domenico Siracusa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16143v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16143v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09955v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09955v3",
                "updated": "2025-05-29T14:51:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    51,
                    49,
                    3,
                    149,
                    0
                ],
                "published": "2024-08-19T12:55:16Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    55,
                    16,
                    0,
                    232,
                    0
                ],
                "title": "MegaAgent: A Large-Scale Autonomous LLM-based Multi-Agent System Without\n  Predefined SOPs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaAgent: A Large-Scale Autonomous LLM-based Multi-Agent System Without\n  Predefined SOPs"
                },
                "summary": "LLM-based multi-agent systems (MAS) have shown promise in tackling complex\ntasks. However, existing solutions often suffer from limited agent coordination\nand heavy reliance on predefined Standard Operating Procedures (SOPs), which\ndemand extensive human input. To address these limitations, we propose\nMegaAgent, a large-scale autonomous LLM-based multi-agent system. MegaAgent\ngenerates agents based on task complexity and enables dynamic task\ndecomposition, parallel execution, efficient communication, and comprehensive\nsystem monitoring of agents. In evaluations, MegaAgent demonstrates exceptional\nperformance, successfully developing a Gobang game within 800 seconds and\nscaling up to 590 agents in a national policy simulation to generate\nmulti-domain policies. It significantly outperforms existing systems, such as\nMetaGPT, in both task completion efficiency and scalability. By eliminating the\nneed for predefined SOPs, MegaAgent demonstrates exceptional scalability and\nautonomy, setting a foundation for advancing true autonomy in MAS. Our code is\navailable at https://github.com/Xtra-Computing/MegaAgent .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based multi-agent systems (MAS) have shown promise in tackling complex\ntasks. However, existing solutions often suffer from limited agent coordination\nand heavy reliance on predefined Standard Operating Procedures (SOPs), which\ndemand extensive human input. To address these limitations, we propose\nMegaAgent, a large-scale autonomous LLM-based multi-agent system. MegaAgent\ngenerates agents based on task complexity and enables dynamic task\ndecomposition, parallel execution, efficient communication, and comprehensive\nsystem monitoring of agents. In evaluations, MegaAgent demonstrates exceptional\nperformance, successfully developing a Gobang game within 800 seconds and\nscaling up to 590 agents in a national policy simulation to generate\nmulti-domain policies. It significantly outperforms existing systems, such as\nMetaGPT, in both task completion efficiency and scalability. By eliminating the\nneed for predefined SOPs, MegaAgent demonstrates exceptional scalability and\nautonomy, setting a foundation for advancing true autonomy in MAS. Our code is\navailable at https://github.com/Xtra-Computing/MegaAgent ."
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Qinbin Li"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Jingsheng Liang"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09955v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09955v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09948v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09948v3",
                "updated": "2025-05-29T14:49:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    49,
                    44,
                    3,
                    149,
                    0
                ],
                "published": "2023-11-16T15:01:48Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    15,
                    1,
                    48,
                    3,
                    320,
                    0
                ],
                "title": "Hijacking Large Language Models via Adversarial In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hijacking Large Language Models via Adversarial In-Context Learning"
                },
                "summary": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs\nfor specific downstream tasks by utilizing labeled examples as demonstrations\n(demos) in the preconditioned prompts. Despite its promising performance,\ncrafted adversarial attacks pose a notable threat to the robustness of LLMs.\nExisting attacks are either easy to detect, require a trigger in user input, or\nlack specificity towards ICL. To address these issues, this work introduces a\nnovel transferable prompt injection attack against ICL, aiming to hijack LLMs\nto generate the target output or elicit harmful responses. In our threat model,\nthe hacker acts as a model publisher who leverages a gradient-based prompt\nsearch method to learn and append imperceptible adversarial suffixes to the\nin-context demos via prompt injection. We also propose effective defense\nstrategies using a few shots of clean demos, enhancing the robustness of LLMs\nduring ICL. Extensive experimental results across various classification and\njailbreak tasks demonstrate the effectiveness of the proposed attack and\ndefense strategies. This work highlights the significant security\nvulnerabilities of LLMs during ICL and underscores the need for further\nin-depth studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs\nfor specific downstream tasks by utilizing labeled examples as demonstrations\n(demos) in the preconditioned prompts. Despite its promising performance,\ncrafted adversarial attacks pose a notable threat to the robustness of LLMs.\nExisting attacks are either easy to detect, require a trigger in user input, or\nlack specificity towards ICL. To address these issues, this work introduces a\nnovel transferable prompt injection attack against ICL, aiming to hijack LLMs\nto generate the target output or elicit harmful responses. In our threat model,\nthe hacker acts as a model publisher who leverages a gradient-based prompt\nsearch method to learn and append imperceptible adversarial suffixes to the\nin-context demos via prompt injection. We also propose effective defense\nstrategies using a few shots of clean demos, enhancing the robustness of LLMs\nduring ICL. Extensive experimental results across various classification and\njailbreak tasks demonstrate the effectiveness of the proposed attack and\ndefense strategies. This work highlights the significant security\nvulnerabilities of LLMs during ICL and underscores the need for further\nin-depth studies."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhou"
                    },
                    {
                        "name": "Yao Qiang"
                    },
                    {
                        "name": "Saleh Zare Zade"
                    },
                    {
                        "name": "Prashant Khanduri"
                    },
                    {
                        "name": "Dongxiao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Dongxiao Zhu"
                },
                "author": "Dongxiao Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09948v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09948v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23503v1",
                "updated": "2025-05-29T14:48:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    48,
                    9,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:48:09Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    48,
                    9,
                    3,
                    149,
                    0
                ],
                "title": "Can Large Language Models Challenge CNNS in Medical Image Analysis?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Challenge CNNS in Medical Image Analysis?"
                },
                "summary": "This study presents a multimodal AI framework designed for precisely\nclassifying medical diagnostic images. Utilizing publicly available datasets,\nthe proposed system compares the strengths of convolutional neural networks\n(CNNs) and different large language models (LLMs). This in-depth comparative\nanalysis highlights key differences in diagnostic performance, execution\nefficiency, and environmental impacts. Model evaluation was based on accuracy,\nF1-score, average execution time, average energy consumption, and estimated\n$CO_2$ emission. The findings indicate that although CNN-based models can\noutperform various multimodal techniques that incorporate both images and\ncontextual information, applying additional filtering on top of LLMs can lead\nto substantial performance gains. These findings highlight the transformative\npotential of multimodal AI systems to enhance the reliability, efficiency, and\nscalability of medical diagnostics in clinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a multimodal AI framework designed for precisely\nclassifying medical diagnostic images. Utilizing publicly available datasets,\nthe proposed system compares the strengths of convolutional neural networks\n(CNNs) and different large language models (LLMs). This in-depth comparative\nanalysis highlights key differences in diagnostic performance, execution\nefficiency, and environmental impacts. Model evaluation was based on accuracy,\nF1-score, average execution time, average energy consumption, and estimated\n$CO_2$ emission. The findings indicate that although CNN-based models can\noutperform various multimodal techniques that incorporate both images and\ncontextual information, applying additional filtering on top of LLMs can lead\nto substantial performance gains. These findings highlight the transformative\npotential of multimodal AI systems to enhance the reliability, efficiency, and\nscalability of medical diagnostics in clinical settings."
                },
                "authors": [
                    {
                        "name": "Shibbir Ahmed"
                    },
                    {
                        "name": "Shahnewaz Karim Sakib"
                    },
                    {
                        "name": "Anindya Bijoy Das"
                    }
                ],
                "author_detail": {
                    "name": "Anindya Bijoy Das"
                },
                "author": "Anindya Bijoy Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23495v1",
                "updated": "2025-05-29T14:44:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    44,
                    52,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:44:52Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    44,
                    52,
                    3,
                    149,
                    0
                ],
                "title": "Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More\n  Reliable Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More\n  Reliable Benchmarking"
                },
                "summary": "Knowledge Graph Question Answering (KGQA) systems rely on high-quality\nbenchmarks to evaluate complex multi-hop reasoning. However, despite their\nwidespread use, popular datasets such as WebQSP and CWQ suffer from critical\nquality issues, including inaccurate or incomplete ground-truth annotations,\npoorly constructed questions that are ambiguous, trivial, or unanswerable, and\noutdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA\ndatasets, including WebQSP and CWQ, we find that the average factual\ncorrectness rate is only 57 %. To address these issues, we introduce KGQAGen,\nan LLM-in-the-loop framework that systematically resolves these pitfalls.\nKGQAGen combines structured knowledge grounding, LLM-guided generation, and\nsymbolic verification to produce challenging and verifiable QA instances. Using\nKGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in\nWikidata, and evaluate a diverse set of KG-RAG models. Experimental results\ndemonstrate that even state-of-the-art systems struggle on this benchmark,\nhighlighting its ability to expose limitations of existing models. Our findings\nadvocate for more rigorous benchmark construction and position KGQAGen as a\nscalable framework for advancing KGQA evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph Question Answering (KGQA) systems rely on high-quality\nbenchmarks to evaluate complex multi-hop reasoning. However, despite their\nwidespread use, popular datasets such as WebQSP and CWQ suffer from critical\nquality issues, including inaccurate or incomplete ground-truth annotations,\npoorly constructed questions that are ambiguous, trivial, or unanswerable, and\noutdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA\ndatasets, including WebQSP and CWQ, we find that the average factual\ncorrectness rate is only 57 %. To address these issues, we introduce KGQAGen,\nan LLM-in-the-loop framework that systematically resolves these pitfalls.\nKGQAGen combines structured knowledge grounding, LLM-guided generation, and\nsymbolic verification to produce challenging and verifiable QA instances. Using\nKGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in\nWikidata, and evaluate a diverse set of KG-RAG models. Experimental results\ndemonstrate that even state-of-the-art systems struggle on this benchmark,\nhighlighting its ability to expose limitations of existing models. Our findings\nadvocate for more rigorous benchmark construction and position KGQAGen as a\nscalable framework for advancing KGQA evaluation."
                },
                "authors": [
                    {
                        "name": "Liangliang Zhang"
                    },
                    {
                        "name": "Zhuorui Jiang"
                    },
                    {
                        "name": "Hongliang Chi"
                    },
                    {
                        "name": "Haoyang Chen"
                    },
                    {
                        "name": "Mohammed Elkoumy"
                    },
                    {
                        "name": "Fali Wang"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Zhengyi Zhou"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Suhang Wang"
                    },
                    {
                        "name": "Yao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yao Ma"
                },
                "author": "Yao Ma",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13459v3",
                "updated": "2025-05-29T14:42:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    42,
                    38,
                    3,
                    149,
                    0
                ],
                "published": "2024-02-21T01:30:03Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    1,
                    30,
                    3,
                    2,
                    52,
                    0
                ],
                "title": "Learning to Poison Large Language Models for Downstream Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Poison Large Language Models for Downstream Manipulation"
                },
                "summary": "The advent of Large Language Models (LLMs) has marked significant\nachievements in language processing and reasoning capabilities. Despite their\nadvancements, LLMs face vulnerabilities to data poisoning attacks, where the\nadversary inserts backdoor triggers into training data to manipulate outputs.\nThis work further identifies additional security risks in LLMs by designing a\nnew data poisoning attack tailored to exploit the supervised fine-tuning (SFT)\nprocess. We propose a novel gradient-guided backdoor trigger learning (GBTL)\nalgorithm to identify adversarial triggers efficiently, ensuring an evasion of\ndetection by conventional defenses while maintaining content integrity. Through\nexperimental validation across various language model tasks, including\nsentiment analysis, domain generation, and question answering, our poisoning\nstrategy demonstrates a high success rate in compromising various LLMs'\noutputs. We further propose two defense strategies against data poisoning\nattacks, including in-context learning (ICL) and continuous learning (CL),\nwhich effectively rectify the behavior of LLMs and significantly reduce the\ndecline in performance. Our work highlights the significant security risks\npresent during SFT of LLMs and the necessity of safeguarding LLMs against data\npoisoning attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has marked significant\nachievements in language processing and reasoning capabilities. Despite their\nadvancements, LLMs face vulnerabilities to data poisoning attacks, where the\nadversary inserts backdoor triggers into training data to manipulate outputs.\nThis work further identifies additional security risks in LLMs by designing a\nnew data poisoning attack tailored to exploit the supervised fine-tuning (SFT)\nprocess. We propose a novel gradient-guided backdoor trigger learning (GBTL)\nalgorithm to identify adversarial triggers efficiently, ensuring an evasion of\ndetection by conventional defenses while maintaining content integrity. Through\nexperimental validation across various language model tasks, including\nsentiment analysis, domain generation, and question answering, our poisoning\nstrategy demonstrates a high success rate in compromising various LLMs'\noutputs. We further propose two defense strategies against data poisoning\nattacks, including in-context learning (ICL) and continuous learning (CL),\nwhich effectively rectify the behavior of LLMs and significantly reduce the\ndecline in performance. Our work highlights the significant security risks\npresent during SFT of LLMs and the necessity of safeguarding LLMs against data\npoisoning attacks."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhou"
                    },
                    {
                        "name": "Yao Qiang"
                    },
                    {
                        "name": "Saleh Zare Zade"
                    },
                    {
                        "name": "Mohammad Amin Roshani"
                    },
                    {
                        "name": "Prashant Khanduri"
                    },
                    {
                        "name": "Douglas Zytko"
                    },
                    {
                        "name": "Dongxiao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Dongxiao Zhu"
                },
                "author": "Dongxiao Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11862v2",
                "updated": "2025-05-29T14:42:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    42,
                    17,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-17T14:53:49Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    53,
                    49,
                    0,
                    48,
                    0
                ],
                "title": "Understanding In-Context Machine Translation for Low-Resource Languages:\n  A Case Study on Manchu",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding In-Context Machine Translation for Low-Resource Languages:\n  A Case Study on Manchu"
                },
                "summary": "In-context machine translation (MT) with large language models (LLMs) is a\npromising approach for low-resource MT, as it can readily take advantage of\nlinguistic resources such as grammar books and dictionaries. Such resources are\nusually selectively integrated into the prompt so that LLMs can directly\nperform translation without any specific training, via their in-context\nlearning capability (ICL). However, the relative importance of each type of\nresource, e.g., dictionary, grammar book, and retrieved parallel examples, is\nnot entirely clear. To address this gap, this study systematically investigates\nhow each resource and its quality affect the translation performance, with the\nManchu language as our case study. To remove any prior knowledge of Manchu\nencoded in the LLM parameters and single out the effect of ICL, we also\nexperiment with an enciphered version of Manchu texts. Our results indicate\nthat high-quality dictionaries and good parallel examples are very helpful,\nwhile grammars hardly help. In a follow-up study, we showcase a promising\napplication of in-context MT: parallel data augmentation as a way to bootstrap\na conventional MT model. When monolingual data abound, generating synthetic\nparallel data through in-context MT offers a pathway to mitigate data scarcity\nand build effective and efficient low-resource neural MT systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context machine translation (MT) with large language models (LLMs) is a\npromising approach for low-resource MT, as it can readily take advantage of\nlinguistic resources such as grammar books and dictionaries. Such resources are\nusually selectively integrated into the prompt so that LLMs can directly\nperform translation without any specific training, via their in-context\nlearning capability (ICL). However, the relative importance of each type of\nresource, e.g., dictionary, grammar book, and retrieved parallel examples, is\nnot entirely clear. To address this gap, this study systematically investigates\nhow each resource and its quality affect the translation performance, with the\nManchu language as our case study. To remove any prior knowledge of Manchu\nencoded in the LLM parameters and single out the effect of ICL, we also\nexperiment with an enciphered version of Manchu texts. Our results indicate\nthat high-quality dictionaries and good parallel examples are very helpful,\nwhile grammars hardly help. In a follow-up study, we showcase a promising\napplication of in-context MT: parallel data augmentation as a way to bootstrap\na conventional MT model. When monolingual data abound, generating synthetic\nparallel data through in-context MT offers a pathway to mitigate data scarcity\nand build effective and efficient low-resource neural MT systems."
                },
                "authors": [
                    {
                        "name": "Renhao Pei"
                    },
                    {
                        "name": "Yihong Liu"
                    },
                    {
                        "name": "Peiqin Lin"
                    },
                    {
                        "name": "François Yvon"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15819v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15819v2",
                "updated": "2025-05-29T14:39:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    39,
                    27,
                    3,
                    149,
                    0
                ],
                "published": "2024-12-20T12:01:01Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    12,
                    1,
                    1,
                    4,
                    355,
                    0
                ],
                "title": "Robustness-enhanced Myoelectric Control with GAN-based Open-set\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness-enhanced Myoelectric Control with GAN-based Open-set\n  Recognition"
                },
                "summary": "Electromyography (EMG) signals are widely used in human motion recognition\nand medical rehabilitation, yet their variability and susceptibility to noise\nsignificantly limit the reliability of myoelectric control systems. Existing\nrecognition algorithms often fail to handle unfamiliar actions effectively,\nleading to system instability and errors. This paper proposes a novel framework\nbased on Generative Adversarial Networks (GANs) to enhance the robustness and\nusability of myoelectric control systems by enabling open-set recognition. The\nmethod incorporates a GAN-based discriminator to identify and reject unknown\nactions, maintaining system stability by preventing misclassifications.\nExperimental evaluations on publicly available and self-collected datasets\ndemonstrate a recognition accuracy of 97.6\\% for known actions and a 23.6\\%\nimprovement in Active Error Rate (AER) after rejecting unknown actions. The\nproposed approach is computationally efficient and suitable for deployment on\nedge devices, making it practical for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electromyography (EMG) signals are widely used in human motion recognition\nand medical rehabilitation, yet their variability and susceptibility to noise\nsignificantly limit the reliability of myoelectric control systems. Existing\nrecognition algorithms often fail to handle unfamiliar actions effectively,\nleading to system instability and errors. This paper proposes a novel framework\nbased on Generative Adversarial Networks (GANs) to enhance the robustness and\nusability of myoelectric control systems by enabling open-set recognition. The\nmethod incorporates a GAN-based discriminator to identify and reject unknown\nactions, maintaining system stability by preventing misclassifications.\nExperimental evaluations on publicly available and self-collected datasets\ndemonstrate a recognition accuracy of 97.6\\% for known actions and a 23.6\\%\nimprovement in Active Error Rate (AER) after rejecting unknown actions. The\nproposed approach is computationally efficient and suitable for deployment on\nedge devices, making it practical for real-world applications."
                },
                "authors": [
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Ziyang Feng"
                    },
                    {
                        "name": "Pin Zhang"
                    },
                    {
                        "name": "Manjiang Cao"
                    },
                    {
                        "name": "Yiming Yuan"
                    },
                    {
                        "name": "Tengfei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Tengfei Chang"
                },
                "author": "Tengfei Chang",
                "arxiv_comment": "11 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15819v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15819v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22353v2",
                "updated": "2025-05-29T14:35:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    35,
                    0,
                    3,
                    149,
                    0
                ],
                "published": "2025-03-28T11:49:56Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    49,
                    56,
                    4,
                    87,
                    0
                ],
                "title": "Firm or Fickle? Evaluating Large Language Models Consistency in\n  Sequential Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Firm or Fickle? Evaluating Large Language Models Consistency in\n  Sequential Interactions"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious tasks, but their deployment in high-stake domains requires consistent\nperformance across multiple interaction rounds. This paper introduces a\ncomprehensive framework for evaluating and improving LLM response consistency,\nmaking three key contributions. First, we propose a novel Position-Weighted\nConsistency (PWC) score that captures both the importance of early-stage\nstability and recovery patterns in multi-turn interactions. Second, we present\na carefully curated benchmark dataset spanning diverse domains and difficulty\nlevels, specifically designed to evaluate LLM consistency under various\nchallenging follow-up scenarios. Third, we introduce Confidence-Aware Response\nGeneration (CARG), a framework that significantly improves response stability\nby incorporating model confidence signals into the generation process.\nEmpirical results demonstrate that CARG significantly improves response\nstability without sacrificing accuracy, underscoring its potential for reliable\nLLM deployment in critical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious tasks, but their deployment in high-stake domains requires consistent\nperformance across multiple interaction rounds. This paper introduces a\ncomprehensive framework for evaluating and improving LLM response consistency,\nmaking three key contributions. First, we propose a novel Position-Weighted\nConsistency (PWC) score that captures both the importance of early-stage\nstability and recovery patterns in multi-turn interactions. Second, we present\na carefully curated benchmark dataset spanning diverse domains and difficulty\nlevels, specifically designed to evaluate LLM consistency under various\nchallenging follow-up scenarios. Third, we introduce Confidence-Aware Response\nGeneration (CARG), a framework that significantly improves response stability\nby incorporating model confidence signals into the generation process.\nEmpirical results demonstrate that CARG significantly improves response\nstability without sacrificing accuracy, underscoring its potential for reliable\nLLM deployment in critical applications."
                },
                "authors": [
                    {
                        "name": "Yubo Li"
                    },
                    {
                        "name": "Yidi Miao"
                    },
                    {
                        "name": "Xueying Ding"
                    },
                    {
                        "name": "Ramayya Krishnan"
                    },
                    {
                        "name": "Rema Padman"
                    }
                ],
                "author_detail": {
                    "name": "Rema Padman"
                },
                "author": "Rema Padman",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23486v1",
                "updated": "2025-05-29T14:34:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    34,
                    54,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:34:54Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    34,
                    54,
                    3,
                    149,
                    0
                ],
                "title": "Autoformalization in the Era of Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization in the Era of Large Language Models: A Survey"
                },
                "summary": "Autoformalization, the process of transforming informal mathematical\npropositions into verifiable formal representations, is a foundational task in\nautomated theorem proving, offering a new perspective on the use of mathematics\nin both theoretical and applied domains. Driven by the rapid progress in\nartificial intelligence, particularly large language models (LLMs), this field\nhas witnessed substantial growth, bringing both new opportunities and unique\nchallenges. In this survey, we provide a comprehensive overview of recent\nadvances in autoformalization from both mathematical and LLM-centric\nperspectives. We examine how autoformalization is applied across various\nmathematical domains and levels of difficulty, and analyze the end-to-end\nworkflow from data preprocessing to model design and evaluation. We further\nexplore the emerging role of autoformalization in enhancing the verifiability\nof LLM-generated outputs, highlighting its potential to improve both the\ntrustworthiness and reasoning capabilities of LLMs. Finally, we summarize key\nopen-source models and datasets supporting current research, and discuss open\nchallenges and promising future directions for the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization, the process of transforming informal mathematical\npropositions into verifiable formal representations, is a foundational task in\nautomated theorem proving, offering a new perspective on the use of mathematics\nin both theoretical and applied domains. Driven by the rapid progress in\nartificial intelligence, particularly large language models (LLMs), this field\nhas witnessed substantial growth, bringing both new opportunities and unique\nchallenges. In this survey, we provide a comprehensive overview of recent\nadvances in autoformalization from both mathematical and LLM-centric\nperspectives. We examine how autoformalization is applied across various\nmathematical domains and levels of difficulty, and analyze the end-to-end\nworkflow from data preprocessing to model design and evaluation. We further\nexplore the emerging role of autoformalization in enhancing the verifiability\nof LLM-generated outputs, highlighting its potential to improve both the\ntrustworthiness and reasoning capabilities of LLMs. Finally, we summarize key\nopen-source models and datasets supporting current research, and discuss open\nchallenges and promising future directions for the field."
                },
                "authors": [
                    {
                        "name": "Ke Weng"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Sirui Li"
                    },
                    {
                        "name": "Wangyue Lu"
                    },
                    {
                        "name": "Haozhe Sun"
                    },
                    {
                        "name": "Hengyu Liu"
                    },
                    {
                        "name": "Tiancheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tiancheng Zhang"
                },
                "author": "Tiancheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23484v1",
                "updated": "2025-05-29T14:34:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    34,
                    25,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:34:25Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    34,
                    25,
                    3,
                    149,
                    0
                ],
                "title": "VCapsBench: A Large-scale Fine-grained Benchmark for Video Caption\n  Quality Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VCapsBench: A Large-scale Fine-grained Benchmark for Video Caption\n  Quality Evaluation"
                },
                "summary": "Video captions play a crucial role in text-to-video generation tasks, as\ntheir quality directly influences the semantic coherence and visual fidelity of\nthe generated videos. Although large vision-language models (VLMs) have\ndemonstrated significant potential in caption generation, existing benchmarks\ninadequately address fine-grained evaluation, particularly in capturing\nspatial-temporal details critical for video generation. To address this gap, we\nintroduce the Fine-grained Video Caption Evaluation Benchmark (VCapsBench), the\nfirst large-scale fine-grained benchmark comprising 5,677 (5K+) videos and\n109,796 (100K+) question-answer pairs. These QA-pairs are systematically\nannotated across 21 fine-grained dimensions (e.g., camera movement, and shot\ntype) that are empirically proven critical for text-to-video generation. We\nfurther introduce three metrics (Accuracy (AR), Inconsistency Rate (IR),\nCoverage Rate (CR)), and an automated evaluation pipeline leveraging large\nlanguage model (LLM) to verify caption quality via contrastive QA-pairs\nanalysis. By providing actionable insights for caption optimization, our\nbenchmark can advance the development of robust text-to-video models. The\ndataset and codes are available at website: https://github.com/GXYM/VCapsBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video captions play a crucial role in text-to-video generation tasks, as\ntheir quality directly influences the semantic coherence and visual fidelity of\nthe generated videos. Although large vision-language models (VLMs) have\ndemonstrated significant potential in caption generation, existing benchmarks\ninadequately address fine-grained evaluation, particularly in capturing\nspatial-temporal details critical for video generation. To address this gap, we\nintroduce the Fine-grained Video Caption Evaluation Benchmark (VCapsBench), the\nfirst large-scale fine-grained benchmark comprising 5,677 (5K+) videos and\n109,796 (100K+) question-answer pairs. These QA-pairs are systematically\nannotated across 21 fine-grained dimensions (e.g., camera movement, and shot\ntype) that are empirically proven critical for text-to-video generation. We\nfurther introduce three metrics (Accuracy (AR), Inconsistency Rate (IR),\nCoverage Rate (CR)), and an automated evaluation pipeline leveraging large\nlanguage model (LLM) to verify caption quality via contrastive QA-pairs\nanalysis. By providing actionable insights for caption optimization, our\nbenchmark can advance the development of robust text-to-video models. The\ndataset and codes are available at website: https://github.com/GXYM/VCapsBench."
                },
                "authors": [
                    {
                        "name": "Shi-Xue Zhang"
                    },
                    {
                        "name": "Hongfa Wang"
                    },
                    {
                        "name": "Duojun Huang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Xiaobin Zhu"
                    },
                    {
                        "name": "Xu-Cheng Yin"
                    }
                ],
                "author_detail": {
                    "name": "Xu-Cheng Yin"
                },
                "author": "Xu-Cheng Yin",
                "arxiv_comment": "submitting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23477v1",
                "updated": "2025-05-29T14:27:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    27,
                    14,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:27:14Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    27,
                    14,
                    3,
                    149,
                    0
                ],
                "title": "Evaluating the performance and fragility of large language models on the\n  self-assessment for neurological surgeons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the performance and fragility of large language models on the\n  self-assessment for neurological surgeons"
                },
                "summary": "The Congress of Neurological Surgeons Self-Assessment for Neurological\nSurgeons (CNS-SANS) questions are widely used by neurosurgical residents to\nprepare for written board examinations. Recently, these questions have also\nserved as benchmarks for evaluating large language models' (LLMs) neurosurgical\nknowledge. This study aims to assess the performance of state-of-the-art LLMs\non neurosurgery board-like questions and to evaluate their robustness to the\ninclusion of distractor statements. A comprehensive evaluation was conducted\nusing 28 large language models. These models were tested on 2,904 neurosurgery\nboard examination questions derived from the CNS-SANS. Additionally, the study\nintroduced a distraction framework to assess the fragility of these models. The\nframework incorporated simple, irrelevant distractor statements containing\npolysemous words with clinical meanings used in non-clinical contexts to\ndetermine the extent to which such distractions degrade model performance on\nstandard medical benchmarks. 6 of the 28 tested LLMs achieved board-passing\noutcomes, with the top-performing models scoring over 15.7% above the passing\nthreshold. When exposed to distractions, accuracy across various model\narchitectures was significantly reduced-by as much as 20.4%-with one model\nfailing that had previously passed. Both general-purpose and medical\nopen-source models experienced greater performance declines compared to\nproprietary variants when subjected to the added distractors. While current\nLLMs demonstrate an impressive ability to answer neurosurgery board-like exam\nquestions, their performance is markedly vulnerable to extraneous, distracting\ninformation. These findings underscore the critical need for developing novel\nmitigation strategies aimed at bolstering LLM resilience against in-text\ndistractions, particularly for safe and effective clinical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Congress of Neurological Surgeons Self-Assessment for Neurological\nSurgeons (CNS-SANS) questions are widely used by neurosurgical residents to\nprepare for written board examinations. Recently, these questions have also\nserved as benchmarks for evaluating large language models' (LLMs) neurosurgical\nknowledge. This study aims to assess the performance of state-of-the-art LLMs\non neurosurgery board-like questions and to evaluate their robustness to the\ninclusion of distractor statements. A comprehensive evaluation was conducted\nusing 28 large language models. These models were tested on 2,904 neurosurgery\nboard examination questions derived from the CNS-SANS. Additionally, the study\nintroduced a distraction framework to assess the fragility of these models. The\nframework incorporated simple, irrelevant distractor statements containing\npolysemous words with clinical meanings used in non-clinical contexts to\ndetermine the extent to which such distractions degrade model performance on\nstandard medical benchmarks. 6 of the 28 tested LLMs achieved board-passing\noutcomes, with the top-performing models scoring over 15.7% above the passing\nthreshold. When exposed to distractions, accuracy across various model\narchitectures was significantly reduced-by as much as 20.4%-with one model\nfailing that had previously passed. Both general-purpose and medical\nopen-source models experienced greater performance declines compared to\nproprietary variants when subjected to the added distractors. While current\nLLMs demonstrate an impressive ability to answer neurosurgery board-like exam\nquestions, their performance is markedly vulnerable to extraneous, distracting\ninformation. These findings underscore the critical need for developing novel\nmitigation strategies aimed at bolstering LLM resilience against in-text\ndistractions, particularly for safe and effective clinical deployment."
                },
                "authors": [
                    {
                        "name": "Krithik Vishwanath"
                    },
                    {
                        "name": "Anton Alyakin"
                    },
                    {
                        "name": "Mrigayu Ghosh"
                    },
                    {
                        "name": "Jin Vivian Lee"
                    },
                    {
                        "name": "Daniel Alexander Alber"
                    },
                    {
                        "name": "Karl L. Sangwon"
                    },
                    {
                        "name": "Douglas Kondziolka"
                    },
                    {
                        "name": "Eric Karl Oermann"
                    }
                ],
                "author_detail": {
                    "name": "Eric Karl Oermann"
                },
                "author": "Eric Karl Oermann",
                "arxiv_comment": "22 pages, 3 main figures, 3 supplemental figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23474v1",
                "updated": "2025-05-29T14:26:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    26,
                    53,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:26:53Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    26,
                    53,
                    3,
                    149,
                    0
                ],
                "title": "Socratic-PRMBench: Benchmarking Process Reward Models with Systematic\n  Reasoning Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Socratic-PRMBench: Benchmarking Process Reward Models with Systematic\n  Reasoning Patterns"
                },
                "summary": "Process Reward Models (PRMs) are crucial in complex reasoning and\nproblem-solving tasks (e.g., LLM agents with long-horizon decision-making) by\nverifying the correctness of each intermediate reasoning step. In real-world\nscenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to\nsolve a problem, potentially suffering from errors under various reasoning\npatterns. Therefore, PRMs are required to identify errors under various\nreasoning patterns during the reasoning process. However, existing benchmarks\nmainly focus on evaluating PRMs with stepwise correctness, ignoring a\nsystematic evaluation of PRMs under various reasoning patterns. To mitigate\nthis gap, we introduce Socratic-PRMBench, a new benchmark to evaluate PRMs\nsystematically under six reasoning patterns, including Transformation,\nDecomposition, Regather, Deduction, Verification, and Integration.\nSocratic-PRMBench}comprises 2995 reasoning paths with flaws within the\naforementioned six reasoning patterns. Through our experiments on both PRMs and\nLLMs prompted as critic models, we identify notable deficiencies in existing\nPRMs. These observations underscore the significant weakness of current PRMs in\nconducting evaluations on reasoning steps under various reasoning patterns. We\nhope Socratic-PRMBench can serve as a comprehensive testbed for systematic\nevaluation of PRMs under diverse reasoning patterns and pave the way for future\ndevelopment of PRMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models (PRMs) are crucial in complex reasoning and\nproblem-solving tasks (e.g., LLM agents with long-horizon decision-making) by\nverifying the correctness of each intermediate reasoning step. In real-world\nscenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to\nsolve a problem, potentially suffering from errors under various reasoning\npatterns. Therefore, PRMs are required to identify errors under various\nreasoning patterns during the reasoning process. However, existing benchmarks\nmainly focus on evaluating PRMs with stepwise correctness, ignoring a\nsystematic evaluation of PRMs under various reasoning patterns. To mitigate\nthis gap, we introduce Socratic-PRMBench, a new benchmark to evaluate PRMs\nsystematically under six reasoning patterns, including Transformation,\nDecomposition, Regather, Deduction, Verification, and Integration.\nSocratic-PRMBench}comprises 2995 reasoning paths with flaws within the\naforementioned six reasoning patterns. Through our experiments on both PRMs and\nLLMs prompted as critic models, we identify notable deficiencies in existing\nPRMs. These observations underscore the significant weakness of current PRMs in\nconducting evaluations on reasoning steps under various reasoning patterns. We\nhope Socratic-PRMBench can serve as a comprehensive testbed for systematic\nevaluation of PRMs under diverse reasoning patterns and pave the way for future\ndevelopment of PRMs."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Ziyang Huang"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23473v1",
                "updated": "2025-05-29T14:26:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    26,
                    46,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:26:46Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    26,
                    46,
                    3,
                    149,
                    0
                ],
                "title": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and\n  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and\n  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions"
                },
                "summary": "Large language models (LLMs) frequently refuse to respond to pseudo-malicious\ninstructions: semantically harmless input queries triggering unnecessary LLM\nrefusals due to conservative safety alignment, significantly impairing user\nexperience. Collecting such instructions is crucial for evaluating and\nmitigating over-refusals, but existing instruction curation methods, like\nmanual creation or instruction rewriting, either lack scalability or fail to\nproduce sufficiently diverse and effective refusal-inducing prompts. To address\nthese limitations, we introduce EVOREFUSE, a prompt optimization approach that\ngenerates diverse pseudo-malicious instructions consistently eliciting\nconfident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm\nexploring the instruction space in more diverse directions than existing\nmethods via mutation strategies and recombination, and iteratively evolves seed\ninstructions to maximize evidence lower bound on LLM refusal probability. Using\nEVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582\npseudo-malicious instructions that outperforms the next-best benchmark with\n140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater\nlexical diversity, and 40.03% improved LLM response confidence scores; and\nEVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with\nresponses for supervised and preference-based alignment training.\nLLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to\n14.31% fewer over-refusals than models trained on the second-best alignment\ndataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals\nmodels trigger over-refusals by overly focusing on sensitive keywords while\nignoring broader context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) frequently refuse to respond to pseudo-malicious\ninstructions: semantically harmless input queries triggering unnecessary LLM\nrefusals due to conservative safety alignment, significantly impairing user\nexperience. Collecting such instructions is crucial for evaluating and\nmitigating over-refusals, but existing instruction curation methods, like\nmanual creation or instruction rewriting, either lack scalability or fail to\nproduce sufficiently diverse and effective refusal-inducing prompts. To address\nthese limitations, we introduce EVOREFUSE, a prompt optimization approach that\ngenerates diverse pseudo-malicious instructions consistently eliciting\nconfident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm\nexploring the instruction space in more diverse directions than existing\nmethods via mutation strategies and recombination, and iteratively evolves seed\ninstructions to maximize evidence lower bound on LLM refusal probability. Using\nEVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582\npseudo-malicious instructions that outperforms the next-best benchmark with\n140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater\nlexical diversity, and 40.03% improved LLM response confidence scores; and\nEVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with\nresponses for supervised and preference-based alignment training.\nLLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to\n14.31% fewer over-refusals than models trained on the second-best alignment\ndataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals\nmodels trigger over-refusals by overly focusing on sensitive keywords while\nignoring broader context."
                },
                "authors": [
                    {
                        "name": "Xiaorui Wu"
                    },
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Fei Li"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Xiaolu Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Yuxiang Peng"
                    },
                    {
                        "name": "Li Zheng"
                    },
                    {
                        "name": "Chong Teng"
                    },
                    {
                        "name": "Donghong Ji"
                    },
                    {
                        "name": "Zhuang Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Li"
                },
                "author": "Zhuang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23471v1",
                "updated": "2025-05-29T14:26:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    26,
                    22,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:26:22Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    26,
                    22,
                    3,
                    149,
                    0
                ],
                "title": "Synthesizing Performance Constraints for Evaluating and Improving Code\n  Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing Performance Constraints for Evaluating and Improving Code\n  Efficiency"
                },
                "summary": "Large Language Models (LLMs) have been increasingly used to optimize code\nefficiency. Evaluating their effectiveness and further suggesting optimization\nopportunities often rely on high-quality tests to demonstrate the performance\nbottlenecks presented in the program. However, existing approaches rely on a\nlimited set of hand-curated inputs or LLM-generated uninteresting\nlength-stressing tests, failing to reveal more nuanced optimization\nopportunities. We present WEDGE, a framework for generating\nperformance-stressing input given the program under test. WEDGE synthesizes\nexplicit performance-characterizing constraints in the form of branch\nconditions to partition the programs' execution space into performance-specific\nregions. When integrated with the coverage-guided fuzzer, reaching different\nregions introduces explicit rewards for test generation to explore inefficient\nimplementations. Our evaluation shows that WEDGE introduces a significant\nslowdown compared to the tests in CodeContests and those claimed to be\noptimized by existing approaches. From the utility perspective, integrating our\ntests substantially improves the existing code optimization approaches that\nrely on test-driven execution feedback. We release PERFFORGE, the performance\ntests generated by WEDGE, to benchmark future approaches for efficient code\ngeneration at https://github.com/UChiSeclab/perfforge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been increasingly used to optimize code\nefficiency. Evaluating their effectiveness and further suggesting optimization\nopportunities often rely on high-quality tests to demonstrate the performance\nbottlenecks presented in the program. However, existing approaches rely on a\nlimited set of hand-curated inputs or LLM-generated uninteresting\nlength-stressing tests, failing to reveal more nuanced optimization\nopportunities. We present WEDGE, a framework for generating\nperformance-stressing input given the program under test. WEDGE synthesizes\nexplicit performance-characterizing constraints in the form of branch\nconditions to partition the programs' execution space into performance-specific\nregions. When integrated with the coverage-guided fuzzer, reaching different\nregions introduces explicit rewards for test generation to explore inefficient\nimplementations. Our evaluation shows that WEDGE introduces a significant\nslowdown compared to the tests in CodeContests and those claimed to be\noptimized by existing approaches. From the utility perspective, integrating our\ntests substantially improves the existing code optimization approaches that\nrely on test-driven execution feedback. We release PERFFORGE, the performance\ntests generated by WEDGE, to benchmark future approaches for efficient code\ngeneration at https://github.com/UChiSeclab/perfforge."
                },
                "authors": [
                    {
                        "name": "Jun Yang"
                    },
                    {
                        "name": "Cheng-Chi Wang"
                    },
                    {
                        "name": "Bogdan Alexandru Stoica"
                    },
                    {
                        "name": "Kexin Pei"
                    }
                ],
                "author_detail": {
                    "name": "Kexin Pei"
                },
                "author": "Kexin Pei",
                "arxiv_comment": "30 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12974v2",
                "updated": "2025-05-29T14:22:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    22,
                    10,
                    3,
                    149,
                    0
                ],
                "published": "2024-10-16T19:09:02Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    19,
                    9,
                    2,
                    2,
                    290,
                    0
                ],
                "title": "BenchmarkCards: Large Language Model and Risk Reporting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BenchmarkCards: Large Language Model and Risk Reporting"
                },
                "summary": "Large language models (LLMs) are powerful tools capable of handling diverse\ntasks. Comparing and selecting appropriate LLMs for specific tasks requires\nsystematic evaluation methods, as models exhibit varying capabilities across\ndifferent domains. However, finding suitable benchmarks is difficult given the\nmany available options. This complexity not only increases the risk of\nbenchmark misuse and misinterpretation but also demands substantial effort from\nLLM users, seeking the most suitable benchmarks for their specific needs. To\naddress these issues, we introduce \\texttt{BenchmarkCards}, an intuitive and\nvalidated documentation framework that standardizes critical benchmark\nattributes such as objectives, methodologies, data sources, and limitations.\nThrough user studies involving benchmark creators and users, we show that\n\\texttt{BenchmarkCards} can simplify benchmark selection and enhance\ntransparency, facilitating informed decision-making in evaluating LLMs. Data &\nCode: https://github.com/SokolAnn/BenchmarkCards",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are powerful tools capable of handling diverse\ntasks. Comparing and selecting appropriate LLMs for specific tasks requires\nsystematic evaluation methods, as models exhibit varying capabilities across\ndifferent domains. However, finding suitable benchmarks is difficult given the\nmany available options. This complexity not only increases the risk of\nbenchmark misuse and misinterpretation but also demands substantial effort from\nLLM users, seeking the most suitable benchmarks for their specific needs. To\naddress these issues, we introduce \\texttt{BenchmarkCards}, an intuitive and\nvalidated documentation framework that standardizes critical benchmark\nattributes such as objectives, methodologies, data sources, and limitations.\nThrough user studies involving benchmark creators and users, we show that\n\\texttt{BenchmarkCards} can simplify benchmark selection and enhance\ntransparency, facilitating informed decision-making in evaluating LLMs. Data &\nCode: https://github.com/SokolAnn/BenchmarkCards"
                },
                "authors": [
                    {
                        "name": "Anna Sokol"
                    },
                    {
                        "name": "Elizabeth Daly"
                    },
                    {
                        "name": "Michael Hind"
                    },
                    {
                        "name": "David Piorkowski"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Nuno Moniz"
                    },
                    {
                        "name": "Nitesh Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh Chawla"
                },
                "author": "Nitesh Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03553v2",
                "updated": "2025-05-29T14:15:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    15,
                    53,
                    3,
                    149,
                    0
                ],
                "published": "2025-04-04T16:03:38Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    3,
                    38,
                    4,
                    94,
                    0
                ],
                "title": "Agentic Knowledgeable Self-awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Knowledgeable Self-awareness"
                },
                "summary": "Large Language Models (LLMs) have achieved considerable performance across\nvarious agentic planning tasks. However, traditional agent planning approaches\nadopt a \"flood irrigation\" methodology that indiscriminately injects gold\ntrajectories, external feedback, and domain knowledge into agent models. This\npractice overlooks the fundamental human cognitive principle of situational\nself-awareness during decision-making-the ability to dynamically assess\nsituational demands and strategically employ resources during decision-making.\nWe propose agentic knowledgeable self-awareness to address this gap, a novel\nparadigm enabling LLM-based agents to autonomously regulate knowledge\nutilization. Specifically, we propose KnowSelf, a data-centric approach that\napplies agents with knowledgeable self-awareness like humans. Concretely, we\ndevise a heuristic situation judgement criterion to mark special tokens on the\nagent's self-explored trajectories for collecting training data. Through a\ntwo-stage training process, the agent model can switch between different\nsituations by generating specific special tokens, achieving optimal planning\neffects with minimal costs. Our experiments demonstrate that KnowSelf can\noutperform various strong baselines on different tasks and models with minimal\nuse of external knowledge. Code is available at\nhttps://github.com/zjunlp/KnowSelf.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved considerable performance across\nvarious agentic planning tasks. However, traditional agent planning approaches\nadopt a \"flood irrigation\" methodology that indiscriminately injects gold\ntrajectories, external feedback, and domain knowledge into agent models. This\npractice overlooks the fundamental human cognitive principle of situational\nself-awareness during decision-making-the ability to dynamically assess\nsituational demands and strategically employ resources during decision-making.\nWe propose agentic knowledgeable self-awareness to address this gap, a novel\nparadigm enabling LLM-based agents to autonomously regulate knowledge\nutilization. Specifically, we propose KnowSelf, a data-centric approach that\napplies agents with knowledgeable self-awareness like humans. Concretely, we\ndevise a heuristic situation judgement criterion to mark special tokens on the\nagent's self-explored trajectories for collecting training data. Through a\ntwo-stage training process, the agent model can switch between different\nsituations by generating specific special tokens, achieving optimal planning\neffects with minimal costs. Our experiments demonstrate that KnowSelf can\noutperform various strong baselines on different tasks and models with minimal\nuse of external knowledge. Code is available at\nhttps://github.com/zjunlp/KnowSelf."
                },
                "authors": [
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Zhisong Qiu"
                    },
                    {
                        "name": "Baochang Ren"
                    },
                    {
                        "name": "Xiaobin Wang"
                    },
                    {
                        "name": "Xiangyuan Ru"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03793v2",
                "updated": "2025-05-29T14:15:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    15,
                    49,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-01T15:07:32Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    15,
                    7,
                    32,
                    3,
                    121,
                    0
                ],
                "title": "LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection"
                },
                "summary": "The proliferation of open-sourced Large Language Models (LLMs) and diverse\ndownstream tasks necessitates efficient model selection, given the\nimpracticality of fine-tuning all candidates due to computational constraints.\nDespite the recent advances in LLM selection, a fundamental research question\nlargely remains nascent: how can we model the dynamic behaviors of LLMs during\nfine-tuning, thereby enhancing our understanding of their generalization\nperformance across diverse downstream tasks? In this work, we propose a novel\ntheoretical framework that provides a proper lens to assess the generalization\ncapabilities of LLMs, thereby enabling accurate and efficient LLM selection for\ndownstream applications. In particular, we first derive a PAC-Bayesian\nGeneralization Bound that unveils fine-tuning dynamics of LLMs and then\nintroduce LENSLLM, a Neural Tangent Kernel (NTK)-based Rectified Scaling Model\nthat enables accurate performance predictions across diverse tasks while\nmaintaining computational efficiency. Extensive empirical results on 3\nlarge-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy\nand reduces up to 88.5% computational cost in LLM selection, outperforming 5\nstate-of-the-art methods. We open-source our proposed LENSLLM model and\ncorresponding results at LensLLM.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of open-sourced Large Language Models (LLMs) and diverse\ndownstream tasks necessitates efficient model selection, given the\nimpracticality of fine-tuning all candidates due to computational constraints.\nDespite the recent advances in LLM selection, a fundamental research question\nlargely remains nascent: how can we model the dynamic behaviors of LLMs during\nfine-tuning, thereby enhancing our understanding of their generalization\nperformance across diverse downstream tasks? In this work, we propose a novel\ntheoretical framework that provides a proper lens to assess the generalization\ncapabilities of LLMs, thereby enabling accurate and efficient LLM selection for\ndownstream applications. In particular, we first derive a PAC-Bayesian\nGeneralization Bound that unveils fine-tuning dynamics of LLMs and then\nintroduce LENSLLM, a Neural Tangent Kernel (NTK)-based Rectified Scaling Model\nthat enables accurate performance predictions across diverse tasks while\nmaintaining computational efficiency. Extensive empirical results on 3\nlarge-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy\nand reduces up to 88.5% computational cost in LLM selection, outperforming 5\nstate-of-the-art methods. We open-source our proposed LENSLLM model and\ncorresponding results at LensLLM.io."
                },
                "authors": [
                    {
                        "name": "Xinyue Zeng"
                    },
                    {
                        "name": "Haohui Wang"
                    },
                    {
                        "name": "Junhong Lin"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Tyler Cody"
                    },
                    {
                        "name": "Dawei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Zhou"
                },
                "author": "Dawei Zhou",
                "arxiv_comment": "Accepted by ICML'2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23461v1",
                "updated": "2025-05-29T14:10:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    10,
                    24,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:10:24Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    10,
                    24,
                    3,
                    149,
                    0
                ],
                "title": "UAQFact: Evaluating Factual Knowledge Utilization of LLMs on\n  Unanswerable Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAQFact: Evaluating Factual Knowledge Utilization of LLMs on\n  Unanswerable Questions"
                },
                "summary": "Handling unanswerable questions (UAQ) is crucial for LLMs, as it helps\nprevent misleading responses in complex situations. While previous studies have\nbuilt several datasets to assess LLMs' performance on UAQ, these datasets lack\nfactual knowledge support, which limits the evaluation of LLMs' ability to\nutilize their factual knowledge when handling UAQ. To address the limitation,\nwe introduce a new unanswerable question dataset UAQFact, a bilingual dataset\nwith auxiliary factual knowledge created from a Knowledge Graph. Based on\nUAQFact, we further define two new tasks to measure LLMs' ability to utilize\ninternal and external factual knowledge, respectively. Our experimental results\nacross multiple LLM series show that UAQFact presents significant challenges,\nas LLMs do not consistently perform well even when they have factual knowledge\nstored. Additionally, we find that incorporating external knowledge may enhance\nperformance, but LLMs still cannot make full use of the knowledge which may\nresult in incorrect responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling unanswerable questions (UAQ) is crucial for LLMs, as it helps\nprevent misleading responses in complex situations. While previous studies have\nbuilt several datasets to assess LLMs' performance on UAQ, these datasets lack\nfactual knowledge support, which limits the evaluation of LLMs' ability to\nutilize their factual knowledge when handling UAQ. To address the limitation,\nwe introduce a new unanswerable question dataset UAQFact, a bilingual dataset\nwith auxiliary factual knowledge created from a Knowledge Graph. Based on\nUAQFact, we further define two new tasks to measure LLMs' ability to utilize\ninternal and external factual knowledge, respectively. Our experimental results\nacross multiple LLM series show that UAQFact presents significant challenges,\nas LLMs do not consistently perform well even when they have factual knowledge\nstored. Additionally, we find that incorporating external knowledge may enhance\nperformance, but LLMs still cannot make full use of the knowledge which may\nresult in incorrect responses."
                },
                "authors": [
                    {
                        "name": "Chuanyuan Tan"
                    },
                    {
                        "name": "Wenbiao Shao"
                    },
                    {
                        "name": "Hao Xiong"
                    },
                    {
                        "name": "Tong Zhu"
                    },
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Kai Shi"
                    },
                    {
                        "name": "Wenliang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenliang Chen"
                },
                "author": "Wenliang Chen",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16555v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16555v3",
                "updated": "2025-05-29T14:05:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    5,
                    50,
                    3,
                    149,
                    0
                ],
                "published": "2024-12-21T09:43:51Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    9,
                    43,
                    51,
                    5,
                    356,
                    0
                ],
                "title": "Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are widely applied in various fields of society\ndue to their powerful reasoning, understanding, and generation capabilities.\nHowever, the security issues associated with these models are becoming\nincreasingly severe. Jailbreaking attacks, as an important method for detecting\nvulnerabilities in LLMs, have been explored by researchers who attempt to\ninduce these models to generate harmful content through various attack methods.\nNevertheless, existing jailbreaking methods face numerous limitations, such as\nexcessive query counts, limited coverage of jailbreak modalities, low attack\nsuccess rates, and simplistic evaluation methods. To overcome these\nconstraints, this paper proposes a multimodal jailbreaking method: JMLLM. This\nmethod integrates multiple strategies to perform comprehensive jailbreak\nattacks across text, visual, and auditory modalities. Additionally, we\ncontribute a new and comprehensive dataset for multimodal jailbreaking\nresearch: TriJail, which includes jailbreak prompts for all three modalities.\nExperiments on the TriJail dataset and the benchmark dataset AdvBench,\nconducted on 13 popular LLMs, demonstrate advanced attack success rates and\nsignificant reduction in time overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely applied in various fields of society\ndue to their powerful reasoning, understanding, and generation capabilities.\nHowever, the security issues associated with these models are becoming\nincreasingly severe. Jailbreaking attacks, as an important method for detecting\nvulnerabilities in LLMs, have been explored by researchers who attempt to\ninduce these models to generate harmful content through various attack methods.\nNevertheless, existing jailbreaking methods face numerous limitations, such as\nexcessive query counts, limited coverage of jailbreak modalities, low attack\nsuccess rates, and simplistic evaluation methods. To overcome these\nconstraints, this paper proposes a multimodal jailbreaking method: JMLLM. This\nmethod integrates multiple strategies to perform comprehensive jailbreak\nattacks across text, visual, and auditory modalities. Additionally, we\ncontribute a new and comprehensive dataset for multimodal jailbreaking\nresearch: TriJail, which includes jailbreak prompts for all three modalities.\nExperiments on the TriJail dataset and the benchmark dataset AdvBench,\nconducted on 13 popular LLMs, demonstrate advanced attack success rates and\nsignificant reduction in time overhead."
                },
                "authors": [
                    {
                        "name": "Yanxu Mao"
                    },
                    {
                        "name": "Peipei Liu"
                    },
                    {
                        "name": "Tiehan Cui"
                    },
                    {
                        "name": "Zhaoteng Yan"
                    },
                    {
                        "name": "Congying Liu"
                    },
                    {
                        "name": "Datao You"
                    }
                ],
                "author_detail": {
                    "name": "Datao You"
                },
                "author": "Datao You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16555v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16555v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21807v2",
                "updated": "2025-05-29T14:02:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    2,
                    15,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-27T22:23:11Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    22,
                    23,
                    11,
                    1,
                    147,
                    0
                ],
                "title": "TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for\n  Explainable Tabular Data Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for\n  Explainable Tabular Data Prediction"
                },
                "summary": "Predictive modeling on tabular data is the cornerstone of many real-world\napplications. Although gradient boosting machines and some recent deep models\nachieve strong performance on tabular data, they often lack interpretability.\nOn the other hand, large language models (LLMs) have demonstrated powerful\ncapabilities to generate human-like reasoning and explanations, but remain\nunder-performed for tabular data prediction. In this paper, we propose a new\napproach that leverages reasoning-based LLMs, trained using reinforcement\nlearning, to perform more accurate and explainable predictions on tabular data.\nOur method introduces custom reward functions that guide the model not only\ntoward high prediction accuracy but also toward human-understandable reasons\nfor its predictions. Experimental results show that our model achieves\npromising performance on financial benchmark datasets, outperforming most\nexisting LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive modeling on tabular data is the cornerstone of many real-world\napplications. Although gradient boosting machines and some recent deep models\nachieve strong performance on tabular data, they often lack interpretability.\nOn the other hand, large language models (LLMs) have demonstrated powerful\ncapabilities to generate human-like reasoning and explanations, but remain\nunder-performed for tabular data prediction. In this paper, we propose a new\napproach that leverages reasoning-based LLMs, trained using reinforcement\nlearning, to perform more accurate and explainable predictions on tabular data.\nOur method introduces custom reward functions that guide the model not only\ntoward high prediction accuracy but also toward human-understandable reasons\nfor its predictions. Experimental results show that our model achieves\npromising performance on financial benchmark datasets, outperforming most\nexisting LLMs."
                },
                "authors": [
                    {
                        "name": "Tommy Xu"
                    },
                    {
                        "name": "Zhitian Zhang"
                    },
                    {
                        "name": "Xiangyu Sun"
                    },
                    {
                        "name": "Lauren Kelly Zung"
                    },
                    {
                        "name": "Hossein Hajimirsadeghi"
                    },
                    {
                        "name": "Greg Mori"
                    }
                ],
                "author_detail": {
                    "name": "Greg Mori"
                },
                "author": "Greg Mori",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04999v2",
                "updated": "2025-05-29T13:57:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    57,
                    4,
                    3,
                    149,
                    0
                ],
                "published": "2024-11-07T18:59:27Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    27,
                    3,
                    312,
                    0
                ],
                "title": "DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile\n  Manipulation"
                },
                "summary": "Significant progress has been made in open-vocabulary mobile manipulation,\nwhere the goal is for a robot to perform tasks in any environment given a\nnatural language description. However, most current systems assume a static\nenvironment, which limits the system's applicability in real-world scenarios\nwhere environments frequently change due to human intervention or the robot's\nown actions. In this work, we present DynaMem, a new approach to open-world\nmobile manipulation that uses a dynamic spatio-semantic memory to represent a\nrobot's environment. DynaMem constructs a 3D data structure to maintain a\ndynamic memory of point clouds, and answers open-vocabulary object localization\nqueries using multimodal LLMs or open-vocabulary features generated by\nstate-of-the-art vision-language models. Powered by DynaMem, our robots can\nexplore novel environments, search for objects not found in memory, and\ncontinuously update the memory as objects move, appear, or disappear in the\nscene. We run extensive experiments on the Stretch SE3 robots in three real and\nnine offline scenes, and achieve an average pick-and-drop success rate of 70%\non non-stationary objects, which is more than a 2x improvement over\nstate-of-the-art static systems. Our code as well as our experiment and\ndeployment videos are open sourced and can be found on our project website:\nhttps://dynamem.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant progress has been made in open-vocabulary mobile manipulation,\nwhere the goal is for a robot to perform tasks in any environment given a\nnatural language description. However, most current systems assume a static\nenvironment, which limits the system's applicability in real-world scenarios\nwhere environments frequently change due to human intervention or the robot's\nown actions. In this work, we present DynaMem, a new approach to open-world\nmobile manipulation that uses a dynamic spatio-semantic memory to represent a\nrobot's environment. DynaMem constructs a 3D data structure to maintain a\ndynamic memory of point clouds, and answers open-vocabulary object localization\nqueries using multimodal LLMs or open-vocabulary features generated by\nstate-of-the-art vision-language models. Powered by DynaMem, our robots can\nexplore novel environments, search for objects not found in memory, and\ncontinuously update the memory as objects move, appear, or disappear in the\nscene. We run extensive experiments on the Stretch SE3 robots in three real and\nnine offline scenes, and achieve an average pick-and-drop success rate of 70%\non non-stationary objects, which is more than a 2x improvement over\nstate-of-the-art static systems. Our code as well as our experiment and\ndeployment videos are open sourced and can be found on our project website:\nhttps://dynamem.github.io/"
                },
                "authors": [
                    {
                        "name": "Peiqi Liu"
                    },
                    {
                        "name": "Zhanqiu Guo"
                    },
                    {
                        "name": "Mohit Warke"
                    },
                    {
                        "name": "Soumith Chintala"
                    },
                    {
                        "name": "Chris Paxton"
                    },
                    {
                        "name": "Nur Muhammad Mahi Shafiullah"
                    },
                    {
                        "name": "Lerrel Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Lerrel Pinto"
                },
                "author": "Lerrel Pinto",
                "arxiv_comment": "Website: https://dynamem.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15484v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15484v3",
                "updated": "2025-05-29T13:54:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    54,
                    51,
                    3,
                    149,
                    0
                ],
                "published": "2024-12-20T01:37:22Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    1,
                    37,
                    22,
                    4,
                    355,
                    0
                ],
                "title": "Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and\n  Dual Evaluation Metrics for Factuality and Coverage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and\n  Dual Evaluation Metrics for Factuality and Coverage"
                },
                "summary": "Multimodal large language models (MLLMs) excel at generating highly detailed\ncaptions but often produce hallucinations. Our analysis reveals that existing\nhallucination detection methods struggle with detailed captions. We attribute\nthis to the increasing reliance of MLLMs on their generated text, rather than\nthe input image, as the sequence length grows. To address this issue, we\npropose a multiagent approach that leverages LLM-MLLM collaboration to correct\ngiven captions. Additionally, we introduce an evaluation framework and a\nbenchmark dataset to facilitate the systematic analysis of detailed captions.\nOur experiments demonstrate that our proposed evaluation method better aligns\nwith human judgments of factuality than existing metrics and that existing\napproaches to improve the MLLM factuality may fall short in hyper-detailed\nimage captioning tasks. In contrast, our proposed method significantly enhances\nthe factual accuracy of captions, even improving those generated by GPT-4V.\nFinally, we highlight a limitation of VQA-centric benchmarking by demonstrating\nthat an MLLM's performance on VQA benchmarks may not correlate with its ability\nto generate detailed image captions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) excel at generating highly detailed\ncaptions but often produce hallucinations. Our analysis reveals that existing\nhallucination detection methods struggle with detailed captions. We attribute\nthis to the increasing reliance of MLLMs on their generated text, rather than\nthe input image, as the sequence length grows. To address this issue, we\npropose a multiagent approach that leverages LLM-MLLM collaboration to correct\ngiven captions. Additionally, we introduce an evaluation framework and a\nbenchmark dataset to facilitate the systematic analysis of detailed captions.\nOur experiments demonstrate that our proposed evaluation method better aligns\nwith human judgments of factuality than existing metrics and that existing\napproaches to improve the MLLM factuality may fall short in hyper-detailed\nimage captioning tasks. In contrast, our proposed method significantly enhances\nthe factual accuracy of captions, even improving those generated by GPT-4V.\nFinally, we highlight a limitation of VQA-centric benchmarking by demonstrating\nthat an MLLM's performance on VQA benchmarks may not correlate with its ability\nto generate detailed image captions."
                },
                "authors": [
                    {
                        "name": "Saehyung Lee"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Jing Shi"
                    },
                    {
                        "name": "Sungroh Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Sungroh Yoon"
                },
                "author": "Sungroh Yoon",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15484v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15484v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12913v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12913v3",
                "updated": "2025-05-29T13:50:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    50,
                    58,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-18T14:54:55Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    54,
                    55,
                    1,
                    49,
                    0
                ],
                "title": "GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training\n  for LLMs On-Device Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training\n  for LLMs On-Device Fine-tuning"
                },
                "summary": "Large Language Models (LLMs) fine-tuning technologies have achieved\nremarkable results. However, traditional LLM fine-tuning approaches face\nsignificant challenges: they require large Floating Point (FP) computation,\nraising privacy concerns when handling sensitive data, and are impractical for\nresource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT)\ntechniques reduce trainable parameters, their reliance on floating-point\narithmetic creates fundamental incompatibilities with edge hardware. In this\nwork, we introduce a novel framework for on-device LLM fine-tuning that\neliminates the need for floating-point operations in both inference and\ntraining, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer\nformat, which efficiently represents model parameters in integer format using\nshared exponents among parameter groups. When combined with LoRA-like adapters,\nthis enables fully integer-based fine-tuning that is both memory and compute\nefficient. We demonstrate that our approach achieves accuracy comparable to\nBF16-based fine-tuning while significantly reducing 1.85x memory usage.\nMoreover, compared to FP8, our method can reduce 5x power consumption and 11x\nchip area with same performance, making large-scale model adaptation feasible\non edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) fine-tuning technologies have achieved\nremarkable results. However, traditional LLM fine-tuning approaches face\nsignificant challenges: they require large Floating Point (FP) computation,\nraising privacy concerns when handling sensitive data, and are impractical for\nresource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT)\ntechniques reduce trainable parameters, their reliance on floating-point\narithmetic creates fundamental incompatibilities with edge hardware. In this\nwork, we introduce a novel framework for on-device LLM fine-tuning that\neliminates the need for floating-point operations in both inference and\ntraining, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer\nformat, which efficiently represents model parameters in integer format using\nshared exponents among parameter groups. When combined with LoRA-like adapters,\nthis enables fully integer-based fine-tuning that is both memory and compute\nefficient. We demonstrate that our approach achieves accuracy comparable to\nBF16-based fine-tuning while significantly reducing 1.85x memory usage.\nMoreover, compared to FP8, our method can reduce 5x power consumption and 11x\nchip area with same performance, making large-scale model adaptation feasible\non edge devices."
                },
                "authors": [
                    {
                        "name": "Sifan Zhou"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Mingjia Shi"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Dawei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yang"
                },
                "author": "Dawei Yang",
                "arxiv_comment": "Accepted by Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12913v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12913v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18212v2",
                "updated": "2025-05-29T13:45:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    45,
                    0,
                    3,
                    149,
                    0
                ],
                "published": "2024-11-27T10:45:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    45,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "SCoTT: Strategic Chain-of-Thought Tasking for Wireless-Aware Robot\n  Navigation in Digital Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCoTT: Strategic Chain-of-Thought Tasking for Wireless-Aware Robot\n  Navigation in Digital Twins"
                },
                "summary": "Path planning under wireless performance constraints is a complex challenge\nin robot navigation. However, naively incorporating such constraints into\nclassical planning algorithms often incurs prohibitive search costs. In this\npaper, we propose SCoTT, a wireless-aware path planning framework that\nleverages vision-language models (VLMs) to co-optimize average path gains and\ntrajectory length using wireless heatmap images and ray-tracing data from a\ndigital twin (DT). At the core of our framework is Strategic Chain-of-Thought\nTasking (SCoTT), a novel prompting paradigm that decomposes the exhaustive\nsearch problem into structured subtasks, each solved via chain-of-thought\nprompting. To establish strong baselines, we compare classical A* and\nwireless-aware extensions of it, and derive DP-WA*, an optimal, iterative\ndynamic programming algorithm that incorporates all path gains and distance\nmetrics from the DT, but at significant computational cost. In extensive\nexperiments, we show that SCoTT achieves path gains within 2% of DP-WA* while\nconsistently generating shorter trajectories. Moreover, SCoTT's intermediate\noutputs can be used to accelerate DP-WA* by reducing its search space, saving\nup to 62% in execution time. We validate our framework using four VLMs,\ndemonstrating effectiveness across both large and small models, thus making it\napplicable to a wide range of compact models at low inference cost. We also\nshow the practical viability of our approach by deploying SCoTT as a ROS node\nwithin Gazebo simulations. Finally, we discuss data acquisition pipelines,\ncompute requirements, and deployment considerations for VLMs in 6G-enabled DTs,\nunderscoring the potential of natural language interfaces for wireless-aware\nnavigation in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path planning under wireless performance constraints is a complex challenge\nin robot navigation. However, naively incorporating such constraints into\nclassical planning algorithms often incurs prohibitive search costs. In this\npaper, we propose SCoTT, a wireless-aware path planning framework that\nleverages vision-language models (VLMs) to co-optimize average path gains and\ntrajectory length using wireless heatmap images and ray-tracing data from a\ndigital twin (DT). At the core of our framework is Strategic Chain-of-Thought\nTasking (SCoTT), a novel prompting paradigm that decomposes the exhaustive\nsearch problem into structured subtasks, each solved via chain-of-thought\nprompting. To establish strong baselines, we compare classical A* and\nwireless-aware extensions of it, and derive DP-WA*, an optimal, iterative\ndynamic programming algorithm that incorporates all path gains and distance\nmetrics from the DT, but at significant computational cost. In extensive\nexperiments, we show that SCoTT achieves path gains within 2% of DP-WA* while\nconsistently generating shorter trajectories. Moreover, SCoTT's intermediate\noutputs can be used to accelerate DP-WA* by reducing its search space, saving\nup to 62% in execution time. We validate our framework using four VLMs,\ndemonstrating effectiveness across both large and small models, thus making it\napplicable to a wide range of compact models at low inference cost. We also\nshow the practical viability of our approach by deploying SCoTT as a ROS node\nwithin Gazebo simulations. Finally, we discuss data acquisition pipelines,\ncompute requirements, and deployment considerations for VLMs in 6G-enabled DTs,\nunderscoring the potential of natural language interfaces for wireless-aware\nnavigation in real-world applications."
                },
                "authors": [
                    {
                        "name": "Aladin Djuhera"
                    },
                    {
                        "name": "Amin Seffo"
                    },
                    {
                        "name": "Vlad C. Andrei"
                    },
                    {
                        "name": "Holger Boche"
                    },
                    {
                        "name": "Walid Saad"
                    }
                ],
                "author_detail": {
                    "name": "Walid Saad"
                },
                "author": "Walid Saad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18893v3",
                "updated": "2025-05-29T13:42:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    42,
                    10,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-24T22:35:32Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    22,
                    35,
                    32,
                    5,
                    144,
                    0
                ],
                "title": "Reality Check: A New Evaluation Ecosystem Is Necessary to Understand\n  AI's Real World Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reality Check: A New Evaluation Ecosystem Is Necessary to Understand\n  AI's Real World Effects"
                },
                "summary": "Conventional AI evaluation approaches concentrated within the AI stack\nexhibit systemic limitations for exploring, navigating and resolving the human\nand societal factors that play out in real world deployment such as in\neducation, finance, healthcare, and employment sectors. AI capability\nevaluations can capture detail about first-order effects, such as whether\nimmediate system outputs are accurate, or contain toxic, biased or\nstereotypical content, but AI's second-order effects, i.e. any long-term\noutcomes and consequences that may result from AI use in the real world, have\nbecome a significant area of interest as the technology becomes embedded in our\ndaily lives. These secondary effects can include shifts in user behavior,\nsocietal, cultural and economic ramifications, workforce transformations, and\nlong-term downstream impacts that may result from a broad and growing set of\nrisks. This position paper argues that measuring the indirect and secondary\neffects of AI will require expansion beyond static, single-turn approaches\nconducted in silico to include testing paradigms that can capture what actually\nmaterializes when people use AI technology in context. Specifically, we\ndescribe the need for data and methods that can facilitate contextual awareness\nand enable downstream interpretation and decision making about AI's secondary\neffects, and recommend requirements for a new ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional AI evaluation approaches concentrated within the AI stack\nexhibit systemic limitations for exploring, navigating and resolving the human\nand societal factors that play out in real world deployment such as in\neducation, finance, healthcare, and employment sectors. AI capability\nevaluations can capture detail about first-order effects, such as whether\nimmediate system outputs are accurate, or contain toxic, biased or\nstereotypical content, but AI's second-order effects, i.e. any long-term\noutcomes and consequences that may result from AI use in the real world, have\nbecome a significant area of interest as the technology becomes embedded in our\ndaily lives. These secondary effects can include shifts in user behavior,\nsocietal, cultural and economic ramifications, workforce transformations, and\nlong-term downstream impacts that may result from a broad and growing set of\nrisks. This position paper argues that measuring the indirect and secondary\neffects of AI will require expansion beyond static, single-turn approaches\nconducted in silico to include testing paradigms that can capture what actually\nmaterializes when people use AI technology in context. Specifically, we\ndescribe the need for data and methods that can facilitate contextual awareness\nand enable downstream interpretation and decision making about AI's secondary\neffects, and recommend requirements for a new ecosystem."
                },
                "authors": [
                    {
                        "name": "Reva Schwartz"
                    },
                    {
                        "name": "Rumman Chowdhury"
                    },
                    {
                        "name": "Akash Kundu"
                    },
                    {
                        "name": "Heather Frase"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Tom David"
                    },
                    {
                        "name": "Gabriella Waters"
                    },
                    {
                        "name": "Afaf Taik"
                    },
                    {
                        "name": "Morgan Briggs"
                    },
                    {
                        "name": "Patrick Hall"
                    },
                    {
                        "name": "Shomik Jain"
                    },
                    {
                        "name": "Kyra Yee"
                    },
                    {
                        "name": "Spencer Thomas"
                    },
                    {
                        "name": "Sundeep Bhandari"
                    },
                    {
                        "name": "Qinghua Lu"
                    },
                    {
                        "name": "Matthew Holmes"
                    },
                    {
                        "name": "Theodora Skeadas"
                    }
                ],
                "author_detail": {
                    "name": "Theodora Skeadas"
                },
                "author": "Theodora Skeadas",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02229v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02229v2",
                "updated": "2025-05-29T13:40:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    40,
                    26,
                    3,
                    149,
                    0
                ],
                "published": "2024-10-03T05:51:26Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    5,
                    51,
                    26,
                    3,
                    277,
                    0
                ],
                "title": "CodePMP: Scalable Preference Model Pretraining for Large Language Model\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodePMP: Scalable Preference Model Pretraining for Large Language Model\n  Reasoning"
                },
                "summary": "Large language models (LLMs) have made significant progress in natural\nlanguage understanding and generation, driven by scalable pretraining and\nadvanced finetuning. However, enhancing reasoning abilities in LLMs,\nparticularly via reinforcement learning from human feedback (RLHF), remains\nchallenging due to the scarcity of high-quality preference data, which is\nlabor-intensive to annotate and crucial for reward model (RM) finetuning. To\nalleviate this issue, we introduce CodePMP, a scalable preference model\npretraining (PMP) pipeline that utilizes a large corpus of synthesized\ncode-preference pairs from publicly available high-quality source code. CodePMP\nimproves RM finetuning efficiency by pretraining preference models on\nlarge-scale synthesized code-preference pairs. We evaluate CodePMP on\nmathematical reasoning tasks (GSM8K, MATH) and logical reasoning tasks (ReClor,\nLogiQA2.0), consistently showing significant improvements in reasoning\nperformance of LLMs and highlighting the importance of scalable preference\nmodel pretraining for efficient reward modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant progress in natural\nlanguage understanding and generation, driven by scalable pretraining and\nadvanced finetuning. However, enhancing reasoning abilities in LLMs,\nparticularly via reinforcement learning from human feedback (RLHF), remains\nchallenging due to the scarcity of high-quality preference data, which is\nlabor-intensive to annotate and crucial for reward model (RM) finetuning. To\nalleviate this issue, we introduce CodePMP, a scalable preference model\npretraining (PMP) pipeline that utilizes a large corpus of synthesized\ncode-preference pairs from publicly available high-quality source code. CodePMP\nimproves RM finetuning efficiency by pretraining preference models on\nlarge-scale synthesized code-preference pairs. We evaluate CodePMP on\nmathematical reasoning tasks (GSM8K, MATH) and logical reasoning tasks (ReClor,\nLogiQA2.0), consistently showing significant improvements in reasoning\nperformance of LLMs and highlighting the importance of scalable preference\nmodel pretraining for efficient reward modeling."
                },
                "authors": [
                    {
                        "name": "Huimu Yu"
                    },
                    {
                        "name": "Xing Wu"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Debing Zhang"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02229v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08120v2",
                "updated": "2025-05-29T13:35:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    35,
                    47,
                    3,
                    149,
                    0
                ],
                "published": "2025-04-10T20:39:18Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    20,
                    39,
                    18,
                    3,
                    100,
                    0
                ],
                "title": "DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and\n  Summarization?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and\n  Summarization?"
                },
                "summary": "Reasoning-enabled large language models (LLMs) excel in logical tasks, yet\ntheir utility for evaluating natural language generation remains unexplored.\nThis study systematically compares reasoning LLMs with non-reasoning\ncounterparts across machine translation and text summarization evaluation\ntasks. We evaluate eight models spanning state-of-the-art reasoning models\n(DeepSeek-R1, OpenAI o3), their distilled variants (8B-70B parameters), and\nequivalent non-reasoning LLMs. Experiments on WMT23 and SummEval benchmarks\nreveal architecture and task-dependent benefits: OpenAI o3-mini models show\nimproved performance with increased reasoning on MT, while DeepSeek-R1 and\ngenerally underperforms compared to its non-reasoning variant except in\nsummarization consistency evaluation. Correlation analysis demonstrates that\nreasoning token usage correlates with evaluation quality only in specific\nmodels, while almost all models generally allocate more reasoning tokens when\nidentifying more quality issues. Distillation maintains reasonable performance\nup to 32B parameter models but degrades substantially at 8B scale. This work\nprovides the first assessment of reasoning LLMs for NLG evaluation and\ncomparison to non-reasoning models. We share our code to facilitate further\nresearch: https://github.com/NL2G/reasoning-eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-enabled large language models (LLMs) excel in logical tasks, yet\ntheir utility for evaluating natural language generation remains unexplored.\nThis study systematically compares reasoning LLMs with non-reasoning\ncounterparts across machine translation and text summarization evaluation\ntasks. We evaluate eight models spanning state-of-the-art reasoning models\n(DeepSeek-R1, OpenAI o3), their distilled variants (8B-70B parameters), and\nequivalent non-reasoning LLMs. Experiments on WMT23 and SummEval benchmarks\nreveal architecture and task-dependent benefits: OpenAI o3-mini models show\nimproved performance with increased reasoning on MT, while DeepSeek-R1 and\ngenerally underperforms compared to its non-reasoning variant except in\nsummarization consistency evaluation. Correlation analysis demonstrates that\nreasoning token usage correlates with evaluation quality only in specific\nmodels, while almost all models generally allocate more reasoning tokens when\nidentifying more quality issues. Distillation maintains reasonable performance\nup to 32B parameter models but degrades substantially at 8B scale. This work\nprovides the first assessment of reasoning LLMs for NLG evaluation and\ncomparison to non-reasoning models. We share our code to facilitate further\nresearch: https://github.com/NL2G/reasoning-eval."
                },
                "authors": [
                    {
                        "name": "Daniil Larionov"
                    },
                    {
                        "name": "Sotaro Takeshita"
                    },
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Yanran Chen"
                    },
                    {
                        "name": "Christoph Leiter"
                    },
                    {
                        "name": "Zhipin Wang"
                    },
                    {
                        "name": "Christian Greisinger"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23433v1",
                "updated": "2025-05-29T13:27:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    27,
                    44,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T13:27:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    27,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "Diversity-Aware Policy Optimization for Large Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity-Aware Policy Optimization for Large Language Model Reasoning"
                },
                "summary": "The reasoning capabilities of large language models (LLMs) have advanced\nrapidly, particularly following the release of DeepSeek R1, which has inspired\na surge of research into data quality and reinforcement learning (RL)\nalgorithms. Despite the pivotal role diversity plays in RL, its influence on\nLLM reasoning remains largely underexplored. To bridge this gap, this work\npresents a systematic investigation into the impact of diversity in RL-based\ntraining for LLM reasoning, and proposes a novel diversity-aware policy\noptimization method. Across evaluations on 12 LLMs, we observe a strong\npositive correlation between the solution diversity and Potential at k (a novel\nmetric quantifying an LLM's reasoning potential) in high-performing models.\nThis finding motivates our method to explicitly promote diversity during RL\ntraining. Specifically, we design a token-level diversity and reformulate it\ninto a practical objective, then we selectively apply it to positive samples.\nIntegrated into the R1-zero training framework, our method achieves a 3.5\npercent average improvement across four mathematical reasoning benchmarks,\nwhile generating more diverse and robust solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning capabilities of large language models (LLMs) have advanced\nrapidly, particularly following the release of DeepSeek R1, which has inspired\na surge of research into data quality and reinforcement learning (RL)\nalgorithms. Despite the pivotal role diversity plays in RL, its influence on\nLLM reasoning remains largely underexplored. To bridge this gap, this work\npresents a systematic investigation into the impact of diversity in RL-based\ntraining for LLM reasoning, and proposes a novel diversity-aware policy\noptimization method. Across evaluations on 12 LLMs, we observe a strong\npositive correlation between the solution diversity and Potential at k (a novel\nmetric quantifying an LLM's reasoning potential) in high-performing models.\nThis finding motivates our method to explicitly promote diversity during RL\ntraining. Specifically, we design a token-level diversity and reformulate it\ninto a practical objective, then we selectively apply it to positive samples.\nIntegrated into the R1-zero training framework, our method achieves a 3.5\npercent average improvement across four mathematical reasoning benchmarks,\nwhile generating more diverse and robust solutions."
                },
                "authors": [
                    {
                        "name": "Jian Yao"
                    },
                    {
                        "name": "Ran Cheng"
                    },
                    {
                        "name": "Xingyu Wu"
                    },
                    {
                        "name": "Jibin Wu"
                    },
                    {
                        "name": "Kay Chen Tan"
                    }
                ],
                "author_detail": {
                    "name": "Kay Chen Tan"
                },
                "author": "Kay Chen Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11647v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11647v2",
                "updated": "2025-05-29T13:26:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    26,
                    52,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-17T10:39:21Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    10,
                    39,
                    21,
                    0,
                    48,
                    0
                ],
                "title": "DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with\n  Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with\n  Model Editing"
                },
                "summary": "Large Language Models (LLMs) are widely applied in decision making, but their\ndeployment is threatened by jailbreak attacks, where adversarial users\nmanipulate model behavior to bypass safety measures. Existing defense\nmechanisms, such as safety fine-tuning and model editing, either require\nextensive parameter modifications or lack precision, leading to performance\ndegradation on general tasks, which is unsuitable to post-deployment safety\nalignment. To address these challenges, we propose DELMAN (Dynamic Editing for\nLLMs JAilbreak DefeNse), a novel approach leveraging direct model editing for\nprecise, dynamic protection against jailbreak attacks. DELMAN directly updates\na minimal set of relevant parameters to neutralize harmful behaviors while\npreserving the model's utility. To avoid triggering a safe response in benign\ncontext, we incorporate KL-divergence regularization to ensure the updated\nmodel remains consistent with the original model when processing benign\nqueries. Experimental results demonstrate that DELMAN outperforms baseline\nmethods in mitigating jailbreak attacks while preserving the model's utility,\nand adapts seamlessly to new attack instances, providing a practical and\nefficient solution for post-deployment model protection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely applied in decision making, but their\ndeployment is threatened by jailbreak attacks, where adversarial users\nmanipulate model behavior to bypass safety measures. Existing defense\nmechanisms, such as safety fine-tuning and model editing, either require\nextensive parameter modifications or lack precision, leading to performance\ndegradation on general tasks, which is unsuitable to post-deployment safety\nalignment. To address these challenges, we propose DELMAN (Dynamic Editing for\nLLMs JAilbreak DefeNse), a novel approach leveraging direct model editing for\nprecise, dynamic protection against jailbreak attacks. DELMAN directly updates\na minimal set of relevant parameters to neutralize harmful behaviors while\npreserving the model's utility. To avoid triggering a safe response in benign\ncontext, we incorporate KL-divergence regularization to ensure the updated\nmodel remains consistent with the original model when processing benign\nqueries. Experimental results demonstrate that DELMAN outperforms baseline\nmethods in mitigating jailbreak attacks while preserving the model's utility,\nand adapts seamlessly to new attack instances, providing a practical and\nefficient solution for post-deployment model protection."
                },
                "authors": [
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Fenghua Weng"
                    },
                    {
                        "name": "Sibei Yang"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Minlie Huang"
                    },
                    {
                        "name": "Wenjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Wang"
                },
                "author": "Wenjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11647v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18325v2",
                "updated": "2025-05-29T13:22:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    22,
                    42,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-23T19:30:49Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    19,
                    30,
                    49,
                    4,
                    143,
                    0
                ],
                "title": "Understanding and Mitigating Overrefusal in LLMs from an Unveiling\n  Perspective of Safety Decision Boundary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Mitigating Overrefusal in LLMs from an Unveiling\n  Perspective of Safety Decision Boundary"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they often refuse to answer legitimate queries-a\nphenomenon known as overrefusal. Overrefusal typically stems from\nover-conservative safety alignment, causing models to treat many reasonable\nprompts as potentially risky. To systematically understand this issue, we probe\nand leverage the models'safety decision boundaries to analyze and mitigate\noverrefusal. Our findings reveal that overrefusal is closely tied to\nmisalignment at these boundary regions, where models struggle to distinguish\nsubtle differences between benign and harmful content. Building on these\ninsights, we present RASS, an automated framework for prompt generation and\nselection that strategically targets overrefusal prompts near the safety\nboundary. By harnessing steering vectors in the representation space, RASS\nefficiently identifies and curates boundary-aligned prompts, enabling more\neffective and targeted mitigation of overrefusal. This approach not only\nprovides a more precise and interpretable view of model safety decisions but\nalso seamlessly extends to multilingual scenarios.We have explored the safety\ndecision boundaries of various LLMs and construct the MORBench evaluation set\nto facilitate robust assessment of model safety and helpfulness across multiple\nlanguages. Code and datasets will be released at\nhttps://anonymous.4open.science/r/RASS-80D3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they often refuse to answer legitimate queries-a\nphenomenon known as overrefusal. Overrefusal typically stems from\nover-conservative safety alignment, causing models to treat many reasonable\nprompts as potentially risky. To systematically understand this issue, we probe\nand leverage the models'safety decision boundaries to analyze and mitigate\noverrefusal. Our findings reveal that overrefusal is closely tied to\nmisalignment at these boundary regions, where models struggle to distinguish\nsubtle differences between benign and harmful content. Building on these\ninsights, we present RASS, an automated framework for prompt generation and\nselection that strategically targets overrefusal prompts near the safety\nboundary. By harnessing steering vectors in the representation space, RASS\nefficiently identifies and curates boundary-aligned prompts, enabling more\neffective and targeted mitigation of overrefusal. This approach not only\nprovides a more precise and interpretable view of model safety decisions but\nalso seamlessly extends to multilingual scenarios.We have explored the safety\ndecision boundaries of various LLMs and construct the MORBench evaluation set\nto facilitate robust assessment of model safety and helpfulness across multiple\nlanguages. Code and datasets will be released at\nhttps://anonymous.4open.science/r/RASS-80D3."
                },
                "authors": [
                    {
                        "name": "Licheng Pan"
                    },
                    {
                        "name": "Yongqi Tong"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Xiaolu Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Zhixuan Chu"
                    }
                ],
                "author_detail": {
                    "name": "Zhixuan Chu"
                },
                "author": "Zhixuan Chu",
                "arxiv_comment": "We have identified significant errors in the results presented in\n  this paper, specifically in the evaluation sections concerning the DPO\n  training of LLaMA2 and Qwen2.5, as well as in the representation space\n  visualization section. Given the extent of these issues, we intend to\n  substantially revise the manuscript's content and structure. Hence, we\n  request to withdraw it from arXiv at this time",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]